{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu sentence-transformers langchain_chroma langchain_community nest_asyncio\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install ragatouille colbert\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!pip install --upgrade chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from tqdm import tqdm\n",
    "from fastapi import FastAPI, Query\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"BAAI/bge-m3\"\n",
    "embedding_function = HuggingFaceBgeEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "# Chroma persistent directory\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "\n",
    "def get_text_collection(user_id: str):\n",
    "    \"\"\"\n",
    "    Initializes a Chroma vector store for textual data using the provided user ID to create a unique collection name.\n",
    "    \"\"\"\n",
    "    collection_name = f\"{user_id}_text_collection\"\n",
    "    return Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "         persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "def embed_data():\n",
    "    \"\"\"\n",
    "    Embeds all data from 'publications.json' into ChromaDB before the server starts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load dataset\n",
    "        with open(\"publications.json\", \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Initialize Chroma collection\n",
    "        user_id = \"default_user\"\n",
    "        logger.info(f\"Initializing ChromaDB for user_id: {user_id}\")\n",
    "        text_store = get_text_collection(user_id)\n",
    "        logger.info(\"ChromaDB vector store initialized successfully.\")\n",
    "\n",
    "        # Prepare and embed documents\n",
    "        documents = []\n",
    "        ids = []\n",
    "\n",
    "        logger.info(f\"Embedding {len(data)} documents from the JSON file...\")\n",
    "\n",
    "        for idx, item in enumerate(data):\n",
    "            # Extract fields\n",
    "            title = item.get(\"Title\", f\"Untitled Document {idx}\")\n",
    "            url = item.get(\"URL\", \"No URL\")\n",
    "            full_abstract = item.get(\"Full Abstract\", \"\").strip()\n",
    "\n",
    "            # Skip documents with insufficient data\n",
    "            if not full_abstract and not title.strip():\n",
    "                logger.warning(f\"Skipping document {idx}: insufficient content.\")\n",
    "                continue\n",
    "\n",
    "            # Combine title and abstract\n",
    "            document_content = f\"{title}\\n{full_abstract or 'No additional content available.'}\"\n",
    "\n",
    "            # Create Document object\n",
    "            document = Document(\n",
    "                page_content=document_content,\n",
    "                metadata={\"title\": title, \"url\": url},\n",
    "            )\n",
    "            documents.append(document)\n",
    "            ids.append(f\"doc_{idx}\")\n",
    "\n",
    "        # Store documents in the vector store\n",
    "        logger.info(\"Storing documents in ChromaDB vector store...\")\n",
    "        text_store.add_documents(documents=documents, ids=ids)\n",
    "        logger.info(f\"Successfully embedded {len(documents)} documents.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error embedding documents: {e}\")\n",
    "        logger.debug(\"Traceback:\", exc_info=True)\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def read_root():\n",
    "    \"\"\"\n",
    "    Root endpoint for testing.\n",
    "    \"\"\"\n",
    "    return {\"message\": \"Welcome to the ChromaDB Semantic Search API\"}\n",
    "\n",
    "@app.get(\"/search\")\n",
    "async def search(query: str, k: int = Query(default=5, description=\"Number of results to fetch\")):\n",
    "    \"\"\"\n",
    "    Performs a similarity search on the indexed data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_id = \"default_user\"  # Use the same user_id as in embed_data\n",
    "        logger.info(f\"Initializing ChromaDB for user_id: {user_id}\")\n",
    "        text_store = get_text_collection(user_id)\n",
    "        logger.info(\"ChromaDB vector store initialized successfully.\")\n",
    "\n",
    "        # Perform similarity search\n",
    "        logger.info(f\"Received search query: {query}\")\n",
    "        results = text_store.similarity_search(query, k=k)\n",
    "\n",
    "        # Format and log results\n",
    "        formatted_results = []\n",
    "        for idx, result in enumerate(results):\n",
    "            title = result.metadata.get(\"title\", \"Unknown Title\")\n",
    "            url = result.metadata.get(\"url\", \"No URL\")\n",
    "            content = result.page_content\n",
    "            logger.debug(f\"Result {idx + 1}: Title: {title}, URL: {url}, Content: {content[:100]}...\")\n",
    "            formatted_results.append({\"title\": title, \"url\": url, \"content\": content})\n",
    "\n",
    "        return {\"results\": formatted_results}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during search: {e}\")\n",
    "        return JSONResponse(status_code=500, content={\"message\": \"An error occurred during search.\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Embed data before starting the server\n",
    "    embed_data()\n",
    "    # Run FastAPI app\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-32' coro=<Server.serve() done, defined at /Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/uvicorn/server.py:67> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/uvicorn/main.py\", line 579, in run\n",
      "    server.run()\n",
      "  File \"/Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "  File \"/Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 396, in __wakeup\n",
      "    self.__step()\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 68, in serve\n",
      "    with self.capture_signals():\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Users/paniz/ReSearch/.venv/lib/python3.12/site-packages/uvicorn/server.py\", line 332, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "KeyboardInterrupt\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from fastapi import FastAPI, Query\n",
    "from fastapi.responses import JSONResponse\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"BAAI/bge-m3\"\n",
    "embedding_function = HuggingFaceBgeEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "# Chroma persistent directory\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "def embed_data():\n",
    "    \"\"\"\n",
    "    Embeds all data from 'publications.json' into ChromaDB before the server starts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(\"publications.json\"):\n",
    "            logger.error(\"Error: publications.json not found.\")\n",
    "            return\n",
    "\n",
    "        # Load dataset\n",
    "        with open(\"publications.json\", \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Initialize Chroma collection\n",
    "        user_id = \"default_user\"\n",
    "        logger.info(f\"Initializing ChromaDB for user_id: {user_id}\")\n",
    "        text_store = get_text_collection(user_id)\n",
    "        logger.info(\"ChromaDB vector store initialized successfully.\")\n",
    "\n",
    "        # Prepare and embed documents\n",
    "        documents = []\n",
    "        ids = []\n",
    "\n",
    "        logger.info(f\"Embedding {len(data)} documents from the JSON file...\")\n",
    "\n",
    "        for idx, item in enumerate(data):\n",
    "            # Extract fields\n",
    "            title = item.get(\"Title\", f\"Untitled Document {idx}\")\n",
    "            url = item.get(\"URL\", \"No URL\")\n",
    "            full_abstract = item.get(\"Full Abstract\", \"\").strip()\n",
    "\n",
    "            # Log the embedding content for debugging\n",
    "            logger.debug(f\"Embedding document {idx + 1}:\")\n",
    "            logger.debug(f\"  Title: {title}\")\n",
    "            logger.debug(f\"  URL: {url}\")\n",
    "            logger.debug(f\"  Abstract: {full_abstract[:100]}...\")\n",
    "\n",
    "            # Handle missing abstracts\n",
    "            if not full_abstract or \"No abstract available\" in full_abstract:\n",
    "                logger.warning(f\"Document {idx} has no abstract. Embedding title and URL only.\")\n",
    "                document_content = f\"{title}\\n{url}\"\n",
    "            else:\n",
    "                document_content = f\"{title}\\n{full_abstract}\"\n",
    "\n",
    "            # Create Document object\n",
    "            document = Document(\n",
    "                page_content=document_content,\n",
    "                metadata={\"title\": title, \"url\": url},\n",
    "            )\n",
    "            documents.append(document)\n",
    "            ids.append(f\"doc_{idx}\")\n",
    "\n",
    "        # Store documents in the vector store\n",
    "        logger.info(\"Storing documents in ChromaDB vector store...\")\n",
    "        text_store.add_documents(documents=documents, ids=ids)\n",
    "        logger.info(f\"Successfully embedded {len(documents)} documents.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error embedding documents: {e}\")\n",
    "        logger.debug(\"Traceback:\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:System cache cleared successfully.\n"
     ]
    }
   ],
   "source": [
    "@staticmethod\n",
    "def clear_system_cache() -> None:\n",
    "    \"\"\"\n",
    "    Clears the system cache for the current user.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.system(\"rm -rf ~/.cache/chroma\")\n",
    "        logger.info(\"System cache cleared successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error clearing system cache: {e}\")\n",
    "        logger.debug(\"Traceback:\", exc_info=True)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from langchain_core.documents import Document\n",
    "# clear_system_cache() method is not available in the current version of Chroma\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# clean cache\n",
    "clear_system_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-m3\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:     Started server process [57326]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52685 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:52690 - \"GET /search?query=machine%20learning%20security&k=3 HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:52690 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52690 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52692 - \"GET /search_with_scores?query=name&k=5 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52697 - \"GET /search_with_scores?query=machine&k=5 HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:52809 - \"GET /search_with_scores?query=code&k=5 HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from fastapi import FastAPI, Query\n",
    "from fastapi.responses import JSONResponse\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"BAAI/bge-m3\"\n",
    "embedding_function = HuggingFaceBgeEmbeddings(model_name=MODEL_NAME)\n",
    "\n",
    "# Chroma persistent directory\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "\n",
    "def get_text_collection(user_id: str):\n",
    "    \"\"\"\n",
    "    Initializes or retrieves a Chroma vector store for textual data using the provided user ID.\n",
    "    \"\"\"\n",
    "    collection_name = f\"{user_id}_text_collection\"\n",
    "    return Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "\n",
    "@app.get(\"/search_with_scores\")\n",
    "async def search_with_scores(\n",
    "    query: str = Query(..., description=\"Search query text\"),\n",
    "    k: int = Query(5, description=\"Number of results to fetch\")\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform a similarity search for a given query and return results with similarity scores.\n",
    "    Add suggestions for documents missing abstracts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize Chroma collection\n",
    "        user_id = \"default_user\"\n",
    "        text_store = get_text_collection(user_id)\n",
    "\n",
    "        # Perform similarity search with scores\n",
    "        results = text_store.similarity_search_with_score(query, k=k)\n",
    "\n",
    "        if not results:\n",
    "            return {\"results\": [], \"message\": \"No matching results found.\"}\n",
    "\n",
    "        # Separate results with and without abstracts\n",
    "        results_with_abstract = []\n",
    "        suggestions = []\n",
    "\n",
    "        for doc, score in results:\n",
    "            if \"Abstract not found\" in doc.page_content or \"failed to load\" in doc.page_content:\n",
    "                suggestions.append({\n",
    "                    \"title\": doc.metadata.get(\"title\", \"Unknown Title\"),\n",
    "                    \"url\": doc.metadata.get(\"url\", \"No URL\"),\n",
    "                    \"similarity_score\": score\n",
    "                })\n",
    "            else:\n",
    "                results_with_abstract.append({\n",
    "                    \"title\": doc.metadata.get(\"title\", \"Unknown Title\"),\n",
    "                    \"url\": doc.metadata.get(\"url\", \"No URL\"),\n",
    "                    \"content_snippet\": doc.page_content[:100],  # Limit to 100 characters for snippet\n",
    "                    \"similarity_score\": score\n",
    "                })\n",
    "\n",
    "        response = {\"results\": results_with_abstract}\n",
    "\n",
    "        # Add suggestions if available\n",
    "        if suggestions:\n",
    "            response[\"suggestions\"] = suggestions\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during similarity search: {e}\")\n",
    "        return JSONResponse(status_code=500, content={\"message\": \"An error occurred during search.\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the persistent directory exists\n",
    "    os.makedirs(PERSIST_DIR, exist_ok=True)\n",
    "    \n",
    "    # Run the FastAPI app\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
