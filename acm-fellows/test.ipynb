{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "# from gsc_crawler import get_google_scholar_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 1)) (4.12.3)\n",
      "Collecting openai==1.47.0 (from -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached openai-1.47.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pandas==2.2.3 (from -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 3))\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting PyYAML==6.0.2 (from -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 4))\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests==2.32.3 (from -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 5))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting scholarly==1.7.11 (from -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached scholarly-1.7.11-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting wordcloud==1.9.3 (from -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7))\n",
      "  Using cached wordcloud-1.9.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from beautifulsoup4==4.12.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 1)) (2.5)\n",
      "Collecting anyio<5,>=3.5.0 (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached jiter-0.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: sniffio in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2)) (1.3.1)\n",
      "Collecting tqdm>4 (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas==2.2.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas==2.2.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 3)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas==2.2.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas==2.2.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 3)) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests==2.32.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests==2.32.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 5)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests==2.32.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests==2.32.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 5)) (2024.2.2)\n",
      "Collecting arrow (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting bibtexparser (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Downloading bibtexparser-1.4.2.tar.gz (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting deprecated (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting fake-useragent (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting free-proxy (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached free_proxy-1.1.2-py3-none-any.whl\n",
      "Collecting python-dotenv (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: selenium in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (4.18.1)\n",
      "Collecting sphinx-rtd-theme (from scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: pillow in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from wordcloud==1.9.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7)) (10.2.0)\n",
      "Requirement already satisfied: matplotlib in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from wordcloud==1.9.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7)) (3.8.3)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2)) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1.9.0->openai==1.47.0->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 2))\n",
      "  Using cached pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 3)) (1.16.0)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from bibtexparser->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from deprecated->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: lxml in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from free-proxy->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (5.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->wordcloud==1.9.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->wordcloud==1.9.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->wordcloud==1.9.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7)) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->wordcloud==1.9.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from matplotlib->wordcloud==1.9.3->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 7)) (24.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests[socks]->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (1.7.1)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from selenium->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from selenium->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (0.11.1)\n",
      "Collecting sphinx<8,>=5 (from sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinx-7.4.7-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting docutils<0.21 (from sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached docutils-0.20.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sphinxcontrib-qthelp (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: Pygments>=2.17 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (2.17.2)\n",
      "Collecting snowballstemmer>=2.2 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting babel>=2.13 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting alabaster~=0.7.14 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting imagesize>=1.3 (from sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6))\n",
      "  Using cached imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from trio~=0.17->selenium->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from trio~=0.17->selenium->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from trio~=0.17->selenium->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/paniz/anaconda3/envs/myenv/lib/python3.12/site-packages (from Jinja2>=3.1->sphinx<8,>=5->sphinx-rtd-theme->scholarly==1.7.11->-r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt (line 6)) (2.1.5)\n",
      "Using cached openai-1.47.0-py3-none-any.whl (375 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
      "Using cached wordcloud-1.9.3-cp312-cp312-macosx_11_0_arm64.whl (169 kB)\n",
      "Using cached anyio-4.6.0-py3-none-any.whl (89 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jiter-0.5.0-cp312-cp312-macosx_11_0_arm64.whl (296 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached docutils-0.20.1-py3-none-any.whl (572 kB)\n",
      "Using cached sphinx-7.4.7-py3-none-any.whl (3.4 MB)\n",
      "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)\n",
      "Using cached alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
      "Using cached babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "Using cached imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Building wheels for collected packages: bibtexparser\n",
      "  Building wheel for bibtexparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bibtexparser: filename=bibtexparser-1.4.2-py3-none-any.whl size=43562 sha256=8ea77894acf0f441e3684898217df09d526bfb6a82fbe29d1106aaad8bc713c8\n",
      "  Stored in directory: /Users/paniz/Library/Caches/pip/wheels/3b/56/86/6e8c134f04e22849f84536287075eefce431b7ba0a11752390\n",
      "Successfully built bibtexparser\n",
      "Installing collected packages: snowballstemmer, fake-useragent, typing-extensions, types-python-dateutil, tqdm, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, requests, PyYAML, python-dotenv, jiter, imagesize, httpcore, docutils, distro, deprecated, bibtexparser, babel, anyio, annotated-types, alabaster, sphinx, pydantic-core, pandas, httpx, free-proxy, arrow, wordcloud, sphinxcontrib-jquery, pydantic, sphinx-rtd-theme, openai, scholarly\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.10.0\n",
      "    Uninstalling typing_extensions-4.10.0:\n",
      "      Successfully uninstalled typing_extensions-4.10.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.1\n",
      "    Uninstalling pandas-2.2.1:\n",
      "      Successfully uninstalled pandas-2.2.1\n",
      "Successfully installed PyYAML-6.0.2 alabaster-0.7.16 annotated-types-0.7.0 anyio-4.6.0 arrow-1.3.0 babel-2.16.0 bibtexparser-1.4.2 deprecated-1.2.14 distro-1.9.0 docutils-0.20.1 fake-useragent-1.5.1 free-proxy-1.1.2 httpcore-1.0.6 httpx-0.27.2 imagesize-1.4.1 jiter-0.5.0 openai-1.47.0 pandas-2.2.3 pydantic-2.9.2 pydantic-core-2.23.4 python-dotenv-1.0.1 requests-2.32.3 scholarly-1.7.11 snowballstemmer-2.2.0 sphinx-7.4.7 sphinx-rtd-theme-2.0.0 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 tqdm-4.66.5 types-python-dateutil-2.9.0.20241003 typing-extensions-4.12.2 wordcloud-1.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /Users/paniz/Documents/GitHub/SE390/ReSearch/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: ['Name', 'Award', 'Year', 'Region', 'DL']\n",
      "Row: ['Milner,\\xa0A\\xa0J', 'ACM A. M. Turing Award', '1991', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81332515695']\n",
      "Row: ['Perlis,\\xa0A.\\xa0J.', 'ACM A. M. Turing Award', '1966', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100086771']\n",
      "Row: ['Shamir,\\xa0Adi', 'ACM A. M. Turing Award', '2002', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81100081898']\n",
      "Row: ['Kay,\\xa0Alan', 'ACM A. M. Turing Award', '2003', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100544599']\n",
      "Row: ['Aho,\\xa0Alfred\\xa0V', 'ACM A. M. Turing Award', '2020', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100024612']\n",
      "Row: ['Newell,\\xa0Allen', 'ACM A. M. Turing Award', '1975', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100393604']\n",
      "Row: ['Pnueli,\\xa0Amir', 'ACM A. M. Turing Award', '1996', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100648459']\n",
      "Row: ['Yao,\\xa0Andrew\\xa0C', 'ACM A. M. Turing Award', '2000', 'Asia', 'https://dl.acm.org/author_page.cfm?id=81100545139']\n",
      "Row: ['Wigderson,\\xa0Avi', 'ACM A. M. Turing Award', '2023', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100459522']\n",
      "Row: ['Liskov,\\xa0Barbara', 'ACM A. M. Turing Award', '2008', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100323833']\n",
      "Row: ['Lampson,\\xa0Butler\\xa0W', 'ACM A. M. Turing Award', '1992', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100081662']\n",
      "Row: ['Hoare,\\xa0C. Antony\\xa0R.', 'ACM A. M. Turing Award', '1980', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81385591905']\n",
      "Row: ['Thacker,\\xa0Charles\\xa0P', 'ACM A. M. Turing Award', '2009', 'North America', 'https://dl.acm.org/author_page.cfm?id=81332531482']\n",
      "Row: ['Bachman,\\xa0Charles\\xa0W', 'ACM A. M. Turing Award', '1973', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100243333']\n",
      "Row: ['Scott,\\xa0Dana\\xa0S', 'ACM A. M. Turing Award', '1976', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100545006']\n",
      "Row: ['Patterson,\\xa0David', 'ACM A. M. Turing Award', '2017', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100565162']\n",
      "Row: ['Ritchie,\\xa0Dennis\\xa0M.', 'ACM A. M. Turing Award', '1983', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100458439']\n",
      "Row: ['Knuth,\\xa0Donald\\xa0E', 'ACM A. M. Turing Award', '1974', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100246710']\n",
      "Row: ['Engelbart,\\xa0Douglas', 'ACM A. M. Turing Award', '1997', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100342853']\n",
      "Row: ['Dongarra,\\xa0Jack', 'ACM A. M. Turing Award', '2021', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100368092']\n",
      "Row: ['Emerson,\\xa0E.\\xa0Allen', 'ACM A. M. Turing Award', '2007', 'North America', 'https://dl.acm.org/author_page.cfm?id=81452614401']\n",
      "Row: ['Codd,\\xa0Edgar\\xa0F', 'ACM A. M. Turing Award', '1981', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100425534']\n",
      "Row: ['Clarke,\\xa0Edmund', 'ACM A. M. Turing Award', '2007', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100393517']\n",
      "Row: ['Dijkstra,\\xa0Edsger\\xa0W', 'ACM A. M. Turing Award', '1972', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81100248871']\n",
      "Row: ['Feigenbaum,\\xa0Edward\\xa0A', 'ACM A. M. Turing Award', '1994', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100346299']\n",
      "Row: ['Catmull,\\xa0Edwin', 'ACM A. M. Turing Award', '2019', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100160637']\n",
      "Row: ['Corbato,\\xa0Fernando\\xa0J', 'ACM A. M. Turing Award', '1990', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100474167']\n",
      "Row: ['Allen,\\xa0Frances', 'ACM A. M. Turing Award', '2006', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100651807']\n",
      "Row: ['Brooks,\\xa0Frederick', 'ACM A. M. Turing Award', '1999', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100077256']\n",
      "Row: ['Hinton,\\xa0Geoffrey\\xa0E', 'ACM A. M. Turing Award', '2018', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100505762']\n",
      "Row: ['Simon,\\xa0Herbert\\xa0A', 'ACM A. M. Turing Award', '1975', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100527918']\n",
      "Row: ['Sutherland,\\xa0Ivan', 'ACM A. M. Turing Award', '1988', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100265287']\n",
      "Row: ['Wilkinson,\\xa0J.\\xa0H.', 'ACM A. M. Turing Award', '1970', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81341498480']\n",
      "Row: ['Ullman,\\xa0Jeffrey\\xa0D', 'ACM A. M. Turing Award', '2020', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100314798']\n",
      "Row: ['Gray,\\xa0Jim', 'ACM A. M. Turing Award', '1998', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100403088']\n",
      "Row: ['Backus,\\xa0John', 'ACM A. M. Turing Award', '1977', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100233661']\n",
      "Row: ['Cocke,\\xa0John', 'ACM A. M. Turing Award', '1987', 'North America', 'https://dl.acm.org/author_page.cfm?id=81342491861']\n",
      "Row: ['Hopcroft,\\xa0John\\xa0E', 'ACM A. M. Turing Award', '1986', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100008350']\n",
      "Row: ['Hennessy,\\xa0John\\xa0L', 'ACM A. M. Turing Award', '2017', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100207767']\n",
      "Row: ['McCarthy,\\xa0John', 'ACM A. M. Turing Award', '1971', 'North America', 'https://dl.acm.org/author_page.cfm?id=81406600200']\n",
      "Row: ['Sifakis,\\xa0Joseph', 'ACM A. M. Turing Award', '2007', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81100396269']\n",
      "Row: ['Pearl,\\xa0Judea', 'ACM A. M. Turing Award', '2011', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100419881']\n",
      "Row: ['Hartmanis,\\xa0Juris', 'ACM A. M. Turing Award', '1993', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100525769']\n",
      "Row: ['Iverson,\\xa0Kenneth\\xa0E.', 'ACM A. M. Turing Award', '1979', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100366906']\n",
      "Row: ['Thompson,\\xa0Kenneth\\xa0Lane', 'ACM A. M. Turing Award', '1983', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100436668']\n",
      "Row: ['Nygaard,\\xa0Kristen', 'ACM A. M. Turing Award', '2001', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81100392381']\n",
      "Row: ['Adleman,\\xa0Leonard\\xa0M.', 'ACM A. M. Turing Award', '2002', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100129131']\n",
      "Row: ['Valiant,\\xa0Leslie\\xa0G', 'ACM A. M. Turing Award', '2010', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100502250']\n",
      "Row: ['Lamport,\\xa0Leslie', 'ACM A. M. Turing Award', '2013', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100244989']\n",
      "Row: ['Blum,\\xa0Manuel', 'ACM A. M. Turing Award', '1995', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100159946']\n",
      "Row: ['Minsky,\\xa0Marvin', 'ACM A. M. Turing Award', '1969', 'North America', 'https://dl.acm.org/author_page.cfm?id=81336491376']\n",
      "Row: ['Wilkes,\\xa0Maurice\\xa0V.', 'ACM A. M. Turing Award', '1967', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81100297470']\n",
      "Row: ['Rabin,\\xa0Michael\\xa0O.', 'ACM A. M. Turing Award', '1976', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100510853']\n",
      "Row: ['Stonebraker,\\xa0Michael', 'ACM A. M. Turing Award', '2014', 'North America', 'https://dl.acm.org/author_page.cfm?id=81337493529']\n",
      "Row: ['Wirth,\\xa0Niklaus\\xa0E', 'ACM A. M. Turing Award', '1984', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81332536058']\n",
      "Row: ['Dahl,\\xa0Ole-Johan', 'ACM A. M. Turing Award', '2001', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81452606808']\n",
      "Row: ['Hanrahan,\\xa0Pat', 'ACM A. M. Turing Award', '2019', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100482576']\n",
      "Row: ['Naur,\\xa0Peter', 'ACM A. M. Turing Award', '2005', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81100158784']\n",
      "Row: ['Hellman,\\xa0Martin', 'ACM A. M. Turing Award', '2015', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100428712']\n",
      "Row: ['Reddy,\\xa0Raj', 'ACM A. M. Turing Award', '1994', 'North America', 'https://dl.acm.org/author_page.cfm?id=81329491459']\n",
      "Row: ['Stearns,\\xa0Richard\\xa0E', 'ACM A. M. Turing Award', '1993', 'North America', 'https://dl.acm.org/author_page.cfm?id=81452610514']\n",
      "Row: ['Karp,\\xa0Richard', 'ACM A. M. Turing Award', '1985', 'North America', 'https://dl.acm.org/author_page.cfm?id=81361598854']\n",
      "Row: ['Hamming,\\xa0Richard\\xa0W', 'ACM A. M. Turing Award', '1968', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100153796']\n",
      "Row: ['Kahn,\\xa0Robert\\xa0E', 'ACM A. M. Turing Award', '2004', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100452904']\n",
      "Row: ['Tarjan,\\xa0Robert\\xa0E', 'ACM A. M. Turing Award', '1986', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100645220']\n",
      "Row: ['Metcalfe,\\xa0Robert\\xa0Melancton', 'ACM A. M. Turing Award', '2022', 'North America', 'https://dl.acm.org/author_page.cfm?id=89758929157']\n",
      "Row: ['Floyd,\\xa0Robert\\xa0W.', 'ACM A. M. Turing Award', '1978', 'North America', 'https://dl.acm.org/author_page.cfm?id=81452611368']\n",
      "Row: ['Rivest,\\xa0Ronald\\xa0L', 'ACM A. M. Turing Award', '2002', 'North America', 'https://dl.acm.org/author_page.cfm?id=81328490215']\n",
      "Row: ['Goldwasser,\\xa0Shafi', 'ACM A. M. Turing Award', '2012', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100237195']\n",
      "Row: ['Micali,\\xa0Silvio', 'ACM A. M. Turing Award', '2012', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100228636']\n",
      "Row: ['Berners-Lee,\\xa0Tim', 'ACM A. M. Turing Award', '2016', 'Europe', 'https://dl.acm.org/author_page.cfm?id=81100026375']\n",
      "Row: ['Cook,\\xa0Stephen\\xa0A', 'ACM A. M. Turing Award', '1982', 'North America', 'https://dl.acm.org/author_page.cfm?id=81451600040']\n",
      "Row: ['Cerf,\\xa0Vinton', 'ACM A. M. Turing Award', '2004', 'North America', 'https://dl.acm.org/author_page.cfm?id=81385601636']\n",
      "Row: ['Diffie,\\xa0Whitfield', 'ACM A. M. Turing Award', '2015', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100513962']\n",
      "Row: ['Kahan,\\xa0William', 'ACM A. M. Turing Award', '1989', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100645838']\n",
      "Row: ['LeCun,\\xa0Yann', 'ACM A. M. Turing Award', '2018', 'North America', 'https://dl.acm.org/author_page.cfm?id=81350597740']\n",
      "Row: ['Bengio,\\xa0Yoshua', 'ACM A. M. Turing Award', '2018', 'North America', 'https://dl.acm.org/author_page.cfm?id=81100287057']\n",
      "Exception occurred: 'citedby'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "from gsc_crawler import get_google_scholar_url\n",
    "\n",
    "# Function to crawl profile data from the award profile URL\n",
    "def profile_crawler(name, profile_url):\n",
    "    response = requests.get(profile_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    last_name, first_name = name.split(\", \")\n",
    "    full_name = soup.find('h1').text.strip()  # Ensure no trailing spaces\n",
    "\n",
    "    awards_info = soup.find_all('section', {'class': 'awards-winners__citation'})\n",
    "    acm_award = next((award for award in awards_info if award.find('h2').a.text == 'ACM A. M. Turing Award'), None)\n",
    "\n",
    "    if acm_award:\n",
    "        location, year = acm_award.find('h3', {'class': 'awards-winners__location'}).text.split(' - ')\n",
    "        citation = ' '.join(acm_award.find('p', {'class': \"awards-winners__citation-short\"}).text.split('\\n')).strip()\n",
    "    else:\n",
    "        location, year, citation = '', '', ''\n",
    "\n",
    "    # Extract Google Scholar data\n",
    "    gsc_data = get_google_scholar_url(full_name)\n",
    "    if not gsc_data and len(full_name.split()) >= 3:\n",
    "        first_last_name = f'{full_name.split()[0]} {full_name.split()[-1]}'\n",
    "        gsc_data = get_google_scholar_url(first_last_name)\n",
    "    \n",
    "    if gsc_data:\n",
    "        gsc_url = f'https://scholar.google.com/citations?user={gsc_data[\"scholar_id\"]}'\n",
    "        affiliation = gsc_data.get(\"affiliation\", \"\")\n",
    "        interests = \" \".join(gsc_data.get('interests', []))\n",
    "    else:\n",
    "        gsc_url, affiliation, interests = '', '', '[]'\n",
    "\n",
    "    return [last_name, first_name, year, location, citation, profile_url, gsc_url, affiliation, interests]\n",
    "\n",
    "\n",
    "# Scraping ACM Turing Award page\n",
    "url = 'https://awards.acm.org/turing/award-recipients'\n",
    "session = requests.Session()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = session.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Locate the table and check if it exists\n",
    "table = soup.find('table', class_='awards-tables--fullWidth')\n",
    "if not table:\n",
    "    print(\"Table not found!\")\n",
    "    exit(1)\n",
    "\n",
    "# Extract table headers (optional step)\n",
    "headers = [th.text.strip() for th in table.find('thead').find_all('th')]\n",
    "\n",
    "# Extract table rows\n",
    "rows = []\n",
    "for tr in table.find('tbody').find_all('tr'):\n",
    "    # Extract Name\n",
    "    name_element = tr.find_all('td')[0].find('a')\n",
    "    recipient_name = name_element.text.strip()\n",
    "    profile_url = f'https://awards.acm.org{name_element[\"href\"]}'\n",
    "    \n",
    "    # Extract Award\n",
    "    award = tr.find_all('td')[1].text.strip()\n",
    "    \n",
    "    # Extract Year\n",
    "    year = tr.find_all('td')[2].text.strip()\n",
    "    \n",
    "    # Extract Region\n",
    "    region = tr.find_all('td')[3].text.strip()\n",
    "    \n",
    "    # Extract DL Link (if available)\n",
    "    dl_element = tr.find_all('td')[4].find('a')\n",
    "    dl_link = dl_element['href'] if dl_element else 'N/A'\n",
    "    \n",
    "    rows.append([recipient_name, award, year, region, dl_link])\n",
    "\n",
    "# Print the extracted data for verification\n",
    "print(\"Headers:\", headers)\n",
    "for row in rows:\n",
    "    print(\"Row:\", row)\n",
    "\n",
    "# Sort rows by year (column index 2)\n",
    "rows.sort(key=lambda row: int(row[2]), reverse=True)\n",
    "\n",
    "# Handling file and checkpoint for resuming\n",
    "it = 0\n",
    "checkpoint = 'last_iteration.txt'\n",
    "fileName = 'acm_turings.csv'\n",
    "fileExist = os.path.isfile(fileName) and os.path.isfile(checkpoint)\n",
    "\n",
    "with open(fileName, 'a' if fileExist else 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row if the file is new\n",
    "    if not fileExist:\n",
    "        writer.writerow(['Index', 'Last Name', 'Given Name', 'Year', 'Region', 'Award', 'Profile URL', 'Digital Library Link', 'Google Scholar Profile', 'Affiliation', 'Interests'])\n",
    "    else:\n",
    "        with open(checkpoint, 'r') as f:\n",
    "            index = int(f.readline().split(':')[-1])\n",
    "            rows = rows[index:]\n",
    "            it = index\n",
    "    \n",
    "    for row in rows:\n",
    "        try:\n",
    "            # Separate last name and first name from the recipient name\n",
    "            name = row[0]\n",
    "            profile_url = row[4]  # Profile URL is the last column in the row\n",
    "\n",
    "            # Clean the name to remove non-ASCII characters\n",
    "            name_clean = ''.join([i if ord(i) < 128 else ' ' for i in name])\n",
    "            \n",
    "            # Crawl profile data\n",
    "            data = profile_crawler(name_clean, profile_url)\n",
    "            it += 1\n",
    "\n",
    "            data.insert(0, it)  # Add index at the start\n",
    "            writer.writerow(data)\n",
    "\n",
    "            if it % 20 == 0:\n",
    "                print(f\"Finished {it} iterations...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Process interrupted manually.\")\n",
    "            with open(checkpoint, 'w') as f:\n",
    "                f.write(f'Last completed iteration: {it}')\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "            with open(checkpoint, 'w') as f:\n",
    "                f.write(f'Failed at iteration: {it}')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1/1393 | Scraped Milner, A J - https://dl.acm.org/author_page.cfm?id=81332515695\n",
      "Progress: 2/1393 | Scraped Sreejith, A V - https://dl.acm.org/author_page.cfm?id=81479663157\n",
      "Progress: 3/1393 | Scraped Malossi, A. Cristiano I. - N/A\n",
      "Progress: 4/1393 | Scraped Perlis, A. J. - https://dl.acm.org/author_page.cfm?id=81100086771\n",
      "Progress: 5/1393 | Scraped Turner, A. Joe - https://dl.acm.org/author_page.cfm?id=81408600192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 150\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData saved to acm_award_recipients.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mscrape_acm_award_recipients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 135\u001b[0m, in \u001b[0;36mscrape_acm_award_recipients\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Scraped \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdl_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Wait before making the next request to avoid blocking\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait 1-3 seconds randomly between requests\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Write data to CSV\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macm_award_recipients.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# User-Agent list for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.48',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='scraper.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Optional proxies\n",
    "PROXIES = [\n",
    "    # Example proxy format\n",
    "    # 'http://user:password@proxyserver:port',\n",
    "    # Add your proxies here or leave it as an empty list for no proxy usage\n",
    "]\n",
    "\n",
    "# Function to get a random User-Agent\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# Function to get a random proxy (optional)\n",
    "def get_random_proxy():\n",
    "    if PROXIES:\n",
    "        return {\"http\": random.choice(PROXIES), \"https\": random.choice(PROXIES)}\n",
    "    return None\n",
    "\n",
    "# Function to scrape a single researcher's additional details\n",
    "def scrape_dl_profile(dl_url):\n",
    "    profile_info = {}\n",
    "    try:\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        proxy = get_random_proxy()\n",
    "        dl_response = requests.get(dl_url, headers=headers, proxies=proxy, timeout=10)\n",
    "        \n",
    "        if dl_response.status_code == 200:\n",
    "            dl_soup = BeautifulSoup(dl_response.content, 'html.parser')\n",
    "            \n",
    "            # Find all elements with classes containing double underscores\n",
    "            double_underscore_elements = dl_soup.find_all(class_=re.compile(r'\\w+__\\w+'))\n",
    "            \n",
    "            # Collecting text from these elements\n",
    "            double_underscore_text = [element.get_text(strip=True) for element in double_underscore_elements]\n",
    "            \n",
    "            # Example fields to store:\n",
    "            profile_info['double_underscore_content'] = \" | \".join(double_underscore_text) if double_underscore_text else 'N/A'\n",
    "            \n",
    "        else:\n",
    "            logging.error(f\"Failed to fetch DL profile: {dl_url}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping DL profile {dl_url}: {e}\")\n",
    "    \n",
    "    return profile_info\n",
    "\n",
    "# Function to scrape the ACM award recipients page\n",
    "def scrape_acm_award_recipients():\n",
    "    url = \"https://awards.acm.org/award-recipients\"\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    proxy = get_random_proxy()\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, proxies=proxy, timeout=10)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch the ACM page: {e}\")\n",
    "        return\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        logging.error(\"Failed to retrieve the ACM awards page.\")\n",
    "        return\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Try to find the table using a more generic approach\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    if table is None:\n",
    "        logging.error(\"No table found on the page. Check the structure or class name.\")\n",
    "        return\n",
    "    \n",
    "    # Now try to find all rows in the table\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    \n",
    "    if not rows:\n",
    "        logging.error(\"No rows found in the table. Verify the page content.\")\n",
    "        return\n",
    "    \n",
    "    # List to store scraped data\n",
    "    recipients_data = []\n",
    "    \n",
    "    for idx, row in enumerate(rows):\n",
    "        cols = row.find_all('td')\n",
    "        name = cols[0].text.strip()\n",
    "        award = cols[1].text.strip()\n",
    "        year = cols[2].text.strip()\n",
    "        region = cols[3].text.strip()\n",
    "        dl_link = cols[4].find('a')['href'] if cols[4].find('a') else None\n",
    "        \n",
    "        if dl_link:\n",
    "            dl_url = f\"https://dl.acm.org{dl_link}\" if dl_link.startswith('/') else dl_link\n",
    "        else:\n",
    "            dl_url = 'N/A'\n",
    "\n",
    "        # Scrape additional details from the DL profile\n",
    "        profile_details = scrape_dl_profile(dl_url) if dl_url != 'N/A' else {'double_underscore_content': 'N/A'}\n",
    "        \n",
    "        # Append all the data together\n",
    "        recipient = {\n",
    "            'name': name,\n",
    "            'award': award,\n",
    "            'year': year,\n",
    "            'region': region,\n",
    "            'dl_profile': dl_url,\n",
    "            **profile_details\n",
    "        }\n",
    "        recipients_data.append(recipient)\n",
    "        \n",
    "        # Logging progress\n",
    "        logging.info(f\"Processed {idx+1}/{len(rows)}: {name} ({dl_url})\")\n",
    "\n",
    "        # Progress output\n",
    "        print(f\"Progress: {idx+1}/{len(rows)} | Scraped {name} - {dl_url}\")\n",
    "        \n",
    "        # Wait before making the next request to avoid blocking\n",
    "        time.sleep(random.uniform(1, 3))  # Wait 1-3 seconds randomly between requests\n",
    "    \n",
    "    # Write data to CSV\n",
    "    with open('acm_award_recipients.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['name', 'award', 'year', 'region', 'dl_profile', 'double_underscore_content']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for data in recipients_data:\n",
    "            writer.writerow(data)\n",
    "    \n",
    "    logging.info(\"Data saved to acm_award_recipients.csv\")\n",
    "    print(\"Data saved to acm_award_recipients.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_acm_award_recipients()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved as acm_award_recipients.html\n",
      "HTML content saved as profile_81332515695.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "# User-Agent list for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.48',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Function to get a random User-Agent\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# Function to fetch and save HTML page to local file\n",
    "def fetch_and_save_html(url, save_as):\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_as, 'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "            print(f\"HTML content saved as {save_as}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page: {url}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timed out for {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "\n",
    "# Function to fetch and save DL profiles for each recipient\n",
    "def scrape_and_save_html():\n",
    "    url = \"https://awards.acm.org/award-recipients\"\n",
    "    fetch_and_save_html(url, \"acm_award_recipients.html\")  # Save the main page locally\n",
    "\n",
    "    # Simulating the DL profile URLs for demo purposes\n",
    "    dl_profiles = [\n",
    "        \"https://dl.acm.org/author_page.cfm?id=81332515695\"\n",
    "       \n",
    "    ]\n",
    "    \n",
    "    for dl_url in dl_profiles:\n",
    "        # Save each DL profile HTML as a local file\n",
    "        profile_id = dl_url.split(\"=\")[-1]\n",
    "        save_as = f\"profile_{profile_id}.html\"\n",
    "        fetch_and_save_html(dl_url, save_as)\n",
    "\n",
    "        # Sleep between requests to avoid overwhelming the server\n",
    "        time.sleep(random.uniform(3, 6))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_and_save_html()  # Fetch and save the pages locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author Profile Data:\n",
      "{'Name': 'Robin Milner', 'Bibliometrics': {'Average Citation per Article': '95', 'Citation count': '9,201', 'Publication counts': '97', 'Publication Years': '1971 - 2013', 'Available for Download': '19', 'Average Downloads per Article': '1,706', 'Downloads (6 weeks)': '403', 'Downloads (12 months)': '9,237', 'Downloads (cumulative)': '32,421'}, 'Image_URL': '/do/10.1145/contrib-81332515695/full/81332515695-1588898507063.jpg', 'Co_Authors': [], 'Keywords': [], 'Publications': [{'Title': 'An inductive characterization of matching in binding bigraphs', 'Details': 'March 2013Formal Aspects of Computing, Volume 25, Issue 2https://doi.org/10.1007/s00165-011-0184-5', 'DOI': 'https://doi.org/10.1007/s00165-011-0184-5'}, {'Title': 'Bigraphical Categories', 'Details': 'September 2009CONCUR 2009: Proceedings of the 20th International Conference on Concurrency Theoryhttps://doi.org/10.1007/978-3-642-04081-8_3', 'DOI': 'https://doi.org/10.1007/978-3-642-04081-8_3'}, {'Title': 'Computing Tomorrow: Future Research Directions in Computer Science', 'Details': 'March 2009'}, {'Title': 'Stochastic Bigraphs', 'Details': 'October 2008Electronic Notes in Theoretical Computer Science (ENTCS), Volume 218https://doi.org/10.1016/j.entcs.2008.10.006', 'DOI': 'https://doi.org/10.1016/j.entcs.2008.10.006'}, {'Title': 'Categories, Software and Meaning', 'Details': 'June 2008Concurrency, Graphs and Modelshttps://doi.org/10.1007/978-3-540-68679-8_50', 'DOI': 'https://doi.org/10.1007/978-3-540-68679-8_50'}, {'Title': 'Bigraphs and Their Algebra', 'Details': 'April 2008Electronic Notes in Theoretical Computer Science (ENTCS), Volume 209https://doi.org/10.1016/j.entcs.2008.04.002', 'DOI': 'https://doi.org/10.1016/j.entcs.2008.04.002'}]}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_author_profile(html_file):\n",
    "    with open(html_file, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    author_data = {}\n",
    "\n",
    "    # Extracting Author's Name\n",
    "    author_name_tag = soup.find('h1', class_='title')\n",
    "    if author_name_tag:\n",
    "        author_data['Name'] = author_name_tag.text.strip()\n",
    "\n",
    "    # Extracting Bibliometrics\n",
    "    bibliometrics = {}\n",
    "    bibliometrics_section = soup.find('div', class_='bibliometrics equal-height-slides')\n",
    "    if bibliometrics_section:\n",
    "        metrics = bibliometrics_section.find_all('div', class_='slide-item')\n",
    "        for metric in metrics:\n",
    "            title = metric.find('div', class_='bibliometrics__title').text.strip()\n",
    "            value = metric.find('div', class_='bibliometrics__count').text.strip()\n",
    "            bibliometrics[title] = value\n",
    "    author_data['Bibliometrics'] = bibliometrics\n",
    "    \n",
    "    # Extracting Author's Image URL\n",
    "    image_tag = soup.find('img', alt=True, class_='image-lazy-loaded')\n",
    "    if image_tag:\n",
    "        author_data['Image_URL'] = image_tag['src']\n",
    "    \n",
    "    # Extracting Co-Authors and Affiliations\n",
    "    co_authors = []\n",
    "    co_author_section = soup.find_all('div', class_='colored-block shadow contrib-metrics__multi-items')\n",
    "    for co_author in co_author_section:\n",
    "        co_author_data = {}\n",
    "        title_tag = co_author.find('h5')\n",
    "        if title_tag and title_tag.text.strip() in ['Most frequent co-Author', 'Most cited colleague', 'Most frequent Affiliation']:\n",
    "            co_author_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "            author_tag = co_author.find('div', class_='box-item')\n",
    "            if author_tag:\n",
    "                co_author_data['Details'] = author_tag.get_text(separator=' ').strip()\n",
    "\n",
    "            co_authors.append(co_author_data)\n",
    "\n",
    "    author_data['Co_Authors'] = co_authors\n",
    "    \n",
    "    # Extracting Top Subjects and Keywords\n",
    "    # top_subjects = []\n",
    "    # subject_section = soup.find_all('div', class_='colored-block shadow')\n",
    "    # for subject_block in subject_section:\n",
    "    #     title_tag = subject_block.find('h3', class_='title-header')\n",
    "    #     if title_tag and title_tag.text.strip() == 'Top subject':\n",
    "    #         top_subjects.append(subject_block.find('div', class_='top-rated-text').text.strip())\n",
    "\n",
    "    # author_data['Top_Subjects'] = top_subjects\n",
    "    \n",
    "    # Extracting Keywords\n",
    "    keywords = []\n",
    "    keyword_section = soup.find('div', class_='colored-block__content')\n",
    "    if keyword_section:\n",
    "        keyword_tags = keyword_section.find_all('div', class_='tag-cloud')\n",
    "        for keyword_tag in keyword_tags:\n",
    "            keyword_text = keyword_tag.get_text(separator=' ').strip()\n",
    "            keywords.append(keyword_text)\n",
    "    author_data['Keywords'] = keywords\n",
    "\n",
    "    # Extracting Publications\n",
    "    publications = []\n",
    "    pub_list_section = soup.find_all('li', class_='grid-item')\n",
    "    for pub in pub_list_section:\n",
    "        pub_data = {}\n",
    "        title_tag = pub.find('h3', class_='issue-item__title')\n",
    "        if title_tag:\n",
    "            pub_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "        date_tag = pub.find('div', class_='issue-item__detail')\n",
    "        if date_tag:\n",
    "            pub_data['Details'] = date_tag.text.strip()\n",
    "        \n",
    "        doi_tag = pub.find('a', class_='issue-item__doi')\n",
    "        if doi_tag:\n",
    "            pub_data['DOI'] = doi_tag.text.strip()\n",
    "\n",
    "        publications.append(pub_data)\n",
    "    \n",
    "    author_data['Publications'] = publications\n",
    "\n",
    "    return author_data\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "html_file = 'profile_81332515695.html'  # Replace with the actual file path\n",
    "author_profile_data = extract_author_profile(html_file)\n",
    "\n",
    "# Print the extracted data\n",
    "print(\"Author Profile Data:\")\n",
    "print(author_profile_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ACM award page: https://awards.acm.org/award-recipients\n",
      "\n",
      "Recipient 1:\n",
      "  Name: Milner, A J\n",
      "  Award: ACM A. M. Turing Award\n",
      "  Year: 1991\n",
      "  Region: Europe\n",
      "  DL Profile: https://dl.acm.org/author_page.cfm?id=81332515695\n",
      "Fetching DL profile: https://dl.acm.org/author_page.cfm?id=81332515695\n",
      "\n",
      "Recipient 2:\n",
      "  Name: Sreejith, A V\n",
      "  Award: ACM India Doctoral Dissertation Award\n",
      "  Year: 2014\n",
      "  Region: Asia\n",
      "  DL Profile: https://dl.acm.org/author_page.cfm?id=81479663157\n",
      "Fetching DL profile: https://dl.acm.org/author_page.cfm?id=81479663157\n",
      "\n",
      "Recipient 3:\n",
      "  Name: Malossi, A. Cristiano I.\n",
      "  Award: ACM Gordon Bell Prize\n",
      "  Year: 2015\n",
      "  Region: Europe\n",
      "  DL Profile: N/A\n",
      "Saved data to acm_author_profiles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# User-Agent list for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.48',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Function to get a random User-Agent\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# Function to extract author profile details from DL profile page\n",
    "def extract_author_profile(dl_url):\n",
    "    try:\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        print(f\"Fetching DL profile: {dl_url}\")\n",
    "        dl_response = requests.get(dl_url, headers=headers, timeout=10)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "\n",
    "        if dl_response.status_code == 200:\n",
    "            soup = BeautifulSoup(dl_response.content, 'html.parser')\n",
    "            \n",
    "            author_data = {}\n",
    "            \n",
    "            # Extracting Author's Name\n",
    "            author_name_tag = soup.find('h1', class_='title')\n",
    "            if author_name_tag:\n",
    "                author_data['Name'] = author_name_tag.text.strip()\n",
    "\n",
    "            # Extracting Bibliometrics\n",
    "            bibliometrics = {}\n",
    "            bibliometrics_section = soup.find('div', class_='bibliometrics equal-height-slides')\n",
    "            if bibliometrics_section:\n",
    "                metrics = bibliometrics_section.find_all('div', class_='slide-item')\n",
    "                for metric in metrics:\n",
    "                    title = metric.find('div', class_='bibliometrics__title').text.strip()\n",
    "                    value = metric.find('div', class_='bibliometrics__count').text.strip()\n",
    "                    bibliometrics[title] = value\n",
    "            author_data['Bibliometrics'] = bibliometrics\n",
    "            \n",
    "            # Extracting Author's Image URL\n",
    "            image_tag = soup.find('img', alt=True, class_='image-lazy-loaded')\n",
    "            if image_tag:\n",
    "                author_data['Image_URL'] = image_tag['src']\n",
    "            \n",
    "            # Extracting Co-Authors and Affiliations\n",
    "            co_authors = []\n",
    "            co_author_section = soup.find_all('div', class_='colored-block shadow contrib-metrics__multi-items')\n",
    "            for co_author in co_author_section:\n",
    "                co_author_data = {}\n",
    "                title_tag = co_author.find('h5')\n",
    "                if title_tag and title_tag.text.strip() in ['Most frequent co-Author', 'Most cited colleague', 'Most frequent Affiliation']:\n",
    "                    co_author_data['Title'] = title_tag.text.strip()\n",
    "                    author_tag = co_author.find('div', class_='box-item')\n",
    "                    if author_tag:\n",
    "                        co_author_data['Details'] = author_tag.get_text(separator=' ').strip()\n",
    "                    co_authors.append(co_author_data)\n",
    "            author_data['Co_Authors'] = co_authors\n",
    "            \n",
    "            # Extracting Keywords\n",
    "            keywords = []\n",
    "            keyword_section = soup.find('div', class_='colored-block__content')\n",
    "            if keyword_section:\n",
    "                keyword_tags = keyword_section.find_all('div', class_='tag-cloud')\n",
    "                for keyword_tag in keyword_tags:\n",
    "                    keyword_text = keyword_tag.get_text(separator=' ').strip()\n",
    "                    keywords.append(keyword_text)\n",
    "            author_data['Keywords'] = keywords\n",
    "\n",
    "            # Extracting Publications\n",
    "            publications = []\n",
    "            pub_list_section = soup.find_all('li', class_='grid-item')\n",
    "            for pub in pub_list_section:\n",
    "                pub_data = {}\n",
    "                title_tag = pub.find('h3', class_='issue-item__title')\n",
    "                if title_tag:\n",
    "                    pub_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "                date_tag = pub.find('div', class_='issue-item__detail')\n",
    "                if date_tag:\n",
    "                    pub_data['Details'] = date_tag.text.strip()\n",
    "                \n",
    "                doi_tag = pub.find('a', class_='issue-item__doi')\n",
    "                if doi_tag:\n",
    "                    pub_data['DOI'] = doi_tag.text.strip()\n",
    "\n",
    "                publications.append(pub_data)\n",
    "            \n",
    "            author_data['Publications'] = publications\n",
    "            \n",
    "            return author_data\n",
    "        else:\n",
    "            print(f\"Failed to fetch DL profile: {dl_url}\")\n",
    "            return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timed out for {dl_url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping DL profile {dl_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to scrape ACM award recipients and extract DL profile data\n",
    "def scrape_and_explore_dl_links_and_save():\n",
    "    url = \"https://awards.acm.org/award-recipients\"\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    \n",
    "    # Send request to the ACM award page\n",
    "    try:\n",
    "        print(f\"Fetching ACM award page: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        time.sleep(random.uniform(2, 5))  # Random sleep between 2-5 seconds\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Request timed out for ACM page\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch the ACM page: {e}\")\n",
    "        return\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the ACM awards page.\")\n",
    "        return\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    if table is None:\n",
    "        print(\"No table found on the page.\")\n",
    "        return\n",
    "    \n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    \n",
    "    if not rows:\n",
    "        print(\"No rows found in the table.\")\n",
    "        return\n",
    "    \n",
    "    rows_to_scrape = rows[:3]  # Limit to first 3 recipients for testing\n",
    "    \n",
    "    all_profiles = []\n",
    "    \n",
    "    for idx, row in enumerate(rows_to_scrape):\n",
    "        cols = row.find_all('td')\n",
    "        name = cols[0].text.strip()\n",
    "        award = cols[1].text.strip()\n",
    "        year = cols[2].text.strip()\n",
    "        region = cols[3].text.strip()\n",
    "        dl_link = cols[4].find('a')['href'] if cols[4].find('a') else None\n",
    "        \n",
    "        if dl_link:\n",
    "            dl_url = f\"https://dl.acm.org{dl_link}\" if dl_link.startswith('/') else dl_link\n",
    "        else:\n",
    "            dl_url = 'N/A'\n",
    "\n",
    "        # Print the data for verification\n",
    "        print(f\"\\nRecipient {idx+1}:\")\n",
    "        print(f\"  Name: {name}\")\n",
    "        print(f\"  Award: {award}\")\n",
    "        print(f\"  Year: {year}\")\n",
    "        print(f\"  Region: {region}\")\n",
    "        print(f\"  DL Profile: {dl_url}\")\n",
    "        \n",
    "        # Explore and extract DL profile for specific information\n",
    "        if dl_url != 'N/A':\n",
    "            author_profile = extract_author_profile(dl_url)\n",
    "            if author_profile:\n",
    "                author_profile['Award'] = award\n",
    "                author_profile['Year'] = year\n",
    "                author_profile['Region'] = region\n",
    "                all_profiles.append(author_profile)\n",
    "\n",
    "        time.sleep(random.uniform(3, 6))  # Sleep between rows\n",
    "\n",
    "    # Save to CSV using pandas\n",
    "    save_profiles_to_csv(all_profiles, 'acm_author_profiles.csv')\n",
    "\n",
    "# Save the data to a CSV file\n",
    "def save_profiles_to_csv(profiles, output_file):\n",
    "    flat_profiles = []\n",
    "    \n",
    "    for profile in profiles:\n",
    "        flat_profile = {\n",
    "            'Name': profile.get('Name'),\n",
    "            'Award': profile.get('Award'),\n",
    "            'Year': profile.get('Year'),\n",
    "            'Region': profile.get('Region'),\n",
    "            'Bibliometrics': str(profile.get('Bibliometrics', {})),  # Convert dict to string\n",
    "            'Image_URL': profile.get('Image_URL'),\n",
    "            'Co_Authors': ', '.join([co_author['Details'] for co_author in profile.get('Co_Authors', [])]),\n",
    "            'Keywords': ', '.join(profile.get('Keywords', [])),\n",
    "            'Publications': ', '.join([pub['Title'] for pub in profile.get('Publications', [])])\n",
    "        }\n",
    "        flat_profiles.append(flat_profile)\n",
    "    \n",
    "    df = pd.DataFrame(flat_profiles)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved data to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_and_explore_dl_links_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ACM award page: https://awards.acm.org/award-recipients\n",
      "\n",
      "Recipient 1:\n",
      "  Name: Milner, A J\n",
      "  Award: ACM A. M. Turing Award\n",
      "  Year: 1991\n",
      "  Region: Europe\n",
      "  DL Profile: https://dl.acm.org/author_page.cfm?id=81332515695\n",
      "Fetching DL profile: https://dl.acm.org/author_page.cfm?id=81332515695\n",
      "\n",
      "Recipient 2:\n",
      "  Name: Sreejith, A V\n",
      "  Award: ACM India Doctoral Dissertation Award\n",
      "  Year: 2014\n",
      "  Region: Asia\n",
      "  DL Profile: https://dl.acm.org/author_page.cfm?id=81479663157\n",
      "Fetching DL profile: https://dl.acm.org/author_page.cfm?id=81479663157\n",
      "\n",
      "Recipient 3:\n",
      "  Name: Malossi, A. Cristiano I.\n",
      "  Award: ACM Gordon Bell Prize\n",
      "  Year: 2015\n",
      "  Region: Europe\n",
      "  DL Profile: N/A\n",
      "Saved data to acm_author_profiles.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# User-Agent list for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.48',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Function to get a random User-Agent\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# Function to extract author profile details from DL profile page\n",
    "def extract_author_profile(dl_url):\n",
    "    try:\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        print(f\"Fetching DL profile: {dl_url}\")\n",
    "        dl_response = requests.get(dl_url, headers=headers, timeout=10)\n",
    "        time.sleep(random.uniform(2, 4))\n",
    "\n",
    "        if dl_response.status_code == 200:\n",
    "            soup = BeautifulSoup(dl_response.content, 'html.parser')\n",
    "            \n",
    "            author_data = {}\n",
    "            \n",
    "            # Extracting Author's Name\n",
    "            author_name_tag = soup.find('h1', class_='title')\n",
    "            if author_name_tag:\n",
    "                author_data['Name'] = author_name_tag.text.strip()\n",
    "\n",
    "            # Extracting Bibliometrics\n",
    "            bibliometrics = {}\n",
    "            bibliometrics_section = soup.find('div', class_='bibliometrics equal-height-slides')\n",
    "            if bibliometrics_section:\n",
    "                metrics = bibliometrics_section.find_all('div', class_='slide-item')\n",
    "                for metric in metrics:\n",
    "                    title = metric.find('div', class_='bibliometrics__title').text.strip()\n",
    "                    value = metric.find('div', class_='bibliometrics__count').text.strip()\n",
    "                    bibliometrics[title] = value\n",
    "            author_data['Bibliometrics'] = bibliometrics\n",
    "            \n",
    "            # Extracting Author's Image URL\n",
    "            image_tag = soup.find('img', alt=True, class_='image-lazy-loaded')\n",
    "            if image_tag:\n",
    "                author_data['Image_URL'] = image_tag['src']\n",
    "            \n",
    "            # Extracting Co-Authors and Affiliations\n",
    "            co_authors = []\n",
    "            co_author_section = soup.find_all('div', class_='colored-block shadow contrib-metrics__multi-items')\n",
    "            for co_author in co_author_section:\n",
    "                co_author_data = {}\n",
    "                title_tag = co_author.find('h5')\n",
    "                if title_tag and title_tag.text.strip() in ['Most frequent co-Author', 'Most cited colleague', 'Most frequent Affiliation']:\n",
    "                    co_author_data['Title'] = title_tag.text.strip()\n",
    "                    author_tag = co_author.find('div', class_='box-item')\n",
    "                    if author_tag:\n",
    "                        co_author_data['Details'] = author_tag.get_text(separator=' ').strip()\n",
    "                    co_authors.append(co_author_data)\n",
    "            author_data['Co_Authors'] = co_authors\n",
    "            \n",
    "            # Extracting Keywords\n",
    "            keywords = []\n",
    "            keyword_section = soup.find('div', class_='colored-block__content')\n",
    "            if keyword_section:\n",
    "                keyword_tags = keyword_section.find_all('div', class_='tag-cloud')\n",
    "                for keyword_tag in keyword_tags:\n",
    "                    keyword_text = keyword_tag.get_text(separator=' ').strip()\n",
    "                    keywords.append(keyword_text)\n",
    "            author_data['Keywords'] = keywords\n",
    "\n",
    "            # Extracting Publications\n",
    "            publications = []\n",
    "            pub_list_section = soup.find_all('li', class_='grid-item')\n",
    "            for pub in pub_list_section:\n",
    "                pub_data = {}\n",
    "                title_tag = pub.find('h3', class_='issue-item__title')\n",
    "                if title_tag:\n",
    "                    pub_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "                date_tag = pub.find('div', class_='issue-item__detail')\n",
    "                if date_tag:\n",
    "                    pub_data['Details'] = date_tag.text.strip()\n",
    "                \n",
    "                doi_tag = pub.find('a', class_='issue-item__doi')\n",
    "                if doi_tag:\n",
    "                    pub_data['DOI'] = doi_tag.text.strip()\n",
    "\n",
    "                publications.append(pub_data)\n",
    "            \n",
    "            author_data['Publications'] = publications\n",
    "            \n",
    "            return author_data\n",
    "        else:\n",
    "            print(f\"Failed to fetch DL profile: {dl_url}\")\n",
    "            return None\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Request timed out for {dl_url}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping DL profile {dl_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to scrape ACM award recipients and extract DL profile data\n",
    "def scrape_and_explore_dl_links_and_save():\n",
    "    url = \"https://awards.acm.org/award-recipients\"\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    \n",
    "    # Send request to the ACM award page\n",
    "    try:\n",
    "        print(f\"Fetching ACM award page: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        time.sleep(random.uniform(2, 5))  # Random sleep between 2-5 seconds\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Request timed out for ACM page\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch the ACM page: {e}\")\n",
    "        return\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to retrieve the ACM awards page.\")\n",
    "        return\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    if table is None:\n",
    "        print(\"No table found on the page.\")\n",
    "        return\n",
    "    \n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    \n",
    "    if not rows:\n",
    "        print(\"No rows found in the table.\")\n",
    "        return\n",
    "    \n",
    "    rows_to_scrape = rows[:3]  # Limit to first 3 recipients for testing\n",
    "    \n",
    "    all_profiles = []\n",
    "    \n",
    "    for idx, row in enumerate(rows_to_scrape):\n",
    "        cols = row.find_all('td')\n",
    "        name = cols[0].text.strip()\n",
    "        award = cols[1].text.strip()\n",
    "        year = cols[2].text.strip()\n",
    "        region = cols[3].text.strip()\n",
    "        dl_link = cols[4].find('a')['href'] if cols[4].find('a') else None\n",
    "        \n",
    "        if dl_link:\n",
    "            dl_url = f\"https://dl.acm.org{dl_link}\" if dl_link.startswith('/') else dl_link\n",
    "        else:\n",
    "            dl_url = 'N/A'\n",
    "\n",
    "        # Print the data for verification\n",
    "        print(f\"\\nRecipient {idx+1}:\")\n",
    "        print(f\"  Name: {name}\")\n",
    "        print(f\"  Award: {award}\")\n",
    "        print(f\"  Year: {year}\")\n",
    "        print(f\"  Region: {region}\")\n",
    "        print(f\"  DL Profile: {dl_url}\")\n",
    "        \n",
    "        # Explore and extract DL profile for specific information\n",
    "        if dl_url != 'N/A':\n",
    "            author_profile = extract_author_profile(dl_url)\n",
    "            if author_profile:\n",
    "                author_profile['Award'] = award\n",
    "                author_profile['Year'] = year\n",
    "                author_profile['Region'] = region\n",
    "                all_profiles.append(author_profile)\n",
    "\n",
    "        time.sleep(random.uniform(3, 6))  # Sleep between rows\n",
    "\n",
    "    # Save to CSV using pandas\n",
    "    save_profiles_to_csv(all_profiles, 'acm_author_profiles.csv')\n",
    "\n",
    "# Save the data to a CSV file\n",
    "def save_profiles_to_csv(profiles, output_file):\n",
    "    flat_profiles = []\n",
    "    \n",
    "    for profile in profiles:\n",
    "        flat_profile = {\n",
    "            'Name': profile.get('Name'),\n",
    "            'Award': profile.get('Award'),\n",
    "            'Year': profile.get('Year'),\n",
    "            'Region': profile.get('Region'),\n",
    "            # Flatten the bibliometrics\n",
    "            'Average Citation per Article': profile['Bibliometrics'].get('Average Citation per Article', 'N/A'),\n",
    "            'Citation Count': profile['Bibliometrics'].get('Citation count', 'N/A'),\n",
    "            'Publication Count': profile['Bibliometrics'].get('Publication counts', 'N/A'),\n",
    "            'Publication Years': profile['Bibliometrics'].get('Publication Years', 'N/A'),\n",
    "            'Downloads (12 months)': profile['Bibliometrics'].get('Downloads (12 months)', 'N/A'),\n",
    "            'Image_URL': profile.get('Image_URL'),\n",
    "            'Co_Authors': ', '.join([co_author['Details'] for co_author in profile.get('Co_Authors', [])]),\n",
    "            'Keywords': ', '.join(profile.get('Keywords', [])),\n",
    "            'Publications': ', '.join([pub['Title'] for pub in profile.get('Publications', [])])\n",
    "        }\n",
    "        flat_profiles.append(flat_profile)\n",
    "    \n",
    "    df = pd.DataFrame(flat_profiles)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved data to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_and_explore_dl_links_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting profile from: https://dl.acm.org/author_page.cfm?id=81332515695\n",
      "Extracting profile from: https://dl.acm.org/author_page.cfm?id=81479663157\n",
      "Extracting profile from: https://dl.acm.org/author_page.cfm?id=81100086771\n",
      "Extracting profile from: https://dl.acm.org/author_page.cfm?id=81408600192\n",
      "Extracting profile from: https://dl.acm.org/author_page.cfm?id=81100604913\n",
      "Extracting profile from: https://dl.acm.org/author_page.cfm?id=81758701057\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProfiles saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     profiles \u001b[38;5;241m=\u001b[39m \u001b[43mextract_all_acm_profiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m profiles:\n\u001b[1;32m    170\u001b[0m         save_profiles_to_json(profiles, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macm_recipient_profiles.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 154\u001b[0m, in \u001b[0;36mextract_all_acm_profiles\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m recipient_urls:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting profile from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 154\u001b[0m     profile \u001b[38;5;241m=\u001b[39m \u001b[43mextract_author_profile_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    156\u001b[0m         all_profiles\u001b[38;5;241m.\u001b[39mappend(profile)\n",
      "Cell \u001b[0;32mIn[15], line 58\u001b[0m, in \u001b[0;36mextract_author_profile_from_url\u001b[0;34m(dl_url)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: get_random_user_agent()}\n\u001b[0;32m---> 58\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve profile page for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdl_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidProxyURL(\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check proxy URL. It is malformed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand could be missing the host.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         )\n\u001b[1;32m    483\u001b[0m     proxy_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy_manager_for(proxy)\n\u001b[1;32m    484\u001b[0m     conn \u001b[38;5;241m=\u001b[39m proxy_manager\u001b[38;5;241m.\u001b[39mconnection_from_host(\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhost_params, pool_kwargs\u001b[38;5;241m=\u001b[39mpool_kwargs\n\u001b[0;32m--> 486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Only scheme should be lower case\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoolmanager\u001b[38;5;241m.\u001b[39mconnection_from_host(\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhost_params, pool_kwargs\u001b[38;5;241m=\u001b[39mpool_kwargs\n\u001b[1;32m    491\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/http/client.py:1423\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1422\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1423\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/socket.py:707\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.12/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "# User-Agent list for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.48',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Function to get a random User-Agent\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)\n",
    "\n",
    "# Function to scrape the ACM award recipients and gather their DL profile URLs\n",
    "def scrape_acm_award_recipients():\n",
    "    url = \"https://awards.acm.org/award-recipients\"\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve ACM awards page. Status code: {response.status_code}\")\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(\"No table found on the page.\")\n",
    "            return None\n",
    "\n",
    "        rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "        recipient_urls = []\n",
    "        \n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            dl_link = cols[4].find('a')['href'] if cols[4].find('a') else None\n",
    "            \n",
    "            if dl_link:\n",
    "                dl_url = f\"https://dl.acm.org{dl_link}\" if dl_link.startswith('/') else dl_link\n",
    "                recipient_urls.append(dl_url)\n",
    "\n",
    "        return recipient_urls\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping ACM recipients: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract author profile from DL profile URL\n",
    "def extract_author_profile_from_url(dl_url):\n",
    "    try:\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        response = requests.get(dl_url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve profile page for {dl_url}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return extract_author_profile_from_soup(soup)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching profile for {dl_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract author profile details from the soup object\n",
    "def extract_author_profile_from_soup(soup):\n",
    "    author_data = {}\n",
    "\n",
    "    # Extracting Author's Name\n",
    "    author_name_tag = soup.find('h1', class_='title')\n",
    "    if author_name_tag:\n",
    "        author_data['Name'] = author_name_tag.text.strip()\n",
    "\n",
    "    # Extracting Bibliometrics\n",
    "    bibliometrics = {}\n",
    "    bibliometrics_section = soup.find('div', class_='bibliometrics equal-height-slides')\n",
    "    if bibliometrics_section:\n",
    "        metrics = bibliometrics_section.find_all('div', class_='slide-item')\n",
    "        for metric in metrics:\n",
    "            title = metric.find('div', class_='bibliometrics__title').text.strip()\n",
    "            value = metric.find('div', class_='bibliometrics__count').text.strip()\n",
    "            bibliometrics[title] = value\n",
    "    author_data['Bibliometrics'] = bibliometrics\n",
    "    \n",
    "    # Extracting Author's Image URL\n",
    "    image_tag = soup.find('img', alt=True, class_='image-lazy-loaded')\n",
    "    if image_tag:\n",
    "        author_data['Image_URL'] = image_tag['src']\n",
    "    \n",
    "    # Extracting Co-Authors and Affiliations\n",
    "    co_authors = []\n",
    "    co_author_section = soup.find_all('div', class_='colored-block shadow contrib-metrics__multi-items')\n",
    "    for co_author in co_author_section:\n",
    "        co_author_data = {}\n",
    "        title_tag = co_author.find('h5')\n",
    "        if title_tag and title_tag.text.strip() in ['Most frequent co-Author', 'Most cited colleague', 'Most frequent Affiliation']:\n",
    "            co_author_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "            author_tag = co_author.find('div', class_='box-item')\n",
    "            if author_tag:\n",
    "                co_author_data['Details'] = author_tag.get_text(separator=' ').strip()\n",
    "\n",
    "            co_authors.append(co_author_data)\n",
    "\n",
    "    author_data['Co_Authors'] = co_authors\n",
    "\n",
    "    # Extracting Keywords\n",
    "    keywords = []\n",
    "    keyword_section = soup.find('div', class_='colored-block__content')\n",
    "    if keyword_section:\n",
    "        keyword_tags = keyword_section.find_all('div', class_='tag-cloud')\n",
    "        for keyword_tag in keyword_tags:\n",
    "            keyword_text = keyword_tag.get_text(separator=' ').strip()\n",
    "            keywords.append(keyword_text)\n",
    "    author_data['Keywords'] = keywords\n",
    "\n",
    "    # Extracting Publications\n",
    "    publications = []\n",
    "    pub_list_section = soup.find_all('li', class_='grid-item')\n",
    "    for pub in pub_list_section:\n",
    "        pub_data = {}\n",
    "        title_tag = pub.find('h3', class_='issue-item__title')\n",
    "        if title_tag:\n",
    "            pub_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "        date_tag = pub.find('div', class_='issue-item__detail')\n",
    "        if date_tag:\n",
    "            pub_data['Details'] = date_tag.text.strip()\n",
    "        \n",
    "        doi_tag = pub.find('a', class_='issue-item__doi')\n",
    "        if doi_tag:\n",
    "            pub_data['DOI'] = doi_tag.text.strip()\n",
    "\n",
    "        publications.append(pub_data)\n",
    "    \n",
    "    author_data['Publications'] = publications\n",
    "\n",
    "    return author_data\n",
    "\n",
    "# Main function to extract profiles for all recipients\n",
    "def extract_all_acm_profiles():\n",
    "    recipient_urls = scrape_acm_award_recipients()\n",
    "    if not recipient_urls:\n",
    "        print(\"No recipients found.\")\n",
    "        return\n",
    "\n",
    "    all_profiles = []\n",
    "    for url in recipient_urls:\n",
    "        print(f\"Extracting profile from: {url}\")\n",
    "        profile = extract_author_profile_from_url(url)\n",
    "        if profile:\n",
    "            all_profiles.append(profile)\n",
    "        time.sleep(random.uniform(2, 5))  # Avoid overloading the server with requests\n",
    "\n",
    "    return all_profiles\n",
    "\n",
    "# Save the profiles to a JSON file\n",
    "def save_profiles_to_json(profiles, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(profiles, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Profiles saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    profiles = extract_all_acm_profiles()\n",
    "    if profiles:\n",
    "        save_profiles_to_json(profiles, 'acm_recipient_profiles.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent list for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.48',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Retry configuration\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BACKOFF = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a random User-Agent\n",
    "def get_random_user_agent():\n",
    "    return random.choice(USER_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retry function with backoff strategy\n",
    "def retry_request(url, headers, max_retries=MAX_RETRIES, timeout=10):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                print(f\"Error: {response.status_code} - Retrying...\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e} - Retrying...\")\n",
    "        retries += 1\n",
    "        time.sleep(RETRY_BACKOFF * retries)  # Exponential backoff\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Google Scholar profile data with retries\n",
    "def get_google_scholar_url(full_name):\n",
    "    base_url = f\"https://api.scholarlydata.com/get_profile?name={full_name}\"\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    response = retry_request(base_url, headers)\n",
    "    \n",
    "    if response:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            if 'scholar_id' in data:\n",
    "                return data\n",
    "            else:\n",
    "                return {}\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Failed to parse the JSON response.\")\n",
    "            return {}\n",
    "    else:\n",
    "        print(f\"Failed to retrieve Google Scholar data for {full_name}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape ACM award recipients\n",
    "def scrape_acm_award_recipients():\n",
    "    url = \"https://awards.acm.org/award-recipients\"\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    response = retry_request(url, headers)\n",
    "    \n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table')\n",
    "    if not table:\n",
    "        print(\"No table found on the page.\")\n",
    "        return None\n",
    "    \n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "    recipient_urls = []\n",
    "    recepient_data = []\n",
    "    selected_range = 3  # Limit to first 3 recipients for testing\n",
    "    for row in rows[:selected_range]:\n",
    "        cols = row.find_all('td')\n",
    "        name = cols[0].text.strip()\n",
    "        award = cols[1].text.strip()\n",
    "        year = cols[2].text.strip()\n",
    "        region = cols[3].text.strip()\n",
    "        dl_link = cols[4].find('a')['href'] if cols[4].find('a') else None\n",
    "\n",
    "        # save the corresponding details \n",
    "        recipient_profile = {\n",
    "            'Name': name,\n",
    "            'Award': award,\n",
    "            'Year': year,\n",
    "            'Region': region,\n",
    "            'DL_Link': dl_link\n",
    "        }\n",
    "        \n",
    "        recepient_data.append(recipient_profile)\n",
    "\n",
    "        if dl_link:\n",
    "            dl_url = f\"https://dl.acm.org{dl_link}\" if dl_link.startswith('/') else dl_link\n",
    "            recipient_urls.append(dl_url)\n",
    "\n",
    "    return recipient_urls, recepient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract author profile from URL with retries\n",
    "def extract_author_profile_from_url(dl_url):\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    response = retry_request(dl_url, headers)\n",
    "    \n",
    "    if not response:\n",
    "        print(f\"Failed to retrieve profile page for {dl_url}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return extract_author_profile_from_soup(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract author profile from DL profile URL\n",
    "def extract_author_profile_from_url(dl_url):\n",
    "    try:\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        response = requests.get(dl_url, headers=headers, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve profile page for {dl_url}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return extract_author_profile_from_soup(soup)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching profile for {dl_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract author profile details from the soup object\n",
    "def extract_author_profile_from_soup(soup):\n",
    "    author_data = {}\n",
    "\n",
    "    # Extracting Author's Name\n",
    "    author_name_tag = soup.find('h1', class_='title')\n",
    "    if author_name_tag:\n",
    "        author_data['Name'] = author_name_tag.text.strip()\n",
    "\n",
    "    # Extracting Bibliometrics\n",
    "    bibliometrics = {}\n",
    "    bibliometrics_section = soup.find('div', class_='bibliometrics equal-height-slides')\n",
    "    if bibliometrics_section:\n",
    "        metrics = bibliometrics_section.find_all('div', class_='slide-item')\n",
    "        for metric in metrics:\n",
    "            title = metric.find('div', class_='bibliometrics__title').text.strip()\n",
    "            value = metric.find('div', class_='bibliometrics__count').text.strip()\n",
    "            bibliometrics[title] = value\n",
    "    author_data['Bibliometrics'] = bibliometrics\n",
    "    \n",
    "    # Extracting Author's Image URL\n",
    "    image_tag = soup.find('img', alt=True, class_='image-lazy-loaded')\n",
    "    if image_tag:\n",
    "        author_data['Image_URL'] = image_tag['src']\n",
    "    \n",
    "    # Extracting Co-Authors and Affiliations\n",
    "    co_authors = []\n",
    "    co_author_section = soup.find_all('div', class_='colored-block shadow contrib-metrics__multi-items')\n",
    "    for co_author in co_author_section:\n",
    "        co_author_data = {}\n",
    "        title_tag = co_author.find('h5')\n",
    "        if title_tag and title_tag.text.strip() in ['Most frequent co-Author', 'Most cited colleague', 'Most frequent Affiliation']:\n",
    "            co_author_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "            author_tag = co_author.find('div', class_='box-item')\n",
    "            if author_tag:\n",
    "                co_author_data['Details'] = author_tag.get_text(separator=' ').strip()\n",
    "\n",
    "            co_authors.append(co_author_data)\n",
    "\n",
    "    author_data['Co_Authors'] = co_authors\n",
    "\n",
    "    # Extracting Keywords\n",
    "    keywords = []\n",
    "    keyword_section = soup.find('div', class_='colored-block__content')\n",
    "    if keyword_section:\n",
    "        keyword_tags = keyword_section.find_all('div', class_='tag-cloud')\n",
    "        for keyword_tag in keyword_tags:\n",
    "            keyword_text = keyword_tag.get_text(separator=' ').strip()\n",
    "            keywords.append(keyword_text)\n",
    "    author_data['Keywords'] = keywords\n",
    "\n",
    "    # Extracting Publications\n",
    "    publications = []\n",
    "    pub_list_section = soup.find_all('li', class_='grid-item')\n",
    "    for pub in pub_list_section:\n",
    "        pub_data = {}\n",
    "        title_tag = pub.find('h3', class_='issue-item__title')\n",
    "        if title_tag:\n",
    "            pub_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "        date_tag = pub.find('div', class_='issue-item__detail')\n",
    "        if date_tag:\n",
    "            pub_data['Details'] = date_tag.text.strip()\n",
    "        \n",
    "        doi_tag = pub.find('a', class_='issue-item__doi')\n",
    "        if doi_tag:\n",
    "            pub_data['DOI'] = doi_tag.text.strip()\n",
    "\n",
    "        publications.append(pub_data)\n",
    "    \n",
    "    author_data['Publications'] = publications\n",
    "\n",
    "    return author_data\n",
    "\n",
    "dl_url = 'https://dl.acm.org/profile/81332515695'\n",
    "author_profile = extract_author_profile_from_url(dl_url)\n",
    "print(author_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Robin Milner', 'Bibliometrics': {'Average Citation per Article': '95', 'Citation count': '9,205', 'Publication counts': '97', 'Publication Years': '1971 - 2013', 'Available for Download': '19', 'Average Downloads per Article': '1,711', 'Downloads (6 weeks)': '422', 'Downloads (12 months)': '9,316', 'Downloads (cumulative)': '32,500'}, 'Image_URL': '/do/10.1145/contrib-81332515695/full/81332515695-1588898507063.jpg', 'Co_Authors': [], 'Keywords': [{'term': '111', 'label': 'Logic', 'count': 16, 'link': None}, {'term': '460', 'label': 'Program semantics', 'count': 12, 'link': None}, {'term': '108', 'label': 'Models of computation', 'count': 7, 'link': None}, {'term': '1199', 'label': 'Parallel computing models', 'count': 7, 'link': None}, {'term': '115', 'label': 'Semantics and reasoning', 'count': 7, 'link': None}, {'term': '1719', 'label': 'Semantics', 'count': 6, 'link': None}, {'term': '109', 'label': 'Formal languages and automata theory', 'count': 5, 'link': None}, {'term': '770', 'label': 'Lambda calculus', 'count': 5, 'link': None}, {'term': '397', 'label': 'Concurrency', 'count': 4, 'link': None}, {'term': '774', 'label': 'Lambda calculus', 'count': 4, 'link': None}, {'term': '680', 'label': 'Formal language definitions', 'count': 3, 'link': None}, {'term': '1716', 'label': 'Language types', 'count': 3, 'link': None}, {'term': '68', 'label': 'Document types', 'count': 2, 'link': None}, {'term': '714', 'label': 'Graph algorithms', 'count': 2, 'link': None}, {'term': '331', 'label': 'Mobile networks', 'count': 2, 'link': None}, {'term': '702', 'label': 'Trees', 'count': 2, 'link': None}, {'term': '333', 'label': 'Wireless access networks', 'count': 2, 'link': None}, {'term': '1202', 'label': 'Algebraic language theory', 'count': 1, 'link': None}, {'term': '100', 'label': 'Interaction design', 'count': 1, 'link': None}, {'term': '1222', 'label': 'Pattern matching', 'count': 1, 'link': None}], 'Publications': [{'Title': 'An inductive characterization of matching in binding bigraphs', 'Details': 'March 2013Formal Aspects of Computing, Volume 25, Issue 2https://doi.org/10.1007/s00165-011-0184-5', 'DOI': 'https://doi.org/10.1007/s00165-011-0184-5'}, {'Title': 'Bigraphical Categories', 'Details': 'September 2009CONCUR 2009: Proceedings of the 20th International Conference on Concurrency Theoryhttps://doi.org/10.1007/978-3-642-04081-8_3', 'DOI': 'https://doi.org/10.1007/978-3-642-04081-8_3'}, {'Title': 'Computing Tomorrow: Future Research Directions in Computer Science', 'Details': 'March 2009'}, {'Title': 'Stochastic Bigraphs', 'Details': 'October 2008Electronic Notes in Theoretical Computer Science (ENTCS), Volume 218https://doi.org/10.1016/j.entcs.2008.10.006', 'DOI': 'https://doi.org/10.1016/j.entcs.2008.10.006'}, {'Title': 'Categories, Software and Meaning', 'Details': 'June 2008Concurrency, Graphs and Modelshttps://doi.org/10.1007/978-3-540-68679-8_50', 'DOI': 'https://doi.org/10.1007/978-3-540-68679-8_50'}, {'Title': 'Bigraphs and Their Algebra', 'Details': 'April 2008Electronic Notes in Theoretical Computer Science (ENTCS), Volume 209https://doi.org/10.1016/j.entcs.2008.04.002', 'DOI': 'https://doi.org/10.1016/j.entcs.2008.04.002'}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "\n",
    "def retry_request(url, headers):\n",
    "    return requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "def extract_author_profile(dl_url):\n",
    "    try:\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        response = retry_request(dl_url, headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve profile page for {dl_url}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        author_data = {}\n",
    "\n",
    "        # Extracting Author's Name\n",
    "        author_name_tag = soup.find('h1', class_='title')\n",
    "        if author_name_tag:\n",
    "            author_data['Name'] = author_name_tag.text.strip()\n",
    "\n",
    "        # Extracting Bibliometrics\n",
    "        bibliometrics = {}\n",
    "        bibliometrics_section = soup.find('div', class_='bibliometrics equal-height-slides')\n",
    "        if bibliometrics_section:\n",
    "            metrics = bibliometrics_section.find_all('div', class_='slide-item')\n",
    "            for metric in metrics:\n",
    "                title = metric.find('div', class_='bibliometrics__title').text.strip()\n",
    "                value = metric.find('div', class_='bibliometrics__count').text.strip()\n",
    "                bibliometrics[title] = value\n",
    "        author_data['Bibliometrics'] = bibliometrics\n",
    "        \n",
    "        # Extracting Author's Image URL\n",
    "        image_tag = soup.find('img', alt=True, class_='image-lazy-loaded')\n",
    "        if image_tag:\n",
    "            author_data['Image_URL'] = image_tag['src']\n",
    "        \n",
    "        # Extracting Co-Authors and Affiliations\n",
    "        co_authors = []\n",
    "        co_author_section = soup.find_all('div', class_='colored-block shadow contrib-metrics__multi-items')\n",
    "        for co_author in co_author_section:\n",
    "            co_author_data = {}\n",
    "            title_tag = co_author.find('h5')\n",
    "            if title_tag and title_tag.text.strip() in ['Most frequent co-Author', 'Most cited colleague', 'Most frequent Affiliation']:\n",
    "                co_author_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "                author_tag = co_author.find('div', class_='box-item')\n",
    "                if author_tag:\n",
    "                    co_author_data['Details'] = author_tag.get_text(separator=' ').strip()\n",
    "\n",
    "                co_authors.append(co_author_data)\n",
    "\n",
    "        author_data['Co_Authors'] = co_authors\n",
    "\n",
    "        # Extracting Keywords\n",
    "        keywords = []\n",
    "        tag_cloud_div = soup.find('div', class_='tag-cloud')\n",
    "        if tag_cloud_div and tag_cloud_div.has_attr('data-tags'):\n",
    "            data_tags = tag_cloud_div['data-tags'].replace('&quot;', '\"')\n",
    "            try:\n",
    "                tags_data = json.loads(data_tags)\n",
    "                for tag in tags_data:\n",
    "                    keyword_info = {\n",
    "                        'term': tag.get('term'),\n",
    "                        'label': tag.get('label'),\n",
    "                        'count': tag.get('count'),\n",
    "                        'link': tag.get('link')\n",
    "                    }\n",
    "                    keywords.append(keyword_info)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse JSON: {e}\")\n",
    "        author_data['Keywords'] = keywords\n",
    "\n",
    "        # Extracting Publications\n",
    "        publications = []\n",
    "        pub_list_section = soup.find_all('li', class_='grid-item')\n",
    "        for pub in pub_list_section:\n",
    "            pub_data = {}\n",
    "            title_tag = pub.find('h3', class_='issue-item__title')\n",
    "            if title_tag:\n",
    "                pub_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "            date_tag = pub.find('div', class_='issue-item__detail')\n",
    "            if date_tag:\n",
    "                pub_data['Details'] = date_tag.text.strip()\n",
    "            \n",
    "            doi_tag = pub.find('a', class_='issue-item__doi')\n",
    "            if doi_tag:\n",
    "                pub_data['DOI'] = doi_tag.text.strip()\n",
    "\n",
    "            publications.append(pub_data)\n",
    "        \n",
    "        author_data['Publications'] = publications\n",
    "\n",
    "        return author_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching profile for {dl_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiles saved to author_profiles.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save profiles to JSON (save in batches to avoid data loss)\n",
    "def save_profiles_to_json(profiles, output_file):\n",
    "    if not profiles:\n",
    "        print(\"No profiles to save.\")\n",
    "        return\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(profiles, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Profiles saved to {output_file}\")\n",
    "\n",
    "dl_url = 'https://dl.acm.org/profile/81332515695'\n",
    "author_profile = extract_author_profile(dl_url)\n",
    "save_profiles_to_json([author_profile], 'author_profiles.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Subject Areas': [{'term': '111', 'label': 'Logic', 'count': 16, 'link': None}, {'term': '460', 'label': 'Program semantics', 'count': 12, 'link': None}, {'term': '108', 'label': 'Models of computation', 'count': 7, 'link': None}, {'term': '1199', 'label': 'Parallel computing models', 'count': 7, 'link': None}, {'term': '115', 'label': 'Semantics and reasoning', 'count': 7, 'link': None}, {'term': '1719', 'label': 'Semantics', 'count': 6, 'link': None}, {'term': '109', 'label': 'Formal languages and automata theory', 'count': 5, 'link': None}, {'term': '770', 'label': 'Lambda calculus', 'count': 5, 'link': None}, {'term': '397', 'label': 'Concurrency', 'count': 4, 'link': None}, {'term': '774', 'label': 'Lambda calculus', 'count': 4, 'link': None}, {'term': '680', 'label': 'Formal language definitions', 'count': 3, 'link': None}, {'term': '1716', 'label': 'Language types', 'count': 3, 'link': None}, {'term': '68', 'label': 'Document types', 'count': 2, 'link': None}, {'term': '714', 'label': 'Graph algorithms', 'count': 2, 'link': None}, {'term': '331', 'label': 'Mobile networks', 'count': 2, 'link': None}, {'term': '702', 'label': 'Trees', 'count': 2, 'link': None}, {'term': '333', 'label': 'Wireless access networks', 'count': 2, 'link': None}, {'term': '1202', 'label': 'Algebraic language theory', 'count': 1, 'link': None}, {'term': '100', 'label': 'Interaction design', 'count': 1, 'link': None}, {'term': '1222', 'label': 'Pattern matching', 'count': 1, 'link': None}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_random_user_agent():\n",
    "    # Function to get a random user agent (placeholder for real user agent rotation logic)\n",
    "    return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "\n",
    "def retry_request(url, headers):\n",
    "    # Placeholder for the retry_request function, which should handle retries and backoff\n",
    "    import requests\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def extract_author_profile():\n",
    "    url = \"https://dl.acm.org/author_page.cfm?id=81332515695\"\n",
    "    headers = {'User-Agent': get_random_user_agent()}\n",
    "    response = retry_request(url, headers)\n",
    "    \n",
    "    html_content = response.content\n",
    "\n",
    "    # Parsing the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the div with class 'tag-cloud'\n",
    "    tag_cloud_div = soup.find('div', class_='tag-cloud')\n",
    "\n",
    "    # Find the parent of parent and extract h4 text\n",
    "    if tag_cloud_div:\n",
    "        parent_of_parent = tag_cloud_div.find_parent().find_parent()\n",
    "        h4_text = parent_of_parent.find('h4').text.strip() if parent_of_parent.find('h4') else \"No h4 found\"\n",
    "\n",
    "    # Extract the JSON-like data from the 'data-tags' attribute\n",
    "    keywords = []\n",
    "    if tag_cloud_div and tag_cloud_div.has_attr('data-tags'):\n",
    "        data_tags = tag_cloud_div['data-tags']\n",
    "        data_tags = data_tags.replace('&quot;', '\"')  # Convert HTML entities to normal characters\n",
    "        \n",
    "        # Parse the JSON string into Python objects\n",
    "        try:\n",
    "            tags_data = json.loads(data_tags)\n",
    "\n",
    "            # Extract relevant information (term, label, count)\n",
    "            for tag in tags_data:\n",
    "                keyword_info = {\n",
    "                    'term': tag.get('term'),\n",
    "                    'label': tag.get('label'),\n",
    "                    'count': tag.get('count'),\n",
    "                    'link': tag.get('link')\n",
    "                }\n",
    "                keywords.append(keyword_info)\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse JSON: {e}\")\n",
    "    else:\n",
    "        print(\"No tag cloud data found.\")\n",
    "    \n",
    "    # Return a dictionary mapping the h4 text to the list of keywords\n",
    "    return {h4_text: keywords}\n",
    "\n",
    "# Run the extraction and print the results\n",
    "result = extract_author_profile()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Milner, A J (1/5)\n",
      "Processing Perlis, A. J. (2/5)\n",
      "Processing Shamir, Adi (3/5)\n",
      "Processing Kay, Alan (4/5)\n",
      "Processing Aho, Alfred V (5/5)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from scholarly import scholarly, ProxyGenerator\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraper.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# User-Agent list for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36',\n",
    "]\n",
    "\n",
    "# Initialize the ProxyGenerator for Google Scholar scraping\n",
    "API_KEY = '2c0689f76068fc9463b07cac6970050e'  # Your ScraperAPI key\n",
    "components = ['name', 'scholar_id', 'affiliation', 'interests', 'citedby']\n",
    "\n",
    "pg = ProxyGenerator()\n",
    "pg.ScraperAPI(API_KEY)\n",
    "scholarly.use_proxy(pg)\n",
    "\n",
    "\n",
    "\n",
    "class WebSessionHandler:\n",
    "    def __init__(self, proxies=None):\n",
    "        self.session = requests.Session()\n",
    "        self.proxies = proxies\n",
    "\n",
    "    def get_random_user_agent(self):\n",
    "        return random.choice(USER_AGENTS)\n",
    "\n",
    "    def fetch(self, url, use_proxy=False, retries=3):\n",
    "        headers = {'User-Agent': self.get_random_user_agent()}\n",
    "        proxy = {\"http\": random.choice(self.proxies), \"https\": random.choice(self.proxies)} if self.proxies else None\n",
    "        for _ in range(retries):\n",
    "            try:\n",
    "                return self.session.get(url, headers=headers, proxies=proxy, timeout=10)\n",
    "            except requests.exceptions.Timeout:\n",
    "                logging.warning(f\"Timeout fetching {url}, retrying...\")\n",
    "                time.sleep(2)\n",
    "        logging.error(f\"Failed to fetch {url} after {retries} retries.\")\n",
    "        return None\n",
    "\n",
    "class ACMProfileScraper:\n",
    "    def __init__(self, session_handler):\n",
    "        self.session_handler = session_handler\n",
    "\n",
    "    def extract_acm_profile(self, profile_url):\n",
    "        response = self.session_handler.fetch(profile_url)\n",
    "        if response and response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return self._parse_acm_profile(soup)\n",
    "        return None\n",
    "\n",
    "    def _parse_acm_profile(self, soup):\n",
    "        full_name = soup.find('h1').text.strip() if soup.find('h1') else \"N/A\"\n",
    "        awards_info = soup.find_all('section', {'class': 'awards-winners__citation'})\n",
    "        acm_award = next((award for award in awards_info if award.find('h2').a.text == 'ACM A. M. Turing Award'), None)\n",
    "        if acm_award:\n",
    "            location, year = acm_award.find('h3', {'class': 'awards-winners__location'}).text.split(' - ')\n",
    "            citation = ' '.join(acm_award.find('p', {'class': \"awards-winners__citation-short\"}).text.split('\\n')).strip()\n",
    "        else:\n",
    "            location, year, citation = 'N/A', 'N/A', 'N/A'\n",
    "        return full_name, year, location, citation\n",
    "\n",
    "    def scrape_acm_award_page(self, base_url, limit=5):\n",
    "        \"\"\"Scrapes a limited number of profiles for testing.\"\"\"\n",
    "        response = self.session_handler.fetch(base_url)\n",
    "        if response and response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table', class_='awards-tables--fullWidth')\n",
    "            if not table:\n",
    "                print(\"Table not found!\")\n",
    "                return []\n",
    "            rows = table.find('tbody').find_all('tr')[:limit]  # Limit for testing\n",
    "            profiles = []\n",
    "            for row in rows:\n",
    "                name_element = row.find_all('td')[0].find('a')\n",
    "                recipient_name = name_element.text.strip()\n",
    "                profile_url = f'https://awards.acm.org{name_element[\"href\"]}'\n",
    "                profiles.append((recipient_name, profile_url))\n",
    "            return profiles\n",
    "        return []\n",
    "    \n",
    "    # Function to scrape ACM award recipients\n",
    "    def scrape_acm_award_recipients():\n",
    "        url = \"https://awards.acm.org/award-recipients\"\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        response = retry_request(url, headers)\n",
    "        \n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(\"No table found on the page.\")\n",
    "            return None\n",
    "        \n",
    "        rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "        recipient_urls = []\n",
    "        recepient_data = []\n",
    "        selected_range = 3  # Limit to first 3 recipients for testing\n",
    "        for row in rows[:selected_range]:\n",
    "            cols = row.find_all('td')\n",
    "            name = cols[0].text.strip()\n",
    "            award = cols[1].text.strip()\n",
    "            year = cols[2].text.strip()\n",
    "            region = cols[3].text.strip()\n",
    "            dl_link = cols[4].find('a')['href'] if cols[4].find('a') else None\n",
    "\n",
    "            # save the corresponding details \n",
    "            recipient_profile = {\n",
    "                'Name': name,\n",
    "                'Award': award,\n",
    "                'Year': year,\n",
    "                'Region': region,\n",
    "                'DL_Link': dl_link\n",
    "            }\n",
    "            \n",
    "            recepient_data.append(recipient_profile)\n",
    "\n",
    "            if dl_link:\n",
    "                dl_url = f\"https://dl.acm.org{dl_link}\" if dl_link.startswith('/') else dl_link\n",
    "                recipient_urls.append(dl_url)\n",
    "\n",
    "        return recipient_urls, recepient_data\n",
    "    \n",
    "    def extract_tags(self, _url):\n",
    "        url = _url\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        response = retry_request(url, headers)\n",
    "        \n",
    "        html_content = response.content\n",
    "\n",
    "        # Parsing the HTML content with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all div elements with class 'tag-cloud'\n",
    "        tag_cloud_divs = soup.find_all('div', class_='tag-cloud')\n",
    "        h4_to_keywords_map = {}\n",
    "\n",
    "        bar_count = soup.find_all('svg', class_='d3-bar-chart')  \n",
    "\n",
    "        data_chart_data = []\n",
    "\n",
    "        if bar_count.has_attr(\"data-chart-data\"):\n",
    "            data_chart_data = bar_count['data-chart-data']\n",
    "            data_chart_data = data_chart_data.replace('&quot;', '\"')\n",
    "            try:\n",
    "                chart_data = json.loads(data_chart_data)\n",
    "                for data in chart_data:\n",
    "                    data_chart_data.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse JSON: {e}\")\n",
    "\n",
    "        data_contrib = soup.find_all('div', class_=\"contrib-metrics__multi-items ajax-done\")\n",
    "        if data_contrib.has_attr(\"data-component\"):\n",
    "            data_component = data_contrib['data-component']\n",
    "            data_component = data_component.replace('&quot;', '\"')\n",
    "            try:\n",
    "                component_data = json.loads(data_component)\n",
    "                for data in component_data:\n",
    "                    data_contrib.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to parse JSON: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "        # Iterate over each 'tag-cloud' div\n",
    "        for tag_cloud_div in tag_cloud_divs:\n",
    "            # Find the grandparent of the 'tag-cloud' div and extract h4 text\n",
    "            parent_of_parent = tag_cloud_div.find_parent().find_parent()\n",
    "            h4_text = parent_of_parent.find('h4').text.strip() if parent_of_parent.find('h4') else \"No h4 found\"\n",
    "\n",
    "            # Extract the JSON-like data from the 'data-tags' attribute\n",
    "            keywords = []\n",
    "            if tag_cloud_div and tag_cloud_div.has_attr('data-tags'):\n",
    "                data_tags = tag_cloud_div['data-tags']\n",
    "                data_tags = data_tags.replace('&quot;', '\"')  # Convert HTML entities to normal characters\n",
    "\n",
    "                # Parse the JSON string into Python objects\n",
    "                try:\n",
    "                    tags_data = json.loads(data_tags)\n",
    "\n",
    "                    # Extract relevant information (term, label, count)\n",
    "                    for tag in tags_data:\n",
    "                        keyword_info = {\n",
    "                            'label': tag.get('label'),\n",
    "                            'count': tag.get('count'),\n",
    "                        }\n",
    "                        keywords.append(keyword_info)\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to parse JSON: {e}\")\n",
    "            else:\n",
    "                print(\"No tag cloud data found.\")\n",
    "\n",
    "            # Map the h4 text to the list of keywords\n",
    "            h4_to_keywords_map[h4_text] = keywords\n",
    "\n",
    "        # Return the complete dictionary mapping h4 text to keywords\n",
    "        return h4_to_keywords_map\n",
    "        \n",
    "\n",
    "class GoogleScholarScraper:\n",
    "    \"\"\"Handles scraping Google Scholar details based on a profile name using ScraperAPI.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.pg = ProxyGenerator()\n",
    "        self.pg.ScraperAPI(API_KEY)\n",
    "        scholarly.use_proxy(self.pg)\n",
    "    # Google Scholar scraping function using the scholarly library\n",
    "    def get_google_scholar_url(name):\n",
    "        author = scholarly.search_author(name)\n",
    "        res = {}\n",
    "\n",
    "        try:\n",
    "            author = next(author)\n",
    "        except StopIteration:\n",
    "            return res\n",
    "        \n",
    "        for comp in components:\n",
    "            if comp in author:\n",
    "                res[comp] = author[comp]\n",
    "        return res\n",
    "\n",
    "    def get_scholar_profile(self, full_name):\n",
    "        \"\"\"Uses `get_google_scholar_url` to extract Google Scholar details.\"\"\"\n",
    "        gsc_data = get_google_scholar_url(full_name)\n",
    "        if not gsc_data and len(full_name.split()) >= 3:\n",
    "            first_last_name = f'{full_name.split()[0]} {full_name.split()[-1]}'\n",
    "            gsc_data = get_google_scholar_url(first_last_name)\n",
    "        if gsc_data:\n",
    "            return {\n",
    "                'gsc_url': f'https://scholar.google.com/citations?user={gsc_data[\"scholar_id\"]}',\n",
    "                'affiliation': gsc_data.get(\"affiliation\", \"\"),\n",
    "                'interests': \", \".join(gsc_data.get('interests', []))\n",
    "            }\n",
    "        return None\n",
    "\n",
    "class DataSaver:\n",
    "    def __init__(self, filename, checkpoint_file):\n",
    "        self.filename = filename\n",
    "        self.checkpoint_file = checkpoint_file\n",
    "\n",
    "    def save_checkpoint(self, iteration):\n",
    "        with open(self.checkpoint_file, 'w') as f:\n",
    "            f.write(f'Last completed iteration: {iteration}')\n",
    "\n",
    "    def read_checkpoint(self):\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            with open(self.checkpoint_file, 'r') as f:\n",
    "                return int(f.readline().split(':')[-1])\n",
    "        return 0\n",
    "\n",
    "    def save_to_csv(self, data, append=False):\n",
    "        file_exists = os.path.isfile(self.filename)\n",
    "        mode = 'a' if append else 'w'\n",
    "        with open(self.filename, mode, newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            if not file_exists:\n",
    "                writer.writerow(['Index', 'Full Name', 'Year', 'Location', 'Citation', 'Profile URL', 'GSC URL', 'Affiliation', 'Interests'])\n",
    "            writer.writerow(data)\n",
    "\n",
    "# Main scraping execution\n",
    "def main():\n",
    "    base_url = 'https://awards.acm.org/turing/award-recipients'\n",
    "    session_handler = WebSessionHandler()\n",
    "    acm_scraper = ACMProfileScraper(session_handler)\n",
    "    google_scholar_scraper = GoogleScholarScraper()\n",
    "    data_saver = DataSaver('acm_turings3.csv', 'last_iteration.txt')\n",
    "\n",
    "    # Read the checkpoint to continue from where it left off\n",
    "    last_iteration = data_saver.read_checkpoint()\n",
    "\n",
    "    # Scrape the ACM award recipients page (limited for testing)\n",
    "    profiles = acm_scraper.scrape_acm_award_page(base_url, limit=2)  # Limit to 5 for testing\n",
    "    if not profiles:\n",
    "        print(\"No profiles found. Check if the page structure has changed.\")\n",
    "        logging.error(\"No profiles found on the ACM award page.\")\n",
    "        return\n",
    "    profiles = profiles[last_iteration:]  # Continue from last checkpoint\n",
    "\n",
    "    for idx, (recipient_name, profile_url) in enumerate(profiles, start=last_iteration):\n",
    "        try:\n",
    "            print(f\"Processing {recipient_name} ({idx + 1}/{len(profiles)})\")\n",
    "\n",
    "            # Scrape ACM profile data\n",
    "            acm_data = acm_scraper.extract_acm_profile(profile_url)\n",
    "            if acm_data:\n",
    "                full_name, year, location, citation = acm_data\n",
    "\n",
    "                # Scrape Google Scholar profile data\n",
    "                gsc_data = google_scholar_scraper.get_scholar_profile(full_name)\n",
    "                gsc_url, affiliation, interests = (gsc_data['gsc_url'], gsc_data['affiliation'], gsc_data['interests']) if gsc_data else ('N/A', 'N/A', 'N/A')\n",
    "\n",
    "                # Save to CSV\n",
    "                row_data = [idx + 1, full_name, year, location, citation, profile_url, gsc_url, affiliation, interests]\n",
    "                data_saver.save_to_csv(row_data, append=True)\n",
    "\n",
    "            # Save the checkpoint every 10 profiles\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                data_saver.save_checkpoint(idx + 1)\n",
    "                print(f\"Checkpoint saved at iteration {idx + 1}\")\n",
    "\n",
    "            # Pause to avoid triggering rate limits\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {recipient_name}: {e}\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve profile page for https://dl.acm.org/profile/81100093619\n",
      "Profiles saved to author_profiles.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# author + publication + co authors \n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "def get_random_user_agent():\n",
    "    return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "\n",
    "def retry_request(url, headers):\n",
    "    return requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "def extract_publications(dl_url):\n",
    "    try:\n",
    "        page_number = 1\n",
    "        publications = []\n",
    "        while True:\n",
    "            paginated_url = f\"{dl_url}&startPage={page_number}\"\n",
    "            headers = {'User-Agent': get_random_user_agent()}\n",
    "            response = retry_request(paginated_url, headers)\n",
    "            if response.status_code != 200:\n",
    "                logging.error(f\"Failed to retrieve page {page_number} for {dl_url}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extract publication data\n",
    "            pub_list_section = soup.find_all('li', class_='search__item issue-item-container')\n",
    "            if not pub_list_section:  # Check if the publication section is None or empty\n",
    "                logging.error(f\"No publication list found on page {page_number}\")\n",
    "                break  # Exit if there are no more publications\n",
    "\n",
    "            for pub in pub_list_section:\n",
    "                pub_data = {}\n",
    "\n",
    "                # Extract title and link\n",
    "                title_tag = pub.find('h5', class_='issue-item__title').find('a')\n",
    "                if title_tag:\n",
    "                    pub_data['Title'] = title_tag.text.strip()\n",
    "                    pub_data['Title_URL'] = title_tag['href'].strip()\n",
    "\n",
    "                # Extract contributors/authors\n",
    "                # Locate the authors list using the <ul> tag with class 'loa'\n",
    "                author_list_section = pub.find('ul', class_='loa')\n",
    "                authors = []\n",
    "\n",
    "                # Ensure that we have found the authors list before iterating\n",
    "                if author_list_section:\n",
    "                    # Loop over the <li> tags within the list to extract visible authors\n",
    "                    for author_item in author_list_section.find_all('li'):\n",
    "                        # Find the <span> tag inside the <li> to extract the author's name\n",
    "                        author_link_tag = author_item.find('a')\n",
    "                        if author_link_tag and author_link_tag.text.strip():\n",
    "                            author_name = author_link_tag.text.strip()\n",
    "                            author_profile_url = author_link_tag['href'].strip()\n",
    "                            authors.append({\n",
    "                                'Name': author_name,\n",
    "                                'Profile_URL': f\"https://dl.acm.org{author_profile_url}\"  # Ensure full URL\n",
    "                            })\n",
    "\n",
    "                # Check if there is a button for more authors (collapsed)\n",
    "                collapsed_authors_button = pub.find('button', class_='removed-items-count')\n",
    "                if collapsed_authors_button:\n",
    "                    logging.warning(f\"Additional authors may be hidden behind a collapsed view for publication: {pub_data.get('Title')}\")\n",
    "\n",
    "                # If no authors are found, fallback to 'No authors listed'\n",
    "                pub_data['Authors'] = authors if authors else \"No authors listed\"\n",
    "\n",
    "                # Extract journal, article number, and pages\n",
    "                details_tag = pub.find('div', class_='issue-item__detail')\n",
    "                if details_tag:\n",
    "                    journal_info = details_tag.find('span', class_='epub-section__title')\n",
    "                    article_info = details_tag.find_all('span', class_='dot-separator') if details_tag else []\n",
    "                    pub_data['Journal_Info'] = journal_info.text.strip() if journal_info else \"No journal info available\"\n",
    "                    if len(article_info) > 0:\n",
    "                        pub_data['Article_No'] = article_info[0].text.strip()\n",
    "                    if len(article_info) > 1:\n",
    "                        pub_data['Pages'] = article_info[1].text.strip()\n",
    "\n",
    "                # Extract DOI link\n",
    "                doi_tag = details_tag.find('a', class_='issue-item__doi') if details_tag else None\n",
    "                if doi_tag:\n",
    "                    doi_link = doi_tag['href'].strip()\n",
    "                    if not doi_link.startswith('https://'):\n",
    "                        doi_link = f\"https://{doi_link}\"\n",
    "                    pub_data['DOI'] = doi_link\n",
    "                else:\n",
    "                    pub_data['DOI'] = \"No DOI available\"\n",
    "\n",
    "                # Extract abstract\n",
    "                abstract_tag = pub.find('div', class_='issue-item__abstract')\n",
    "                if abstract_tag:\n",
    "                    pub_data['Abstract'] = abstract_tag.find('p').text.strip()\n",
    "\n",
    "                # Extract citation and download metrics\n",
    "                metrics_tag = pub.find('div', class_='issue-item__footer')\n",
    "                if metrics_tag:\n",
    "                    citations = metrics_tag.find('span', class_='citation')\n",
    "                    downloads = metrics_tag.find('span', class_='metric')\n",
    "                    pub_data['Citations'] = citations.find('span').text.strip() if citations else \"0\"\n",
    "                    pub_data['Downloads'] = downloads.find('span').text.strip() if downloads else \"0\"\n",
    "\n",
    "                publications.append(pub_data)\n",
    "\n",
    "            page_number += 1  # Move to the next page\n",
    "\n",
    "        return publications\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching publications for {dl_url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Example usage\n",
    "publications_url = 'https://dl.acm.org/profile/81100093619/publications?Role=author'\n",
    "publications = extract_publications(publications_url)\n",
    "print(json.dumps(publications, indent=2))\n",
    "\n",
    "\n",
    "def extract_author_profile(dl_url):\n",
    "    try:\n",
    "        headers = {'User-Agent': get_random_user_agent()}\n",
    "        response = retry_request(dl_url, headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve profile page for {dl_url}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        author_data = {}\n",
    "\n",
    "        # Extracting Author's Name\n",
    "        author_name_tag = soup.find('h1', class_='title')\n",
    "        if author_name_tag:\n",
    "            author_data['Name'] = author_name_tag.text.strip()\n",
    "\n",
    "        # Extracting Bibliometrics\n",
    "        bibliometrics = {}\n",
    "        bibliometrics_section = soup.find('div', class_='bibliometrics equal-height-slides')\n",
    "        if bibliometrics_section:\n",
    "            metrics = bibliometrics_section.find_all('div', class_='slide-item')\n",
    "            for metric in metrics:\n",
    "                title = metric.find('div', class_='bibliometrics__title').text.strip()\n",
    "                value = metric.find('div', class_='bibliometrics__count').text.strip()\n",
    "                bibliometrics[title] = value\n",
    "        author_data['Bibliometrics'] = bibliometrics\n",
    "        \n",
    "        # Extracting Author's Image URL\n",
    "        image_tag = soup.find('img', alt=True, class_='image-lazy-loaded')\n",
    "        if image_tag:\n",
    "            author_data['Image_URL'] = image_tag['src']\n",
    "        \n",
    "        # Extracting Co-Authors and Affiliations\n",
    "        co_authors = []\n",
    "        co_author_section = soup.find_all('div', class_='colored-block shadow contrib-metrics__multi-items')\n",
    "        for co_author in co_author_section:\n",
    "            co_author_data = {}\n",
    "            title_tag = co_author.find('h5')\n",
    "            if title_tag and title_tag.text.strip() in ['Most frequent co-Author', 'Most cited colleague', 'Most frequent Affiliation']:\n",
    "                co_author_data['Title'] = title_tag.text.strip()\n",
    "\n",
    "                author_tag = co_author.find('div', class_='box-item')\n",
    "                if author_tag:\n",
    "                    co_author_data['Details'] = author_tag.get_text(separator=' ').strip()\n",
    "\n",
    "                co_authors.append(co_author_data)\n",
    "\n",
    "        author_data['Co_Authors'] = co_authors\n",
    "\n",
    "        # Extracting Keywords\n",
    "        keywords = []\n",
    "        tag_cloud_div = soup.find('div', class_='tag-cloud')\n",
    "        if tag_cloud_div and tag_cloud_div.has_attr('data-tags'):\n",
    "            data_tags = tag_cloud_div['data-tags'].replace('&quot;', '\"')\n",
    "            try:\n",
    "                tags_data = json.loads(data_tags)\n",
    "                for tag in tags_data:\n",
    "                    keyword_info = {\n",
    "                        'term': tag.get('term'),\n",
    "                        'label': tag.get('label'),\n",
    "                        'count': tag.get('count'),\n",
    "                        'link': tag.get('link')\n",
    "                    }\n",
    "                    keywords.append(keyword_info)\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"Failed to parse JSON: {e}\")\n",
    "        author_data['Keywords'] = keywords\n",
    "\n",
    "        # Extracting Bar Chart Data\n",
    "        bar_count = soup.find_all('svg', class_='d3-bar-chart')\n",
    "        data_chart_data = []\n",
    "        for bar_chart in bar_count:\n",
    "            if bar_chart.has_attr(\"data-chart-data\"):\n",
    "                chart_data = bar_chart['data-chart-data'].replace('&quot;', '\"')\n",
    "                try:\n",
    "                    chart_data = json.loads(chart_data)\n",
    "                    data_chart_data.extend(chart_data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.error(f\"Failed to parse bar chart data: {e}\")\n",
    "        author_data['Bar_Chart_Data'] = data_chart_data\n",
    "\n",
    "        # Extracting Contribution Metrics\n",
    "        data_contrib = soup.find_all('div', class_=\"contrib-metrics__multi-items ajax-done\")\n",
    "        contrib_data = []\n",
    "        for contrib in data_contrib:\n",
    "            if contrib.has_attr(\"data-component\"):\n",
    "                component_data = contrib['data-component'].replace('&quot;', '\"')\n",
    "                try:\n",
    "                    component_data = json.loads(component_data)\n",
    "                    contrib_data.extend(component_data)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    logging.error(f\"Failed to parse contribution metrics data: {e}\")\n",
    "        author_data['Contribution_Metrics'] = contrib_data\n",
    "\n",
    "        # Extracting Publications\n",
    "        publications_url = f\"{dl_url}/publications?Role=author\"\n",
    "        author_data['Publications'] = extract_publications(publications_url)\n",
    "\n",
    "        return author_data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching profile for {dl_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "dl_url = 'https://dl.acm.org/profile/81100093619'\n",
    "author_profile = extract_author_profile(dl_url)\n",
    "print(json.dumps(author_profile, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
