title,authors,year,abstract,ss_id
Evaluating Embedding APIs for Information Retrieval,"[{'name': 'Ehsan Kamalloo', 'dblp_profile': 'https://dblp.org/pid/61/7404.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'David Alfonso-Hermelo', 'dblp_profile': 'https://dblp.org/pid/241/1750.html'}, {'name': 'Mehdi Rezagholizadeh', 'dblp_profile': 'https://dblp.org/pid/134/0625.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best albeit at a higher cost. We hope our work lays the groundwork for thoroughly evaluating APIs that are critical in search and more broadly, in information retrieval.",3a14af18549daee147c6c747592be314418eeac3
GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration,"[{'name': 'Aleksandra Piktus', 'dblp_profile': 'https://dblp.org/pid/241/7090.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Christopher Akiki', 'dblp_profile': 'https://dblp.org/pid/277/0703.html'}, {'name': 'Akintunde Oladipo', 'dblp_profile': 'https://dblp.org/pid/341/4148.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Hailey Schoelkopf', 'dblp_profile': 'https://dblp.org/pid/323/1550.html'}, {'name': 'Stella Biderman', 'dblp_profile': 'https://dblp.org/pid/239/5641.html'}, {'name': 'Martin Potthast', 'dblp_profile': 'https://dblp.org/pid/87/6573.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We leverage the existing functionalities of both platforms while proposing novel features further facilitating their integration. Our goal is to give NLP researchers tools that will allow them to develop retrieval-based instrumentation for their data analytics needs with ease and agility.We include a Jupyter Notebook-based walk through the core interoperability features, available on GitHub: https://github.com/huggingface/gaia.We then demonstrate how the ideas we present can be operationalized to create a powerful tool for qualitative data analysis in NLP. We present GAIA Search - a search engine built following previously laid out principles, giving access to four popular large-scale text collections. GAIA serves a dual purpose of illustrating the potential of methodologies we discuss but also as a standalone qualitative analysis tool that can be leveraged by NLP researchers aiming to understand datasets prior to using them in training. GAIA is hosted live on Hugging Face Spaces: https://huggingface.co/spaces/spacerini/gaia.",751563cf0c32fe4dfa43d3416c916f8eb053e5f3
Precise Zero-Shot Dense Retrieval without Relevance Labels,"[{'name': 'Luyu Gao', 'dblp_profile': 'https://dblp.org/pid/213/8857.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jamie Callan', 'dblp_profile': 'https://dblp.org/pid/c/JamesPCallan.html'}]",2023,"While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn).",5c32c653735b43a0a8923ca65ac191bd4bf15311
Operator Selection and Ordering in a Pipeline Approach to Efficiency Optimizations for Transformers,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,,aed940439307c975a0e3eca18c40c6f18c4903f6
What the DAAM: Interpreting Stable Diffusion Using Cross Attention,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Linqing Liu', 'dblp_profile': 'https://dblp.org/pid/36/7028.html'}, {'name': 'Akshat Pandey', 'dblp_profile': 'https://dblp.org/pid/263/3240.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Gefei Yang', 'dblp_profile': 'https://dblp.org/pid/229/5810.html'}, {'name': 'Karun Kumar', 'dblp_profile': 'https://dblp.org/pid/305/6007.html'}, {'name': 'Pontus Stenetorp', 'dblp_profile': 'https://dblp.org/pid/44/8358.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ferhan Ture', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}]",2023,"Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8-64.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4-4.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head–dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at https://github.com/castorini/daam.",37d626720f5003373336098ce7c01a1a38e6b63d
"""Low-Resource"" Text Classification: A Parameter-Free Classification Method with Compressors","[{'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Matthew Y. R. Yang', 'dblp_profile': 'https://dblp.org/pid/322/0271.html'}, {'name': 'Mikhail Tsirlin', 'dblp_profile': 'https://dblp.org/pid/336/6084.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Yiqin Dai', 'dblp_profile': 'https://dblp.org/pid/225/0795.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,,147af99d852c516e16d90b128504a43c82ceffb8
CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Barlas Oguz', 'dblp_profile': 'https://dblp.org/pid/69/9892.html'}, {'name': 'Asish Ghoshal', 'dblp_profile': 'https://dblp.org/pid/35/10890.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Yashar Mehdad', 'dblp_profile': 'https://dblp.org/pid/28/7560.html'}, {'name': 'Wen-tau Yih', 'dblp_profile': 'https://dblp.org/pid/07/7129.html'}, {'name': 'Xilun Chen', 'dblp_profile': 'https://dblp.org/pid/96/10207-2.html'}]",2023,"Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and dense (e.g. DPR) retrievers and have achieved state-of-the-art performance on various retrieval tasks.These methods, however, are orders of magnitude slower and need much more space to store their indices compared to their single-vector counterparts.In this paper, we unify different multi-vector retrieval models from a token routing viewpoint and propose conditional token interaction via dynamic lexical routing, namely CITADEL, for efficient and effective multi-vector retrieval.CITADEL learns to route different token vectors to the predicted lexical keys such that a query token vector only interacts with document token vectors routed to the same key.This design significantly reduces the computation cost while maintaining high accuracy.Notably, CITADEL achieves the same or slightly better performance than the previous state of the art, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR) evaluations, while being nearly 40 times faster. Source code and data are available at https://github.com/facebookresearch/dpr-scale/tree/citadel.",741a5536c2e58e1595e7396345f1e5d40a3aa775
Answer Retrieval for Math Questions Using Structural and Dense Retrieval,"[{'name': 'Wei Zhong', 'dblp_profile': 'https://dblp.org/pid/60/1416.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,,1296b4f809e11eea701d40552d1920536efa78fc
PyGaggle: A Gaggle of Resources for Open-Domain Question Answering,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Haonan Chen', 'dblp_profile': 'https://dblp.org/pid/121/7527.html'}, {'name': 'Lingwei Gu', 'dblp_profile': 'https://dblp.org/pid/342/6327.html'}, {'name': 'Manveer Singh Tamber', 'dblp_profile': 'https://dblp.org/pid/314/0617.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,,6a0c18aa1bf69798b15ef2355d5256ae8fa80186
Pre-processing Matters! Improved Wikipedia Corpora for Open-Domain Question Answering,"[{'name': 'Manveer Singh Tamber', 'dblp_profile': 'https://dblp.org/pid/314/0617.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,,532a24fbf508c40234e9126c7a3f80dd92e8545f
One Blade for One Purpose: Advancing Math Information Retrieval using Hybrid Search,"[{'name': 'Wei Zhong', 'dblp_profile': 'https://dblp.org/pid/60/1416.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Neural retrievers have been shown to be effective for math-aware search. Their ability to cope with math symbol mismatches, to represent highly contextualized semantics, and to learn effective representations are critical to improving math information retrieval. However, the most effective retriever for math remains impractical as it depends on token-level dense representations for each math token, which leads to prohibitive storage demands, especially considering that math content generally consumes more tokens. In this work, we try to alleviate this efficiency bottleneck while boosting math information retrieval effectiveness via hybrid search. To this end, we propose MABOWDOR, a Math-Aware Bestof-Worlds Domain Optimized Retriever, which has an unsupervised structure search component, a dense retriever, and optionally a sparse retriever on top of a domain-adapted backbone learned by context-enhanced pretraining, each addressing a different need in retrieving heterogeneous data from math documents. Our hybrid search outperforms the previous state-of-the-art math IR system while eliminating efficiency bottlenecks. Our system is available at https://github.com/approach0/pya0.",72a9187b489992cad3d54420611d5039eb6b9d86
SLIM: Sparsified Late Interaction for Multi-Vector Retrieval with Inverted Indexes,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"This paper introduces Sparsified Late Interaction for Multi-vector (SLIM) retrieval with inverted indexes. Multi-vector retrieval methods have demonstrated their effectiveness on various retrieval datasets, and among them, ColBERT is the most established method based on the late interaction of contextualized token embeddings of pre-trained language models. However, efficient ColBERT implementations require complex engineering and cannot take advantage of off-the-shelf search libraries, impeding their practical use. To address this issue, SLIM first maps each contextualized token vector to a sparse, high-dimensional lexical space before performing late interaction between these sparse token embeddings. We then introduce an efficient two-stage retrieval architecture that includes inverted index retrieval followed by a score refinement module to approximate the sparsified late interaction, which is fully compatible with off-the-shelf lexical search libraries such as Lucene. SLIM achieves competitive accuracy on MS MARCO Passages and BEIR compared to ColBERT while being much smaller and faster on CPUs. To our knowledge, we are the first to explore using sparse token representations for multi-vector retrieval. Source code and data are integrated into the Pyserini IR toolkit.",e1f513fe13b735b7829c5b7d27040512f189efcb
MMEAD: MS MARCO Entity Annotations and Disambiguations,"[{'name': 'Chris Kamphuis', 'dblp_profile': 'https://dblp.org/pid/156/3395.html'}, {'name': 'Aileen Lin', 'dblp_profile': 'https://dblp.org/pid/236/4217.html'}, {'name': 'Siwen Yang', 'dblp_profile': 'https://dblp.org/pid/208/9759.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}, {'name': 'Faegheh Hasibi', 'dblp_profile': 'https://dblp.org/pid/118/9049.html'}]",2023,"MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for entity links for the MS MARCO datasets. We specify a format to store and share links for both document and passage collections of MS MARCO. Following this specification, we release entity links to Wikipedia for documents and passages in both MS MARCO collections (v1 and v2). Entity links have been produced by the REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing users to load the link data and entity embeddings effortlessly. Using MMEAD takes only a few lines of code. Finally, we show how MMEAD can be used for IR research that uses entity information. We show how to improve recall@1000 and MRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this resource. We also demonstrate how entity expansions can be used for interactive search applications.",4839344495444c25ee3da374e9462042157d62ca
SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval,"[{'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Kexin Wang', 'dblp_profile': 'https://dblp.org/pid/44/8799.html'}, {'name': 'Iryna Gurevych', 'dblp_profile': 'https://dblp.org/pid/85/6201.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here: https://github.com/thakur-nandan/sprint.",19a6bc42e14d84453b8647ebb98e453c1449762f
AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation,"[{'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Carlos Lassance', 'dblp_profile': 'https://dblp.org/pid/177/5760.html'}, {'name': 'Rafael Sampaio de Rezende', 'dblp_profile': 'https://dblp.org/pid/243/3241.html'}, {'name': 'Krishna Srinivasan', 'dblp_profile': 'https://dblp.org/pid/50/145.html'}, {'name': 'Miriam Redi', 'dblp_profile': 'https://dblp.org/pid/85/9997.html'}, {'name': 'Stéphane Clinchant', 'dblp_profile': 'https://dblp.org/pid/97/2910.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"This paper presents the AToMiC (Authoring Tools for Multi media Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision--language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image--text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image--document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research. Finally, our dataset provides the basis for a dedicated track at the 2023 Text Retrieval Conference (TREC), and is publicly available at https://github.com/TREC-AToMiC/AToMiC.",17abec01047a09c1c57a8150957e1980e618d043
Tevatron: An Efficient and Flexible Toolkit for Neural Retrieval,"[{'name': 'Luyu Gao', 'dblp_profile': 'https://dblp.org/pid/213/8857.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jamie Callan', 'dblp_profile': 'https://dblp.org/pid/c/JamesPCallan.html'}]",2023,"Recent rapid advances in deep pre-trained language models and the introduction of large datasets have powered research in embedding-based neural retrieval. While many excellent research papers have emerged, most of them come with their own implementations, which are typically optimized for some particular research goals instead of efficiency or code organization. In this paper, we introduce Tevatron, a neural retrieval toolkit that is optimized for efficiency, flexibility, and code simplicity. Tevatron enables model training and evaluation for a variety of ranking components such as dense retrievers, sparse retrievers, and rerankers. It also provides a standardized pipeline that includes text processing, model training, corpus/query encoding, and search. In addition, Tevatron incorporates well-studied methods for improving retriever effectiveness such as hard negative mining and knowledge distillation. We provide an overview of Tevatron in this paper, demonstrating its effectiveness and efficiency on multiple IR and QA datasets. We highlight Tevatron's flexible design, which enables easy generalization across datasets, model architectures, and accelerator platforms (GPUs and TPUs). Overall, we believe that Tevatron can serve as a solid software foundation for research on neural retrieval systems, including their design, modeling, and optimization.",e3e1b523c4af48061b17771a5c7ffa4aced03eef
Which Model Shall I Choose? Cost/Quality Trade-offs for Text Classification Tasks,"[{'name': 'Shi Zong', 'dblp_profile': 'https://dblp.org/pid/177/9106.html'}, {'name': 'Josh Seltzer', 'dblp_profile': 'https://dblp.org/pid/323/9652.html'}, {'name': 'Jiahua Pan', 'dblp_profile': 'https://dblp.org/pid/277/1923.html'}, {'name': 'Kathy Cheng', 'dblp_profile': 'https://dblp.org/pid/323/9661.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Industry practitioners always face the problem of choosing the appropriate model for deployment under different considerations, such as to maximize a metric that is crucial for production, or to reduce the total cost given financial concerns. In this work, we focus on the text classification task and present a quantitative analysis for this challenge. Using classification accuracy as the main metric, we evaluate the classifiers' performances for a variety of models, including large language models, along with their associated costs, including the annotation cost, training (fine-tuning) cost, and inference cost. We then discuss the model choices for situations like having a large number of samples needed for inference. We hope our work will help people better understand the cost/quality trade-offs for the text classification task.",e6f9e70182d2b10a9b97541091ecc73c37c6028a
SLIM: Sparsified Late Interaction for Multi-Vector Retrieval with Inverted Indexes,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"This paper introduces Sparsified Late Interaction for Multi-vector (SLIM) retrieval with inverted indexes. Multi-vector retrieval methods have demonstrated their effectiveness on various retrieval datasets, and among them, ColBERT is the most established method based on the late interaction of contextualized token embeddings of pre-trained language models. However, efficient ColBERT implementations require complex engineering and cannot take advantage of off-the-shelf search libraries, impeding their practical use. To address this issue, SLIM first maps each contextualized token vector to a sparse, high-dimensional lexical space before performing late interaction between these sparse token embeddings. We then introduce an efficient two-stage retrieval architecture that includes inverted index retrieval followed by a score refinement module to approximate the sparsified late interaction, which is fully compatible with off-the-shelf lexical search libraries such as Lucene. SLIM achieves competitive accuracy on MS MARCO Passages and BEIR compared to ColBERT while being much smaller and faster on CPUs. To our knowledge, we are the first to explore using sparse token representations for multi-vector retrieval. Source code and data are integrated into the Pyserini IR toolkit.",e1f513fe13b735b7829c5b7d27040512f189efcb
Improving Out-of-Distribution Generalization of Neural Rerankers with Contextualized Late Interaction,"[{'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Recent progress in information retrieval finds that embedding query and document representation into multi-vector yields a robust bi-encoder retriever on out-of-distribution datasets. In this paper, we explore whether late interaction, the simplest form of multi-vector, is also helpful to neural rerankers that only use the [CLS] vector to compute the similarity score. Although intuitively, the attention mechanism of rerankers at the previous layers already gathers the token-level information, we find adding late interaction still brings an extra 5% improvement in average on out-of-distribution datasets, with little increase in latency and no degradation in in-domain effectiveness. Through extensive experiments and analysis, we show that the finding is consistent across different model sizes and first-stage retrievers of diverse natures and that the improvement is more prominent on longer queries.",d68753acfbd0a3d007842884d56a776b45982e6e
How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Akari Asai', 'dblp_profile': 'https://dblp.org/pid/213/8066.html'}, {'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Barlas Oguz', 'dblp_profile': 'https://dblp.org/pid/69/9892.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Yashar Mehdad', 'dblp_profile': 'https://dblp.org/pid/28/7560.html'}, {'name': 'Wen-tau Yih', 'dblp_profile': 'https://dblp.org/pid/07/7129.html'}, {'name': 'Xilun Chen', 'dblp_profile': 'https://dblp.org/pid/96/10207-2.html'}]",2023,"Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our dense retriever trained with diverse augmentation, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++).",a4867148c2f692efc6c22c3935a59be2d04ea3e9
Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face,"[{'name': 'Christopher Akiki', 'dblp_profile': 'https://dblp.org/pid/277/0703.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Aleksandra Piktus', 'dblp_profile': 'https://dblp.org/pid/241/7090.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Akintunde Oladipo', 'dblp_profile': 'https://dblp.org/pid/341/4148.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Martin Potthast', 'dblp_profile': 'https://dblp.org/pid/87/6573.html'}]",2023,"We present Spacerini, a modular framework for seamless building and deployment of interactive search applications, designed to facilitate the qualitative analysis of large scale research datasets. Spacerini integrates features from both the Pyserini toolkit and the Hugging Face ecosystem to ease the indexing text collections and deploy them as search engines for ad-hoc exploration and to make the retrieval of relevant data points quick and efficient. The user-friendly interface enables searching through massive datasets in a no-code fashion, making Spacerini broadly accessible to anyone looking to qualitatively audit their text collections. This is useful both to IR~researchers aiming to demonstrate the capabilities of their indexes in a simple and interactive way, and to NLP~researchers looking to better understand and audit the failure modes of large language models. The framework is open source and available on GitHub: https://github.com/castorini/hf-spacerini, and includes utilities to load, pre-process, index, and deploy local and web search applications. A portfolio of applications created with Spacerini for a multitude of use cases can be found by visiting https://hf.co/spacerini.",c198c1193f924953b649002413979e7733c93a48
Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'David Alfonso-Hermelo', 'dblp_profile': 'https://dblp.org/pid/241/1750.html'}, {'name': 'Vitor Jeronymo', 'dblp_profile': 'https://dblp.org/pid/321/4286.html'}, {'name': 'Ehsan Kamalloo', 'dblp_profile': 'https://dblp.org/pid/61/7404.html'}, {'name': 'Carlos Lassance', 'dblp_profile': 'https://dblp.org/pid/177/5760.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Mehdi Rezagholizadeh', 'dblp_profile': 'https://dblp.org/pid/134/0625.html'}, {'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}]",2023,"The advent of multilingual language models has generated a resurgence of interest in cross-lingual information retrieval (CLIR), which is the task of searching documents in one language with queries from another. However, the rapid pace of progress has led to a confusing panoply of methods and reproducibility has lagged behind the state of the art. In this context, our work makes two important contributions: First, we provide a conceptual framework for organizing different approaches to cross-lingual retrieval using multi-stage architectures for mono-lingual retrieval as a scaffold. Second, we implement simple yet effective reproducible baselines in the Anserini and Pyserini IR toolkits for test collections from the TREC 2022 NeuCLIR Track, in Persian, Russian, and Chinese. Our efforts are built on a collaboration of the two teams that submitted the most effective runs to the TREC evaluation. These contributions provide a firm foundation for future advances.",610bfeb562ce11cd3a6e7588427daf489e3b4e94
AToMiC: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation,"[{'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Carlos Lassance', 'dblp_profile': 'https://dblp.org/pid/177/5760.html'}, {'name': 'Rafael Sampaio de Rezende', 'dblp_profile': 'https://dblp.org/pid/243/3241.html'}, {'name': 'Krishna Srinivasan', 'dblp_profile': 'https://dblp.org/pid/50/145.html'}, {'name': 'Miriam Redi', 'dblp_profile': 'https://dblp.org/pid/85/9997.html'}, {'name': 'Stéphane Clinchant', 'dblp_profile': 'https://dblp.org/pid/97/2910.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"This paper presents the AToMiC (Authoring Tools for Multi media Content) dataset, designed to advance research in image/text cross-modal retrieval. While vision--language pretrained transformers have led to significant improvements in retrieval effectiveness, existing research has relied on image-caption datasets that feature only simplistic image--text relationships and underspecified user models of retrieval tasks. To address the gap between these oversimplified settings and real-world applications for multimedia content creation, we introduce a new approach for building retrieval test collections. We leverage hierarchical structures and diverse domains of texts, styles, and types of images, as well as large-scale image--document associations embedded in Wikipedia. We formulate two tasks based on a realistic user model and validate our dataset through retrieval experiments using baseline models. AToMiC offers a testbed for scalable, diverse, and reproducible multimedia retrieval research. Finally, our dataset provides the basis for a dedicated track at the 2023 Text Retrieval Conference (TREC), and is publicly available at https://github.com/TREC-AToMiC/AToMiC.",17abec01047a09c1c57a8150957e1980e618d043
Anserini Gets Dense Retrieval: Integration of Lucene's HNSW Indexes,"[{'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Tommaso Teofili', 'dblp_profile': 'https://dblp.org/pid/201/6374.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Anserini is a Lucene-based toolkit for reproducible information retrieval research in Java that has been gaining traction in the community. It provides retrieval capabilities for both""traditional""bag-of-words retrieval models such as BM25 as well as retrieval using learned sparse representations such as SPLADE. With Pyserini, which provides a Python interface to Anserini, users gain access to both sparse and dense retrieval models, as Pyserini implements bindings to the Faiss vector search library alongside Lucene inverted indexes in a uniform, consistent interface. Nevertheless, hybrid fusion techniques that integrate sparse and dense retrieval models need to stitch together results from two completely different""software stacks"", which creates unnecessary complexities and inefficiencies. However, the introduction of HNSW indexes for dense vector search in Lucene promises the integration of both dense and sparse retrieval within a single software framework. We explore exactly this integration in the context of Anserini. Experiments on the MS MARCO passage and BEIR datasets show that our Anserini HNSW integration supports (reasonably) effective and (reasonably) efficient approximate nearest neighbor search for dense retrieval models, using only Lucene.",962052c6ef89d3ae317759df42bc762aab59dc30
Zero-Shot Listwise Document Reranking with a Large Language Model,"[{'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Supervised ranking methods based on bi-encoder or cross-encoder architectures have shown success in multi-stage text ranking tasks, but they require large amounts of relevance judgments as training data. In this work, we propose Listwise Reranker with a Large Language Model (LRL), which achieves strong reranking effectiveness without using any task-specific training data. Different from the existing pointwise ranking methods, where documents are scored independently and ranked according to the scores, LRL directly generates a reordered list of document identifiers given the candidate documents. Experiments on three TREC web search datasets demonstrate that LRL not only outperforms zero-shot pointwise methods when reranking first-stage retrieval results, but can also act as a final-stage reranker to improve the top-ranked results of a pointwise method for improved efficiency. Additionally, we apply our approach to subsets of MIRACL, a recent multilingual retrieval dataset, with results showing its potential to generalize across different languages.",8be0ec99f80710887e3a8e6bac5fba51a8fd7186
Evaluating Embedding APIs for Information Retrieval,"[{'name': 'Ehsan Kamalloo', 'dblp_profile': 'https://dblp.org/pid/61/7404.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'David Alfonso-Hermelo', 'dblp_profile': 'https://dblp.org/pid/241/1750.html'}, {'name': 'Mehdi Rezagholizadeh', 'dblp_profile': 'https://dblp.org/pid/134/0625.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best albeit at a higher cost. We hope our work lays the groundwork for thoroughly evaluating APIs that are critical in search and more broadly, in information retrieval.",3a14af18549daee147c6c747592be314418eeac3
SmartProbe: A Virtual Moderator for Market Research Surveys,"[{'name': 'Josh Seltzer', 'dblp_profile': 'https://dblp.org/pid/323/9652.html'}, {'name': 'Jiahua Pan', 'dblp_profile': 'https://dblp.org/pid/277/1923.html'}, {'name': 'Kathy Cheng', 'dblp_profile': 'https://dblp.org/pid/323/9661.html'}, {'name': 'Yuxiao Sun', 'dblp_profile': 'https://dblp.org/pid/339/8545.html'}, {'name': 'Santosh Kolagati', 'dblp_profile': 'https://dblp.org/pid/330/3367.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Shi Zong', 'dblp_profile': 'https://dblp.org/pid/177/9106.html'}]",2023,"Market research surveys are a powerful methodology for understanding consumer perspectives at scale, but are limited by depth of understanding and insights. A virtual moderator can introduce elements of qualitative research into surveys, developing a rapport with survey participants and dynamically asking probing questions, ultimately to elicit more useful information for market researchers. In this work, we introduce ${\tt SmartProbe}$, an API which leverages the adaptive capabilities of large language models (LLMs), and incorporates domain knowledge from market research, in order to generate effective probing questions in any market research survey. We outline the modular processing flow of $\tt SmartProbe$, and evaluate the quality and effectiveness of its generated probing questions. We believe our efforts will inspire industry practitioners to build real-world applications based on the latest advances in LLMs. Our demo is publicly available at https://nexxt.in/smartprobe-demo",cac71be7153c598921fef05a6eca494efac9cb11
How Does Generative Retrieval Scale to Millions of Passages?,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Kai Hui', 'dblp_profile': 'https://dblp.org/pid/37/10077.html'}, {'name': 'Jai Gupta', 'dblp_profile': 'https://dblp.org/pid/154/6787.html'}, {'name': 'Ádám Dániel Lelkes', 'dblp_profile': 'https://dblp.org/pid/147/5184.html'}, {'name': 'Honglei Zhuang', 'dblp_profile': 'https://dblp.org/pid/10/9988.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}, {'name': 'Vinh Q. Tran', 'dblp_profile': 'https://dblp.org/pid/77/2885-2.html'}]",2023,"Popularized by the Differentiable Search Index, the emerging paradigm of generative retrieval re-frames the classic information retrieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many different approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document corpora on the order of 100k in size. We conduct the first empirical study of generative retrieval techniques across various corpus scales, ultimately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several findings about scaling generative retrieval to millions of passages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modifications when accounting for compute cost, and the limits of naively scaling model parameters with respect to retrieval performance. While we find that generative retrieval is competitive with state-of-the-art dual encoders on small corpora, scaling to millions of passages remains an important and unsolved challenge. We believe these findings will be valuable for the community to clarify the current state of generative retrieval, highlight the unique challenges, and inspire new research directions.",20a7b1e274aff828466bba3760992aa54e14951a
Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain,"[{'name': 'Vanessa Liao', 'dblp_profile': 'https://dblp.org/pid/348/6893.html'}, {'name': 'Syed Shariyar Murtaza', 'dblp_profile': 'https://dblp.org/pid/10/4078.html'}, {'name': 'Yifan Nie', 'dblp_profile': 'https://dblp.org/pid/152/7965.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"A common way to use large pre-trained language models for downstream tasks is to fine tune them using additional layers. This may not work well if downstream domain is a specialized domain whereas the large language model has been pre-trained on a generic corpus. In this paper, we discuss the use of regular expression patterns employed as features for domain knowledge during the process of fine tuning, in addition to domain specific text. Our experiments on real scenario production data show that this method of fine tuning improves the downstream text classification tasks as compared to fine tuning only on domain specific text. We also show that the use of attention network for fine tuning improves results compared to simple linear layers.",a74c49d96ab32c9bf398a23c9b46f36bfeb6ec4b
GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration,"[{'name': 'Aleksandra Piktus', 'dblp_profile': 'https://dblp.org/pid/241/7090.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Christopher Akiki', 'dblp_profile': 'https://dblp.org/pid/277/0703.html'}, {'name': 'Akintunde Oladipo', 'dblp_profile': 'https://dblp.org/pid/341/4148.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Hailey Schoelkopf', 'dblp_profile': 'https://dblp.org/pid/323/1550.html'}, {'name': 'Stella Biderman', 'dblp_profile': 'https://dblp.org/pid/239/5641.html'}, {'name': 'Martin Potthast', 'dblp_profile': 'https://dblp.org/pid/87/6573.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We leverage the existing functionalities of both platforms while proposing novel features further facilitating their integration. Our goal is to give NLP researchers tools that will allow them to develop retrieval-based instrumentation for their data analytics needs with ease and agility.We include a Jupyter Notebook-based walk through the core interoperability features, available on GitHub: https://github.com/huggingface/gaia.We then demonstrate how the ideas we present can be operationalized to create a powerful tool for qualitative data analysis in NLP. We present GAIA Search - a search engine built following previously laid out principles, giving access to four popular large-scale text collections. GAIA serves a dual purpose of illustrating the potential of methodologies we discuss but also as a standalone qualitative analysis tool that can be leveraged by NLP researchers aiming to understand datasets prior to using them in training. GAIA is hosted live on Hugging Face Spaces: https://huggingface.co/spaces/spacerini/gaia.",751563cf0c32fe4dfa43d3416c916f8eb053e5f3
Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard,"[{'name': 'Ehsan Kamalloo', 'dblp_profile': 'https://dblp.org/pid/61/7404.html'}, {'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Carlos Lassance', 'dblp_profile': 'https://dblp.org/pid/177/5760.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"BEIR is a benchmark dataset for zero-shot evaluation of information retrieval models across 18 different domain/task combinations. In recent years, we have witnessed the growing popularity of a representation learning approach to building retrieval models, typically using pretrained transformers in a supervised setting. This naturally begs the question: How effective are these models when presented with queries and documents that differ from the training data? Examples include searching in different domains (e.g., medical or legal text) and with different types of queries (e.g., keywords vs. well-formed questions). While BEIR was designed to answer these questions, our work addresses two shortcomings that prevent the benchmark from achieving its full potential: First, the sophistication of modern neural methods and the complexity of current software infrastructure create barriers to entry for newcomers. To this end, we provide reproducible reference implementations that cover the two main classes of approaches: learned dense and sparse models. Second, there does not exist a single authoritative nexus for reporting the effectiveness of different models on BEIR, which has led to difficulty in comparing different methods. To remedy this, we present an official self-service BEIR leaderboard that provides fair and consistent comparisons of retrieval models. By addressing both shortcomings, our work facilitates future explorations in a range of interesting research questions that BEIR enables.",bda7b805cee90412e7043a333a809d0e78852eb3
SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval,"[{'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Kexin Wang', 'dblp_profile': 'https://dblp.org/pid/44/8799.html'}, {'name': 'Iryna Gurevych', 'dblp_profile': 'https://dblp.org/pid/85/6201.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"Traditionally, sparse retrieval systems relied on lexical representations to retrieve documents, such as BM25, dominated information retrieval tasks. With the onset of pre-trained transformer models such as BERT, neural sparse retrieval has led to a new paradigm within retrieval. Despite the success, there has been limited software supporting different sparse retrievers running in a unified, common environment. This hinders practitioners from fairly comparing different sparse models and obtaining realistic evaluation results. Another missing piece is, that a majority of prior work evaluates sparse retrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO. However, a key requirement in practical retrieval systems requires models that can generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. In this work, we provide SPRINT, a unified python toolkit based on Pyserini and Lucene, supporting a common interface for evaluating neural sparse retrieval. The toolkit currently includes five built-in models: uniCOIL, DeepImpact, SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models by defining their term weighting method. Using our toolkit, we establish strong and reproducible zero-shot sparse retrieval baselines across the well-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2 achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neural sparse retrievers. In this work, we further uncover the reasons behind its performance gain. We show that SPLADEv2 produces sparse representations with a majority of tokens outside of the original query and document which is often crucial for its performance gains, i.e. a limitation among its other sparse counterparts. We provide our SPRINT toolkit, models, and data used in our experiments publicly here: https://github.com/thakur-nandan/sprint.",19a6bc42e14d84453b8647ebb98e453c1449762f
HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution,"[{'name': 'Ehsan Kamalloo', 'dblp_profile': 'https://dblp.org/pid/61/7404.html'}, {'name': 'Aref Jafari', 'dblp_profile': 'https://dblp.org/pid/270/0275.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2023,"The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources. Building generative information-seeking models demands openly accessible datasets, which currently remain lacking. In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset. HAGRID is constructed based on human and LLM collaboration. We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability. HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.",b408198f2c891720738c385272afeeebcd747268
Approximating Human-Like Few-shot Learning with GPT-based Compression,"[{'name': 'Cynthia Huang', 'dblp_profile': 'https://dblp.org/pid/347/7854.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/181/2821.html'}]",2023,"In this work, we conceptualize the learning process as information compression. We seek to equip generative pre-trained models with human-like learning capabilities that enable data compression during inference. We present a novel approach that utilizes the Generative Pre-trained Transformer (GPT) to approximate Kolmogorov complexity, with the aim of estimating the optimal Information Distance for few-shot learning. We first propose using GPT as a prior for lossless text compression, achieving a noteworthy compression ratio. Experiment with LLAMA2-7B backbone achieves a compression ratio of 15.5 on enwik9. We justify the pre-training objective of GPT models by demonstrating its equivalence to the compression length, and, consequently, its ability to approximate the information distance for texts. Leveraging the approximated information distance, our method allows the direct application of GPT models in quantitative text similarity measurements. Experiment results show that our method overall achieves superior performance compared to embedding and prompt baselines on challenging NLP tasks, including semantic similarity, zero and one-shot text classification, and zero-shot text ranking.",f170594b13efb4a93ef9819179fc929cac6809bf
Vector Search with OpenAI Embeddings: Lucene Is All You Need,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Tommaso Teofili', 'dblp_profile': 'https://dblp.org/pid/201/6374.html'}, {'name': 'Jasper Xian', 'dblp_profile': 'https://dblp.org/pid/355/6093.html'}]",2023,"We provide a reproducible, end-to-end demonstration of vector search with OpenAI embeddings using Lucene on the popular MS MARCO passage ranking test collection. The main goal of our work is to challenge the prevailing narrative that a dedicated vector store is necessary to take advantage of recent advances in deep neural networks as applied to search. Quite the contrary, we show that hierarchical navigable small-world network (HNSW) indexes in Lucene are adequate to provide vector search capabilities in a standard bi-encoder architecture. This suggests that, from a simple cost-benefit analysis, there does not appear to be a compelling reason to introduce a dedicated vector store into a modern""AI stack""for search, since such applications have already received substantial investments in existing, widely deployed infrastructure.",a08bde10a47059b0ba1e58b425dd080ec9b42339
Unsupervised Chunking with Hierarchical RNN,"[{'name': 'Zijun Wu', 'dblp_profile': 'https://dblp.org/pid/40/7525.html'}, {'name': 'Anup Anand Deshmukh', 'dblp_profile': 'https://dblp.org/pid/233/1236.html'}, {'name': 'Yongkang Wu', 'dblp_profile': 'https://dblp.org/pid/259/1114.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Lili Mou', 'dblp_profile': 'https://dblp.org/pid/127/0779.html'}]",2023,"In Natural Language Processing (NLP), predicting linguistic structures, such as parsing and chunking, has mostly relied on manual annotations of syntactic structures. This paper introduces an unsupervised approach to chunking, a syntactic task that involves grouping words in a non-hierarchical manner. We present a two-layer Hierarchical Recurrent Neural Network (HRNN) designed to model word-to-chunk and chunk-to-sentence compositions. Our approach involves a two-stage training process: pretraining with an unsupervised parser and finetuning on downstream NLP tasks. Experiments on the CoNLL-2000 dataset reveal a notable improvement over existing unsupervised methods, enhancing phrase F1 score by up to 6 percentage points. Further, finetuning with downstream tasks results in an additional performance improvement. Interestingly, we observe that the emergence of the chunking structure is transient during the neural model's downstream-task training. This study contributes to the advancement of unsupervised syntactic structure discovery and opens avenues for further research in linguistic theory.",01580fbe1b01f37caeb76ec5587be552ea3452be
MMEAD: MS MARCO Entity Annotations and Disambiguations,"[{'name': 'Chris Kamphuis', 'dblp_profile': 'https://dblp.org/pid/156/3395.html'}, {'name': 'Aileen Lin', 'dblp_profile': 'https://dblp.org/pid/236/4217.html'}, {'name': 'Siwen Yang', 'dblp_profile': 'https://dblp.org/pid/208/9759.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}, {'name': 'Faegheh Hasibi', 'dblp_profile': 'https://dblp.org/pid/118/9049.html'}]",2023,"MMEAD, or MS MARCO Entity Annotations and Disambiguations, is a resource for entity links for the MS MARCO datasets. We specify a format to store and share links for both document and passage collections of MS MARCO. Following this specification, we release entity links to Wikipedia for documents and passages in both MS MARCO collections (v1 and v2). Entity links have been produced by the REL and BLINK systems. MMEAD is an easy-to-install Python package, allowing users to load the link data and entity embeddings effortlessly. Using MMEAD takes only a few lines of code. Finally, we show how MMEAD can be used for IR research that uses entity information. We show how to improve recall@1000 and MRR@10 on more complex queries on the MS MARCO v1 passage dataset by using this resource. We also demonstrate how entity expansions can be used for interactive search applications.",4839344495444c25ee3da374e9462042157d62ca
VoxelCache: Accelerating Online Mapping in Robotics and 3D Reconstruction Tasks,"[{'name': 'Sankeerth Durvasula', 'dblp_profile': 'https://dblp.org/pid/331/3911.html'}, {'name': 'Raymond Kiguru', 'dblp_profile': 'https://dblp.org/pid/331/3858.html'}, {'name': 'Samarth Mathur', 'dblp_profile': 'https://dblp.org/pid/331/3519.html'}, {'name': 'Jenny Xu', 'dblp_profile': 'https://dblp.org/pid/321/4121.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Nandita Vijaykumar', 'dblp_profile': 'https://dblp.org/pid/163/0027.html'}]",2022,"Real-time 3D mapping is a critical component in many important applications today including robotics, AR/VR, and 3D visualization. 3D mapping involves continuously fusing depth maps obtained from depth sensors in phones, robots, and autonomous vehicles into a single 3D representative model of the scene. Many important applications, e.g., global path planning and trajectory generation in micro aerial vehicles, require the construction of large maps at high resolutions. In this work, we identify mapping, i.e., construction and updates of 3D maps to be a critical bottleneck in these applications. The memory required and access times of these maps limit the size of the environment and the resolution with which the environment can be feasibly mapped, especially in resource constrained environments such as autonomous robot platforms and portable devices. To address this challenge, we propose VoxelCache: a hardware-software technique to accelerate map data access times in 3D mapping applications. We observe that mapping applications typically access voxels in the map that are spatially co-located to each other. We leverage this temporal locality in voxel accesses to cache indices to blocks of voxels to enable quick lookup and avoid expensive access times. We evaluate VoxelCache on popularly used mapping and reconstruction applications on both GPUs and CPUs. We demonstrate an average speedup of 1.47X (up to 1.66X) and 1.79X (up to 1.91X) on CPUs and GPUs respectively.",e46cd6f065e57adf1321c42890db1a2209e17f7b
Pseudo-Relevance Feedback with Dense Retrievers in Pyserini,"[{'name': 'Hang Li', 'dblp_profile': 'https://dblp.org/pid/83/5560-9.html'}, {'name': 'Shengyao Zhuang', 'dblp_profile': 'https://dblp.org/pid/262/6236.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Guido Zuccon', 'dblp_profile': 'https://dblp.org/pid/22/6562.html'}]",2022,"Transformer-based Dense Retrievers (DRs) are attracting extensive attention because of their effectiveness paired with high efficiency. In this context, few Pseudo-Relevance Feedback (PRF) methods applied to DRs have emerged. However, the absence of a general framework for performing PRF with DRs has made the empirical evaluation, comparison and reproduction of these methods challenging and time-consuming, especially across different DR models developed by different teams of researchers. To tackle this and speed up research into PRF methods for DRs, we showcase a new PRF framework that we implemented as a feature in Pyserini – an easy-to-use Python Information Retrieval toolkit. In particular, we leverage Pyserini’s DR framework and expand it with a PRF framework that abstracts the PRF process away from the specific DR model used. This new functionality in Pyserini allows to easily experiment with PRF methods across different DR models and datasets. Our framework comes with a number of recently proposed PRF methods built into it. Experiments within our framework show that this new PRF feature improves the effectiveness of the DR models currently available in Pyserini.",b0fe57b2ee2217a325b6f8e09f9232c35904fc62
"Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF","[{'name': 'Wei Zhong', 'dblp_profile': 'https://dblp.org/pid/60/1416.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"This work describes the participation of our team in the ARQMath 2022 Lab, where we have applied two highly complementary methods for effective math answer and formula retrieval. More specifically, a lexical sparse retriever (Approach Zero) capable of first-stage structure matching is combined with a fine-tuned bi-encoder dense retriever (ColBERT) to capture contextual similarity and semantic matching. The dense retrieval model is further pretrained to adapt to math domain content containing L A TEX tokens. In the Open Domain QA task, we take an extractive approach and filter sentences using heuristic rules applied to top-ranked answers returned from our retrievers. We provide an analysis of both the effectiveness and efficiency of our models. In this contest, our effectiveness is ranked at the top among all three tasks.",c27f3904490b8e5f9f39fe2b36722090c189e916
REBL: Entity Linking at Scale (prototype),"[{'name': 'Chris Kamphuis', 'dblp_profile': 'https://dblp.org/pid/156/3395.html'}, {'name': 'Faegheh Hasibi', 'dblp_profile': 'https://dblp.org/pid/118/9049.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2022,,a95731ecfc3f228e720276c7d93d5b28f5defb11
Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback: A Reproducibility Study,"[{'name': 'Hang Li', 'dblp_profile': 'https://dblp.org/pid/83/5560-9.html'}, {'name': 'Shengyao Zhuang', 'dblp_profile': 'https://dblp.org/pid/262/6236.html'}, {'name': 'Ahmed Mourad', 'dblp_profile': 'https://dblp.org/pid/121/4296.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Guido Zuccon', 'dblp_profile': 'https://dblp.org/pid/22/6562.html'}]",2022,,471dea6589d6f19e78db1f47fbc7cff0d9f1aab3
Another Look at DPR: Reproduction of Training and Replication of Retrieval,"[{'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Kai Sun', 'dblp_profile': 'https://dblp.org/pid/09/1171.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,,74043ae76b95ad34ee9f31d89549aa1c1c820fed
Squeezing Water from a Stone: A Bag of Tricks for Further Improving Cross-Encoder Effectiveness for Reranking,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Yuqi Liu', 'dblp_profile': 'https://dblp.org/pid/35/9071.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Yilin Li', 'dblp_profile': 'https://dblp.org/pid/120/8519.html'}, {'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,,d0629d89120409b6bb2c7cc3b309b15839ff73cb
"SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale","[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Karun Kumar', 'dblp_profile': 'https://dblp.org/pid/305/6007.html'}, {'name': 'Gefei Yang', 'dblp_profile': 'https://dblp.org/pid/229/5810.html'}, {'name': 'Akshat Pandey', 'dblp_profile': 'https://dblp.org/pid/263/3240.html'}, {'name': 'Yajie Mao', 'dblp_profile': 'https://dblp.org/pid/232/2455.html'}, {'name': 'Vladislav Belyaev', 'dblp_profile': 'https://dblp.org/pid/277/0618.html'}, {'name': 'Madhuri Emmadi', 'dblp_profile': 'https://dblp.org/pid/334/1610.html'}, {'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Ferhan Ture', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"End-to-end automatic speech recognition systems represent the state of the art, but they rely on thousands of hours of manually annotated speech for training, as well as heavyweight computation for inference. Of course, this impedes commercialization since most companies lack vast human and computational resources. In this paper, we explore training and deploying an ASR system in the label-scarce, compute-limited setting. To reduce human labor, we use a third-party ASR system as a weak supervision source, supplemented with labeling functions derived from implicit user feedback. To accelerate inference, we propose to route production-time queries across a pool of CUDA graphs of varying input lengths, the distribution of which best matches the traffic's. Compared to our third-party ASR, we achieve a relative improvement in word-error rate of 8% and a speedup of 600%. Our system, called SpeechNet, currently serves 12 million queries per day on our voice-enabled smart television. To our knowledge, this is the first time a large-scale, Wav2vec-based deployment has been described in the academic literature.",89beaeacb16e0ee4bbd8fe38a6c3b86de0d3373a
Certified Error Control of Candidate Set Pruning for Two-Stage Relevance Ranking,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Hongyang Zhang', 'dblp_profile': 'https://dblp.org/pid/23/10537-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"In information retrieval (IR), candidate set pruning has been commonly used to speed up two-stage relevance ranking. However, such an approach lacks accurate error control and often trades accuracy against computational efficiency in an empirical fashion, missing theoretical guarantees. In this paper, we propose the concept of certified error control of candidate set pruning for relevance ranking, which means that the test error after pruning is guaranteed to be controlled under a user-specified threshold with high probability. Both in-domain and out-of-domain experiments show that our method successfully prunes the first-stage retrieved candidate sets to improve the second-stage reranking speed while satisfying the pre-specified accuracy constraints in both settings. For example, on MS MARCO Passage v1, our method reduces the average candidate set size from 1000 to 27, increasing reranking speed by about 37 times, while keeping MRR@10 greater than a pre-specified value of 0.38 with about 90% empirical coverage. In contrast, empirical baselines fail to meet such requirements. Code and data are available at: https://github.com/alexlimh/CEC-Ranking.",0c430ea48e37ff43d3ab373ea1441468aa923653
Improving Precancerous Case Characterization via Transformer-based Ensemble Learning,"[{'name': 'Yizhen Zhong', 'dblp_profile': 'https://dblp.org/pid/200/4364.html'}, {'name': 'Jiajie Xiao', 'dblp_profile': 'https://dblp.org/pid/287/0404.html'}, {'name': 'Thomas Vetterli', 'dblp_profile': 'https://dblp.org/pid/99/2512.html'}, {'name': 'Mahan Matin', 'dblp_profile': 'https://dblp.org/pid/336/2644.html'}, {'name': 'Ellen Loo', 'dblp_profile': 'https://dblp.org/pid/336/1834.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Richard Bourgon', 'dblp_profile': 'https://dblp.org/pid/00/7066.html'}, {'name': 'Ofer Shapira', 'dblp_profile': 'https://dblp.org/pid/02/9059.html'}]",2022,"The application of natural language processing (NLP) to cancer pathology reports has been focused on detecting cancer cases, largely ignoring precancerous cases. Improving the characterization of precancerous adenomas assists in developing diagnostic tests for early cancer detection and prevention, especially for colorectal cancer (CRC). Here we developed transformer-based deep neural network NLP models to perform the CRC phenotyping, with the goal of extracting precancerous lesion attributes and distinguishing cancer and precancerous cases. We achieved 0.914 macro-F1 scores for classifying patients into negative, non-advanced adenoma, advanced adenoma and CRC. We further improved the performance to 0.923 using an ensemble of classifiers for cancer status classification and lesion size named entity recognition (NER). Our results demonstrated the potential of using NLP to leverage real-world health record data to facilitate the development of diagnostic tests for early cancer prevention.",9d06ef8ecab568b371727df9a47dff3b2e26fc61
Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval,"[{'name': 'Wei Zhong', 'dblp_profile': 'https://dblp.org/pid/60/1416.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"With the recent success of dense retrieval methods based on bi-encoders, studies have applied this approach to various interesting downstream retrieval tasks with good efficiency and in-domain effectiveness. Recently, we have also seen the presence of dense retrieval models in Math Information Retrieval (MIR) tasks, but the most effective systems remain classic retrieval methods that consider hand-crafted structure features. In this work, we try to combine the best of both worlds:\ a well-defined structure search method for effective formula search and efficient bi-encoder dense retrieval models to capture contextual similarities. Specifically, we have evaluated two representative bi-encoder models for token-level and passage-level dense retrieval on recent MIR tasks. Our results show that bi-encoder models are highly complementary to existing structure search methods, and we are able to advance the state-of-the-art on MIR datasets.",859cbed6555319c3c31b84c1001a1df9a34889e2
XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Rui Zhang', 'dblp_profile': 'https://dblp.org/pid/60/2536-37.html'}, {'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation. Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models. However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages. To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema. We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English). This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts. We also include global translation exemplars for a target language to facilitate the translation process for large language models. To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines. Data and code are publicly available at https://github.com/Impavidity/XRICL.",38e1a9c5599fc7597b7c5ffd37951ba5f528094c
Cross-lingual Text-to-SQL Semantic Parsing with Representation Mixup,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Linfeng Song', 'dblp_profile': 'https://dblp.org/pid/136/3610.html'}, {'name': 'Lifeng Jin', 'dblp_profile': 'https://dblp.org/pid/66/7607.html'}, {'name': 'Haitao Mi', 'dblp_profile': 'https://dblp.org/pid/19/70.html'}, {'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dong Yu', 'dblp_profile': 'https://dblp.org/pid/71/4598-1.html'}]",2022,"We focus on the cross-lingual Text-to-SQL semantic parsing task, where the parsers are expected to generate SQL for non-English utterances based on English database schemas. Intuitively, English translation as side information is an effective way to bridge the language gap, but noise introduced by the translation system may affect parser effectiveness. In this work, we propose a Representation Mixup Framework (REX) for effectively exploiting translations in the cross-lingual Text-to-SQL task. Particularly, it uses a general encoding layer, a transition layer, and a target-centric layer to properly guide the information flow of the English translation. Experimental results on CSPIDER and VSPIDER show that our framework can benefit from cross-lingual training and improve the effectiveness of semantic parsers, achieving state-of-the-art performance.",603c11bc7d00c50fd66687a1dda8181e2b593e03
AfriCLIRMatrix: Enabling Cross-Lingual Information Retrieval for African Languages,"[{'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Shuo Sun', 'dblp_profile': 'https://dblp.org/pid/04/4493.html'}, {'name': 'Kevin Duh', 'dblp_profile': 'https://dblp.org/pid/58/3217.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Language diversity in NLP is critical in enabling the development of tools for a wide range of users.However, there are limited resources for building such tools for many languages, particularly those spoken in Africa.For search, most existing datasets feature few or no African languages, directly impacting researchers’ ability to build and improve information access capabilities in those languages.Motivated by this, we created AfriCLIRMatrix, a test collection for cross-lingual information retrieval research in 15 diverse African languages.In total, our dataset contains 6 million queries in English and 23 million relevance judgments automatically mined from Wikipedia inter-language links, covering many more African languages than any existing information retrieval test collection.In addition, we release BM25, dense retrieval, and sparse–dense hybrid baselines to provide a starting point for the development of future systems.We hope that these efforts can spur additional work in search for African languages.AfriCLIRMatrix can be downloaded at https://github.com/castorini/africlirmatrix.",cf387294994faf86b3c8332ba72fc17b1442651c
Temporal Early Exiting for Streaming Speech Commands Recognition,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Karun Kumar', 'dblp_profile': 'https://dblp.org/pid/305/6007.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Piyush Vyas', 'dblp_profile': 'https://dblp.org/pid/270/7757.html'}, {'name': 'Wenyan Li', 'dblp_profile': 'https://dblp.org/pid/21/6731.html'}, {'name': 'Gefei Yang', 'dblp_profile': 'https://dblp.org/pid/229/5810.html'}, {'name': 'Yajie Mao', 'dblp_profile': 'https://dblp.org/pid/232/2455.html'}, {'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Limited-vocabulary speech commands recognition is the task of classifying a short utterance as one of several speech commands, for which neural networks obtain state-of-the-art results. In particular, recurrent neural networks represent a common approach for streaming commands recognition systems. In this paper, we explore resource-efficient methods to short-circuit such systems in the time domain when the model is confident in its prediction. We propose applying a frame-level labeling objective to further improve the efficiency–accuracy trade-off. On two datasets in limited-vocabulary commands recognition, our best method achieves an average time savings of 45% of the utterance without reducing the absolute accuracy by more than 0.6 points. We show that the per-instance savings depend on the length of the unique prefix in the phonemes across a dataset.",3ea4d0bbb313dbaa28e3eb9680879dff80cab84d
Integration of text and geospatial search for hydrographic datasets using the lucene search library,"[{'name': 'Matthew Y. R. Yang', 'dblp_profile': 'https://dblp.org/pid/322/0271.html'}, {'name': 'Siwen Yang', 'dblp_profile': 'https://dblp.org/pid/208/9759.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"We present a hybrid text and geospatial search application for hydrographic datasets built on the open-source Lucene search library. Our goal is to demonstrate that it is possible to build custom GIS applications by integrating existing open-source components and data sources, which contrasts with existing approaches based on monolithic platforms such as ArcGIS and QGIS. Lucene provides rich index structures and search capabilities for free text and geometries; the former has already been integrated and exposed via our group’s Anserini and Pyserini IR toolkits. In this work, we extend these toolkits to include geospatial capabilities. Combining knowledge extracted from Wikidata with the HydroSHEDS dataset, our application enables text and geospatial search of rivers worldwide. CCS CONCEPTS • Information systems → Geographic information systems; Users and interactive retrieval.",a6739fcfa6fe2fd680e424a3a101bcb5fc9276f2
Few-Shot Non-Parametric Learning with Deep Latent Variable Model,"[{'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Yiqin Dai', 'dblp_profile': 'https://dblp.org/pid/225/0795.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/181/2821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Most real-world problems that machine learning algorithms are expected to solve face the situation with 1) unknown data distribution; 2) little domain-specific knowledge; and 3) datasets with limited annotation. We propose Non-Parametric learning by Compression with Latent Variables (NPC-LV), a learning framework for any dataset with abundant unlabeled data but very few labeled ones. By only training a generative model in an unsupervised way, the framework utilizes the data distribution to build a compressor. Using a compressor-based distance metric derived from Kolmogorov complexity, together with few labeled data, NPC-LV classifies without further training. We show that NPC-LV outperforms supervised methods on all three datasets on image classification in low data regime and even outperform semi-supervised learning methods on CIFAR-10. We demonstrate how and when negative evidence lowerbound (nELBO) can be used as an approximate compressed length for classification. By revealing the correlation between compression rate and classification accuracy, we illustrate that under NPC-LV, the improvement of generative models can enhance downstream classification accuracy.",d6885ea92c9c720b58f5ed278f5aea42dfdf9918
Neural Query Synthesis and Domain-Specific Ranking Templates for Multi-Stage Clinical Trial Matching,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Yilin Li', 'dblp_profile': 'https://dblp.org/pid/120/8519.html'}, {'name': 'Yuetong Wang', 'dblp_profile': 'https://dblp.org/pid/62/2659.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"In this work, we propose an effective multi-stage neural ranking system for the clinical trial matching problem. First, we introduce NQS, a neural query synthesis method that leverages a zero-shot document expansion model to generate multiple sentence-long queries from lengthy patient descriptions. These queries are independently issued to a search engine and the results are fused. We find that on the TREC 2021 Clinical Trials Track, this method outperforms strong traditional baselines like BM25 and BM25 + RM3 by about 12 points in nDCG@10, a relative improvement of 34%. This simple method is so effective that even a state-of-the-art neural relevance ranking method trained on the medical subset of MS MARCO passage, when reranking the results of NQS, fails to improve on the ranked list. Second, we introduce a two-stage neural reranking pipeline trained on clinical trial matching data using tailored ranking templates. In this setting, we can train a pointwise reranker using just 1.1k positive examples and obtain effectiveness improvements over NQS by 24 points. This end-to-end multi-stage system demonstrates a 20% relative effectiveness gain compared to the second-best submission at TREC 2021, making it an important step towards better automated clinical trial matching.",35a318073ab5b18cf364699e145a750940bcedb3
"To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers","[{'name': 'Hang Li', 'dblp_profile': 'https://dblp.org/pid/83/5560-9.html'}, {'name': 'Shuai Wang', 'dblp_profile': 'https://dblp.org/pid/42/1503.html'}, {'name': 'Shengyao Zhuang', 'dblp_profile': 'https://dblp.org/pid/262/6236.html'}, {'name': 'Ahmed Mourad', 'dblp_profile': 'https://dblp.org/pid/121/4296.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Guido Zuccon', 'dblp_profile': 'https://dblp.org/pid/22/6562.html'}]",2022,"Current pre-trained language model approaches to information retrieval can be broadly divided into two categories: sparse retrievers (to which belong also non-neural approaches such as bag-of-words methods, e.g., BM25) and dense retrievers. Each of these categories appears to capture different characteristics of relevance. Previous work has investigated how relevance signals from sparse retrievers could be combined with those from dense retrievers via interpolation. Such interpolation would generally lead to higher retrieval effectiveness. In this paper we consider the problem of combining the relevance signals from sparse and dense retrievers in the context of Pseudo Relevance Feedback (PRF). This context poses two key challenges: (1) When should interpolation occur: before, after, or both before and after the PRF process? (2) Which sparse representation should be considered: a zero-shot bag-of-words model (BM25), or a learned sparse representation? To answer these questions we perform a thorough empirical evaluation considering an effective and scalable neural PRF approach (Vector-PRF), three effective dense retrievers (ANCE, TCTv2, DistillBERT), and one state-of-the-art learned sparse retriever (uniCOIL). The empirical findings from our experiments suggest that, regardless of sparse representation and dense retriever, interpolation both before and after PRF achieves the highest effectiveness across most datasets and metrics.",8d715cbbec0a66f685e58b6148ad6a5be0d8ead2
Another Look at Information Retrieval as Statistical Translation,"[{'name': 'Yuqi Liu', 'dblp_profile': 'https://dblp.org/pid/35/9071.html'}, {'name': 'Chengcheng Hu', 'dblp_profile': 'https://dblp.org/pid/56/266.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Over two decades ago, Berger and Lafferty proposed ""information retrieval as statistical translation"" (IRST), a simple and elegant method for ad hoc retrieval based on the noisy channel model. At the time, they lacked the large-scale human-annotated datasets necessary to properly train their models. In this paper, we ask the simple question: What if Berger and Lafferty had access to datasets such as the MS MARCO passage ranking dataset that we take for granted today? The answer to this question tells us how much of recent improvements in ranking can be solely attributed to having more data available, as opposed to improvements in models (e.g., pretrained transformers) and optimization techniques (e.g., contrastive loss). In fact, Boytsov and Kolter recently began to answer this question with a replication of Berger and Lafferty's model, and this work can be viewed as another independent replication effort, with generalizations to additional conditions not previously explored, including replacing the sum of translation probabilities with ColBERT's MaxSim operator. We confirm that while neural models (particularly pretrained transformers) have indeed led to great advances in retrieval effectiveness, the IRST model proposed decades ago is quite effective if provided sufficient training data.",a19438a28ba9eb5eeefa9324840758b1b5fe3c65
Fostering Coopetition While Plugging Leaks: The Design and Implementation of the MS MARCO Leaderboards,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Daniel Campos', 'dblp_profile': 'https://dblp.org/pid/64/1314.html'}, {'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Bhaskar Mitra', 'dblp_profile': 'https://dblp.org/pid/147/9120-1.html'}, {'name': 'Emine Yilmaz', 'dblp_profile': 'https://dblp.org/pid/36/3270.html'}]",2022,"We articulate the design and implementation of the MS MARCO document ranking and passage ranking leaderboards. In contrast to ""standard"" community-wide evaluations such as those at TREC, which can be characterized as simultaneous games, leaderboards represent sequential games, where every player move is immediately visible to the entire community. The fundamental challenge with this setup is that every leaderboard submission leaks information about the held-out evaluation set, which conflicts with the fundamental tenant in machine learning about separation of training and test data. These ""leaks"", accumulated over long periods of time, threaten the validity of the insights that can be derived from the leaderboards. In this paper, we share our experiences grappling with this issue over the past few years and how our considerations are operationalized into a coherent submission policy. Our work provides a useful guide to help the community understand the design choices made in the popular MS MARCO leaderboards and offers lessons for designers of future leaderboards.",3f662d9d43762226ad5ffa344869c691e5808b27
Too Many Relevants: Whither Cranfield Test Collections?,"[{'name': 'Ellen M. Voorhees', 'dblp_profile': 'https://dblp.org/pid/60/3753.html'}, {'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"This paper presents the lessons regarding the construction and use of large Cranfield-style test collections learned from the TREC 2021 Deep Learning track. The corpus used in the 2021 edition of the track was much bigger than the corpus used previously and it contains many more relevant documents. The process used to select documents to judge that had been used in earlier years of the track failed to produce a reliable collection because most topics have too many relevant documents. Judgment budgets were exceeded before an adequate sample of the relevant set could be found, so there are likely many unknown relevant documents in the unjudged portion of the corpus. As a result, the collection is not reusable, and furthermore, recall-based measures are unreliable even for the retrieval systems that were used to build the collection. Yet, early-precision measures cannot distinguish among system results because the maximum score is easily obtained for many topics. And since the existing tools for appraising the quality of test collections depend on systems' scores, they also fail when there are too many relevant documents. Collection builders will need new strategies and tools for building reliable test collections for continued use of the Cranfield paradigm on ever-larger corpora. Ensuring that the definition of 'relevant' truly reflects the desired systems' rankings is a provisional strategy for continued collection building.",979fd6c0eb8571c170c46903ec2c62a4f7b35bc6
Document Expansion Baselines and Learned Sparse Lexical Representations for MS MARCO V1 and V2,"[{'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"With doc2query, we train a neural sequence-to-sequence model that, given an input span of text, predicts a natural language query that the text might answer. These predictions can be viewed as document expansions that feed standard bag-of-words term weighting models such as BM25 or neural retrieval models based on learned sparse lexical representations such as uniCOIL. Previous experiments on the MS MARCO datasets have demonstrated the effectiveness of these methods, and they serve as baselines that are widely used by the community today. Following the recent release of the MS MARCO V2 passage and document ranking test collections, we have refreshed our doc2query and uniCOIL models. This work describes a number of resources that support competitive, reproducible baselines for both the MS MARCO V1 and V2 test collections using our Anserini and Pyserini IR toolkits. Together, they provide a solid foundation for future research on neural retrieval models using the MS MARCO datasets and beyond.",423f94251781e9169f2a96bb3208f7feb2a89b0e
A Common Framework for Exploring Document-at-a-Time and Score-at-a-Time Retrieval Methods,"[{'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}, {'name': 'Joel Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'Pradeesh Parameswaran', 'dblp_profile': 'https://dblp.org/pid/255/6500.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Document-at-a-time (DaaT) and score-at-a-time (SaaT) query evaluation techniques are different approaches to top-k retrieval with inverted indexes. While modern systems are dominated by DaaT, the academic literature has seen decades of debate about the merits of each. Recently, there has been renewed interest in SaaT methods for learned sparse lexical models, where studies have shown that transformers generate ""wacky weights"" that appear to reduce opportunities for optimizations in DaaT methods. However, researchers currently lack an easy-to-use SaaT system to support further exploration. This is the gap that our work fills. Starting with a modern SaaT system (JASS), we built Python bindings in order to integrate into the DaaT Pyserini IR toolkit (Lucene). The result is a common frontend to both a DaaT and a SaaT system. We demonstrate how recent experiments with a wide range of learned sparse lexical models can be easily reproduced. Our contribution is a framework that enables future research comparing DaaT and SaaT methods in the context of modern neural retrieval models.",f162b74b73e53e5980e9b3346be370934753acf1
Flipping the Script: Inverse Information Seeking Dialogues for Market Research,"[{'name': 'Josh Seltzer', 'dblp_profile': 'https://dblp.org/pid/323/9652.html'}, {'name': 'Kathy Cheng', 'dblp_profile': 'https://dblp.org/pid/323/9661.html'}, {'name': 'Shi Zong', 'dblp_profile': 'https://dblp.org/pid/177/9106.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,,
Overview of the TREC 2022 Deep Learning Track,"[{'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Bhaskar Mitra', 'dblp_profile': 'https://dblp.org/pid/147/9120-1.html'}, {'name': 'Emine Yilmaz', 'dblp_profile': 'https://dblp.org/pid/36/3270.html'}, {'name': 'Daniel Campos', 'dblp_profile': 'https://dblp.org/pid/64/1314.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ellen M. Voorhees', 'dblp_profile': 'https://dblp.org/pid/60/3753.html'}, {'name': 'Ian Soboroff', 'dblp_profile': 'https://dblp.org/pid/94/2865.html'}]",2022,,2bfc734f35961291224d131bd38290aacd758a9f
Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'David Alfonso-Hermelo', 'dblp_profile': 'https://dblp.org/pid/241/1750.html'}, {'name': 'Vitor Jeronymo', 'dblp_profile': 'https://dblp.org/pid/321/4286.html'}, {'name': 'Ehsan Kamalloo', 'dblp_profile': 'https://dblp.org/pid/61/7404.html'}, {'name': 'Carlos Lassance', 'dblp_profile': 'https://dblp.org/pid/177/5760.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Mehdi Rezagholizadeh', 'dblp_profile': 'https://dblp.org/pid/134/0625.html'}, {'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}]",2022,"The advent of multilingual language models has generated a resurgence of interest in cross-lingual information retrieval (CLIR), which is the task of searching documents in one language with queries from another. However, the rapid pace of progress has led to a confusing panoply of methods and reproducibility has lagged behind the state of the art. In this context, our work makes two important contributions: First, we provide a conceptual framework for organizing different approaches to cross-lingual retrieval using multi-stage architectures for mono-lingual retrieval as a scaffold. Second, we implement simple yet effective reproducible baselines in the Anserini and Pyserini IR toolkits for test collections from the TREC 2022 NeuCLIR Track, in Persian, Russian, and Chinese. Our efforts are built on a collaboration of the two teams that submitted the most effective runs to the TREC evaluation. These contributions provide a firm foundation for future advances.",610bfeb562ce11cd3a6e7588427daf489e3b4e94
Aligning the Research and Practice of Building Search Applications: Elasticsearch and Pyserini,"[{'name': 'Josh Devins', 'dblp_profile': 'https://dblp.org/pid/313/9159.html'}, {'name': 'Julie Tibshirani', 'dblp_profile': 'https://dblp.org/pid/117/4067.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"We demonstrate, via competitive bag-of-words first-stage retrieval baselines for the MS MARCO document ranking task, seamless replicability and interoperability between Elasticsearch and the Pyserini IR toolkit, which are both built on the open-source Lucene search library. This integration highlights the benefits of recent efforts to promote the use of Lucene in information retrieval research to better align the research and practice of building search applications. Closer alignment between academia and industry is mutually beneficial: Academic researchers gain a smoother path to real-world impact because their contributions can be more easily deployed in production applications. Industry practitioners gain an easy way to benchmark their innovations in a rigorous and vendor-neutral manner by exploiting evaluation resources and infrastructure built by the academic community. This two-way exchange between academia and industry allows both parties to ""have their cakes and eat them too"".",c36e493820c2b5981d43282d33b8b020d65d61e1
Can Old TREC Collections Reliably Evaluate Modern Neural Retrieval Models?,"[{'name': 'Ellen M. Voorhees', 'dblp_profile': 'https://dblp.org/pid/60/3753.html'}, {'name': 'Ian Soboroff', 'dblp_profile': 'https://dblp.org/pid/94/2865.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Neural retrieval models are generally regarded as fundamentally different from the retrieval techniques used in the late 1990's when the TREC ad hoc test collections were constructed. They thus provide the opportunity to empirically test the claim that pooling-built test collections can reliably evaluate retrieval systems that did not contribute to the construction of the collection (in other words, that such collections can be reusable). To test the reusability claim, we asked TREC assessors to judge new pools created from new search results for the TREC-8 ad hoc collection. These new search results consisted of five new runs (one each from three transformer-based models and two baseline runs that use BM25) plus the set of TREC-8 submissions that did not previously contribute to pools. The new runs did retrieve previously unseen documents, but the vast majority of those documents were not relevant. The ranking of all runs by mean evaluation score when evaluated using the official TREC-8 relevance judgment set and the newly expanded relevance set are almost identical, with Kendall's tau correlations greater than 0.99. Correlations for individual topics are also high. The TREC-8 ad hoc collection was originally constructed using deep pools over a diverse set of runs, including several effective manual runs. Its judgment budget, and hence construction cost, was relatively large. However, it does appear that the expense was well-spent: even with the advent of neural techniques, the collection has stood the test of time and remains a reliable evaluation instrument as retrieval techniques have advanced.",e1ca2570b1916398377a7bbd3d3731a389b0bdc2
Tevatron: An Efficient and Flexible Toolkit for Dense Retrieval,"[{'name': 'Luyu Gao', 'dblp_profile': 'https://dblp.org/pid/213/8857.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jamie Callan', 'dblp_profile': 'https://dblp.org/pid/c/JamesPCallan.html'}]",2022,"Recent rapid advancements in deep pre-trained language models and the introductions of large datasets have powered research in embedding-based dense retrieval. While several good research papers have emerged, many of them come with their own software stacks. These stacks are typically optimized for some particular research goals instead of efficiency or code structure. In this paper, we present Tevatron, a dense retrieval toolkit optimized for efficiency, flexibility, and code simplicity. Tevatron provides a standardized pipeline for dense retrieval including text processing, model training, corpus/query encoding, and search. This paper presents an overview of Tevatron and demonstrates its effectiveness and efficiency across several IR and QA data sets. We also show how Tevatron's flexible design enables easy generalization across datasets, model architectures, and accelerator platforms(GPU/TPU). We believe Tevatron can serve as an effective software foundation for dense retrieval system research including design, modeling, and optimization.",d2d913ce77dc2bf4b4bb954e3ce531dcc47b8fc3
Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval,"[{'name': 'Wei Zhong', 'dblp_profile': 'https://dblp.org/pid/60/1416.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"With the recent success of dense retrieval methods based on bi-encoders, studies have applied this approach to various interesting downstream retrieval tasks with good efficiency and in-domain effectiveness. Recently, we have also seen the presence of dense retrieval models in Math Information Retrieval (MIR) tasks, but the most effective systems remain classic retrieval methods that consider hand-crafted structure features. In this work, we try to combine the best of both worlds:\ a well-defined structure search method for effective formula search and efficient bi-encoder dense retrieval models to capture contextual similarities. Specifically, we have evaluated two representative bi-encoder models for token-level and passage-level dense retrieval on recent MIR tasks. Our results show that bi-encoder models are highly complementary to existing structure search methods, and we are able to advance the state-of-the-art on MIR datasets.",859cbed6555319c3c31b84c1001a1df9a34889e2
Towards Best Practices for Training Multilingual Dense Retrieval Models,"[{'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Kelechi Ogueji', 'dblp_profile': 'https://dblp.org/pid/255/5120.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Dense retrieval models using a transformer-based bi-encoder architecture have emerged as an active area of research. In this paper, we focus on the task of monolingual retrieval in a variety of typologically diverse languages using such an architecture. Although recent work with multilingual transformers demonstrates that they exhibit strong cross-lingual generalization capabilities, there remain many open research questions, which we tackle here. Our study is organized as a “best practices” guide for training multilingual dense retrieval models, broken down into three main scenarios: when a multilingual transformer is available, but training data in the form of relevance judgments are not available in the language and domain of interest (“have model, no data”); when both models and training data are available (“have model and data”); and, when training data are available not but models (“have data, no model”). In considering these scenarios, we gain a better understanding of the role of multi-stage fine-tuning, the strength of cross-lingual transfer under various conditions, the usefulness of out-of-language data, and the advantages of multilingual vs. monolingual transformers. Our recommendations offer a guide for practitioners building search applications, particularly for low-resource languages, and while our work leaves open a number of research questions, we provide a solid foundation for future work.",943487997ecd26e871a2ab16160bd5640020369d
"To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers","[{'name': 'Hang Li', 'dblp_profile': 'https://dblp.org/pid/83/5560-9.html'}, {'name': 'Shuai Wang', 'dblp_profile': 'https://dblp.org/pid/42/1503.html'}, {'name': 'Shengyao Zhuang', 'dblp_profile': 'https://dblp.org/pid/262/6236.html'}, {'name': 'Ahmed Mourad', 'dblp_profile': 'https://dblp.org/pid/121/4296.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Guido Zuccon', 'dblp_profile': 'https://dblp.org/pid/22/6562.html'}]",2022,"Current pre-trained language model approaches to information retrieval can be broadly divided into two categories: sparse retrievers (to which belong also non-neural approaches such as bag-of-words methods, e.g., BM25) and dense retrievers. Each of these categories appears to capture different characteristics of relevance. Previous work has investigated how relevance signals from sparse retrievers could be combined with those from dense retrievers via interpolation. Such interpolation would generally lead to higher retrieval effectiveness. In this paper we consider the problem of combining the relevance signals from sparse and dense retrievers in the context of Pseudo Relevance Feedback (PRF). This context poses two key challenges: (1) When should interpolation occur: before, after, or both before and after the PRF process? (2) Which sparse representation should be considered: a zero-shot bag-of-words model (BM25), or a learned sparse representation? To answer these questions we perform a thorough empirical evaluation considering an effective and scalable neural PRF approach (Vector-PRF), three effective dense retrievers (ANCE, TCTv2, DistillBERT), and one state-of-the-art learned sparse retriever (uniCOIL). The empirical findings from our experiments suggest that, regardless of sparse representation and dense retriever, interpolation both before and after PRF achieves the highest effectiveness across most datasets and metrics.",8d715cbbec0a66f685e58b6148ad6a5be0d8ead2
Certified Error Control of Candidate Set Pruning for Two-Stage Relevance Ranking,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Hongyang Zhang', 'dblp_profile': 'https://dblp.org/pid/23/10537-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"In information retrieval (IR), candidate set pruning has been commonly used to speed up two-stage relevance ranking. However, such an approach lacks accurate error control and often trades accuracy against computational efficiency in an empirical fashion, missing theoretical guarantees. In this paper, we propose the concept of certified error control of candidate set pruning for relevance ranking, which means that the test error after pruning is guaranteed to be controlled under a user-specified threshold with high probability. Both in-domain and out-of-domain experiments show that our method successfully prunes the first-stage retrieved candidate sets to improve the second-stage reranking speed while satisfying the pre-specified accuracy constraints in both settings. For example, on MS MARCO Passage v1, our method reduces the average candidate set size from 1000 to 27, increasing reranking speed by about 37 times, while keeping MRR@10 greater than a pre-specified value of 0.38 with about 90% empirical coverage. In contrast, empirical baselines fail to meet such requirements. Code and data are available at: https://github.com/alexlimh/CEC-Ranking.",0c430ea48e37ff43d3ab373ea1441468aa923653
Domain Adaptation for Memory-Efficient Dense Retrieval,"[{'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Nils Reimers', 'dblp_profile': 'https://dblp.org/pid/129/5173.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Dense retrievers encode documents into ﬁxed dimensional embeddings. However, storing all the document embeddings within an index produces bulky indexes which are expensive to serve. Recently, BPR (Yamada et al., 2021) and JPQ (Zhan et al., 2021a) have been proposed which train the model to produce binary document vectors, which reduce the index 32 × and more. The authors showed these binary embedding models signiﬁcantly outperform more traditional index compression techniques like Product Quantization (PQ). Previous work evaluated these approaches just in-domain, i.e. the methods were evaluated on tasks for which training data is available. In practice, retrieval models are often used in an out-of-domain setting, where they have been train on a publicly available dataset, like MS MARCO, but are then used for some custom dataset for which no training data is available. In this work, we show that binary embedding models like BPR and JPQ can perform signif-icantly worse than baselines once there is a domain-shift involved. We propose a modiﬁ-cation to the training procedure of BPR and JPQ and combine it with a corpus speciﬁc generative procedure which allow the adaptation of BPR and JPQ to any corpus without requiring labeled training data. Our domain-adapted strategy known as GPL is model agnostic, achieves an improvement by up-to 19.3 and 11.6 points in nDCG@10 across the BEIR benchmark in comparison to BPR and JPQ while maintaining its 32x memory efﬁciency. JPQ+GPL even outperforms our upper baseline: uncompressed TAS-B model on average by 2.0 points. 1",34ff09ddf6ae64299564cb673044055b4615fc3c
A Dense Representation Framework for Lexical and Semantic Matching,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Lexical and semantic matching capture different successful approaches to text retrieval and the fusion of their results has proven to be more effective and robust than either alone. Prior work performs hybrid retrieval by conducting lexical and semantic matching using different systems (e.g., Lucene and Faiss, respectively) and then fusing their model outputs. In contrast, our work integrates lexical representations with dense semantic representations by densifying high-dimensional lexical representations into what we call low-dimensional dense lexical representations (DLRs). Our experiments show that DLRs can effectively approximate the original lexical representations, preserving effectiveness while improving query latency. Furthermore, we can combine dense lexical and semantic representations to generate dense hybrid representations (DHRs) that are more flexible and yield faster retrieval compared to existing hybrid techniques. In addition, we explore jointly training lexical and semantic representations in a single model and empirically show that the resulting DHRs are able to combine the advantages of the individual components. Our best DHR model is competitive with state-of-the-art single-vector and multi-vector dense retrievers in both in-domain and zero-shot evaluation settings. Furthermore, our model is both faster and requires smaller indexes, making our dense representation framework an attractive approach to text retrieval. Our code is available at https://github.com/castorini/dhr.",6e386d1720d9a81386960fcb4daec979ce0731ca
Few-Shot Non-Parametric Learning with Deep Latent Variable Model,"[{'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Yiqin Dai', 'dblp_profile': 'https://dblp.org/pid/225/0795.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/181/2821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Most real-world problems that machine learning algorithms are expected to solve face the situation with 1) unknown data distribution; 2) little domain-specific knowledge; and 3) datasets with limited annotation. We propose Non-Parametric learning by Compression with Latent Variables (NPC-LV), a learning framework for any dataset with abundant unlabeled data but very few labeled ones. By only training a generative model in an unsupervised way, the framework utilizes the data distribution to build a compressor. Using a compressor-based distance metric derived from Kolmogorov complexity, together with few labeled data, NPC-LV classifies without further training. We show that NPC-LV outperforms supervised methods on all three datasets on image classification in low data regime and even outperform semi-supervised learning methods on CIFAR-10. We demonstrate how and when negative evidence lowerbound (nELBO) can be used as an approximate compressed length for classification. By revealing the correlation between compression rate and classification accuracy, we illustrate that under NPC-LV, the improvement of generative models can enhance downstream classification accuracy.",d6885ea92c9c720b58f5ed278f5aea42dfdf9918
Building an Efficiency Pipeline: Commutativity and Cumulativeness of Efficiency Operators for Transformers,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"There exists a wide variety of efficiency methods for natural language processing (NLP) tasks, such as pruning, distillation, dynamic inference, quantization, etc. We can consider an efficiency method as an operator applied on a model. Naturally, we may construct a pipeline of multiple efficiency methods, i.e., to apply multiple operators on the model sequentially. In this paper, we study the plausibility of this idea, and more importantly, the commutativity and cumulativeness of efficiency operators. We make two interesting observations: (1) Efficiency operators are commutative -- the order of efficiency methods within the pipeline has little impact on the final results; (2) Efficiency operators are also cumulative -- the final results of combining several efficiency methods can be estimated by combining the results of individual methods. These observations deepen our understanding of efficiency operators and provide useful guidelines for their real-world applications.",f9986ca86ef1a617252630c72b3084b039976e5f
Aggretriever: A Simple Approach to Aggregate Textual Representation for Robust Dense Passage Retrieval,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,,
What the DAAM: Interpreting Stable Diffusion Using Cross Attention,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Akshat Pandey', 'dblp_profile': 'https://dblp.org/pid/263/3240.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Gefei Yang', 'dblp_profile': 'https://dblp.org/pid/229/5810.html'}, {'name': 'Karun Kumar', 'dblp_profile': 'https://dblp.org/pid/305/6007.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ferhan Ture', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}]",2022,"Diffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8-64.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4-4.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head–dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at https://github.com/castorini/daam.",37d626720f5003373336098ce7c01a1a38e6b63d
Better Than Whitespace: Information Retrieval for Languages without Custom Tokenizers,"[{'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Tokenization is a crucial step in information retrieval, especially for lexical matching algorithms, where the quality of indexable tokens directly impacts the effectiveness of a retrieval system. Since different languages have unique properties, the design of the tokenization algorithm is usually language-specific and requires at least some lingustic knowledge. However, only a handful of the 7000+ languages on the planet benefit from specialized, custom-built tokenization algorithms, while the other languages are stuck with a""default""whitespace tokenizer, which cannot capture the intricacies of different languages. To address this challenge, we propose a different approach to tokenization for lexical matching retrieval algorithms (e.g., BM25): using the WordPiece tokenizer, which can be built automatically from unsupervised data. We test the approach on 11 typologically diverse languages in the MrTyDi collection: results show that the mBERT tokenizer provides strong relevance signals for retrieval""out of the box"", outperforming whitespace tokenization on most languages. In many cases, our approach also improves retrieval effectiveness when combined with existing custom-built tokenizers.",45a3c9be0cc9a9b31e8c524b4a5ab7c43231fc84
Query Expansion Using Contextual Clue Sampling with Language Models,"[{'name': 'Linqing Liu', 'dblp_profile': 'https://dblp.org/pid/36/7028.html'}, {'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Sebastian Riedel', 'dblp_profile': 'https://dblp.org/pid/18/3348-1.html'}, {'name': 'Pontus Stenetorp', 'dblp_profile': 'https://dblp.org/pid/44/8358.html'}]",2022,"Query expansion is an effective approach for mitigating vocabulary mismatch between queries and documents in information retrieval. One recent line of research uses language models to generate query-related contexts for expansion. Along this line, we argue that expansion terms from these contexts should balance two key aspects: diversity and relevance. The obvious way to increase diversity is to sample multiple contexts from the language model. However, this comes at the cost of relevance, because there is a well-known tendency of models to hallucinate incorrect or irrelevant contexts. To balance these two considerations, we propose a combination of an effective filtering strategy and fusion of the retrieved documents based on the generation probability of each context. Our lexical matching based approach achieves a similar top-5/top-20 retrieval accuracy and higher top-100 accuracy compared with the well-established dense retrieval model DPR, while reducing the index size by more than 96%. For end-to-end QA, the reader model also benefits from our method and achieves the highest Exact-Match score against several competitive baselines.",754fad16ccde2328b302162571650254acd38203
VoxelCache: Accelerating Online Mapping in Robotics and 3D Reconstruction Tasks,"[{'name': 'Sankeerth Durvasula', 'dblp_profile': 'https://dblp.org/pid/331/3911.html'}, {'name': 'Raymond Kiguru', 'dblp_profile': 'https://dblp.org/pid/331/3858.html'}, {'name': 'Samarth Mathur', 'dblp_profile': 'https://dblp.org/pid/331/3519.html'}, {'name': 'Jenny Xu', 'dblp_profile': 'https://dblp.org/pid/321/4121.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Nandita Vijaykumar', 'dblp_profile': 'https://dblp.org/pid/163/0027.html'}]",2022,"Real-time 3D mapping is a critical component in many important applications today including robotics, AR/VR, and 3D visualization. 3D mapping involves continuously fusing depth maps obtained from depth sensors in phones, robots, and autonomous vehicles into a single 3D representative model of the scene. Many important applications, e.g., global path planning and trajectory generation in micro aerial vehicles, require the construction of large maps at high resolutions. In this work, we identify mapping, i.e., construction and updates of 3D maps to be a critical bottleneck in these applications. The memory required and access times of these maps limit the size of the environment and the resolution with which the environment can be feasibly mapped, especially in resource constrained environments such as autonomous robot platforms and portable devices. To address this challenge, we propose VoxelCache: a hardware-software technique to accelerate map data access times in 3D mapping applications. We observe that mapping applications typically access voxels in the map that are spatially co-located to each other. We leverage this temporal locality in voxel accesses to cache indices to blocks of voxels to enable quick lookup and avoid expensive access times. We evaluate VoxelCache on popularly used mapping and reconstruction applications on both GPUs and CPUs. We demonstrate an average speedup of 1.47X (up to 1.66X) and 1.79X (up to 1.91X) on CPUs and GPUs respectively.",e46cd6f065e57adf1321c42890db1a2209e17f7b
Making a MIRACL: Multilingual Information Retrieval Across a Continuum of Languages,"[{'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Nandan Thakur', 'dblp_profile': 'https://dblp.org/pid/276/6898.html'}, {'name': 'Odunayo Ogundepo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Ehsan Kamalloo', 'dblp_profile': 'https://dblp.org/pid/61/7404.html'}, {'name': 'David Alfonso-Hermelo', 'dblp_profile': 'https://dblp.org/pid/241/1750.html'}, {'name': 'Xiaoguang Li', 'dblp_profile': 'https://dblp.org/pid/46/1349.html'}, {'name': 'Qun Liu', 'dblp_profile': 'https://dblp.org/pid/75/4402-1.html'}, {'name': 'Mehdi Rezagholizadeh', 'dblp_profile': 'https://dblp.org/pid/134/0625.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"MIRACL (Multilingual Information Retrieval Across a Continuum of Languages) is a multilingual dataset we have built for the WSDM 2023 Cup challenge that focuses on ad hoc retrieval across 18 different languages, which collectively encompass over three billion native speakers around the world. These languages have diverse typologies, originate from many different language families, and are associated with varying amounts of available resources -- including what researchers typically characterize as high-resource as well as low-resource languages. Our dataset is designed to support the creation and evaluation of models for monolingual retrieval, where the queries and the corpora are in the same language. In total, we have gathered over 700k high-quality relevance judgments for around 77k queries over Wikipedia in these 18 languages, where all assessments have been performed by native speakers hired by our team. Our goal is to spur research that will improve retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have been traditionally underserved. This overview paper describes the dataset and baselines that we share with the community. The MIRACL website is live at http://miracl.ai/.",d821afef2e506c0bbbeb88b4622f2a5a5bb8358b
XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Rui Zhang', 'dblp_profile': 'https://dblp.org/pid/60/2536-37.html'}, {'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation. Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models. However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages. To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema. We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English). This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts. We also include global translation exemplars for a target language to facilitate the translation process for large language models. To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines. Data and code are publicly available at https://github.com/Impavidity/XRICL.",38e1a9c5599fc7597b7c5ffd37951ba5f528094c
On the Interaction Between Differential Privacy and Gradient Compression in Deep Learning,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"While differential privacy and gradient compression are separately well-researched topics in machine learning, the study of interaction between these two topics is still relatively new. We perform a detailed empirical study on how the Gaussian mechanism for differential privacy and gradient compression jointly impact test accuracy in deep learning. The existing literature in gradient compression mostly evaluates compression in the absence of differential privacy guarantees, and demonstrate that sufficiently high compression rates reduce accuracy. Similarly, existing literature in differential privacy evaluates privacy mechanisms in the absence of compression, and demonstrates that sufficiently strong privacy guarantees reduce accuracy. In this work, we observe while gradient compression generally has a negative impact on test accuracy in non-private training, it can sometimes improve test accuracy in differentially private training. Specifically, we observe that when employing aggressive sparsification or rank reduction to the gradients, test accuracy is less affected by the Gaussian noise added for differential privacy. These observations are explained through an analysis how differential privacy and compression effects the bias and variance in estimating the average gradient. We follow this study with a recommendation on how to improve test accuracy under the context of differentially private deep learning and gradient compression. We evaluate this proposal and find that it can reduce the negative impact of noise added by differential privacy mechanisms on test accuracy by up to 24.6%, and reduce the negative impact of gradient sparsification on test accuracy by up to 15.1%.",8ac975f8ece08246554f1ce9585fb13616a51097
CITADEL: Conditional Token Interaction via Dynamic Lexical Routing for Efficient and Effective Multi-Vector Retrieval,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Barlas Oguz', 'dblp_profile': 'https://dblp.org/pid/69/9892.html'}, {'name': 'Asish Ghoshal', 'dblp_profile': 'https://dblp.org/pid/35/10890.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Yashar Mehdad', 'dblp_profile': 'https://dblp.org/pid/28/7560.html'}, {'name': 'Wen-tau Yih', 'dblp_profile': 'https://dblp.org/pid/07/7129.html'}, {'name': 'Xilun Chen', 'dblp_profile': 'https://dblp.org/pid/96/10207-2.html'}]",2022,"Multi-vector retrieval methods combine the merits of sparse (e.g. BM25) and dense (e.g. DPR) retrievers and have achieved state-of-the-art performance on various retrieval tasks.These methods, however, are orders of magnitude slower and need much more space to store their indices compared to their single-vector counterparts.In this paper, we unify different multi-vector retrieval models from a token routing viewpoint and propose conditional token interaction via dynamic lexical routing, namely CITADEL, for efficient and effective multi-vector retrieval.CITADEL learns to route different token vectors to the predicted lexical keys such that a query token vector only interacts with document token vectors routed to the same key.This design significantly reduces the computation cost while maintaining high accuracy.Notably, CITADEL achieves the same or slightly better performance than the previous state of the art, ColBERT-v2, on both in-domain (MS MARCO) and out-of-domain (BEIR) evaluations, while being nearly 40 times faster. Source code and data are available at https://github.com/facebookresearch/dpr-scale/tree/citadel.",741a5536c2e58e1595e7396345f1e5d40a3aa775
"SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale","[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Karun Kumar', 'dblp_profile': 'https://dblp.org/pid/305/6007.html'}, {'name': 'Gefei Yang', 'dblp_profile': 'https://dblp.org/pid/229/5810.html'}, {'name': 'Akshat Pandey', 'dblp_profile': 'https://dblp.org/pid/263/3240.html'}, {'name': 'Yajie Mao', 'dblp_profile': 'https://dblp.org/pid/232/2455.html'}, {'name': 'Vladislav Belyaev', 'dblp_profile': 'https://dblp.org/pid/277/0618.html'}, {'name': 'Madhuri Emmadi', 'dblp_profile': 'https://dblp.org/pid/334/1610.html'}, {'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Ferhan Ture', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"End-to-end automatic speech recognition systems represent the state of the art, but they rely on thousands of hours of manually annotated speech for training, as well as heavyweight computation for inference. Of course, this impedes commercialization since most companies lack vast human and computational resources. In this paper, we explore training and deploying an ASR system in the label-scarce, compute-limited setting. To reduce human labor, we use a third-party ASR system as a weak supervision source, supplemented with labeling functions derived from implicit user feedback. To accelerate inference, we propose to route production-time queries across a pool of CUDA graphs of varying input lengths, the distribution of which best matches the traffic's. Compared to our third-party ASR, we achieve a relative improvement in word-error rate of 8% and a speedup of 600%. Our system, called SpeechNet, currently serves 12 million queries per day on our voice-enabled smart television. To our knowledge, this is the first time a large-scale, Wav2vec-based deployment has been described in the academic literature.",89beaeacb16e0ee4bbd8fe38a6c3b86de0d3373a
Improving Precancerous Case Characterization via Transformer-based Ensemble Learning,"[{'name': 'Yizhen Zhong', 'dblp_profile': 'https://dblp.org/pid/200/4364.html'}, {'name': 'Jiajie Xiao', 'dblp_profile': 'https://dblp.org/pid/287/0404.html'}, {'name': 'Thomas Vetterli', 'dblp_profile': 'https://dblp.org/pid/99/2512.html'}, {'name': 'Mahan Matin', 'dblp_profile': 'https://dblp.org/pid/336/2644.html'}, {'name': 'Ellen Loo', 'dblp_profile': 'https://dblp.org/pid/336/1834.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Richard Bourgon', 'dblp_profile': 'https://dblp.org/pid/00/7066.html'}, {'name': 'Ofer Shapira', 'dblp_profile': 'https://dblp.org/pid/02/9059.html'}]",2022,"The application of natural language processing (NLP) to cancer pathology reports has been focused on detecting cancer cases, largely ignoring precancerous cases. Improving the characterization of precancerous adenomas assists in developing diagnostic tests for early cancer detection and prevention, especially for colorectal cancer (CRC). Here we developed transformer-based deep neural network NLP models to perform the CRC phenotyping, with the goal of extracting precancerous lesion attributes and distinguishing cancer and precancerous cases. We achieved 0.914 macro-F1 scores for classifying patients into negative, non-advanced adenoma, advanced adenoma and CRC. We further improved the performance to 0.923 using an ensemble of classifiers for cancer status classification and lesion size named entity recognition (NER). Our results demonstrated the potential of using NLP to leverage real-world health record data to facilitate the development of diagnostic tests for early cancer prevention.",9d06ef8ecab568b371727df9a47dff3b2e26fc61
Less is More: Parameter-Free Text Classification with Gzip,"[{'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Matthew Y. R. Yang', 'dblp_profile': 'https://dblp.org/pid/322/0271.html'}, {'name': 'Mikhail Tsirlin', 'dblp_profile': 'https://dblp.org/pid/336/6084.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Deep neural networks (DNNs) are often used for text classification tasks as they usually achieve high levels of accuracy. However, DNNs can be computationally intensive with billions of parameters and large amounts of labeled data, which can make them expensive to use, to optimize and to transfer to out-of-distribution (OOD) cases in practice. In this paper, we propose a non-parametric alternative to DNNs that's easy, light-weight and universal in text classification: a combination of a simple compressor like gzip with a $k$-nearest-neighbor classifier. Without any training, pre-training or fine-tuning, our method achieves results that are competitive with non-pretrained deep learning methods on six in-distributed datasets. It even outperforms BERT on all five OOD datasets, including four low-resource languages. Our method also performs particularly well in few-shot settings where labeled data are too scarce for DNNs to achieve a satisfying accuracy.",fddff6eabac1607d8cdf4f576254eaf9612e6e15
Precise Zero-Shot Dense Retrieval without Relevance Labels,"[{'name': 'Luyu Gao', 'dblp_profile': 'https://dblp.org/pid/213/8857.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jamie Callan', 'dblp_profile': 'https://dblp.org/pid/c/JamesPCallan.html'}]",2022,"While dense retrieval has been shown to be effective and efficient across tasks and languages, it remains difficult to create effective fully zero-shot dense retrieval systems when no relevance labels are available. In this paper, we recognize the difficulty of zero-shot learning and encoding relevance. Instead, we propose to pivot through Hypothetical Document Embeddings (HyDE). Given a query, HyDE first zero-shot prompts an instruction-following language model (e.g., InstructGPT) to generate a hypothetical document. The document captures relevance patterns but is “fake” and may contain hallucinations. Then, an unsupervised contrastively learned encoder (e.g., Contriever) encodes the document into an embedding vector. This vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This second step grounds the generated document to the actual corpus, with the encoder’s dense bottleneck filtering out the hallucinations. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense retriever Contriever and shows strong performance comparable to fine-tuned retrievers across various tasks (e.g. web search, QA, fact verification) and in non-English languages (e.g., sw, ko, ja, bn).",5c32c653735b43a0a8923ca65ac191bd4bf15311
Building a Culture of Reproducibility in Academic Research,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2022,"Reproducibility is an ideal that no researcher would dispute""in the abstract"", but when aspirations meet the cold hard reality of the academic grind, reproducibility often""loses out"". In this essay, I share some personal experiences grappling with how to operationalize reproducibility while balancing its demands against other priorities. My research group has had some success building a""culture of reproducibility""over the past few years, which I attempt to distill into lessons learned and actionable advice, organized around answering three questions: why, what, and how. I believe that reproducibility efforts should yield easy-to-use, well-packaged, and self-contained software artifacts that allow others to reproduce and generalize research findings. At the core, my approach centers on self interest: I argue that the primary beneficiaries of reproducibility efforts are, in fact, those making the investments. I believe that (unashamedly) appealing to self interest, augmented with expectations of reciprocity, increases the chances of success. Building from repeatability, social processes and standardized tools comprise the two important additional ingredients that help achieve aspirational ideals. The dogfood principle nicely ties these ideas together.",3963265da22df7dbecc29780cd9a457853078642
Fostering Community Engagement through Datathon Events: The Archives Unleashed Experience,"[{'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This article explores the impact that a series of Archives Unleashed datathon events have had on community engagement both within the web archiving field, and more specifically, on the professional practices of attendees. We present results from surveyed datathon participants, in addition to related evidence from our events, to discuss how our participants saw the datathons as dramatically impacting both their professional practices as well as the broader web archiving community. Drawing on and adapting two leading community engagement models, we combine them to introduce a new understanding of how to build and engage users in an open-source digital humanities project. Our model illustrates both the activities undertaken by our project as well as the related impact they have on the field. The model can be broadly applied to other digital humanities projects seeking to engage their communities.",e15b57e7cc156676f986abd3da442cea20812c6c
The proper care and feeding of CAMELS: How limited training data affects streamflow prediction,"[{'name': 'Martin Gauch', 'dblp_profile': 'https://dblp.org/pid/235/0335.html'}, {'name': 'Juliane Mai', 'dblp_profile': 'https://dblp.org/pid/227/4944.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,,4e1cdcb84fefaa0b813dd2b6baed6f13092ca46c
A proposed conceptual framework for a representational approach to information retrieval,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access ""technologies"" prior to the dawn of computing.",c107835a05ca6fda6e73b64e2ed9884de4fcec0f
Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term Importance Estimation and Neural Query Rewriting,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Ming-Feng Tsai', 'dblp_profile': 'https://dblp.org/pid/16/3313.html'}, {'name': 'Chuan-Ju Wang', 'dblp_profile': 'https://dblp.org/pid/03/5904.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Conversational search plays a vital role in conversational information seeking. As queries in information seeking dialogues are ambiguous for traditional ad hoc information retrieval (IR) systems due to the coreference and omission resolution problems inherent in natural language dialogue, resolving these ambiguities is crucial. In this article, we tackle conversational passage retrieval, an important component of conversational search, by addressing query ambiguities with query reformulation integrated into a multi-stage ad hoc IR system. Specifically, we propose two conversational query reformulation (CQR) methods: (1) term importance estimation and (2) neural query rewriting. For the former, we expand conversational queries using important terms extracted from the conversational context with frequency-based signals. For the latter, we reformulate conversational queries into natural, stand-alone, human-understandable queries with a pretrained sequence-to-sequence model. Detailed analyses of the two CQR methods are provided quantitatively and qualitatively, explaining their advantages, disadvantages, and distinct behaviors. Moreover, to leverage the strengths of both CQR methods, we propose combining their output with reciprocal rank fusion, yielding state-of-the-art retrieval effectiveness, 30% improvement in terms of NDCG@3 compared to the best submission of Text REtrieval Conference (TREC) Conversational Assistant Track (CAsT) 2019.",1973f251069086d850bde83fabfb05ceaf7b8859
Segatron: Segment-Aware Transformer for Language Modeling and Understanding,"[{'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Wen Gao', 'dblp_profile': 'https://dblp.org/pid/g/WenGao.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}]",2021,"Transformers are powerful for sequence modeling. Nearly all state-of-the-art language models and pre-trained language models are based on the Transformer architecture. However, it distinguishes sequential tokens only with the token position index. We hypothesize that better contextual representations can be generated from the Transformer with richer positional information. To verify this, we propose a segment-aware Transformer (Segatron), by replacing the original token position encoding with a combined position encoding of paragraph, sentence, and token. We first introduce the segment-aware mechanism to Transformer-XL, which is a popular Transformer-based language model with memory extension and relative position encoding. We find that our method can further improve the Transformer-XL base model and large model, achieving 17.1 perplexity on the WikiText-103 dataset. We further investigate the pre-training masked language modeling task with Segatron. Experimental results show that BERT pre-trained with Segatron (SegaBERT) can outperform BERT with vanilla Transformer on various NLP tasks, and outperforms RoBERTa on zero-shot sentence representation learning. Our code is available on GitHub.",320efa53dea3e8f836790682fbd4196132c49749
Semantics of the Unwritten: The Effect of End of Paragraph and Sequence Tokens on Text Generation with GPT2,"[{'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Wen Gao', 'dblp_profile': 'https://dblp.org/pid/g/WenGao.html'}, {'name': 'Jie Liu', 'dblp_profile': 'https://dblp.org/pid/03/2134.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}]",2021,"The semantics of a text is manifested not only by what is read but also by what is not read. In this article, we will study how those implicit “not read” information such as end-of-paragraph () and end-of-sequence () affect the quality of text generation. Specifically, we find that the pre-trained language model GPT2 can generate better continuations by learning to generate the in the fine-tuning stage. Experimental results on English story generation show that can lead to higher BLEU scores and lower perplexity. We also conduct experiments on a self-collected Chinese essay dataset with Chinese-GPT2, a character level LM without and during pre-training. Experimental results show that the Chinese GPT2 can generate better essay endings with .",188c4b9a13bf98e0c8818d43baf75b1930342912
Exploring Listwise Evidence Reasoning with T5 for Fact Verification,"[{'name': 'Kelvin Jiang', 'dblp_profile': 'https://dblp.org/pid/219/7297.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This work explores a framework for fact verification that leverages pretrained sequence-to-sequence transformer models for sentence selection and label prediction, two key sub-tasks in fact verification. Most notably, improving on previous pointwise aggregation approaches for label prediction, we take advantage of T5 using a listwise approach coupled with data augmentation. With this enhancement, we observe that our label prediction stage is more robust to noise and capable of verifying complex claims by jointly reasoning over multiple pieces of evidence. Experimental results on the FEVER task show that our system attains a FEVER score of 75.87% on the blind test set. This puts our approach atop the competitive FEVER leaderboard at the time of our work, scoring higher than the second place submission by almost two points in label accuracy and over one point in FEVER score.",0c1d53fc87ca482037360b3547111158b505b26e
The Art of Abstention: Selective Prediction and Error Regularization for Natural Language Processing,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy–efficiency trade-offs. Source code for this paper can be found at https://github.com/castorini/transformers-selective.",122ed3c2e45badc1292422d7e9a5f3a43c402128
Scientific Claim Verification with VerT5erini,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our system outperforms a strong baseline in each of the three sub-tasks. We further show VerT5erini’s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus.",e4cb6bfe88a8ed729d34d5a9ff74a992932b70ce
How Does BERT Rerank Passages? An Attribution Analysis with Information Bottlenecks,"[{'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Fine-tuned pre-trained transformers achieve the state of the art in passage reranking. Unfortunately, how they make their predictions remains vastly unexplained, especially at the end-to-end, input-to-output level. Little known is how tokens, layers, and passages precisely contribute to the final prediction. In this paper, we address this gap by leveraging the recently developed information bottlenecks for attribution (IBA) framework. On BERT-based models for passage reranking, we quantitatively demonstrate the framework’s veracity in extracting attribution maps, from which we perform detailed, token-wise analysis about how predictions are made. Overall, we find that BERT still cares about exact token matching for reranking; the [CLS] token mainly gathers information for predictions at the last layer; top-ranked passages are robust to token removal; and BERT fine-tuned on MSMARCO has positional bias towards the start of the passage.",27b7fc608e66298a0a92776d986bca61b521efa0
Approach Zero and Anserini at the CLEF-2021 ARQMath Track: Applying Substructure Search and BM25 on Operator Tree Path Tokens,"[{'name': 'Wei Zhong', 'dblp_profile': 'https://dblp.org/pid/60/1416.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Richard Zanibbi', 'dblp_profile': 'https://dblp.org/pid/34/3480.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This paper reports on substructure-aware math search system Approach Zero that is applied to our submission for ARQMath lab at CLEF 2021. We have participated in both Task 1 (math ARQ) and Task 2 (formula retrieval) this year. In addition to substructure retrieval, we have added a traditional full-text search pass based on the Anserini toolkit [1]. We use the same path features extracted from Operator Tree (OPT) to index and retrieve math formulas in Anserini, and we interpolate Anserini results with structural results from Approach Zero. Automatic and table-based keyword expansion methods for math formulas have also been explored. Additionally, we report preliminary results from using previous years’ labels and applying learning to rank for our first-stage search results. In this lab, we obtain the most effective search results in Task 2 (formula retrieval) among submissions from 7 participants including the baseline system. Our experiments have also shown a great improvement over the baseline result we produced from previous year.",ca4034f6b3ccf5f4b990ccfb4975768525bdea5e
Serverless BM25 Search and BERT Reranking,"[{'name': 'Mayank Anand', 'dblp_profile': 'https://dblp.org/pid/302/5128.html'}, {'name': 'Jiarui Zhang', 'dblp_profile': 'https://dblp.org/pid/194/0368.html'}, {'name': 'Shane Ding', 'dblp_profile': 'https://dblp.org/pid/281/1500.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"The retrieve–rerank pipeline is a well-established architecture for search applications, typically with first-stage retrieval using keyword search followed by reranking with a transformer-based model. In deploying such an architecture in the cloud, developers must devote considerable effort to resource provisioning and management: typically, the goal is to optimize the infrastructure configuration (number and type of server instance) to achieve certain performance characteristics (latency, throughput, etc.) while reducing operating costs. In this paper, we introduce a serverless prototype of the retrieve–rerank pipeline for search using Amazon Web Services (AWS), comprised of BM25 for first-stage retrieval using Lucene followed by reranking with the monoBERT model using Hugging Face Transformers. The advantage of a serverless design is that a cloud provider shoulders the burden of operational management, for example, allocating server instances and scaling with query load. We experimentally show with the popular MS MARCO passage ranking test collection that compared to a traditional server-based deployment, our serverless implementation (1) retains the same level of effectiveness, (2) can reduce average latency by exploiting massive parallelism, and (3) incurs comparable costs if the service is expected to be idle for some fraction of the time. Our implementation is open-sourced at https://github.com/castorini/serverless-bert-reranking.",45f2e6f7cccb446f0e9bd94bfed41c3edcb2dc43
On the Separation of Logical and Physical Ranking Models for Text Retrieval Applications,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Joel Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'Antonio Mallia', 'dblp_profile': 'https://dblp.org/pid/204/0179.html'}]",2021,"Text retrieval using bags of words is typically formulated as inner products between vector representations of queries and documents, realized in query evaluation algorithms that traverse postings in an inverted index. Viewed in database terms, this captures a tight coupling between the “logical” aspects of ranking (i",6096f3be4b39ffb09a2ba3042b1d4cb6a18c9cdf
Rescuing historical climate observations to support hydrological research: a case study of solar radiation data,"[{'name': 'Ogundepo Odunayo', 'dblp_profile': 'https://dblp.org/pid/299/8249.html'}, {'name': 'Naveela N. Sookoo', 'dblp_profile': 'https://dblp.org/pid/299/8384.html'}, {'name': 'Gautam Bathla', 'dblp_profile': 'https://dblp.org/pid/299/8455.html'}, {'name': 'Anthony Cavallin', 'dblp_profile': 'https://dblp.org/pid/299/8371.html'}, {'name': 'Bhaleka D. Persaud', 'dblp_profile': 'https://dblp.org/pid/299/8227.html'}, {'name': 'Kathy Szigeti', 'dblp_profile': 'https://dblp.org/pid/299/8439.html'}, {'name': 'Philippe Van Cappellen', 'dblp_profile': 'https://dblp.org/pid/299/8125.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"The acceleration of climate change and its impact highlight the need for long-term reliable climate data at high spatiotemporal resolution to answer key science questions in cold regions hydrology. Prior to the digital age, climate records were archived on paper. For example, from the 1950s to the 1990s, solar radiation data from recording stations worldwide were published in booklets by the former Union of Soviet Socialist Republics (USSR) Hydrometeorological Service. As a result, the data are not easily accessible by most researchers. The overarching aim of this research is to develop techniques to convert paper-based climate records into a machine-readable format to support environmental research in cold regions. This study compares the performance of a proprietary optical character recognition (OCR) service with an open-source OCR tool for digitizing hydrometeorological data. We built a digitization pipeline combining different image preprocessing techniques, semantic segmentation, and an open-source OCR engine for extracting data and metadata recorded in the scanned documents. Each page contains blocks of text with station names and tables containing the climate data. The process begins with image preprocessing to reduce noise and to improve quality before the page content is segmented to detect tables and finally run through an OCR engine for text extraction. We outline the digitization process and report on initial results, including different segmentation approaches, preprocessing image algorithms, and OCR techniques to ensure accurate extraction and organization of relevant metadata from thousands of scanned climate records. We evaluated the performance of Tesseract OCR and ABBYY FineReader on text extraction. We find that although ABBY FineReader has better accuracy on the sample data, our custom extraction pipeline using Tesseract is efficient and scalable because it is flexible and allows for more customization.",c96ebc1238245835960d768d1adb773b6b58edc5
BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"The slow speed of BERT has motivated much research on accelerating its inference, and the early exiting idea has been proposed to make trade-offs between model quality and efficiency. This paper aims to address two weaknesses of previous work: (1) existing fine-tuning strategies for early exiting models fail to take full advantage of BERT; (2) methods to make exiting decisions are limited to classification tasks. We propose a more advanced fine-tuning strategy and a learning-to-exit module that extends early exiting to tasks other than classification. Experiments demonstrate improved early exiting for BERT, with better trade-offs obtained by the proposed fine-tuning strategy, successful application to regression tasks, and the possibility to combine it with other acceleration methods. Source code can be found at https://github.com/castorini/berxit.",7b37c0a4976c4d2a5a440d494fbb0f3daede2a00
Don't Change Me! User-Controllable Selective Paraphrase Generation,"[{'name': 'Mohan Zhang', 'dblp_profile': 'https://dblp.org/pid/218/0654.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Zihang Fu', 'dblp_profile': 'https://dblp.org/pid/258/1213.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Zhengkai Tu', 'dblp_profile': 'https://dblp.org/pid/258/1305.html'}]",2021,,
Comparing Score Aggregation Approaches for Document Retrieval with Pretrained Transformers,"[{'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,,8b84b9b1fd3ac5822dada565d48bcb159f493c04
Learning to Rank in the Age of Muppets: Effectiveness-Efficiency Tradeoffs in Multi-Stage Ranking,"[{'name': 'Yue Zhang', 'dblp_profile': 'https://dblp.org/pid/47/722.html'}, {'name': 'Chengcheng Hu', 'dblp_profile': 'https://dblp.org/pid/56/266.html'}, {'name': 'Yuqi Liu', 'dblp_profile': 'https://dblp.org/pid/35/9071.html'}, {'name': 'Hui Fang', 'dblp_profile': 'https://dblp.org/pid/03/2511.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,,
Multi-Task Dense Retrieval via Model Uncertainty Fusion for Open-Domain Question Answering,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/181/2821.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Multi-task dense retrieval models can be used to retrieve documents from a common corpus (e.g., Wikipedia) for different open-domain question-answering (QA) tasks. However, Karpukhin et al. (2020) shows that jointly learning different QA tasks with one dense model is not always beneﬁcial due to corpus inconsistency. For example, SQuAD only focuses on a small set of Wikipedia articles while datasets like NQ and Trivia cover more entries, and joint training on their union can cause performance degradation. To solve this problem, we propose to train individual dense passage retrievers (DPR) for different tasks and aggregate their predictions during test time, where we use uncertainty estimation as weights to in-dicate how probable a speciﬁc query belongs to each expert’s expertise. Our method reaches state-of-the-art performance on 5 benchmark QA datasets, with up to 10% improvement in top-100 accuracy compared to a joint-training multi-task DPR on SQuAD. We also show that our method handles corpus inconsistency better than the joint-training DPR on a mixed subset of different QA datasets. Code and data are available at https://github.com/ alexlimh/DPR_MUF .",e47d338ca879a184c1977ffa8623d2a225b0a319
Voice Query Auto Completion,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Karun Kumar', 'dblp_profile': 'https://dblp.org/pid/305/6007.html'}, {'name': 'Kendra Chalkley', 'dblp_profile': 'https://dblp.org/pid/305/5915.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Liming Zhang', 'dblp_profile': 'https://dblp.org/pid/66/6011.html'}, {'name': 'Wenyan Li', 'dblp_profile': 'https://dblp.org/pid/21/6731.html'}, {'name': 'Gefei Yang', 'dblp_profile': 'https://dblp.org/pid/229/5810.html'}, {'name': 'Yajie Mao', 'dblp_profile': 'https://dblp.org/pid/232/2455.html'}, {'name': 'Junho Shin', 'dblp_profile': 'https://dblp.org/pid/09/7989.html'}, {'name': 'Geoffrey Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Query auto completion (QAC) is the task of predicting a search engine user’s final query from their intermediate, incomplete query. In this paper, we extend QAC to the streaming voice search setting, where automatic speech recognition systems produce intermediate transcriptions as users speak. Naively applying existing methods fails because the intermediate transcriptions often don’t form prefixes or even substrings of the final transcription. To address this issue, we propose to condition QAC approaches on intermediate transcriptions to complete voice queries. We evaluate our models on a speech-enabled smart television with real-life voice search traffic, finding that this ASR-aware conditioning improves the completion quality. Our best method obtains an 18% relative improvement in mean reciprocal rank over previous methods.",6b6e13bd552d1ad640b3fca34049559dc2b3a560
Contextualized Query Embeddings for Conversational Search,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval modules. Despite its effectiveness, such a pipeline often includes multiple neural models that require long inference times. In addition, independently optimizing each module ignores dependencies among them. To address these shortcomings, we propose to integrate conversational query reformulation directly into a dense retrieval model. To aid in this goal, we create a dataset with pseudo-relevance labels for conversational search to overcome the lack of training data and to explore different training strategies. We demonstrate that our model effectively rewrites conversational queries as dense representations in conversational search and open-domain question answering datasets. Finally, after observing that our model learns to adjust the L2 norm of query token embeddings, we leverage this property for hybrid retrieval and to support error analysis.",dd0e36831fa19da2e56fa925397407961a506bb6
Simple and Effective Unsupervised Redundancy Elimination to Compress Dense Vectors for Passage Retrieval,"[{'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Kai Sun', 'dblp_profile': 'https://dblp.org/pid/09/1171.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Recent work has shown that dense passage retrieval techniques achieve better ranking accuracy in open-domain question answering compared to sparse retrieval techniques such as BM25, but at the cost of large space and memory requirements. In this paper, we analyze the redundancy present in encoded dense vectors and show that the default dimension of 768 is unnecessarily large. To improve space efficiency, we propose a simple unsupervised compression pipeline that consists of principal component analysis (PCA), product quantization, and hybrid search. We further investigate other supervised baselines and find surprisingly that unsupervised PCA outperforms them in some settings. We perform extensive experiments on five question answering datasets and demonstrate that our best pipeline achieves good accuracy–space trade-offs, for example, 48\times compression with less than 3% drop in top-100 retrieval accuracy on average or 96\times compression with less than 4% drop. Code and data are available at http://pyserini.io/.",93ddb1188e4c6197a49fe261686e676cdf466623
Unsupervised Chunking as Syntactic Structure Induction with a Knowledge-Transfer Approach,"[{'name': 'Anup Anand Deshmukh', 'dblp_profile': 'https://dblp.org/pid/233/1236.html'}, {'name': 'Qianqiu Zhang', 'dblp_profile': 'https://dblp.org/pid/305/9721.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/181/2821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Lili Mou', 'dblp_profile': 'https://dblp.org/pid/127/0779.html'}]",2021,"In this paper, we address unsupervised chunking as a new task of syntactic structure induction, which is helpful for understanding the linguistic structures of human languages as well as processing low-resource languages. We propose a knowledge-transfer approach that heuristically induces chunk labels from state-of-the-art unsupervised parsing models; a hierarchical recurrent neural network (HRNN) learns from such induced chunk labels to smooth out the noise of the heuristics. Experiments show that our approach largely bridges the gap between supervised and unsupervised chunking. 1",60af7ea858c52df04bdb5d8c874c0549a83e1105
The Simplest Thing That Can Possibly Work: (Pseudo-)Relevance Feedback via Text Classification,"[{'name': 'Xiao Han', 'dblp_profile': 'https://dblp.org/pid/01/2095.html'}, {'name': 'Yuqi Liu', 'dblp_profile': 'https://dblp.org/pid/35/9071.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Motivated by recent commentary that has questioned today's pursuit of ever-more complex models and mathematical formalisms in applied machine learning and whether meaningful empirical progress is actually being made, this paper tackles the decades-old problem of pseudo-relevance feedback with ""the simplest thing that can possibly work"". We present a technique based on training a document relevance classifier for each information need using pseudo-labels from an initial ranked list and then applying the classifier to rerank the retrieved documents. Experiments demonstrate significant improvements across a number of standard newswire collections, with initial rankings supplied by bag-of-words BM25 as well as from query expansion. Further evaluations in the TREC-COVID challenge using human relevance judgments verify the effectiveness and robustness of our proposed technique. While this simple idea draws elements from several well-known threads in the literature, to our knowledge this exact combination has not previously been proposed and rigorously evaluated.",7e8a05e4ce38dffc4e7e3134c6135350133c7183
In-Batch Negatives for Knowledge Distillation with Tightly-Coupled Teachers for Dense Retrieval,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"We present an efficient training approach to text retrieval with dense representations that applies knowledge distillation using the ColBERT late-interaction ranking model. Specifically, we propose to transfer the knowledge from a bi-encoder teacher to a student by distilling knowledge from ColBERT’s expressive MaxSim operator into a simple dot product. The advantage of the bi-encoder teacher–student setup is that we can efficiently add in-batch negatives during knowledge distillation, enabling richer interactions between teacher and student models. In addition, using ColBERT as the teacher reduces training cost compared to a full cross-encoder. Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019 Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and efficiently.",2c74e7258e7c89ca2da1784fb0cb4106a23ac382
Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling,"[{'name': 'Sebastian Hofstätter', 'dblp_profile': 'https://dblp.org/pid/238/6322.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Allan Hanbury', 'dblp_profile': 'https://dblp.org/pid/55/6683.html'}]",2021,"A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",4deed74a3eee7e629dce2b8ef1e437ca74b2e64a
MS MARCO: Benchmarking Ranking Models in the Large-Data Regime,"[{'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Bhaskar Mitra', 'dblp_profile': 'https://dblp.org/pid/147/9120-1.html'}, {'name': 'Emine Yilmaz', 'dblp_profile': 'https://dblp.org/pid/36/3270.html'}, {'name': 'Daniel Campos', 'dblp_profile': 'https://dblp.org/pid/64/1314.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Evaluation efforts such as TREC, CLEF, NTCIR and FIRE, alongside public leaderboard such as MS MARCO, are intended to encourage research and track our progress, addressing big questions in our field. However, the goal is not simply to identify which run is ""best"", achieving the top score. The goal is to move the field forward by developing new robust techniques, that work in many different settings, and are adopted in research and practice. This paper uses the MS MARCO and TREC Deep Learning Track as our case study, comparing it to the case of TREC ad hoc ranking in the 1990s. We show how the design of the evaluation effort can encourage or discourage certain outcomes, and raising questions about internal and external validity of results. We provide some analysis of certain pitfalls, and a statement of best practices for avoiding such pitfalls. We summarize the progress of the effort so far, and describe our desired end state of ""robust usefulness"", along with steps that might be required to get us there.",591cb4d880dbebcc934623738f0a6f4e92fdc40f
Vera: Prediction Techniques for Reducing Harmful Misinformation in Consumer Health Search,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"The COVID-19 pandemic has brought about a proliferation of harmful news articles online, with sources lacking credibility and misrepresenting scientific facts. Misinformation has real consequences for consumer health search, i.e., users searching for health information. In the context of multi-stage ranking architectures, there has been little work exploring whether they prioritize correct and credible information over misinformation. We find that, indeed, training models on standard relevance ranking datasets like MS MARCO passage---which have been curated to contain mostly credible information---yields models that might also promote harmful misinformation. To rectify this, we propose a label prediction technique that can separate helpful from harmful content. Our design leverages pretrained sequence-to-sequence transformer models for both relevance ranking and label prediction. Evaluated at the TREC 2020 Health Misinformation Track, our techniques represent the top-ranked system: Our best submitted run was 19.2 points higher than the second-best run based on the primary metric, a 68% relative improvement. Additional post-hoc experiments show that we can boost effectiveness by another 3.5 points.",7a4c0523dbb99e2b437ab8e222420e193a517686
Significant Improvements over the State of the Art? A Case Study of the MS MARCO Document Ranking Leaderboard,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Daniel Campos', 'dblp_profile': 'https://dblp.org/pid/64/1314.html'}, {'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Bhaskar Mitra', 'dblp_profile': 'https://dblp.org/pid/147/9120-1.html'}, {'name': 'Emine Yilmaz', 'dblp_profile': 'https://dblp.org/pid/36/3270.html'}]",2021,"Leaderboards are a ubiquitous part of modern research in applied machine learning. By design, they sort entries into some linear order, where the top-scoring entry is recognized as the ""state of the art"" (SOTA). Due to the rapid progress being made today, particularly with neural models, the top entry in a leaderboard is replaced with some regularity. These are touted as improvements in the state of the art. Such pronouncements, however, are almost never qualified with significance testing. In the context of the MS MARCO document ranking leaderboard, we pose a specific question: How do we know if a run is significantly better than the current SOTA? Against the backdrop of recent IR debates on scale types, our study proposes an evaluation framework that explicitly treats certain outcomes as distinct and avoids aggregating them into a single-point metric. Empirical analysis of SOTA runs from the MS MARCO document ranking leaderboard reveals insights about how one run can be ""significantly better"" than another that are obscured by the current official evaluation metric (MRR@100).",6ee287800c2aecef59d2b1d98c0ebfcab4294f53
Pyserini: A Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}]",2021,"Pyserini is a Python toolkit for reproducible information retrieval research with sparse and dense representations. It aims to provide effective, reproducible, and easy-to-use first-stage retrieval in a multi-stage ranking architecture. Our toolkit is self-contained as a standard Python package and comes with queries, relevance judgments, pre-built indexes, and evaluation scripts for many commonly used IR test collections. We aim to support, out of the box, the entire research lifecycle of efforts aimed at improving ranking with modern neural approaches. In particular, Pyserini supports sparse retrieval (e.g., BM25 scoring using bag-of-words representations), dense retrieval (e.g., nearest-neighbor search on transformer-encoded representations), as well as hybrid retrieval that integrates both approaches. This paper provides an overview of toolkit features and presents empirical results that illustrate its effectiveness on two popular ranking tasks. Around this toolkit, our group has built a culture of reproducibility through shared norms and tools that enable rigorous automated testing.",d69c0ed04ecc852e8c921900d3e7967f74f81263
Chatty Goose: A Python Framework for Conversational Search,"[{'name': 'Edwin Zhang', 'dblp_profile': 'https://dblp.org/pid/262/6436.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Chatty Goose is an open-source Python conversational search framework that provides strong, reproducible reranking pipelines built on recent advances in neural models. The framework comprises extensible modular components that integrate with popular libraries such as Transformers by HuggingFace and ParlAI by Facebook. Our aim is to lower the barrier of entry for research in conversational search by providing reproducible baselines that researchers can build on top of. We provide an overview of the framework and demonstrate how to instantiate a new system from scratch. Chatty Goose incorporates improvements to components that we introduced in the TREC 2019 Conversational Assistance Track (CAsT), where our submission represented the top-performing system. Using our framework, a comparable run can be reproduced with just a few lines of code.",007a06e9674f58d496ef36ef54a2fd1363299c08
PYA0: A Python Toolkit for Accessible Math-Aware Search,"[{'name': 'Wei Zhong', 'dblp_profile': 'https://dblp.org/pid/60/1416.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Mathematical Information Retrieval (MIR) has been actively studied in recent years and many fruitful results have emerged. Among those, the Approach Zero system is one of the few math-aware search engines that is able to perform substructure matching efficiently. Furthermore, it has been deployed in ARQMath2020, the most recent community-wide MIR evaluation, as a strong baseline due to its empirical effectiveness and ability to handle structured math content. However, in order to implement a retrieval model that handles structured queries efficiently, Approach Zero is written in C from the ground up, requiring special pipelines for processing math content and queries. Thus, the system is not conveniently accessible and reusable to the community as a research tool. In this paper, we present PyA0, an easy-to-use Python toolkit built on Approach Zero that improves its accessibility to researchers. We introduce the toolkit interface and report evaluation results on popular MIR datasets to demonstrate the effectiveness and efficiency of our toolkit. We have made PyA0 source code publicly accessible at https://github.com/approach0/pya0, which includes a link to a notebook demo.",398c7443dfb39319632d0766f6facb2ee2783e71
Pretrained Transformers for Text Ranking: BERT and Beyond,"[{'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This tutorial, based on a forthcoming book, provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. We provide a synthesis of existing work as a single point of entry for both researchers and practitioners. Our coverage is grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. Two themes pervade our treatment: techniques for handling long documents and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of their application are well understood. Nevertheless, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, we also attempt to prognosticate the future.",88b0eea7050c3322162c0cf18b2ccb014c4d17ad
Overview of the TREC 2021 Deep Learning Track,"[{'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Bhaskar Mitra', 'dblp_profile': 'https://dblp.org/pid/147/9120-1.html'}, {'name': 'Emine Yilmaz', 'dblp_profile': 'https://dblp.org/pid/36/3270.html'}, {'name': 'Daniel Campos', 'dblp_profile': 'https://dblp.org/pid/64/1314.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This is the third year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we refreshed both the document and the passage collections which also led to a nearly four times increase in the document collection size and nearly 16 times increase in the size of the passage collection. Deep neural ranking models that employ large scale pretraininig continued to outperform traditional retrieval methods this year. We also found that single stage retrieval can achieve good performance on both tasks although they still do not perform at par with multistage retrieval pipelines. Finally, the increase in the collection size and the general data refresh raised some questions about completeness of NIST judgments and the quality of the training labels that were mapped to the new collections from the old ones which we discuss in this report. The best “nnlm” run outperforms the best “trad run” on 41 out of 57 ( 72% ) queries for the document ranking task—a drop-off from 84% as in the previous two years. For the passage ranking task, the best “nnlm” run wins on 47 out of 53 ( 89% ) queries against the best “trad” run, which is marginally higher than 84% in 2019 and 88% in 2020.",c3df8f8b7f1ed68f0ffeef6afcf60039f955b397
Pretrained Transformers for Text Ranking: BERT and Beyond,"[{'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This tutorial, based on a forthcoming book, provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. We provide a synthesis of existing work as a single point of entry for both researchers and practitioners. Our coverage is grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. Two themes pervade our treatment: techniques for handling long documents and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of their application are well understood. Nevertheless, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, we also attempt to prognosticate the future.",88b0eea7050c3322162c0cf18b2ccb014c4d17ad
The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"We propose a design pattern for tackling text ranking problems, dubbed""Expando-Mono-Duo"", that has been empirically validated for a number of ad hoc retrieval tasks in different domains. At the core, our design relies on pretrained sequence-to-sequence models within a standard multi-stage ranking architecture.""Expando""refers to the use of document expansion techniques to enrich keyword representations of texts prior to inverted indexing.""Mono""and""Duo""refer to components in a reranking pipeline based on a pointwise model and a pairwise model that rerank initial candidates retrieved using keyword search. We present experimental results from the MS MARCO passage and document ranking tasks, the TREC 2020 Deep Learning Track, and the TREC-COVID challenge that validate our design. In all these tasks, we achieve effectiveness that is at or near the state of the art, in some cases using a zero-shot approach that does not exploit any training data from the target task. To support replicability, implementations of our design pattern are open-sourced in the Pyserini IR toolkit and PyGaggle neural reranking library.",e08eed9608382beea1febca49119c665fbabd031
Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representations,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}]",2021,"Pyserini is an easy-to-use Python toolkit that supports replicable IR research by providing effective first-stage retrieval in a multi-stage ranking architecture. Our toolkit is self-contained as a standard Python package and comes with queries, relevance judgments, pre-built indexes, and evaluation scripts for many commonly used IR test collections. We aim to support, out of the box, the entire research lifecycle of efforts aimed at improving ranking with modern neural approaches. In particular, Pyserini supports sparse retrieval (e.g., BM25 scoring using bag-of-words representations), dense retrieval (e.g., nearest-neighbor search on transformer-encoded representations), as well as hybrid retrieval that integrates both approaches. This paper provides an overview of toolkit features and presents empirical results that illustrate its effectiveness on two popular ranking tasks. We also describe how our group has built a culture of replicability through shared norms and tools that enable rigorous automated testing.",5ac627f229fa8d54f5ad43f7f99e9b29d93ada29
Significant Improvements over the State of the Art? A Case Study of the MS MARCO Document Ranking Leaderboard,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Daniel Campos', 'dblp_profile': 'https://dblp.org/pid/64/1314.html'}, {'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Bhaskar Mitra', 'dblp_profile': 'https://dblp.org/pid/147/9120-1.html'}, {'name': 'Emine Yilmaz', 'dblp_profile': 'https://dblp.org/pid/36/3270.html'}]",2021,"Leaderboards are a ubiquitous part of modern research in applied machine learning. By design, they sort entries into some linear order, where the top-scoring entry is recognized as the ""state of the art"" (SOTA). Due to the rapid progress being made today, particularly with neural models, the top entry in a leaderboard is replaced with some regularity. These are touted as improvements in the state of the art. Such pronouncements, however, are almost never qualified with significance testing. In the context of the MS MARCO document ranking leaderboard, we pose a specific question: How do we know if a run is significantly better than the current SOTA? Against the backdrop of recent IR debates on scale types, our study proposes an evaluation framework that explicitly treats certain outcomes as distinct and avoids aggregating them into a single-point metric. Empirical analysis of SOTA runs from the MS MARCO document ranking leaderboard reveals insights about how one run can be ""significantly better"" than another that are obscured by the current official evaluation metric (MRR@100).",6ee287800c2aecef59d2b1d98c0ebfcab4294f53
Investigating the Limitations of the Transformers with Simple Arithmetic Tasks,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"The ability to perform arithmetic tasks is a remarkable trait of human intelligence and might form a critical component of more complex reasoning tasks. In this work, we investigate if the surface form of a number has any influence on how sequence-to-sequence language models learn simple arithmetic tasks such as addition and subtraction across a wide range of values. We find that how a number is represented in its surface form has a strong influence on the model's accuracy. In particular, the model fails to learn addition of five-digit numbers when using subwords (e.g.,""32""), and it struggles to learn with character-level representations (e.g.,""3 2""). By introducing position tokens (e.g.,""3 10e1 2""), the model learns to accurately add and subtract numbers up to 60 digits. We conclude that modern pretrained language models can easily learn arithmetic from very few examples, as long as we use the proper surface representation. This result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement. Moreover, we show that regardless of the number of parameters and training examples, models cannot learn addition rules that are independent of the length of the numbers seen during training. Code to reproduce our experiments is available at https://github.com/castorini/transformers-arithmetic",2cc3ab9fa41ba2804e301f7eae9598636e62422a
A Replication Study of Dense Passage Retriever,"[{'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Kai Sun', 'dblp_profile': 'https://dblp.org/pid/09/1171.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Text retrieval using learned dense representations has recently emerged as a promising alternative to""traditional""text retrieval using sparse bag-of-words representations. One recent work that has garnered much attention is the dense passage retriever (DPR) technique proposed by Karpukhin et al. (2020) for end-to-end open-domain question answering. We present a replication study of this work, starting with model checkpoints provided by the authors, but otherwise from an independent implementation in our group's Pyserini IR toolkit and PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that contribute to a better understanding of DPR: First, it appears that the original authors under-report the effectiveness of the BM25 baseline and hence also dense--sparse hybrid retrieval results. Second, by incorporating evidence from the retriever and an improved answer span scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the original work.",89a19523b0cfb587d272b9ceb950c7bc4e8e221e
Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling,"[{'name': 'Sebastian Hofstätter', 'dblp_profile': 'https://dblp.org/pid/238/6322.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Allan Hanbury', 'dblp_profile': 'https://dblp.org/pid/55/6683.html'}]",2021,"A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further.",4deed74a3eee7e629dce2b8ef1e437ca74b2e64a
Contextualized Query Embeddings for Conversational Search,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval modules. Despite its effectiveness, such a pipeline often includes multiple neural models that require long inference times. In addition, independently optimizing each module ignores dependencies among them. To address these shortcomings, we propose to integrate conversational query reformulation directly into a dense retrieval model. To aid in this goal, we create a dataset with pseudo-relevance labels for conversational search to overcome the lack of training data and to explore different training strategies. We demonstrate that our model effectively rewrites conversational queries as dense representations in conversational search and open-domain question answering datasets. Finally, after observing that our model learns to adjust the L2 norm of query token embeddings, we leverage this property for hybrid retrieval and to support error analysis.",dd0e36831fa19da2e56fa925397407961a506bb6
MS MARCO: Benchmarking Ranking Models in the Large-Data Regime,"[{'name': 'Nick Craswell', 'dblp_profile': 'https://dblp.org/pid/c/NickCraswell.html'}, {'name': 'Bhaskar Mitra', 'dblp_profile': 'https://dblp.org/pid/147/9120-1.html'}, {'name': 'Emine Yilmaz', 'dblp_profile': 'https://dblp.org/pid/36/3270.html'}, {'name': 'Daniel Campos', 'dblp_profile': 'https://dblp.org/pid/64/1314.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Evaluation efforts such as TREC, CLEF, NTCIR and FIRE, alongside public leaderboard such as MS MARCO, are intended to encourage research and track our progress, addressing big questions in our field. However, the goal is not simply to identify which run is ""best"", achieving the top score. The goal is to move the field forward by developing new robust techniques, that work in many different settings, and are adopted in research and practice. This paper uses the MS MARCO and TREC Deep Learning Track as our case study, comparing it to the case of TREC ad hoc ranking in the 1990s. We show how the design of the evaluation effort can encourage or discourage certain outcomes, and raising questions about internal and external validity of results. We provide some analysis of certain pitfalls, and a statement of best practices for avoiding such pitfalls. We summarize the progress of the effort so far, and describe our desired end state of ""robust usefulness"", along with steps that might be required to get us there.",591cb4d880dbebcc934623738f0a6f4e92fdc40f
"A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for Information Retrieval Techniques","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}]",2021,"Recent developments in representational learning for information retrieval can be organized in a conceptual framework that establishes two pairs of contrasts: sparse vs. dense representations and unsupervised vs. learned representations. Sparse learned representations can further be decomposed into expansion and term weighting components. This framework allows us to understand the relationship between recently proposed techniques such as DPR, ANCE, DeepCT, DeepImpact, and COIL, and furthermore, gaps revealed by our analysis point to""low hanging fruit""in terms of techniques that have yet to be explored. We present a novel technique dubbed""uniCOIL"", a simple extension of COIL that achieves to our knowledge the current state-of-the-art in sparse retrieval on the popular MS MARCO passage ranking dataset. Our implementation using the Anserini IR toolkit is built on the Lucene search library and thus fully compatible with standard inverted indexes.",89d373d61c68465fd49da1257aa959e5abefd155
Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval,"[{'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"We present Mr. TyDi, a multi-lingual benchmark dataset for mono-lingual retrieval in eleven typologically diverse languages, designed to evaluate ranking with learned dense representations. The goal of this resource is to spur research in dense retrieval techniques in non-English languages, motivated by recent observations that existing techniques for representation learning perform poorly when applied to out-of-distribution data. As a starting point, we provide zero-shot baselines for this new dataset based on a multi-lingual adaptation of DPR that we call “mDPR”. Experiments show that although the effectiveness of mDPR is much lower than BM25, dense representations nevertheless appear to provide valuable relevance signals, improving BM25 results in sparse–dense hybrids. In addition to analyses of our results, we also discuss future challenges and present a research agenda in multi-lingual dense retrieval. Mr. TyDi can be downloaded at https://github.com/castorini/mr.tydi.",1b09222cfe10f11c4cb0b18a9727d2baf6b991ac
Cross-Lingual Training with Dense Retrieval for Document Retrieval,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Rui Zhang', 'dblp_profile': 'https://dblp.org/pid/60/2536-37.html'}, {'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Dense retrieval has shown great success in passage ranking in English. However, its effectiveness in document retrieval for non-English languages remains unexplored due to the limitation in training resources. In this work, we explore different transfer techniques for document ranking from English annotations to multiple non-English languages. Our experiments on the test collections in six languages (Chinese, Arabic, French, Hindi, Bengali, Spanish) from diverse language families reveal that zero-shot model-based transfer using mBERT improves the search quality in non-English mono-lingual retrieval. Also, we find that weakly-supervised target language transfer yields competitive performances against the generation-based target language transfer that requires external translators and query generators.",66ee9661ebcdbd0a171c9ee1fc17c26778789c67
A Proposed Conceptual Framework for a Representational Approach to Information Retrieval,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"This paper outlines a conceptual framework for understanding recent developments in information retrieval and natural language processing that attempts to integrate dense and sparse retrieval methods. I propose a representational approach that breaks the core text retrieval problem into a logical scoring model and a physical retrieval model. The scoring model is defined in terms of encoders, which map queries and documents into a representational space, and a comparison function that computes query-document scores. The physical retrieval model defines how a system produces the top-k scoring documents from an arbitrarily large corpus with respect to a query. The scoring model can be further analyzed along two dimensions: dense vs. sparse representations and supervised (learned) vs. unsupervised approaches. I show that many recently proposed retrieval methods, including multi-stage ranking designs, can be seen as different parameterizations in this framework, and that a unified view suggests a number of open research questions, providing a roadmap for future work. As a bonus, this conceptual framework establishes connections to sentence similarity tasks in natural language processing and information access ""technologies"" prior to the dawn of computing.",c107835a05ca6fda6e73b64e2ed9884de4fcec0f
Encoder Adaptation of Dense Passage Retrieval for Open-Domain Question Answering,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"One key feature of dense passage retrievers (DPR) is the use of separate question and passage encoder in a bi-encoder design. Previous work on generalization of DPR mainly focus on testing both encoders in tandem on out-of-distribution (OOD) question-answering (QA) tasks, which is also known as domain adaptation. However, it is still unknown how DPR's individual question/passage encoder affects generalization. Specifically, in this paper, we want to know how an in-distribution (IND) question/passage encoder would generalize if paired with an OOD passage/question encoder from another domain. We refer to this challenge as \textit{encoder adaptation}. To answer this question, we inspect different combinations of DPR's question and passage encoder learned from five benchmark QA datasets on both in-domain and out-of-domain questions. We find that the passage encoder has more influence on the lower bound of generalization while the question encoder seems to affect the upper bound in general. For example, applying an OOD passage encoder usually hurts the retrieval accuracy while an OOD question encoder sometimes even improves the accuracy.",0effb07e34ed93ec11b1f2371ec80781e0268561
Wacky Weights in Learned Sparse Representations and the Revenge of Score-at-a-Time Query Evaluation,"[{'name': 'Joel Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Recent advances in retrieval models based on learned sparse representations generated by transformers have led us to, once again, consider score-at-a-time query evaluation techniques for the top-k retrieval problem. Previous studies comparing document-at-a-time and score-at-a-time approaches have consistently found that the former approach yields lower mean query latency, although the latter approach has more predictable query latency. In our experiments with four different retrieval models that exploit representational learning with bags of words, we find that transformers generate""wacky weights""that appear to greatly reduce the opportunities for skipping and early exiting optimizations that lie at the core of standard document-at-a-time techniques. As a result, score-at-a-time approaches appear to be more competitive in terms of query evaluation latency than in previous studies. We find that, if an effectiveness loss of up to three percent can be tolerated, a score-at-a-time approach can yield substantial gains in mean query latency while at the same time dramatically reducing tail latency.",a5c030e1fbf8354227559e374ab573e9230efa72
Densifying Sparse Representations for Passage Retrieval by Representational Slicing,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Learned sparse and dense representations capture different successful approaches to text retrieval and the fusion of their results has proven to be more effective and robust. Prior work combines dense and sparse retrievers by fusing their model scores. As an alternative, this paper presents a simple approach to densifying sparse representations for text retrieval that does not involve any training. Our densified sparse representations (DSRs) are interpretable and can be easily combined with dense representations for end-to-end retrieval. We demonstrate that our approach can jointly learn sparse and dense representations within a single model and then combine them for dense retrieval. Experimental results suggest that combining our DSRs and dense representations yields a balanced tradeoff between effectiveness and efficiency.",b88d69b643b3a7d0872931aeed2cdf1876588e26
Improving Query Representations for Dense Retrieval with Pseudo Relevance Feedback: A Reproducibility Study,"[{'name': 'Hang Li', 'dblp_profile': 'https://dblp.org/pid/83/5560-9.html'}, {'name': 'Shengyao Zhuang', 'dblp_profile': 'https://dblp.org/pid/262/6236.html'}, {'name': 'Ahmed Mourad', 'dblp_profile': 'https://dblp.org/pid/121/4296.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Guido Zuccon', 'dblp_profile': 'https://dblp.org/pid/22/6562.html'}]",2021,,471dea6589d6f19e78db1f47fbc7cff0d9f1aab3
Sparsifying Sparse Representations for Passage Retrieval by Top-k Masking,"[{'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2021,"Sparse lexical representation learning has demonstrated much progress in improving passage retrieval effectiveness in recent models such as DeepImpact, uniCOIL, and SPLADE. This paper describes a straightforward yet effective approach for sparsifying lexical representations for passage retrieval, building on SPLADE by introducing a top-$k$ masking scheme to control sparsity and a self-learning method to coax masked representations to mimic unmasked representations. A basic implementation of our model is competitive with more sophisticated approaches and achieves a good balance between effectiveness and efficiency. The simplicity of our methods opens the door for future explorations in lexical representation learning for passage retrieval.",f71ed8967b26226da15f81e99eb41f656467e148
Building community at distance: a datathon during COVID-19,"[{'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,
Navigation-based candidate expansion and pretrained language models for citation recommendation,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,3d733e63d44bddd235a41e47cf0718ff5e70b8e1
The ubiquity of large graphs and surprising challenges of graph processing: extended survey,"[{'name': 'Siddhartha Sahu', 'dblp_profile': 'https://dblp.org/pid/199/6334.html'}, {'name': 'Amine Mhedhbi', 'dblp_profile': 'https://dblp.org/pid/206/6790.html'}, {'name': 'Semih Salihoglu', 'dblp_profile': 'https://dblp.org/pid/55/6560.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'M. Tamer Özsu', 'dblp_profile': 'https://dblp.org/pid/o/MTamerOzsu.html'}]",2020,,3fc098d403921f7d601549de134f0a3075e77363
DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.",90a1491ac32e732c93773354e4e665794ed4d490
Showing Your Work Doesn't Always Work,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Xinyu Liu', 'dblp_profile': 'https://dblp.org/pid/98/738.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,
"Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data","[{'name': 'Hamidreza Shahidi', 'dblp_profile': 'https://dblp.org/pid/249/5644.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/181/2821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures. In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and unstructured data. We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively. Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models. Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks. Code is available at https://github.com/h-shahidi/2birds-gen.",24cbc48bb0cb1c275fb88e50965dfe1af57fd784
Evaluating Pretrained Transformer Models for Citation Recommendation,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,". Citation recommendation systems for the scientiﬁc literature, to help authors ﬁnd papers that should be cited, have the potential to speed up discoveries and uncover new routes for scientiﬁc exploration. We treat this task as a ranking problem, which we tackle with a two-stage approach: candidate generation followed by re-ranking. Within this framework, we adapt to the scientiﬁc domain a proven combination based on “bag of words” retrieval followed by re-scoring with a BERT model. We experimentally show the eﬀects of domain adaptation, both in terms of pretraining on in-domain data and exploiting in-domain vocabulary. In addition, we evaluate eleven pretrained transformer models and an-alyze some unexpected failure cases. On three diﬀerent collections from diﬀerent scientiﬁc disciplines, our models perform close to or at the state of the art in the citation recommendation task.",50fe89d2109f1f186e5e91aa8a7853d92a85daee
"We Could, but Should We?: Ethical Considerations for Providing Access to GeoCities and Other Historical Digital Collections","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Douglas W. Oard', 'dblp_profile': 'https://dblp.org/pid/o/DouglasWOard.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Katie Shilton', 'dblp_profile': 'https://dblp.org/pid/38/1450.html'}]",2020,"We live in an era in which the ways that we can make sense of our past are evolving as more artifacts from that past become digital. At the same time, the responsibilities of traditional gatekeepers who have negotiated the ethics of historical data collection and use, such as librarians and archivists, are increasingly being sidelined by the system builders who decide whether and how to provide access to historical digital collections, often without sufficient reflection on the ethical issues at hand. It is our aim to better prepare system builders to grapple with these issues. This paper focuses discussions around one such digital collection from the dawn of the web, asking what sorts of analyses can and should be conducted on archival copies of the GeoCities web hosting platform that dates to 1994.",79f14789a2cedb128f3bbc3f484927d30c97e156
Update Delivery Mechanisms for Prospective Information Needs: A Reproducibility Study,"[{'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Yinan Zhang', 'dblp_profile': 'https://dblp.org/pid/19/6790.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Real-time summarization systems monitor continuous streams of documents with the goal of delivering relevant, novel, and timely updates to users. These updates can either be sent to users' mobile devices as push notifications or be silently deposited in an inbox to be consumed - the important difference is whether the user is interrupted by the delivery. Previously, a two-year study examining user attention under these different mechanisms revealed interesting findings about users' information consumption behavior, but the conclusions were marred by a few methodological shortcomings. We present a reproducibility study that follows the same design as the original evaluation, but corrects its flaws. We find that most conclusions from the original study are confirmed, although there are some surprising differences as well. Overall, the magnitude of the observed effects are not as strong as in the original study.",fee8b4a4d59245d7d6cb1dc6b2853fee5324a701
Flexible IR Pipelines with Capreolus,"[{'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}, {'name': 'Kevin Martin Jose', 'dblp_profile': 'https://dblp.org/pid/139/9683.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"While a number of recent open-source toolkits for training and using neural information retrieval models have greatly simplified experiments with neural reranking methods, they essentially hard code a ""search-then-rerank'' experimental pipeline. These pipelines consist of an efficient first-stage ranking method, like BM25, followed by a neural reranking method. Deviations from this setup often require hacks; some improvements, like adding a second reranking step that uses a more expensive neural method, are infeasible without major code changes. In order to improve the flexibility of such toolkits, we propose implementing experimental pipelines as dependency graphs of functional ""IR primitives,'' which we call modules, that can be used and combined as needed. For example, a neural IR pipeline may rerank results from a Searcher module that efficiently retrieves results from an Index module that it depends on. In turn, the Index depends on a Collection to index, which is provided by the pipeline. This Searcher module is self-contained: the pipeline does not need to know about or interact with the Index of the Searcher, which is transparently shared among Searcher modules when possible (e.g., a BM25 and a QL Searcher might share the same Index). Similarly, a Reranker module might depend on a Trainer (e.g., Tensorflow), feature Extractor, Tokenizer, etc. In both cases, the pipeline needs to interact only with the Reranker or Searcher directly; the complexity of their dependencies is hidden and intelligently managed. We rewrite the Capreolus toolkit to take this approach and demonstrate its use. %in a series of code examples and experiments.",8ffa054a33c19812d78b29c411f0661bed1ed3ee
Designing Templates for Eliciting Commonsense Knowledge from Pretrained Sequence-to-Sequence Models,"[{'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Ming-Feng Tsai', 'dblp_profile': 'https://dblp.org/pid/16/3313.html'}, {'name': 'Chuan-Ju Wang', 'dblp_profile': 'https://dblp.org/pid/03/5904.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"While internalized “implicit knowledge” in pretrained transformers has led to fruitful progress in many natural language understanding tasks, how to most effectively elicit such knowledge remains an open question. Based on the text-to-text transfer transformer (T5) model, this work explores a template-based approach to extract implicit knowledge for commonsense reasoning on multiple-choice (MC) question answering tasks. Experiments on three representative MC datasets show the surprisingly good performance of our simple template, coupled with a logit normalization technique for disambiguation. Furthermore, we verify that our proposed template can be easily extended to other MC tasks with contexts such as supporting facts in open-book question answering settings. Starting from the MC task, this work initiates further research to find generic natural language templates that can effectively leverage stored knowledge in pretrained models.",67e5e67ac9fcfb241984d791c1d211a434901639
From MAXSCORE to Block-Max Wand: The Story of How Lucene Significantly Improved Query Evaluation Performance,"[{'name': 'Adrien Grand', 'dblp_profile': 'https://dblp.org/pid/262/6484.html'}, {'name': 'Robert Muir', 'dblp_profile': 'https://dblp.org/pid/262/5973.html'}, {'name': 'Jim Ferenczi', 'dblp_profile': 'https://dblp.org/pid/262/6444.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,7075a35256a595b1a4bc5a7ab02c82f570a28a76
Which BM25 Do You Mean? A Large-Scale Reproducibility Study of Scoring Variants,"[{'name': 'Chris Kamphuis', 'dblp_profile': 'https://dblp.org/pid/156/3395.html'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}, {'name': 'Leonid Boytsov', 'dblp_profile': 'https://dblp.org/pid/31/9869.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,58737fba500075136ee0f33f7801a5ac7f82ab68
"Reproducibility is a Process, Not an Achievement: The Replicability of IR Reproducibility Experiments","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Qian Zhang', 'dblp_profile': 'https://dblp.org/pid/04/2024.html'}]",2020,,d59a57a4603f4268d7fba3155b54517e4c5a7b69
Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset,"[{'name': 'Edwin Zhang', 'dblp_profile': 'https://dblp.org/pid/262/6436.html'}, {'name': 'Nikhil Gupta', 'dblp_profile': 'https://dblp.org/pid/49/6983-9.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Xiao Han', 'dblp_profile': 'https://dblp.org/pid/01/2095.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Kuang Lu', 'dblp_profile': 'https://dblp.org/pid/160/8097.html'}, {'name': 'Yue Zhang', 'dblp_profile': 'https://dblp.org/pid/47/722.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Hui Fang', 'dblp_profile': 'https://dblp.org/pid/03/2511-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the multi-round TREC-COVID challenge: Our infrastructure and baselines have been adopted by many participants, including some of the best systems. In round 3, we submitted the highest-scoring run that took advantage of previous training data and the second-highest fully automatic run. In rounds 4 and 5, we submitted the highest-scoring fully automatic runs.",e3e36944102c9baee49dfec397fc0e4bb63a7c77
Early Exiting BERT for Efficient Document Ranking,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Pre-trained language models such as BERT have shown their effectiveness in various tasks. Despite their power, they are known to be computationally intensive, which hinders real-world applications. In this paper, we introduce early exiting BERT for document ranking. With a slight modification, BERT becomes a model with multiple output paths, and each inference sample can exit early from these paths. In this way, computation can be effectively allocated among samples, and overall system latency is significantly reduced while the original quality is maintained. Our experiments on two document ranking datasets demonstrate up to 2.5x inference speedup with minimal quality degradation. The source code of our implementation can be found at https://github.com/castorini/earlyexiting-monobert.",f84bb7872052b0cc094d0750501635b466268bea
A Little Bit Is Worse Than None: Ranking with Limited Training Data,"[{'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Researchers have proposed simple yet effective techniques for the retrieval problem based on using BERT as a relevance classifier to rerank initial candidates from keyword search. In this work, we tackle the challenge of fine-tuning these models for specific domains in a data and computationally efficient manner. Typically, researchers fine-tune models using corpus-specific labeled data from sources such as TREC. We first answer the question: How much data of this type do we need? Recognizing that the most computationally efficient training is no training, we explore zero-shot ranking using BERT models that have already been fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that “some” labeled in-domain data can be worse than none at all.",241072e21b8f35d91fe072b77ef926d017c12100
Cydex: Neural Search Infrastructure for the Scholarly Literature,"[{'name': 'Shane Ding', 'dblp_profile': 'https://dblp.org/pid/281/1500.html'}, {'name': 'Edwin Zhang', 'dblp_profile': 'https://dblp.org/pid/262/6436.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Cydex is a platform that provides neural search infrastructure for domain-specific scholarly literature. The platform represents an abstraction of Covidex, our recently developed full-stack open-source search engine for the COVID-19 Open Research Dataset (CORD-19) from AI2. While Covidex takes advantage of the latest best practices for keyword search using the popular Lucene search library as well as state-of-the-art neural ranking models using T5, parts of the system were hard coded to only work with CORD-19. This paper describes our efforts to generalize Covidex into Cydex, which can be applied to scholarly literature in different domains. By decoupling corpus-specific configurations from the frontend implementation, we are able to demonstrate the generality of Cydex on two very different corpora: the ACL Anthology and a collection of hydrology abstracts. Our platform is entirely open source and available at cydex.ai.",b4fa5accb3f83ab398772a913b8aa443c34e4000
Document Ranking with a Pretrained Sequence-to-Sequence Model,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model’s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.",f6e0164466e827112fd415afdc28ddf8e0eb1ba3
Cross-Lingual Training of Neural Models for Document Ranking,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We tackle the challenge of cross-lingual training of neural document ranking models for mono-lingual retrieval, specifically leveraging relevance judgments in English to improve search in non-English languages. Our work successfully applies multi-lingual BERT (mBERT) to document ranking and additionally compares against a number of alternatives: translating the training data, translating documents, multi-stage hybrids, and ensembles. Experiments on test collections in six different languages from diverse language families reveal many interesting findings: model-based relevance transfer using mBERT can significantly improve search quality in (non-English) mono-lingual retrieval, but other “low resource” approaches are competitive as well.",fb654cdfebd804a5485ef774da04537bf5c85536
Inserting Information Bottleneck for Attribution in Transformers,"[{'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.",24967a55839cf9d8c26187888aac67b3ee4999b9
Generalized and Scalable Optimal Sparse Decision Trees,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chudi Zhong', 'dblp_profile': 'https://dblp.org/pid/267/5474.html'}, {'name': 'Diane Hu', 'dblp_profile': 'https://dblp.org/pid/26/9953.html'}, {'name': 'Cynthia Rudin', 'dblp_profile': 'https://dblp.org/pid/62/6936.html'}, {'name': 'Margo I. Seltzer', 'dblp_profile': 'https://dblp.org/pid/s/MargoISeltzer.html'}]",2020,"Decision tree optimization is notoriously difficult from a computational perspective but essential for the field of interpretable machine learning. Despite efforts over the past 40 years, only recently have optimization breakthroughs been made that have allowed practical algorithms to find optimal decision trees. These new techniques have the potential to trigger a paradigm shift where it is possible to construct sparse decision trees to efficiently optimize a variety of objective functions without relying on greedy splitting and pruning heuristics that often lead to suboptimal solutions. The contribution in this work is to provide a general framework for decision tree optimization that addresses the two significant open problems in the area: treatment of imbalanced data and fully optimizing over continuous variables. We present techniques that produce optimal decision trees over a variety of objectives including F-score, AUC, and partial area under the ROC convex hull. We also introduce a scalable algorithm that produces provably optimal results in the presence of continuous variables and speeds up decision tree construction by several orders of magnitude relative to the state-of-the art.",1b59747fb24f77406d7234b12421d1cbd7738946
Approximate Nearest Neighbor Search and Lightweight Dense Vector Reranking in Multi-Stage Retrieval Architectures,"[{'name': 'Zhengkai Tu', 'dblp_profile': 'https://dblp.org/pid/258/1305.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Zihang Fu', 'dblp_profile': 'https://dblp.org/pid/258/1213.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"In the context of a multi-stage retrieval architecture, we explore candidate generation based on approximate nearest neighbor (ANN) search and lightweight reranking based on dense vector representations. These results serve as input to slower but more accurate rerankers such as those based on transformers. Our goal is to characterize the effectiveness-efficiency tradeoff space in this context. We find that, on sentence-length segments of text, ANN techniques coupled with dense vector reranking dominate approaches based on inverted indexes, and thus our proposed design should be preferred. For paragraph-length segments, ANN-based and index-based techniques share the Pareto frontier, which means that the choice of alternatives depends on the desired operating point.",d20025487e31cbbf186d1a4e902fc55159a82a9b
"The Archives Unleashed Project: Technology, Process, and Community to Improve Scholarly Access to Web Archives","[{'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}]",2020,"The Archives Unleashed project aims to improve scholarly access to web archives through a multi-pronged strategy involving tool creation, process modeling, and community building---all proceeding concurrently in mutually-reinforcing efforts. As we near the end of our initially-conceived three-year project, we report on our progress and share lessons learned along the way. The main contribution articulated in this paper is a process model that decomposes scholarly inquiries into four main activities: filter, extract, aggregate, and visualize. Based on the insight that these activities can be disaggregated across time, space, and tools, it is possible to generate ""derivative products"", using our Archives Unleashed Toolkit, that serve as useful starting points for scholarly inquiry. Scholars can download these products from the Archives Unleashed Cloud and manipulate them just like any other dataset, thus providing access to web archives without requiring any specialized knowledge. Over the past few years, our platform has processed over a thousand different collections from over two hundred users, totaling around 300 terabytes of web archives.",ad84471593d07525bc78099eed0643f65e142df6
Content-Based Exploration of Archival Images Using Neural Networks,"[{'name': 'Tobi Adewoye', 'dblp_profile': 'https://dblp.org/pid/271/4592.html'}, {'name': 'Xiao Han', 'dblp_profile': 'https://dblp.org/pid/01/2095.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We present DAIRE (Deep Archival Image Retrieval Engine), an image exploration tool based on latent representations derived from neural networks, which allows scholars to ""query"" using an image of interest to rapidly find related images within a web archive. This work represents one part of our broader effort to move away from text-centric analyses of web archives and scholarly tools that are direct reflections of methods for accessing the live web. This short piece describes the implementation of our system and a case study on a subset of the GeoCities web archive.",b4bda0f201446b77548794724bd8238a42c7e6c5
An Open-Source Interface to the Canadian Surface Prediction Archive,"[{'name': 'Martin Gauch', 'dblp_profile': 'https://dblp.org/pid/235/0335.html'}, {'name': 'James Bai', 'dblp_profile': 'https://dblp.org/pid/271/4522.html'}, {'name': 'Juliane Mai', 'dblp_profile': 'https://dblp.org/pid/227/4944.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Data-intensive research and decision-making continue to gain adoption across diverse organizations. As researchers and practitioners increasingly rely on analyzing large data products to both answer scientific questions and for operational needs, data acquisition and pre-processing become critical tasks. For environmental science, the Canadian Surface Prediction Archive (CaSPAr) facilitates easy access to custom subsets of numerical weather predictions. We demonstrate a new open-source interface for CaSPAr that provides easy-to-use map-based querying capabilities and automates data ingestion into the CaSPAr batch processing server.",eb17dca07831e7d80b14cfdb93d3efbfb7db9649
SimClusters: Community-Based Representations for Heterogeneous Recommendations at Twitter,"[{'name': 'Venu Satuluri', 'dblp_profile': 'https://dblp.org/pid/94/1656.html'}, {'name': 'Yao Wu', 'dblp_profile': 'https://dblp.org/pid/93/582.html'}, {'name': 'Xun Zheng', 'dblp_profile': 'https://dblp.org/pid/08/9895.html'}, {'name': 'Yilei Qian', 'dblp_profile': 'https://dblp.org/pid/272/9916.html'}, {'name': 'Brian Wichers', 'dblp_profile': 'https://dblp.org/pid/273/0153.html'}, {'name': 'Qieyun Dai', 'dblp_profile': 'https://dblp.org/pid/93/9989.html'}, {'name': 'Gui Ming Tang', 'dblp_profile': 'https://dblp.org/pid/273/0006.html'}, {'name': 'Jerry Jiang', 'dblp_profile': 'https://dblp.org/pid/185/4257.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Personalized recommendation products at Twitter target a multitude of heterogeneous items: Tweets, Events, Topics, Hashtags, and users. Each of these targets varies in their cardinality (which affects the scale of the problem) and their ""shelf life'' (which constrains the latency of generating the recommendations). Although Twitter has built a variety of recommendation systems before dating back a decade, solutions to the broader problem were mostly tackled piecemeal. In this paper, we present SimClusters, a general-purpose representation layer based on overlapping communities into which users as well as heterogeneous content can be captured as sparse, interpretable vectors to support a multitude of recommendation tasks. We propose a novel algorithm for community discovery based on Metropolis-Hastings sampling, which is both more accurate and significantly faster than off-the-shelf alternatives. SimClusters scales to networks with billions of users and has been effective across a variety of deployed applications at Twitter.",8c1ca95db7eb4cb243587d45b9aa8418d7b0a735
Exploring the Limits of Simple Learners in Knowledge Distillation for Document Classification with DocBERT,"[{'name': 'Ashutosh Adhikari', 'dblp_profile': 'https://dblp.org/pid/230/3772.html'}, {'name': 'Achyudh Ram', 'dblp_profile': 'https://dblp.org/pid/220/8475.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'William L. Hamilton', 'dblp_profile': 'https://dblp.org/pid/137/3314.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Fine-tuned variants of BERT are able to achieve state-of-the-art accuracy on many natural language processing tasks, although at significant computational costs. In this paper, we verify BERT’s effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation—a popular model compression method. The results show that BERT-level effectiveness can be achieved by a single-layer LSTM with at least 40\times fewer FLOPS and only {\sim}3\% parameters. More importantly, this study analyzes the limits of knowledge distillation as we distill BERT’s knowledge all the way down to linear models—a relevant baseline for the task. We report substantial improvement in effectiveness for even the simplest models, as they capture the knowledge learnt by BERT.",10b79d7b377fac08c2569fdc9cb7b3cd903c9637
A Lightweight Environment for Learning Experimental IR Research Practices,"[{'name': 'Zeynep Akkalyoncu Yilmaz', 'dblp_profile': 'https://dblp.org/pid/245/1857.html'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Tools, computing environments, and datasets form the three critical ingredients for teaching and learning the practical aspects of experimental IR research. Assembling these ingredients can often be challenging, particularly in the context of short courses that cannot afford large startup costs. As an initial attempt to address these issues, we describe materials that we have developed for the ""Introduction to IR"" session at the ACM SIGIR/SIGKDD Africa Summer School on Machine Learning for Data Mining and Search (AFIRM 2020), which builds on three components: the open-source Lucene search library, cloud-based notebooks, and the MS MARCO dataset. We offer a self-reflective evaluation of our efforts and hope that our lessons shared can benefit future efforts.",2bbc8d4d6249206190790e3bcf608fccfbacd32e
Supporting Interoperability Between Open-Source Search Engines with the Common Index File Format,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Joel M. Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'Chris Kamphuis', 'dblp_profile': 'https://dblp.org/pid/156/3395.html'}, {'name': 'Craig Macdonald', 'dblp_profile': 'https://dblp.org/pid/02/2224.html'}, {'name': 'Antonio Mallia', 'dblp_profile': 'https://dblp.org/pid/204/0179.html'}, {'name': 'Michal Siedlaczek', 'dblp_profile': 'https://dblp.org/pid/234/2710.html'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2020,"There exists a natural tension between encouraging a diverse ecosystem of open-source search engines and supporting fair, replicable comparisons across those systems. To balance these two goals, we examine two approaches to providing interoperability between the inverted indexes of several systems. The first takes advantage of internal abstractions around index structures and building wrappers that allow one system to directly read the indexes of another. The second involves sharing indexes across systems via a data exchange specification that we have developed, called the Common Index File Format (CIFF). We demonstrate the first approach with the Java systems Anserini and Terrier, and the second approach with Anserini, JASSv2, OldDog, PISA, and Terrier. Together, these systems provide a wide range of implementations and features, with different research goals. Overall, we recommend CIFF as a low-effort approach to support independent innovation while enabling the types of fair evaluations that are critical for driving the field forward.",87bfc3de39c4beab43744bbca73da0f5c88ce126
TREC 2020 Notebook: CAsT Track,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"This notebook describes our participation (h2oloo) in TREC CAsT 2020. We first illustrate our multi-stage pipeline for conversational search: sequence-to-sequence query reformulation followed by an ad hoc text ranking pipeline; then, detail our proposed method for canonical response entry. Empirically, we show that our method effectively reformulates conversational queries considering both historical user utterances and system responses, yielding final ranking result 0.363 and 0.494 in terms of MAP and NDCG@3 respectively, which is our best submission to CAsT 2020.",821f9468f885570d376317fe0a5d471b5d019c31
"H2oloo at TREC 2020: When all you got is a hammer... Deep Learning, Health Misinformation, and Precision Medicine","[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Hang Cui', 'dblp_profile': 'https://dblp.org/pid/93/2906.html'}, {'name': 'Ruizhou Xu', 'dblp_profile': 'https://dblp.org/pid/295/4691.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"The h2oloo team from the University of Waterloo participated in the TREC 2020 Deep Learning, Health Misinformation, and Precision Medicine Tracks. Our primary goal was to validate sequence-to-sequence based retrieval techniques that we have been working on in the context of multi-stage retrieval dubbed “ExpandoMono-Duo” [6, 10] comprising a candidate document generation stage (driven by “bag of words” techniques) followed by a pointwise and then a pairwise reranking stage built around T5 [11], a powerful sequence-to-sequence transformer language model. For the Health Misinformation task, we also employ learnings from our fact verification system, VerT5erini [9]. All of our experiments employed the open-source Anserini IR toolkit [14, 16], which is based on the popular open-source Lucene search library, for initial retrieval that feeds the T5-based rerankers. Besides being the state of the art in various other collections (e.g., Robust04 and TREC-COVID), we found our models achieved much better effectiveness compared to the BM25 baselines as well as the median scores in all three tracks, demonstrating the versatility and the zero-shot transfer capabilities of our multi-stage ranking system.",4b91247cc0692f34eb3e39485aa3d2d4e8ac9dc5
Capreolus: A Toolkit for End-to-End Neural Ad Hoc Retrieval,"[{'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}, {'name': 'Siddhant Arora', 'dblp_profile': 'https://dblp.org/pid/235/7362.html'}, {'name': 'Xinyu Zhang', 'dblp_profile': 'https://dblp.org/pid/58/4582.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Kevin Martin Jose', 'dblp_profile': 'https://dblp.org/pid/139/9683.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We present Capreolus, a toolkit designed to facilitate end-to-end it ad hoc retrieval experiments with neural networks by providing implementations of prominent neural ranking models within a common framework. Our toolkit adopts a standard reranking architecture via tight integration with the Anserini toolkit for candidate document generation using standard bag-of-words approaches. Using Capreolus, we are able to reproduce Yang et al.'s recent SIGIR 2019 finding that, in a reranking scenario on the test collection from the TREC 2004 Robust Track, many neural retrieval models do not significantly outperform a strong query expansion baseline. Furthermore, we find that this holds true for five additional models implemented in Capreolus. We describe the architecture and design of our toolkit, which includes a Web interface to facilitate comparisons between rankings returned by different models.",421c3f395b4483c2b723ecb3472cb88708b50264
Distant Supervision for Multi-Stage Fine-Tuning in Retrieval-Based Question Answering,"[{'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Nicholas Jing Yuan', 'dblp_profile': 'https://dblp.org/pid/131/4855.html'}, {'name': 'Baoxing Huai', 'dblp_profile': 'https://dblp.org/pid/152/3689.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We tackle the problem of question answering directly on a large document collection, combining simple “bag of words” passage retrieval with a BERT-based reader for extracting answer spans. In the context of this architecture, we present a data augmentation technique using distant supervision to automatically annotate paragraphs as either positive or negative examples to supplement existing training data, which are then used together to fine-tune BERT. We explore a number of details that are critical to achieving high accuracy in this setup: the proper sequencing of different datasets during fine-tuning, the balance between “difficult” vs. “easy” examples, and different approaches to gathering negative examples. Experimental results show that, with the appropriate settings, we can achieve large gains in effectiveness on two English and two Chinese QA datasets. We are able to achieve results at or near the state of the art without any modeling advances, which once again affirms the cliché “there’s no data like more data”.",63076064a7775f4ab6a094d93a233d32b7031686
"The Archives Unleashed Project: Technology, Process, and Community to Improve Scholarly Access to Web Archives","[{'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}]",2020,"The Archives Unleashed project aims to improve scholarly access to web archives through a multi-pronged strategy involving tool creation, process modeling, and community building---all proceeding concurrently in mutually-reinforcing efforts. As we near the end of our initially-conceived three-year project, we report on our progress and share lessons learned along the way. The main contribution articulated in this paper is a process model that decomposes scholarly inquiries into four main activities: filter, extract, aggregate, and visualize. Based on the insight that these activities can be disaggregated across time, space, and tools, it is possible to generate ""derivative products"", using our Archives Unleashed Toolkit, that serve as useful starting points for scholarly inquiry. Scholars can download these products from the Archives Unleashed Cloud and manipulate them just like any other dataset, thus providing access to web archives without requiring any specialized knowledge. Over the past few years, our platform has processed over a thousand different collections from over two hundred users, totaling around 300 terabytes of web archives.",ad84471593d07525bc78099eed0643f65e142df6
Navigation-Based Candidate Expansion and Pretrained Language Models for Citation Recommendation,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,3d733e63d44bddd235a41e47cf0718ff5e70b8e1
A Prototype of Serverless Lucene,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"This paper describes a working prototype that adapts Lucene, the world's most popular and most widely deployed open-source search library, to operate within a serverless environment in the cloud. Although the serverless search concept is not new, this work represents a substantial improvement over a previous implementation in eliminating most custom code and in enabling interactive search. While there remain limitations to the design, it nevertheless challenges conventional thinking about search architectures for particular operating points.",11909b4cf231c7fc91933098402c75429fd4c364
Rapid Adaptation of BERT for Information Extraction on Domain-Specific Business Documents,"[{'name': 'Ruixue Zhang', 'dblp_profile': 'https://dblp.org/pid/30/2401.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Luyun Lin', 'dblp_profile': 'https://dblp.org/pid/258/0619.html'}, {'name': 'Zhengkai Tu', 'dblp_profile': 'https://dblp.org/pid/258/1305.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Zihang Fu', 'dblp_profile': 'https://dblp.org/pid/258/1213.html'}, {'name': 'Yuhao Xie', 'dblp_profile': 'https://dblp.org/pid/238/6385.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Techniques for automatically extracting important content elements from business documents such as contracts, statements, and filings have the potential to make business operations more efficient. This problem can be formulated as a sequence labeling task, and we demonstrate the adaption of BERT to two types of business documents: regulatory filings and property lease agreements. There are aspects of this problem that make it easier than ""standard"" information extraction tasks and other aspects that make it more difficult, but on balance we find that modest amounts of annotated data (less than 100 documents) are sufficient to achieve reasonable accuracy. We integrate our models into an end-to-end cloud platform that provides both an easy-to-use annotation interface as well as an inference interface that allows users to upload documents and inspect model outputs.",0413170928fa88cde7c964cb987d094e8e3d76f0
Document Ranking with a Pretrained Sequence-to-Sequence Model,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model’s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.",f6e0164466e827112fd415afdc28ddf8e0eb1ba3
Supporting Interoperability Between Open-Source Search Engines with the Common Index File Format,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Joel M. Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'Chris Kamphuis', 'dblp_profile': 'https://dblp.org/pid/156/3395.html'}, {'name': 'Craig Macdonald', 'dblp_profile': 'https://dblp.org/pid/02/2224.html'}, {'name': 'Antonio Mallia', 'dblp_profile': 'https://dblp.org/pid/204/0179.html'}, {'name': 'Michal Siedlaczek', 'dblp_profile': 'https://dblp.org/pid/234/2710.html'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2020,"There exists a natural tension between encouraging a diverse ecosystem of open-source search engines and supporting fair, replicable comparisons across those systems. To balance these two goals, we examine two approaches to providing interoperability between the inverted indexes of several systems. The first takes advantage of internal abstractions around index structures and building wrappers that allow one system to directly read the indexes of another. The second involves sharing indexes across systems via a data exchange specification that we have developed, called the Common Index File Format (CIFF). We demonstrate the first approach with the Java systems Anserini and Terrier, and the second approach with Anserini, JASSv2, OldDog, PISA, and Terrier. Together, these systems provide a wide range of implementations and features, with different research goals. Overall, we recommend CIFF as a low-effort approach to support independent innovation while enabling the types of fair evaluations that are critical for driving the field forward.",87bfc3de39c4beab43744bbca73da0f5c88ce126
TTTTTackling WinoGrande Schemas,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Ming-Feng Tsai', 'dblp_profile': 'https://dblp.org/pid/16/3313.html'}, {'name': 'Chuan-Ju Wang', 'dblp_profile': 'https://dblp.org/pid/03/5904.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We applied the T5 sequence-to-sequence model to tackle the AI2 WinoGrande Challenge by decomposing each example into two input text strings, each containing a hypothesis, and using the probabilities assigned to the ""entailment"" token as a score of the hypothesis. Our first (and only) submission to the official leaderboard yielded 0.7673 AUC on March 13, 2020, which is the best known result at this time and beats the previous state of the art by over five points.",9ae293dbcb0a3d2311775a7f4d23854fe9b0ee3d
Conversational Question Reformulation via Sequence-to-Sequence Architectures and Pretrained Language Models,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Ming-Feng Tsai', 'dblp_profile': 'https://dblp.org/pid/16/3313.html'}, {'name': 'Chuan-Ju Wang', 'dblp_profile': 'https://dblp.org/pid/03/5904.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"This paper presents an empirical study of conversational question reformulation (CQR) with sequence-to-sequence architectures and pretrained language models (PLMs). We leverage PLMs to address the strong token-to-token independence assumption made in the common objective, maximum likelihood estimation, for the CQR task. In CQR benchmarks of task-oriented dialogue systems, we evaluate fine-tuned PLMs on the recently-introduced CANARD dataset as an in-domain task and validate the models using data from the TREC 2019 CAsT Track as an out-domain task. Examining a variety of architectures with different numbers of parameters, we demonstrate that the recent text-to-text transfer transformer (T5) achieves the best results both on CANARD and CAsT with fewer parameters, compared to similar transformer architectures.",1db81c2e030f37bc14a01c6e43171a8079e7cccd
Semantics of the Unwritten,"[{'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Wen Gao', 'dblp_profile': 'https://dblp.org/pid/g/WenGao.html'}, {'name': 'Jie Liu', 'dblp_profile': 'https://dblp.org/pid/03/2134.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}]",2020,,
Rapidly Deploying a Neural Search Engine for the COVID-19 Open Research Dataset: Preliminary Thoughts and Lessons Learned,"[{'name': 'Edwin Zhang', 'dblp_profile': 'https://dblp.org/pid/262/6436.html'}, {'name': 'Nikhil Gupta', 'dblp_profile': 'https://dblp.org/pid/49/6983-9.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,
Rapidly Bootstrapping a Question Answering Dataset for COVID-19,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Edwin Zhang', 'dblp_profile': 'https://dblp.org/pid/262/6436.html'}, {'name': 'Nikhil Gupta', 'dblp_profile': 'https://dblp.org/pid/49/6983-9.html'}, {'name': 'Phuong Cam', 'dblp_profile': 'https://dblp.org/pid/263/7190.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We present CovidQA, the beginnings of a question answering dataset specifically designed for COVID-19, built by hand from knowledge gathered from Kaggle's COVID-19 Open Research Dataset Challenge. To our knowledge, this is the first publicly available resource of its type, and intended as a stopgap measure for guiding research until more substantial evaluation resources become available. While this dataset, comprising 124 question-article pairs as of the present version 0.1 release, does not have sufficient examples for supervised machine learning, we believe that it can be helpful for evaluating the zero-shot or transfer capabilities of existing models on topics specifically related to COVID-19. This paper describes our methodology for constructing the dataset and presents the effectiveness of a number of baselines, including term-based techniques and various transformer-based models. The dataset is available at this http URL",0f995b05821b58b02e914422b56fba615d0e8d7f
DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications. However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications. We propose a simple but effective method, DeeBERT, to accelerate BERT inference. Our approach allows samples to exit earlier without passing through the entire model. Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/DeeBERT.",90a1491ac32e732c93773354e4e665794ed4d490
Showing Your Work Doesn't Always Work,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Xinyu Liu', 'dblp_profile': 'https://dblp.org/pid/98/738.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,
SegaBERT: Pre-training of Segment-aware BERT for Language Understanding,"[{'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Wen Gao', 'dblp_profile': 'https://dblp.org/pid/g/WenGao.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}]",2020,"Pre-trained language models have achieved state-of-the-art results in various natural language processing tasks. Most of them are based on the Transformer architecture, which distinguishes tokens with the token position index of the input sequence. However, sentence index and paragraph index are also important to indicate the token position in a document. We hypothesize that better contextual representations can be generated from the text encoder with richer positional information. To verify this, we propose a segment-aware BERT, by replacing the token position embedding of Transformer with a combination of paragraph index, sentence index, and token index embeddings. We pre-trained the SegaBERT on the masked language modeling task in BERT but without any affiliated tasks. Experimental results show that our pre-trained model can outperform the original BERT model on various NLP tasks.",1cc0b98b938b984e5da85f86c1a24099b9b4b582
Query Reformulation using Query History for Passage Retrieval in Conversational Search,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Ming-Feng Tsai', 'dblp_profile': 'https://dblp.org/pid/16/3313.html'}, {'name': 'Chuan-Ju Wang', 'dblp_profile': 'https://dblp.org/pid/03/5904.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Passage retrieval in a conversational context is essential for many downstream applications; it is however extremely challenging due to limited data resources. To address this problem, we present an effective multi-stage pipeline for passage ranking in conversational search that integrates a widely-used IR system with a conversational query reformulation module. Along these lines, we propose two simple yet effective query reformulation approaches: historical query expansion (HQE) and neural transfer reformulation (NTR). Whereas HQE applies query expansion, a traditional IR query reformulation technique, NTR transfers human knowledge of conversational query understanding to a neural query reformulation model. The proposed HQE method was the top-performing submission of automatic systems in CAsT Track at TREC 2019. Building on this, our NTR approach improves an additional 18% over that best entry in terms of NDCG@3. We further analyze the distinct behaviors of the two approaches, and show that fusing their output reduces the performance gap (measured in NDCG@3) between the manually-rewritten and automatically-generated queries to 4 from 22 points when compared with the best CAsT submission.",c0a5be99a0ea38d1adbffb6110ce3a1d227e13f7
Generalized Optimal Sparse Decision Trees,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chudi Zhong', 'dblp_profile': 'https://dblp.org/pid/267/5474.html'}, {'name': 'Diane Hu', 'dblp_profile': 'https://dblp.org/pid/26/9953.html'}, {'name': 'Cynthia Rudin', 'dblp_profile': 'https://dblp.org/pid/62/6936.html'}, {'name': 'Margo I. Seltzer', 'dblp_profile': 'https://dblp.org/pid/s/MargoISeltzer.html'}]",2020,"Decision tree optimization is notoriously difficult from a computational perspective but essential for the field of interpretable machine learning. Despite efforts over the past 40 years, only recently have optimization breakthroughs been made that have allowed practical algorithms to find optimal decision trees. These new techniques have the potential to trigger a paradigm shift where it is possible to construct sparse decision trees to efficiently optimize a variety of objective functions without relying on greedy splitting and pruning heuristics that often lead to suboptimal solutions. The contribution in this work is to provide a general framework for decision tree optimization that addresses the two significant open problems in the area: treatment of imbalanced data and fully optimizing over continuous variables. We present techniques that produce optimal decision trees over a variety of objectives including F-score, AUC, and partial area under the ROC convex hull. We also introduce a scalable algorithm that produces provably optimal results in the presence of continuous variables and speeds up decision tree construction by several orders of magnitude relative to the state-of-the art.",5d31f47e0cd4a4bffc68a3f5ff4f37e714215df7
A Data Scientist's Guide to Streamflow Prediction,"[{'name': 'Martin Gauch', 'dblp_profile': 'https://dblp.org/pid/235/0335.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"In recent years, the paradigms of data-driven science have become essential components of physical sciences, particularly in geophysical disciplines such as climatology. The field of hydrology is one of these disciplines where machine learning and data-driven models have attracted significant attention. This offers significant potential for data scientists' contributions to hydrologic research. As in every interdisciplinary research effort, an initial mutual understanding of the domain is key to successful work later on. In this work, we focus on the element of hydrologic rainfall--runoff models and their application to forecast floods and predict streamflow, the volume of water flowing in a river. This guide aims to help interested data scientists gain an understanding of the problem, the hydrologic concepts involved, and the details that come up along the way. We have captured lessons that we have learned while ""coming up to speed"" on streamflow prediction and hope that our experiences will be useful to the community.",1bb95b1c5719b12c11766a87fd2c77971116666a
Covidex: Neural Ranking Models and Keyword Search Infrastructure for the COVID-19 Open Research Dataset,"[{'name': 'Edwin Zhang', 'dblp_profile': 'https://dblp.org/pid/262/6436.html'}, {'name': 'Nikhil Gupta', 'dblp_profile': 'https://dblp.org/pid/49/6983-9.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Xiao Han', 'dblp_profile': 'https://dblp.org/pid/01/2095.html'}, {'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Kuang Lu', 'dblp_profile': 'https://dblp.org/pid/160/8097.html'}, {'name': 'Yue Zhang', 'dblp_profile': 'https://dblp.org/pid/47/722.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Hui Fang', 'dblp_profile': 'https://dblp.org/pid/03/2511-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We present Covidex, a search engine that exploits the latest neural ranking models to provide information access to the COVID-19 Open Research Dataset curated by the Allen Institute for AI. Our system has been online and serving users since late March 2020. The Covidex is the user application component of our three-pronged strategy to develop technologies for helping domain experts tackle the ongoing global pandemic. In addition, we provide robust and easy-to-use keyword search infrastructure that exploits mature fusion-based methods as well as standalone neural ranking models that can be incorporated into other applications. These techniques have been evaluated in the multi-round TREC-COVID challenge: Our infrastructure and baselines have been adopted by many participants, including some of the best systems. In round 3, we submitted the highest-scoring run that took advantage of previous training data and the second-highest fully automatic run. In rounds 4 and 5, we submitted the highest-scoring fully automatic runs.",e3e36944102c9baee49dfec397fc0e4bb63a7c77
To Paraphrase or Not To Paraphrase: User-Controllable Selective Paraphrase Generation,"[{'name': 'Mohan Zhang', 'dblp_profile': 'https://dblp.org/pid/218/0654.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Zhengkai Tu', 'dblp_profile': 'https://dblp.org/pid/258/1305.html'}, {'name': 'Zihang Fu', 'dblp_profile': 'https://dblp.org/pid/258/1213.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"In this article, we propose a paraphrase generation technique to keep the key phrases in source sentences during paraphrasing. We also develop a model called TAGPA with such technique, which has multiple pre-configured or trainable key phrase detector and a paraphrase generator. The paraphrase generator aims to keep the key phrases and increase the diversity of the paraphrased sentences. The key phrases can be entities provided by our user, like company names, people's names, domain-specific terminologies, etc., or can be learned from a given dataset.",8369578a29e57724c8bdb983d63ad9a3ae983223
"Howl: A Deployed, Open-Source Wake Word Detection System","[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Afsaneh Razi', 'dblp_profile': 'https://dblp.org/pid/208/6446.html'}, {'name': 'Julia Cambre', 'dblp_profile': 'https://dblp.org/pid/142/3478.html'}, {'name': 'Ian Bicking', 'dblp_profile': 'https://dblp.org/pid/270/2557.html'}, {'name': 'Jofish Kaye', 'dblp_profile': 'https://dblp.org/pid/k/JosephKaye.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We describe Howl, an open-source wake word detection toolkit with native support for open speech datasets such as Mozilla Common Voice (MCV) and Google Speech Commands (GSC). We report benchmark results of various models supported by our toolkit on GSC and our own freely available wake word detection dataset, built from MCV. One of our models is deployed in Firefox Voice, a plugin enabling speech interactivity for the Firefox web browser. Howl represents, to the best of our knowledge, the first fully productionized, open-source wake word detection toolkit with a web browser deployment target. Our codebase is at howl.ai.",ea2690304b0f08cf298247bcc0d341e2a2981299
Pretrained Transformers for Text Ranking: BERT and Beyond,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Andrew Yates', 'dblp_profile': 'https://dblp.org/pid/49/7109.html'}]",2020,"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This tutorial, based on a forthcoming book, provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has, without exaggeration, revolutionized the fields of natural language processing (NLP), information retrieval (IR), and beyond. We provide a synthesis of existing work as a single point of entry for both researchers and practitioners. Our coverage is grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly. Two themes pervade our treatment: techniques for handling long documents and techniques for addressing the tradeoff between effectiveness (result quality) and efficiency (query latency). Although transformer architectures and pretraining techniques are recent innovations, many aspects of their application are well understood. Nevertheless, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, we also attempt to prognosticate the future.",88b0eea7050c3322162c0cf18b2ccb014c4d17ad
Rainfall-Runoff Prediction at Multiple Timescales with a Single Long Short-Term Memory Network,"[{'name': 'Martin Gauch', 'dblp_profile': 'https://dblp.org/pid/235/0335.html'}, {'name': 'Frederik Kratzert', 'dblp_profile': 'https://dblp.org/pid/238/0238.html'}, {'name': 'Daniel Klotz', 'dblp_profile': 'https://dblp.org/pid/238/0300.html'}, {'name': 'Grey Nearing', 'dblp_profile': 'https://dblp.org/pid/245/2731.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Sepp Hochreiter', 'dblp_profile': 'https://dblp.org/pid/h/SeppHochreiter.html'}]",2020,"Abstract. Long Short-Term Memory (LSTM) networks have been applied to daily discharge prediction with remarkable success.
Many practical applications, however, require predictions at more granular timescales.
For instance, accurate prediction of short but extreme flood peaks can make a lifesaving difference, yet such peaks may escape the coarse temporal resolution of daily predictions.
Naively training an LSTM on hourly data, however, entails very long input sequences that make learning difficult and computationally expensive.
In this study, we propose two multi-timescale LSTM (MTS-LSTM) architectures that jointly predict multiple timescales within one model, as they process long-past inputs at a different temporal resolution than more recent inputs.
In a benchmark on 516 basins across the continental United States, these models achieved significantly higher Nash–Sutcliffe efficiency (NSE) values than the US National Water Model.
Compared to naive prediction with distinct LSTMs per timescale, the multi-timescale architectures are computationally more efficient with no loss in accuracy.
Beyond prediction quality, the multi-timescale LSTM can process different input variables at different timescales, which is especially relevant to operational applications where the lead time of meteorological forcings depends on their temporal resolution.
",5ab6e8f7fbaef1aa85dc84de9168c726f803d212
Latte-Mix: Measuring Sentence Semantic Similarity with Latent Categorical Mixtures,"[{'name': 'Minghan Li', 'dblp_profile': 'https://dblp.org/pid/214/2450.html'}, {'name': 'He Bai', 'dblp_profile': 'https://dblp.org/pid/73/5171-2.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"Measuring sentence semantic similarity using pre-trained language models such as BERT generally yields unsatisfactory zero-shot performance, and one main reason is ineffective token aggregation methods such as mean pooling. In this paper, we demonstrate under a Bayesian framework that distance between primitive statistics such as the mean of word embeddings are fundamentally flawed for capturing sentence-level semantic similarity. To remedy this issue, we propose to learn a categorical variational autoencoder (VAE) based on off-the-shelf pre-trained language models. We theoretically prove that measuring the distance between the latent categorical mixtures, namely Latte-Mix, can better reflect the true sentence semantic similarity. In addition, our Bayesian framework provides explanations for why models finetuned on labelled sentence pairs have better zero-shot performance. We also empirically demonstrate that these finetuned models could be further improved by Latte-Mix. Our method not only yields the state-of-the-art zero-shot performance on semantic similarity datasets such as STS, but also enjoy the benefits of fast training and having small memory footprints.",e439ff2a56b26e8839d894c1d82967b5a8816939
Distilling Dense Representations for Ranking using Tightly-Coupled Teachers,"[{'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"We present an approach to ranking with dense representations that applies knowledge distillation to improve the recently proposed late-interaction ColBERT model. Specifically, we distill the knowledge from ColBERT's expressive MaxSim operator for computing relevance scores into a simple dot product, thus enabling single-step ANN search. Our key insight is that during distillation, tight coupling between the teacher model and the student model enables more flexible distillation strategies and yields better learned representations. We empirically show that our approach improves query latency and greatly reduces the onerous storage requirements of ColBERT, while only making modest sacrifices in terms of effectiveness. By combining our dense representations with sparse representations derived from document expansion, we are able to approach the effectiveness of a standard cross-encoder reranker using BERT that is orders of magnitude slower.",83f915d30720f1aa1c6f6a4342d7f9e52add756e
Scientific Claim Verification with VERT5ERINI,"[{'name': 'Ronak Pradeep', 'dblp_profile': 'https://dblp.org/pid/270/1757.html'}, {'name': 'Xueguang Ma', 'dblp_profile': 'https://dblp.org/pid/44/9030.html'}, {'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,"This work describes the adaptation of a pretrained sequence-to-sequence model to the task of scientific claim verification in the biomedical domain. We propose a system called VerT5erini that exploits T5 for abstract retrieval, sentence selection, and label prediction, which are three critical sub-tasks of claim verification. We evaluate our pipeline on SciFACT, a newly curated dataset that requires models to not just predict the veracity of claims but also provide relevant sentences from a corpus of scientific literature that support the prediction. Empirically, our system outperforms a strong baseline in each of the three sub-tasks. We further show VerT5erini’s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus.",e4cb6bfe88a8ed729d34d5a9ff74a992932b70ce
Inserting Information Bottlenecks for Attribution in Transformers,"[{'name': 'Zhiying Jiang', 'dblp_profile': 'https://dblp.org/pid/21/1662.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2020,,
"The neural hype, justified!: a recantation","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,,
Multi-Perspective Relevance Matching with Hierarchical ConvNets for Social Media Search,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Yuhao Zhang', 'dblp_profile': 'https://dblp.org/pid/139/5876-4.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have mostly been applied to “standard” ad hoc retrieval tasks over web pages and newswire articles. This paper proposes MP-HCNN (Multi-Perspective Hierarchical Convolutional Neural Network), a novel neural ranking model specifically designed for ranking short social media posts. We identify document length, informal language, and heterogeneous relevance signals as features that distinguish documents in our domain, and present a model specifically designed with these characteristics in mind. Our model uses hierarchical convolutional layers to learn latent semantic soft-match relevance signals at the character, word, and phrase levels. A poolingbased similarity measurement layer integrates evidence from multiple types of matches between the query, the social media post, as well as URLs contained in the post. Extensive experiments using Twitter data from the TREC Microblog Tracks 2011–2014 show that our model significantly outperforms prior feature-based as well as existing neural ranking models. To our best knowledge, this paper presents the first substantial work tackling search over social media posts using neural ranking models. Our code and data are publicly available.1",2f61e9f3cf90b42db2571efd19c2c7513c3be09b
Natural Language Generation for Effective Knowledge Distillation,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Yao Lu', 'dblp_profile': 'https://dblp.org/pid/26/5662.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Knowledge distillation can effectively transfer knowledge from BERT, a deep language representation model, to traditional, shallow word embedding-based neural networks, helping them approach or exceed the quality of other heavyweight language representation models. As shown in previous work, critical to this distillation procedure is the construction of an unlabeled transfer dataset, which enables effective knowledge transfer. To create transfer set examples, we propose to sample from pretrained language models fine-tuned on task-specific text. Unlike previous techniques, this directly captures the purpose of the transfer set. We hypothesize that this principled, general approach outperforms rule-based techniques. On four datasets in sentiment classification, sentence similarity, and linguistic acceptability, we show that our approach improves upon previous methods. We outperform OpenAI GPT, a deep pretrained transformer, on three of the datasets, while using a single-layer bidirectional LSTM that runs at least ten times faster.",7d767f64e88fdec81a24190c629dcfe23c940793
Identification and Ranking of Biomedical Informatics Researcher Citation Statistics through a Google Scholar Scraper,"[{'name': 'Allison B. McCoy', 'dblp_profile': 'https://dblp.org/pid/08/11278.html'}, {'name': 'Dean F. Sittig', 'dblp_profile': 'https://dblp.org/pid/04/1456.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Wright', 'dblp_profile': 'https://dblp.org/pid/46/7031.html'}]",2019,"To overcome limitations of previously developed scientific productivity ranking services, we created the Biomedical Informatics Researchers ranking website (rank.informatics-review.com). The website is composed of four key components that work together to create the automatically updating ranking website: 1) list of biomedical informatics researchers, 2) Google Scholar scraper, 3) display page, and 4) updater. The interactive website has facilitated identification of leaders in each of the key citation statistics categories (i.e., number of citations, h-index, and i10-index), and it has allowed other groups, such as tenure and promotions committees, to more effectively and efficiently evaluate researchers and interpret the various citation statistics reported by candidates. Creation of the biomedical informatics researcher ranking website highlights the vast differences in scholarly productivity among members of the biomedical informatics research community. Future efforts are underway to add new functionality to the website and to expand the work to identify top papers in biomedical informatics.",5749bf08ef69d6005e5e084f071a505328053670
Reproducing and Generalizing Semantic Term Matching in Axiomatic Information Retrieval,"[{'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,,872251f0e4a36dfba1b6493358edbdbf1ccbc09b
Simple Techniques for Cross-Collection Relevance Feedback,"[{'name': 'Ruifan Yu', 'dblp_profile': 'https://dblp.org/pid/232/4130.html'}, {'name': 'Yuhao Xie', 'dblp_profile': 'https://dblp.org/pid/238/6385.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,,c4ddd9c8eb0fa8e9d50a7fe1548e61af563f9a9c
Applying BERT to Document Retrieval with Birch,"[{'name': 'Zeynep Akkalyoncu Yilmaz', 'dblp_profile': 'https://dblp.org/pid/245/1857.html'}, {'name': 'Shengjin Wang', 'dblp_profile': 'https://dblp.org/pid/38/2763.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"We present Birch, a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit to demonstrate end-to-end search over large document collections. Birch implements simple ranking models that achieve state-of-the-art effectiveness on standard TREC newswire and social media test collections. This demonstration focuses on technical challenges in the integration of NLP and IR capabilities, along with the design rationale behind our approach to tightly-coupled integration between Python (to support neural networks) and the Java Virtual Machine (to support document retrieval using the open-source Lucene search library). We demonstrate integration of Birch with an existing search interface as well as interactive notebooks that highlight its capabilities in an easy-to-understand manner.",df12d1d972708250f3769eaaa34f4b156cf695fe
Honkling: In-Browser Personalization for Ubiquitous Keyword Spotting,"[{'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Used for simple commands recognition on devices from smart speakers to mobile phones, keyword spotting systems are everywhere. Ubiquitous as well are web applications, which have grown in popularity and complexity over the last decade. However, despite their obvious advantages in natural language interaction, voice-enabled web applications are still few and far between. We attempt to bridge this gap with Honkling, a novel, JavaScript-based keyword spotting system. Purely client-side and cross-device compatible, Honkling can be deployed directly on user devices. Our in-browser implementation enables seamless personalization, which can greatly improve model quality; in the presence of underrepresented, non-American user accents, we can achieve up to an absolute 10% increase in accuracy in the personalized model with only a few examples.",fcf9f980a1f0f24d7234bf0b026e30d7fe5a1843
Incorporating Contextual and Syntactic Structures Improves Semantic Similarity Modeling,"[{'name': 'Linqing Liu', 'dblp_profile': 'https://dblp.org/pid/36/7028.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Semantic similarity modeling is central to many NLP problems such as natural language inference and question answering. Syntactic structures interact closely with semantics in learning compositional representations and alleviating long-range dependency issues. How-ever, such structure priors have not been well exploited in previous work for semantic mod-eling. To examine their effectiveness, we start with the Pairwise Word Interaction Model, one of the best models according to a recent reproducibility study, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce residual connections to the deep convolutional neural network component of the model. Extensive evaluations on eight benchmark datasets show that incorporating structural information contributes to consistent improvements over strong baselines.",0563230acc891263057e28bf4df21582c81ddfa8
Cross-Domain Modeling of Sentence-Level Evidence for Document Retrieval,"[{'name': 'Zeynep Akkalyoncu Yilmaz', 'dblp_profile': 'https://dblp.org/pid/245/1857.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"This paper applies BERT to ad hoc document retrieval on news articles, which requires addressing two challenges: relevance judgments in existing test collections are typically provided only at the document level, and documents often exceed the length that BERT was designed to handle. Our solution is to aggregate sentence-level evidence to rank documents. Furthermore, we are able to leverage passage-level relevance judgments fortuitously available in other domains to fine-tune BERT models that are able to capture cross-domain notions of relevance, and can be directly used for ranking news articles. Our simple neural ranking models achieve state-of-the-art effectiveness on three standard test collections.",142cd4a2a1bf744836b2143d795742a3f5e33bae
Aligning Cross-Lingual Entities with Multi-Aspect Information,"[{'name': 'Hsiu-Wei Yang', 'dblp_profile': 'https://dblp.org/pid/204/0080.html'}, {'name': 'Yanyan Zou', 'dblp_profile': 'https://dblp.org/pid/48/6274.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Wei Lu', 'dblp_profile': 'https://dblp.org/pid/98/6613-11.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Xu Sun', 'dblp_profile': 'https://dblp.org/pid/37/1971-1.html'}]",2019,"Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode entities from multilingual KGs into the same vector space, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our method significantly outperforms existing systems.",cf4dcc7d67f0776ffe7aa5b4ee3217f9bd757282
Bridging the Gap between Relevance Matching and Semantic Matching for Short Text Similarity Modeling,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Linqing Liu', 'dblp_profile': 'https://dblp.org/pid/36/7028.html'}, {'name': 'Yi Tay', 'dblp_profile': 'https://dblp.org/pid/188/6350.html'}, {'name': 'Hsiu-Wei Yang', 'dblp_profile': 'https://dblp.org/pid/204/0080.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"A core problem of information retrieval (IR) is relevance matching, which is to rank documents by relevance to a user’s query. On the other hand, many NLP problems, such as question answering and paraphrase identification, can be considered variants of semantic matching, which is to measure the semantic distance between two pieces of short texts. While at a high level both relevance and semantic matching require modeling textual similarity, many existing techniques for one cannot be easily adapted to the other. To bridge this gap, we propose a novel model, HCAN (Hybrid Co-Attention Network), that comprises (1) a hybrid encoder module that includes ConvNet-based and LSTM-based encoders, (2) a relevance matching module that measures soft term matches with importance weighting at multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-of-the-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders.",90820988957e74db0fbb1df79175610e08016ba4
What Part of the Neural Network Does This? Understanding LSTMs by Measuring and Dissecting Neurons,"[{'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}]",2019,"Memory neurons of long short-term memory (LSTM) networks encode and process information in powerful yet mysterious ways. While there has been work to analyze their behavior in carrying low-level information such as linguistic properties, how they directly contribute to label prediction remains unclear. We find inspiration from biologists and study the affinity between individual neurons and labels, propose a novel metric to quantify the sensitivity of neurons to each label, and conduct experiments to show the validity of our proposed metric. We discover that some neurons are trained to specialize on a subset of labels, and while dropping an arbitrary neuron has little effect on the overall accuracy of the model, dropping label-specialized neurons predictably and significantly degrades prediction accuracy on the associated label. We further examine the consistency of neuron-label affinity across different models. These observations provide insight into the inner mechanisms of LSTMs.",ac3d6c311631fa6eb645d16c40f54b5465767888
Universal voice-enabled user interfaces using JavaScript,"[{'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Countless voice-enabled user interfaces rely on keyword spotting (KWS) systems for wake word detection and simple command recognition. As a practical matter, these applications run on ""edge"" devices, where dozens of different platforms exist; typically, platform-dependent implementation are required whenever keyword spotting capabilities are needed. This impedes the rapid deployment of voice-enabled interfaces. Fortunately, with the development of several recent frameworks, JavaScript enables us to deploy neural networks for keyword spotting to support a wide range of speech-based user interfaces. We present three voice-enabled applications that use a unified, JavaScript-based KWS system: an in-browser game, a desktop virtual assistant, and a smart lightbulb controller. We are, to the best of our knowledge, the first to demonstrate the feasibility of JavaScript-based keyword spotting for universal voice-enabled user interfaces.",9c88bbc2a867f9c139a2d52079f0333ca2bb7aaf
The Cost of a WARC: Analyzing Web Archives in the Cloud,"[{'name': 'Ryan Deschamps', 'dblp_profile': 'https://dblp.org/pid/163/4231.html'}, {'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}]",2019,"The value of web archives to support scholarship in the humanities and social sciences is slowly being realized by the increasing availability of scalable tools and platforms. The cost of providing scholarly access is a critical component of developing a long-term sustainability strategy. This paper attempts to answer a straightforward question: How much does it cost to analyze web archives in the cloud? To make this question more concrete, we examine the creation of three derivatives (extraction of collection statistics, full text, and the webgraph) that serve as the starting points of many scholarly inquiries. Our analysis shows that these typical derivatives costs around US$7 per TB using our Archives Unleashed Toolkit. We describe in detail the methodology and assumptions made to arrive at this figure. To our knowledge, we are the first to quantify the economics of scholarly access to web archives, and we believe that this information is valuable for service planning by archives, libraries, and other institutions.",efc4146087f83573e5b27aab9011d01d7c2126ca
Building Community and Tools for Analyzing Web Archives Through Datathons,"[{'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Nathalie Casemajor', 'dblp_profile': 'https://dblp.org/pid/178/3611.html'}, {'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Matthew S. Weber', 'dblp_profile': 'https://dblp.org/pid/74/10840.html'}, {'name': 'Nicholas Worby', 'dblp_profile': 'https://dblp.org/pid/227/2043.html'}]",2019,"Starting in March 2016, the Archives Unleashed team and our collaborators have brought together social scientists, humanists, archivists, librarians, computer scientists, and other stakeholders to explore web archives as research objects. Three objectives motivated our team to develop and organize these events: facilitating scholarly access, community building, and skills training. We believe that we have been successful on all three fronts. For each event, over the course of two to three days, participants formed interdisciplinary teams and explored web archives using a variety of methods and tools. This paper details our experiences in designing these ""datathons"", with an intent to share lessons learned, highlight interdisciplinary approaches to research and education on web archives, and describe future opportunities.",b55aff76a66a0432f399a9535f71269e1a85c154
The Archives Unleashed Notebook: Madlibs for Jumpstarting Scholarly Exploration of Web Archives,"[{'name': 'Ryan Deschamps', 'dblp_profile': 'https://dblp.org/pid/163/4231.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Samantha Fritz', 'dblp_profile': 'https://dblp.org/pid/246/6800.html'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}]",2019,"This paper introduces the Archives Unleashed Notebook, which is designed to work with derivative datasets from the Archives Unleashed Cloud, a platform for analyzing web archives. These datasets contain common starting points for scholarly inquiry, including full text content and the domain-level webgraph. Our notebooks interactively walk a scholar through the process of interrogating a collection using a fill-in-the-blanks 'madlibs' approach to promote engagement. Scholars start with a notebook populated with common analyses, in which they can make minor changes to variables to alter the subject of study in systematic ways.",fd2ef4000b36e942985343964d098ed84f2ef7c3
Scalable Content-Based Analysis of Images in Web Archives with TensorFlow and the Archives Unleashed Toolkit,"[{'name': 'Hsiu-Wei Yang', 'dblp_profile': 'https://dblp.org/pid/204/0080.html'}, {'name': 'Linqing Liu', 'dblp_profile': 'https://dblp.org/pid/36/7028.html'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"We demonstrate the integration of the Archives Unleashed Toolkit, a scalable platform for exploring web archives, with Google's TensorFlow deep learning toolkit to provide scholars with content-based image analysis capabilities. By applying pretrained deep neural networks for object detection, we are able to extract images of common objects from a 4TB web archive of GeoCities, which we then compile into browsable collages. This case study illustrates the types of interesting analyses enabled by combining big data and deep learning capabilities.",361574d708f137a06d8f889b5531bd5383fa8aea
Warclight: A Rails Engine for Web Archive Discovery,"[{'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"This paper describes the development of Warclight, a portmanteau of the open-source Blacklight platform and the ISO-standard Web ARChive file format. Warclight allows users to explore web archives that have been indexed into Apache Solr using the UK Web Archive's Web Archive Discovery tool. Referencing previous work, we explain how the standard search engine results page is inadequate to support scholarly inquiries. Instead, Warclight provides full-text and faceted search, as well as faceted browsing, to enable exploration and discovery. Given the large sizes of many web archives, we share experiences with deploying our tool at scale using a federated architecture.",ff7de3e6a7a8ed962162665561dc0738e45c73fc
Detecting Customer Complaint Escalation with Recurrent Neural Networks and Manually-Engineered Features,"[{'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Chunwei Lu', 'dblp_profile': 'https://dblp.org/pid/160/9936.html'}, {'name': 'Anqi Cui', 'dblp_profile': 'https://dblp.org/pid/06/7210.html'}, {'name': 'Han Li', 'dblp_profile': 'https://dblp.org/pid/07/1429.html'}, {'name': 'Xi Chen', 'dblp_profile': 'https://dblp.org/pid/16/3283.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Muzi Wang', 'dblp_profile': 'https://dblp.org/pid/242/4280.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jian Pei', 'dblp_profile': 'https://dblp.org/pid/p/JianPei.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Consumers dissatisfied with the normal dispute resolution process provided by an e-commerce company’s customer service agents have the option of escalating their complaints by filing grievances with a government authority. This paper tackles the challenge of monitoring ongoing text chat dialogues to identify cases where the customer expresses such an intent, providing triage and prioritization for a separate pool of specialized agents specially trained to handle more complex situations. We describe a hybrid model that tackles this challenge by integrating recurrent neural networks with manually-engineered features. Experiments show that both components are complementary and contribute to overall recall, outperforming competitive baselines. A trial online deployment of our model demonstrates its business value in improving customer service.",b2ca0a827ca86d515a0e378ef206323a86d5d2af
End-to-End Open-Domain Question Answering with BERTserini,"[{'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Aileen Lin', 'dblp_profile': 'https://dblp.org/pid/236/4217.html'}, {'name': 'Xingyu Li', 'dblp_profile': 'https://dblp.org/pid/45/2385.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",2fe7dba5a58aee5156594b4d78634ecd6c7dcabd
Simple Attention-Based Representation Learning for Ranking Short Social Media Posts,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"This paper explores the problem of ranking short social media posts with respect to user queries using neural networks. Instead of starting with a complex architecture, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic “soft” matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but are also much faster.",6e22f27b65093cc2d24a49b281e7b782ad06c4d4
Rethinking Complex Neural Network Architectures for Document Classification,"[{'name': 'Ashutosh Adhikari', 'dblp_profile': 'https://dblp.org/pid/230/3772.html'}, {'name': 'Achyudh Ram', 'dblp_profile': 'https://dblp.org/pid/220/8475.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Neural network models for many NLP tasks have grown increasingly complex in recent years, making training and deployment more difficult. A number of recent papers have questioned the necessity of such architectures and found that well-executed, simpler models are quite effective. We show that this is also the case for document classification: in a large-scale reproducibility study of several recent neural models, we find that a simple BiLSTM architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. Surprisingly, our simple model is able to achieve these results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, to our knowledge we are the first to apply them in this context. Our work provides an open-source platform and the foundation for future work in document classification.",b7bdf98ef84909d4ec0b2ebd5157ee3cb38522b8
Overview of the 2019 Open-Source IR Replicability Challenge (OSIRRC 2019),"[{'name': 'Ryan Clancy', 'dblp_profile': 'https://dblp.org/pid/245/1822.html'}, {'name': 'Nicola Ferro', 'dblp_profile': 'https://dblp.org/pid/f/NicolaFerro.html'}, {'name': 'Claudia Hauff', 'dblp_profile': 'https://dblp.org/pid/73/906.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Tetsuya Sakai', 'dblp_profile': 'https://dblp.org/pid/18/6321.html'}, {'name': 'Ze Zhong Wu', 'dblp_profile': 'https://dblp.org/pid/245/1784.html'}]",2019,"The Open-Source IR Replicability Challenge (OSIRRC 2019), organized as a workshop at SIGIR 2019, aims to improve the replicability of ad hoc retrieval experiments in information retrieval by gathering a community of researchers to jointly develop a common Docker specification and build Docker images that encapsulate a diversity of systems and retrieval models. We articulate the goals of this workshop and describe the “jig” that encodes the Docker specification. In total, 13 teams from around the world submitted 17 images, most of which were designed to produce retrieval runs for the TREC 2004 Robust Track test collection. This exercise demonstrates the feasibility of orchestrating large, community-based replication experiments with Docker technology. We envision OSIRRC becoming an ongoing community-wide effort to ensure experimental replicability and sustained progress on standard test collections.",cae9b657efe5c96c494eeb920930e8e2e879823b
University of Waterloo Docker Images for OSIRRC at SIGIR 2019,"[{'name': 'Ryan Clancy', 'dblp_profile': 'https://dblp.org/pid/245/1822.html'}, {'name': 'Zeynep Akkalyoncu Yilmaz', 'dblp_profile': 'https://dblp.org/pid/245/1857.html'}, {'name': 'Ze Zhong Wu', 'dblp_profile': 'https://dblp.org/pid/245/1784.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"1 OVERVIEW The University of Waterloo team submitted a total of four Docker images to the Open-Source IR Replicability Challenge (OSIRRC) at SIGIR 2019. This short overview outlines the functionality of each image. As the READMEs in all our source repositories provide details on the technical design of our images and the retrieval models used in our runs, we intentionally do not duplicate this information here. Our primary submission is a packaging of Anserini [11, 12], an open-source information retrieval toolkit built around Lucene to facilitate replicable research. This anserini-docker image resides at the following URL:",5f0d07f9e832c2e26b796de83c89c69eb72df2f2
Yelling at Your TV: An Analysis of Speech Recognition Errors and Subsequent User Behavior on Entertainment Systems,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Millions of consumers issue voice queries through television-based entertainment systems such as the Comcast X1, the Amazon Fire TV, and Roku TV. Automatic speech recognition (ASR) systems are responsible for transcribing these voice queries into text to feed downstream natural language understanding modules. However, ASR is far from perfect, often producing incorrect transcriptions and forcing users to take corrective action. To better understand their impact on sessions, this paper characterizes speech recognition errors as well as subsequent user responses. We provide both quantitative and qualitative analyses, examining the acoustic as well as lexical attributes of the utterances. This work represents, to our knowledge, the first analysis of speech recognition errors from real users on a widely-deployed entertainment system.",0f1a7429d835d742906fbd3d68282ba6aac16b9d
The Impact of Score Ties on Repeatability in Document Ranking,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}]",2019,"Document ranking experiments should be repeatable. However, the interaction between multi-threaded indexing and score ties during retrieval may yield non-deterministic rankings, making repeatability not as trivial as one might imagine. In the context of the open-source Lucene search engine, score ties are broken by internal document ids, which are assigned at index time. Due to multi-threaded indexing, which makes experimentation with large modern document collections practical, internal document ids are not assigned consistently between different index instances of the same collection, and thus score ties are broken unpredictably. This short paper examines the effectiveness impact of such score ties, quantifying the variability that can be attributed to this phenomenon. The obvious solution to this non-determinism and to ensure repeatable document ranking is to break score ties using external collection document ids. This approach, however, comes with measurable efficiency costs due to the necessity of consulting external identifiers during query evaluation.",ca9c3f29f9d10ba4a69cb6871e0edb01bb7b68a2
"Critically Examining the ""Neural Hype"": Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models","[{'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Kuang Lu', 'dblp_profile': 'https://dblp.org/pid/160/8097.html'}, {'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed skepticism that neural ranking models were actually improving ad hoc retrieval effectiveness in limited data scenarios. He provided anecdotal evidence that authors of neural IR papers demonstrate ""wins"" by comparing against weak baselines. This paper provides a rigorous evaluation of those claims in two ways: First, we conducted a meta-analysis of papers that have reported experimental results on the TREC Robust04 test collection. We do not find evidence of an upward trend in effectiveness over time. In fact, the best reported results are from a decade ago and no recent neural approach comes close. Second, we applied five recent neural models to rerank the strong baselines that Lin used to make his arguments. A significant improvement was observed for one of the models, demonstrating additivity in gains. While there appears to be merit to neural IR approaches, at least some of the gains reported in the literature appear illusory.",c9fb00ca4625c90e5d44ead3bd7e076a744ba169
Solr Integration in the Anserini Information Retrieval Toolkit,"[{'name': 'Ryan Clancy', 'dblp_profile': 'https://dblp.org/pid/245/1822.html'}, {'name': 'Toke Eskildsen', 'dblp_profile': 'https://dblp.org/pid/89/3967.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Anserini is an open-source information retrieval toolkit built around Lucene to facilitate replicable research. In this demonstration, we examine different architectures for Solr integration in order to address two current limitations of the system: the lack of an interactive search interface and support for distributed retrieval. Two architectures are explored: In the first approach, Anserini is used as a frontend to index directly into a running Solr instance. In the second approach, Lucene indexes built directly with Anserini can be copied into a Solr installation and placed under its management. We discuss the tradeoffs associated with each architecture and report the results of a performance evaluation comparing indexing throughput. To illustrate the additional capabilities enabled by Anserini/Solr integration, we present a search interface built using the open-source Blacklight discovery interface.",31fc159be43811170b9906c5809c1583e9778151
Information Retrieval Meets Scalable Text Analytics: Solr Integration with Spark,"[{'name': 'Ryan Clancy', 'dblp_profile': 'https://dblp.org/pid/245/1822.html'}, {'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Zeynep Akkalyoncu Yilmaz', 'dblp_profile': 'https://dblp.org/pid/245/1857.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Despite the broad adoption of both Apache Spark and Apache Solr, there is little integration between these two platforms to support scalable, end-to-end text analytics. We believe this is a missed opportunity, as there is substantial synergy in building analytical pipelines where the results of potentially complex faceted queries feed downstream text processing components. This demonstration explores exactly such an integration: we evaluate performance under different analytical scenarios and present three simple case studies that illustrate the range of possible analyses enabled by seamlessly connecting Spark to Solr.",999df2c1d84096253a7ba5369ce3de0b47fe95f6
Challenges and Opportunities in Understanding Spoken Queries Directed at Modern Entertainment Platforms,"[{'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Modern in-home entertainment platforms---representing the evolution of the humble television of yesteryear---are packed with features and content: they offer a dizzying array of programs spanning hundreds of channels as well as a catalog of on-demand programs offering tens of thousands of options. Furthermore, the entertainment platform may serve as an in-home hub, providing capabilities ranging from playing music to controlling the home security system. At a high level, our goal is to provide natural speech-based access to these myriad features as an alternative to physical button entry on a remote control.",1ea3380472e9b0f1b816b28e87059e279e1fd81b
The SIGIR 2019 Open-Source IR Replicability Challenge (OSIRRC 2019),"[{'name': 'Ryan Clancy', 'dblp_profile': 'https://dblp.org/pid/245/1822.html'}, {'name': 'Nicola Ferro', 'dblp_profile': 'https://dblp.org/pid/f/NicolaFerro.html'}, {'name': 'Claudia Hauff', 'dblp_profile': 'https://dblp.org/pid/73/906.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Tetsuya Sakai', 'dblp_profile': 'https://dblp.org/pid/18/6321.html'}, {'name': 'Ze Zhong Wu', 'dblp_profile': 'https://dblp.org/pid/245/1784.html'}]",2019,"The importance of repeatability, replicability, and reproducibility is broadly recognized in the computational sciences, both in supporting desirable scientific methodology as well as sustaining empirical progress. This workshop tackles the replicability challenge for ad hoc document retrieval, via a common Docker interface specification to support images that capture systems performing ad hoc retrieval experiments on standard test collections.",dd4bc6c7b0c7505bc8fedaebdf0a1918eefd0376
Query and Answer Expansion from Conversation History,"[{'name': 'Jheng-Hong Yang', 'dblp_profile': 'https://dblp.org/pid/227/0821.html'}, {'name': 'Sheng-Chieh Lin', 'dblp_profile': 'https://dblp.org/pid/61/10361.html'}, {'name': 'Chuan-Ju Wang', 'dblp_profile': 'https://dblp.org/pid/03/5904.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ming-Feng Tsai', 'dblp_profile': 'https://dblp.org/pid/16/3313.html'}]",2019,"In this paper, we present our methods, experimental analysis, and final submissions for the Conversational Assistance Track (CAsT) at TREC 2019. In addition to language understanding, extracting knowledge from historical dialogues (e.g., previous queries, search-ing results) is a key to the conversational IR task. However, limited annotated data in the CAsT task makes machine learning or other data-driven approaches infeasible. Along this line, we propose two ad hoc and intuitive approaches: Historical Query Expansion and Historical Answer Expansion, to improve the performance of the conversational IR system with limited training data. Our empirical result on the CAsT training set shows that the proposed methods significantly improve the quality of conversational search in terms of retrieval (recall@1000: 0 . 774 → 0 . 844) and ranking (mAP: 0 . 187 → 0 . 197) compared to our strong baseline. As a result, our submitted entries outperform the median performance of all the 21 teams.",3a0b18510d20475f236edcae97da64a8039d2439
End-to-End Open-Domain Question Answering with BERTserini,"[{'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Aileen Lin', 'dblp_profile': 'https://dblp.org/pid/236/4217.html'}, {'name': 'Xingyu Li', 'dblp_profile': 'https://dblp.org/pid/45/2385.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",2fe7dba5a58aee5156594b4d78634ecd6c7dcabd
Matching Entities Across Different Knowledge Graphs with Graph Embeddings,"[{'name': 'Michael Azmy', 'dblp_profile': 'https://dblp.org/pid/185/6923.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ihab F. Ilyas', 'dblp_profile': 'https://dblp.org/pid/i/IhabFIlyas.html'}]",2019,"This paper explores the problem of matching entities across different knowledge graphs. Given a query entity in one knowledge graph, we wish to find the corresponding real-world entity in another knowledge graph. We formalize this problem and present two large-scale datasets for this task based on exiting cross-ontology links between DBpedia and Wikidata, focused on several hundred thousand ambiguous entities. Using a classification-based approach, we find that a simple multi-layered perceptron based on representations derived from RDF2Vec graph embeddings of entities in each knowledge graph is sufficient to achieve high accuracy, with only small amounts of training data. The contributions of our work are datasets for examining this problem and strong baselines on which future work can be based.",45db2b7a9350ed3929bc96219f722efbec1f1720
Simple Applications of BERT for Ad Hoc Document Retrieval,"[{'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval. This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle. We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores. Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of.",ea57734824426a427f8b9139da1ae574cc929543
Distilling Task-Specific Knowledge from BERT into Simple Neural Networks,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Yao Lu', 'dblp_profile': 'https://dblp.org/pid/26/5662.html'}, {'name': 'Linqing Liu', 'dblp_profile': 'https://dblp.org/pid/36/7028.html'}, {'name': 'Lili Mou', 'dblp_profile': 'https://dblp.org/pid/127/0779.html'}, {'name': 'Olga Vechtomova', 'dblp_profile': 'https://dblp.org/pid/64/3140.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.",a08293b2c9c5bcddb023cc7eb3354d4d86bfae89
Simple BERT Models for Relation Extraction and Semantic Role Labeling,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"We present simple BERT-based models for relation extraction and semantic role labeling. In recent years, state-of-the-art performance has been achieved using neural models by incorporating lexical and syntactic features such as part-of-speech tags and dependency trees. In this paper, extensive experiments on datasets for these two tasks show that without using any external features, a simple BERT-based model can achieve state-of-the-art performance. To our knowledge, we are the first to successfully apply BERT in this manner. Our models provide strong baselines for future research.",fddbcabe0fc9be0684855ae3dd059fb525a69e5b
Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering,"[{'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Yuqing Xie', 'dblp_profile': 'https://dblp.org/pid/236/4350-1.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Kun Xiong', 'dblp_profile': 'https://dblp.org/pid/162/8987.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/l/MingLi1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.",f5eaf727b80240a13e9f631211c9ecec7e3b9feb
Document Expansion by Query Prediction,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}]",2019,"One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.",b092b6b843e9421bf42bf96f57ed4658a3e0bdf7
DocBERT: BERT for Document Classification,"[{'name': 'Ashutosh Adhikari', 'dblp_profile': 'https://dblp.org/pid/230/3772.html'}, {'name': 'Achyudh Ram', 'dblp_profile': 'https://dblp.org/pid/220/8475.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.",1a9954d86466a7e4de6f98ddee452ceb50e15d86
The Simplest Thing That Can Possibly Work: Pseudo-Relevance Feedback Using Text Classification,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Motivated by recent commentary that has questioned today's pursuit of ever-more complex models and mathematical formalisms in applied machine learning and whether meaningful empirical progress is actually being made, this paper tries to tackle the decades-old problem of pseudo-relevance feedback with ""the simplest thing that can possibly work"". I present a technique based on training a document relevance classifier for each information need using pseudo-labels from an initial ranked list and then applying the classifier to rerank the retrieved documents. Experiments demonstrate significant improvements across a number of newswire collections, with initial rankings supplied by ""bag of words"" BM25 as well as from a well-tuned query expansion model. While this simple technique draws elements from several well-known threads in the literature, to my knowledge this exact combination has not previously been proposed and evaluated.",c421a58239e681d4caa1afae13e84d5983a4b8ab
"Critically Examining the ""Neural Hype"": Weak Baselines and the Additivity of Effectiveness Gains from Neural Ranking Models","[{'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Kuang Lu', 'dblp_profile': 'https://dblp.org/pid/160/8097.html'}, {'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed skepticism that neural ranking models were actually improving ad hoc retrieval effectiveness in limited data scenarios. He provided anecdotal evidence that authors of neural IR papers demonstrate ""wins"" by comparing against weak baselines. This paper provides a rigorous evaluation of those claims in two ways: First, we conducted a meta-analysis of papers that have reported experimental results on the TREC Robust04 test collection. We do not find evidence of an upward trend in effectiveness over time. In fact, the best reported results are from a decade ago and no recent neural approach comes close. Second, we applied five recent neural models to rerank the strong baselines that Lin used to make his arguments. A significant improvement was observed for one of the models, demonstrating additivity in gains. While there appears to be merit to neural IR approaches, at least some of the gains reported in the literature appear illusory.",c9fb00ca4625c90e5d44ead3bd7e076a744ba169
"Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data","[{'name': 'Hamidreza Shahidi', 'dblp_profile': 'https://dblp.org/pid/249/5644.html'}, {'name': 'Ming Li', 'dblp_profile': 'https://dblp.org/pid/181/2821.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures. In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks. In this work, we show that this is also the case for text generation from structured and unstructured data. We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively. Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models. Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks. Code is available at https://github.com/h-shahidi/2birds-gen.",24cbc48bb0cb1c275fb88e50965dfe1af57fd784
Aligning Cross-Lingual Entities with Multi-Aspect Information,"[{'name': 'Hsiu-Wei Yang', 'dblp_profile': 'https://dblp.org/pid/204/0080.html'}, {'name': 'Yanyan Zou', 'dblp_profile': 'https://dblp.org/pid/48/6274.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Wei Lu', 'dblp_profile': 'https://dblp.org/pid/98/6613-11.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Xu Sun', 'dblp_profile': 'https://dblp.org/pid/37/1971-1.html'}]",2019,"Multilingual knowledge graphs (KGs), such as YAGO and DBpedia, represent entities in different languages. The task of cross-lingual entity alignment is to match entities in a source language with their counterparts in target languages. In this work, we investigate embedding-based approaches to encode entities from multilingual KGs into the same vector space, where equivalent entities are close to each other. Specifically, we apply graph convolutional networks (GCNs) to combine multi-aspect information of entities, including topological connections, relations, and attributes of entities, to learn entity embeddings. To exploit the literal descriptions of entities expressed in different languages, we propose two uses of a pretrained multilingual BERT model to bridge cross-lingual gaps. We further propose two strategies to integrate GCN-based and BERT-based modules to boost performance. Extensive experiments on two benchmark datasets demonstrate that our method significantly outperforms existing systems.",cf4dcc7d67f0776ffe7aa5b4ee3217f9bd757282
Lucene for Approximate Nearest-Neighbors Search on Arbitrary Dense Vectors,"[{'name': 'Tommaso Teofili', 'dblp_profile': 'https://dblp.org/pid/201/6374.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"We demonstrate three approaches for adapting the open-source Lucene search library to perform approximate nearest-neighbor search on arbitrary dense vectors, using similarity search on word embeddings as a case study. At its core, Lucene is built around inverted indexes of a document collection's (sparse) term-document matrix, which is incompatible with the lower-dimensional dense vectors that are common in deep learning applications. We evaluate three techniques to overcome these challenges that can all be natively integrated into Lucene: the creation of documents populated with fake words, LSH applied to lexical realizations of dense vectors, and k-d trees coupled with dimensionality reduction. Experiments show that the ""fake words"" approach represents the best balance between effectiveness and efficiency. These techniques are integrated into the Anserini open-source toolkit and made available to the community.",89cab01ed15323e90abdf2a12fd3819836034aae
The Performance Envelope of Inverted Indexing on Modern Hardware,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Lori Paniak', 'dblp_profile': 'https://dblp.org/pid/251/5587.html'}, {'name': 'Gordon Boerke', 'dblp_profile': 'https://dblp.org/pid/251/5462.html'}]",2019,"This paper explores the performance envelope of ""traditional"" inverted indexing on modern hardware using the implementation in the open-source Lucene search library. We benchmark indexing throughput on a single high-end multi-core commodity server in a number of configurations varying the media of the source collection and target index, examining a network-attacked store, a direct-attached disk array, and an SSD. Experiments show that the largest determinants of performance are the physical characteristics of the source and target media, and that physically isolating the two yields the highest indexing throughput. Results suggest that current indexing techniques have reached physical device limits, and that further algorithmic improvements in performance are unlikely without rethinking the inverted indexing pipeline in light of observed bottlenecks.",e21022fee9d581f9b4680f9510ccbaea8a1cea94
Multi-Stage Document Ranking with BERT,"[{'name': 'Rodrigo Frassetto Nogueira', 'dblp_profile': 'https://dblp.org/pid/175/1545.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Kyunghyun Cho', 'dblp_profile': 'https://dblp.org/pid/41/9736.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"The advent of deep neural networks pre-trained via language modeling tasks has spurred a number of successful applications in natural language processing. This work explores one such popular model, BERT, in the context of document ranking. We propose two variants, called monoBERT and duoBERT, that formulate the ranking problem as pointwise and pairwise classification, respectively. These two models are arranged in a multi-stage ranking architecture to form an end-to-end search system. One major advantage of this design is the ability to trade off quality against latency by controlling the admission of candidates into each pipeline stage, and by doing so, we are able to find operating points that offer a good balance between these two competing metrics. On two large-scale datasets, MS MARCO and TREC CAR, experiments show that our model produces results that are either at or comparable to the state of the art. Ablation studies show the contributions of each component and characterize the latency/quality tradeoff space.",63a2fabbe4b1615a84d5f4d90987733cf09e3ff8
Explicit Pairwise Word Interaction Modeling Improves Pretrained Transformers for English Semantic Similarity Tasks,"[{'name': 'Yinan Zhang', 'dblp_profile': 'https://dblp.org/pid/19/6790.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"In English semantic similarity tasks, classic word embedding-based approaches explicitly model pairwise ""interactions"" between the word representations of a sentence pair. Transformer-based pretrained language models disregard this notion, instead modeling pairwise word interactions globally and implicitly through their self-attention mechanism. In this paper, we hypothesize that introducing an explicit, constrained pairwise word interaction mechanism to pretrained language models improves their effectiveness on semantic similarity tasks. We validate our hypothesis using BERT on four tasks in semantic textual similarity and answer sentence selection. We demonstrate consistent improvements in quality by adding an explicit pairwise word interaction module to BERT.",a200767587cd8e897f96b3f3aa36002d78a76f06
Cross-Lingual Relevance Transfer for Document Retrieval,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Recent work has shown the surprising ability of multi-lingual BERT to serve as a zero-shot cross-lingual transfer model for a number of language processing tasks. We combine this finding with a similarly-recently proposal on sentence-level relevance modeling for document retrieval to demonstrate the ability of multi-lingual BERT to transfer models of relevance across languages. Experiments on test collections in five different languages from diverse language families (Chinese, Arabic, French, Hindi, and Bengali) show that models trained with English data improve ranking quality, without any special processing, both for (non-English) mono-lingual retrieval as well as cross-lingual retrieval.",6b556a6dd7221aa431941251555e52492a2878f5
What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning,"[{'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Pretrained transformer-based language models have achieved state of the art across countless tasks in natural language processing. These models are highly expressive, comprising at least a hundred million parameters and a dozen layers. Recent evidence suggests that only a few of the final layers need to be fine-tuned for high quality on downstream tasks. Naturally, a subsequent research question is, ""how many of the last layers do we need to fine-tune?"" In this paper, we precisely answer this question. We examine two recent pretrained language models, BERT and RoBERTa, across standard tasks in textual entailment, semantic similarity, sentiment analysis, and linguistic acceptability. We vary the number of final layers that are fine-tuned, then study the resulting change in task-specific effectiveness. We show that only a fourth of the final layers need to be fine-tuned to achieve 90% of the original quality. Surprisingly, we also find that fine-tuning all layers does not always help.",4a4646a5ce6b57e369403e4efea1a2e4559fe9f1
Attentive Student Meets Multi-Task Teacher: Improved Knowledge Distillation for Pretrained Models,"[{'name': 'Linqing Liu', 'dblp_profile': 'https://dblp.org/pid/36/7028.html'}, {'name': 'Huan Wang', 'dblp_profile': 'https://dblp.org/pid/70/6155.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Richard Socher', 'dblp_profile': 'https://dblp.org/pid/79/128.html'}, {'name': 'Caiming Xiong', 'dblp_profile': 'https://dblp.org/pid/80/7282.html'}]",2019,"In this paper, we explore the knowledge distillation approach under the multi-task learning setting. We distill the BERT model refined by multi-task learning on seven datasets of the GLUE benchmark into a bidirectional LSTM with attention mechanism. Unlike other BERT distillation methods which specifically designed for Transformer-based architectures, we provide a general learning framework. Our approach is model agnostic and can be easily applied on different future teacher models. Compared to a strong, similarly BiLSTM-based approach, we achieve better quality under the same computational constraints. Compared to the present state of the art, we reach comparable results with much faster inference speed.",b69ed195aacff0b6410eb3b8431ea968e42620af
The Proper Care and Feeding of CAMELS: How Limited Training Data Affects Streamflow Prediction,"[{'name': 'Martin Gauch', 'dblp_profile': 'https://dblp.org/pid/235/0335.html'}, {'name': 'Juliane Mai', 'dblp_profile': 'https://dblp.org/pid/227/4944.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,,4e1cdcb84fefaa0b813dd2b6baed6f13092ca46c
Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits,"[{'name': 'Achyudh Ram', 'dblp_profile': 'https://dblp.org/pid/220/8475.html'}, {'name': 'Ji Xin', 'dblp_profile': 'https://dblp.org/pid/218/7227.html'}, {'name': 'Meiyappan Nagappan', 'dblp_profile': 'https://dblp.org/pid/27/2828.html'}, {'name': 'Yaoliang Yu', 'dblp_profile': 'https://dblp.org/pid/90/4989.html'}, {'name': 'Rocío Cabrera Lozoya', 'dblp_profile': 'https://dblp.org/pid/120/4475.html'}, {'name': 'Antonino Sabetta', 'dblp_profile': 'https://dblp.org/pid/05/4081.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2019,"Public vulnerability databases such as CVE and NVD account for only 60% of security vulnerabilities present in open-source projects, and are known to suffer from inconsistent quality. Over the last two years, there has been considerable growth in the number of known vulnerabilities across projects available in various repositories such as NPM and Maven Central. Such an increasing risk calls for a mechanism to infer the presence of security threats in a timely manner. We propose novel hierarchical deep learning models for the identification of security-relevant commits from either the commit diff or the source code for the Java classes. By comparing the performance of our model against code2vec, a state-of-the-art model that learns from path-based representations of code, and a logistic regression baseline, we show that deep learning models show promising results in identifying security-related commits. We also conduct a comparative analysis of how various deep learning models learn across different input representations and the effect of regularization on the generalization of our models.",ca4ddad3aff3c1808ade7b2cfbfc93ca36d01d70
Scale Up or Scale Out for Graph Processing?,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"This column explores a simple question: scale up or scale out for graph processing? Should we simply throw beefier individual multi-core, large-memory machines at graph processing tasks and focus on developing more efficient multi-threaded algorithms, or are investments in distributed graph processing frameworks and accompanying algorithms worthwhile? For rhetorical convenience, I adopt customary definitions, referring to the former as scale up and the latter as scale out. Under what circumstances should we prefer one approach over the other?",28a5eb484459eac210c055d07fb0e9fbbd4b7f33
Evaluation-as-a-Service for the Computational Sciences: Overview and Outlook,"[{'name': 'Frank Hopfgartner', 'dblp_profile': 'https://dblp.org/pid/85/5730.html'}, {'name': 'Allan Hanbury', 'dblp_profile': 'https://dblp.org/pid/55/6683.html'}, {'name': 'Henning Müller', 'dblp_profile': 'https://dblp.org/pid/93/4446.html'}, {'name': 'Ivan Eggel', 'dblp_profile': 'https://dblp.org/pid/27/7331.html'}, {'name': 'Krisztian Balog', 'dblp_profile': 'https://dblp.org/pid/85/4125.html'}, {'name': 'Torben Brodt', 'dblp_profile': 'https://dblp.org/pid/135/6607.html'}, {'name': 'Gordon V. Cormack', 'dblp_profile': 'https://dblp.org/pid/c/GVCormack.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jayashree Kalpathy-Cramer', 'dblp_profile': 'https://dblp.org/pid/50/3269.html'}, {'name': 'Noriko Kando', 'dblp_profile': 'https://dblp.org/pid/40/4968.html'}, {'name': 'Makoto P. Kato', 'dblp_profile': 'https://dblp.org/pid/14/7537.html'}, {'name': 'Anastasia Krithara', 'dblp_profile': 'https://dblp.org/pid/32/6354.html'}, {'name': 'Tim Gollub', 'dblp_profile': 'https://dblp.org/pid/17/10440.html'}, {'name': 'Martin Potthast', 'dblp_profile': 'https://dblp.org/pid/87/6573.html'}, {'name': 'Evelyne Viegas', 'dblp_profile': 'https://dblp.org/pid/00/4233.html'}, {'name': 'Simon Mercer', 'dblp_profile': 'https://dblp.org/pid/78/4920.html'}]",2018,,
Anserini: Reproducible Ranking Baselines Using Lucene,"[{'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}, {'name': 'Hui Fang', 'dblp_profile': 'https://dblp.org/pid/03/2511-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"This work tackles the perennial problem of reproducible baselines in information retrieval research, focusing on bag-of-words ranking models. Although academic information retrieval researchers have a long history of building and sharing systems, they are primarily designed to facilitate the publication of research papers. As such, these systems are often incomplete, inflexible, poorly documented, difficult to use, and slow, particularly in the context of modern web-scale collections. Furthermore, the growing complexity of modern software ecosystems and the resource constraints most academic research groups operate under make maintaining open-source systems a constant struggle. However, except for a small number of companies (mostly commercial web search engines) that deploy custom infrastructure, Lucene has become the de facto platform in industry for building search applications. Lucene has an active developer base, a large audience of users, and diverse capabilities to work with heterogeneous collections at scale. However, it lacks systematic support for ad hoc experimentation using standard test collections. We describe Anserini, an information retrieval toolkit built on Lucene that fills this gap. Our goal is to simplify ad hoc experimentation and allow researchers to easily reproduce results with modern bag-of-words ranking models on diverse test collections. With Anserini, we demonstrate that Lucene provides a suitable framework for supporting information retrieval research. Experiments show that our system efficiently indexes large web collections, provides modern ranking models that are on par with research implementations in terms of effectiveness, and supports low-latency query evaluation to facilitate rapid experimentation",1490a399d8e9b7e5eb5059b5556909adc04c6b12
The Neural Hype and Comparisons Against Weak Baselines,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Recently, the machine learning community paused in a moment of self-reflection. In a widelydiscussed paper at ICLR 2018, Sculley et al. [13] wrote: ""We observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the field as a whole."" Their primary complaint is the development of a ""research and publication culture that emphasizes wins"" (emphasis in original), which typically means ""demonstrating that a new method beats previous methods on a given task or benchmark"". An apt description might be ""leaderboard chasing""-and for many vision and NLP tasks, this isn't a metaphor. There are literally centralized leaderboards1 that track incremental progress, down to the fifth decimal point, some persisting over years, accumulating dozens of entries. Sculley et al. remind us that ""the goal of science is not wins, but knowledge"". The structure of the scientific enterprise today (pressure to publish, pace of progress, etc.) means that ""winning"" and ""doing good science"" are often not fully aligned. To wit, they cite a number of papers showing that recent advances in neural networks could very well be attributed to mundane issues like better hyperparameter optimization. Many results can't be reproduced, and some observed improvements might just be noise.",22d31d29606ca7c9617f249408f6b1d70c586862
Serverless Data Analytics with Flint,"[{'name': 'Youngbin Kim', 'dblp_profile': 'https://dblp.org/pid/89/8603.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Serverless architectures organized around loosely-coupled function invocations represent an emerging design for many applications. Recent work mostly focuses on user-facing products and event-driven processing pipelines. In this paper, we explore a completely different part of the application space and examine the feasibility of analytical processing on big data using a serverless architecture. We present Flint, a prototype Spark execution engine that takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model. With Flint, a developer uses PySpark exactly as before, but without needing an actual Spark cluster. We describe the design, implementation, and performance of Flint, along with the challenges associated with serverless analytics.",3e9629366f3f2ddf48122b9d6ad973bca212037e
Farewell Freebase: Migrating the SimpleQuestions Dataset to DBpedia,"[{'name': 'Michael Azmy', 'dblp_profile': 'https://dblp.org/pid/185/6923.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ihab F. Ilyas', 'dblp_profile': 'https://dblp.org/pid/i/IhabFIlyas.html'}]",2018,"Question answering over knowledge graphs is an important problem of interest both commercially and academically. There is substantial interest in the class of natural language questions that can be answered via the lookup of a single fact, driven by the availability of the popular SimpleQuestions dataset. The problem with this dataset, however, is that answer triples are provided from Freebase, which has been defunct for several years. As a result, it is difficult to build “real-world” question answering systems that are operationally deployable. Furthermore, a defunct knowledge graph means that much of the infrastructure for querying, browsing, and manipulating triples no longer exists. To address this problem, we present SimpleDBpediaQA, a new benchmark dataset for simple question answering over knowledge graphs that was created by mapping SimpleQuestions entities and predicates from Freebase to DBpedia. Although this mapping is conceptually straightforward, there are a number of nuances that make the task non-trivial, owing to the different conceptual organizations of the two knowledge graphs. To lay the foundation for future research using this dataset, we leverage recent work to provide simple yet strong baselines with and without neural networks.",bf6bc1cb32c15fccb20c9d0f57a8f52056be5948
"Computing without Servers, V8, Rocket Ships, and Other Batsh*t Crazy Ideas in Data Systems","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"1 TWO TRENDS Computing is simultaneously becoming more centralized and more distributed. A relatively small handful of companies are building increasingly-large datacenters to power centralized cloud services at mind-boggling scales. At the other end of the spectrum, personal computing devices continue to proliferate: Although mobile phones and tablets are reaching saturation, smart personal devices and home gadgets, not to mention other devices falling under the Internet of Things umbrella, are increasingly ubiquitous. My talk discusses these two trends in the context of information retrieval systems, andmore broadly, data systems that store, process, analyze, search, and manipulate large amounts of data. I share some research by my colleagues, students, and myself exploring implications of these trends that are “unconventional”, “off the beaten path”, or simply batsh*t crazy.",ea2567295df03f9f5983e9f1d4654df60f13cfad
RecService: Distributed Real-Time Graph Processing at Twitter,"[{'name': 'Ajeet Grewal', 'dblp_profile': 'https://dblp.org/pid/149/5948.html'}, {'name': 'Jerry Jiang', 'dblp_profile': 'https://dblp.org/pid/185/4257.html'}, {'name': 'Gary Lam', 'dblp_profile': 'https://dblp.org/pid/83/6999.html'}, {'name': 'Tristan Jung', 'dblp_profile': 'https://dblp.org/pid/223/0732.html'}, {'name': 'Lohith Vuddemarri', 'dblp_profile': 'https://dblp.org/pid/223/0692.html'}, {'name': 'Quannan Li', 'dblp_profile': 'https://dblp.org/pid/26/6832.html'}, {'name': 'Aaditya Landge', 'dblp_profile': 'https://dblp.org/pid/223/0726.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"We present RecService, a distributed real-time graph processing engine that drives billions of recommendations on Twitter. Real-time recommendations are framed in terms of a user’s social context and real-time events incident on that social context, generated from ad hoc point queries and long-lived standing queries. Results form the basis of downstream processes that power a variety of recommendation products. A noteworthy aspect of the system’s design is a partitioning scheme whereby manipulations of graph adjacency lists are local to a cluster node. This eliminates cross-node network traffic in query execution, enabling horizontal scalability and avoiding “hot spots” caused by vertices with large degrees.",cf084450b2a8bbc88bf0e1c316ccdf95efe5bb62
An Experimental Analysis of the Power Consumption of Convolutional Neural Networks for Keyword Spotting,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Weijie Wang', 'dblp_profile': 'https://dblp.org/pid/18/689.html'}, {'name': 'Zhucheng Tu', 'dblp_profile': 'https://dblp.org/pid/195/7964.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Nearly all previous work on small-footprint keyword spotting with neural networks quantify model footprint in terms of the number of parameters and multiply operations for a feedforward inference pass. These values are, however, proxy measures since empirical performance in actual deployments is determined by many factors. In this paper, we study the power consumption of a family of convolutional neural networks for keyword spotting on a Raspberry Pi. We find that both proxies are good predictors of energy usage, although the number of multiplies is more predictive than the number of model parameters. We also confirm that models with the highest accuracies are, unsurprisingly, the most power hungry.",c95cb204a9b45a81fc8b12eec57ae6f421730d2c
Deep Residual Learning for Small-Footprint Keyword Spotting,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"We explore the application of deep residual learning and dilated convolutions to the keyword spotting task, using the recently-released Google Speech Commands Dataset as our benchmark. Our best residual network (ResNet) implementation significantly outperforms Google's previous convolutional neural networks in terms of accuracy. By varying model depth and width, we can achieve compact models that also outperform previous small-footprint variants. To our knowledge, we are the first to examine these approaches for keyword spotting, and our results establish an open-source state-of-the-art reference to support the development of future speech-based interfaces.",4ee8622f5dacb44e4af6bc9ee1c8f48a48983d9a
Multi-Task Learning with Neural Networks for Voice Query Understanding on an Entertainment Platform,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"We tackle the challenge of understanding voice queries posed against the Comcast Xfinity X1 entertainment platform, where consumers direct speech input at their ""voice remotes"". Such queries range from specific program navigation (i.e., watch a movie) to requests with vague intents and even queries that have nothing to do with watching TV. We present successively richer neural network architectures to tackle this challenge based on two key insights: The first is that session context can be exploited to disambiguate queries and recover from ASR errors, which we operationalize with hierarchical recurrent neural networks. The second insight is that query understanding requires evidence integration across multiple related tasks, which we identify as program prediction, intent classification, and query tagging. We present a novel multi-task neural architecture that jointly learns to accomplish all three tasks. Our initial model, already deployed in production, serves millions of queries daily with an improved customer experience. The novel multi-task learning model, first described here, is evaluated through carefully-controlled laboratory experiments, which demonstrates further gains in effectiveness and increased system capabilities.",beccae35572827f1ddffc50c0bba7be612bb80bf
Pay-Per-Request Deployment of Neural Network Models Using Serverless Architectures,"[{'name': 'Zhucheng Tu', 'dblp_profile': 'https://dblp.org/pid/195/7964.html'}, {'name': 'Mengping Li', 'dblp_profile': 'https://dblp.org/pid/220/2291.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"We demonstrate the serverless deployment of neural networks for model inferencing in NLP applications using Amazon’s Lambda service for feedforward evaluation and DynamoDB for storing word embeddings. Our architecture realizes a pay-per-request pricing model, requiring zero ongoing costs for maintaining server instances. All virtual machine management is handled behind the scenes by the cloud provider without any direct developer intervention. We describe a number of techniques that allow efficient use of serverless resources, and evaluations confirm that our design is both scalable and inexpensive.",cbbf21a41b7779f64008a2720480d46223c8d615
CNNs for NLP in the Browser: Client-Side Deployment and Visualization Opportunities,"[{'name': 'Yiyun Liang', 'dblp_profile': 'https://dblp.org/pid/220/2189.html'}, {'name': 'Zhucheng Tu', 'dblp_profile': 'https://dblp.org/pid/195/7964.html'}, {'name': 'Laetitia Huang', 'dblp_profile': 'https://dblp.org/pid/220/2342.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"We demonstrate a JavaScript implementation of a convolutional neural network that performs feedforward inference completely in the browser. Such a deployment means that models can run completely on the client, on a wide range of devices, without making backend server requests. This design is useful for applications with stringent latency requirements or low connectivity. Our evaluations show the feasibility of JavaScript as a deployment target. Furthermore, an in-browser implementation enables seamless integration with the JavaScript ecosystem for information visualization, providing opportunities to visually inspect neural networks and better understand their inner workings.",b0723029fab0b3b5deb904ea0b8c96c660fc92ef
Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks,"[{'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"We examine the problem of question answering over knowledge graphs, focusing on simple questions that can be answered by the lookup of a single fact. Adopting a straightforward decomposition of the problem into entity detection, entity linking, relation prediction, and evidence combination, we explore simple yet strong baselines. On the popular SimpleQuestions dataset, we find that basic LSTMs and GRUs plus a few heuristics yield accuracies that approach the state of the art, and techniques that do not use neural networks also perform reasonably well. These results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity.",1154add9af6c19c8b264331308ec99776583c52d
Update Delivery Mechanisms for Prospective Information Needs: An Analysis of Attention in Mobile Users,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}]",2018,"Real-time summarization systems that monitor document streams to identify relevant content have a few options for delivering system updates to users. In a mobile context, systems could send push notifications to users' mobile devices, hoping to grab their attention immediately. Alternatively, systems could silently deposit updates into ""inboxes"" that users can access at their leisure. We refer to these mechanisms as push-based vs. pull-based, and present a two-year contrastive study that attempts to understand the effects of the delivery mechanism on mobile user behavior, in the context of the TREC Real-Time Summarization Tracks. Through a cluster analysis, we are able to identify three distinct and coherent patterns of behavior. As expected, we find that users are likely to ignore push notifications, but for those updates that users do pay attention to, content is consumed within a short amount of time. Interestingly, users bombarded with push notifications are less likely to consume updates on their own initiative and less likely to engage in long reading sessions---which is a common pattern for users who pull content from their inboxes. We characterize users as exhibiting ""eager"" or ""apathetic"" information consumption behavior as an explanation of these observations, and attempt to operationalize our findings into design recommendations.",71a6c7e16f9461bba32d9b90fb7ec67664a7426c
What Do Viewers Say to Their TVs?: An Analysis of Voice Queries to Entertainment Systems,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"A recently-introduced product of Comcast, a large cable company in the United States, is a ""voice remote"" that accepts spoken queries from viewers. We present an analysis of a large query log from this service to answer the question: ""What do viewers say to their TVs?"" In addition to a descriptive characterization of queries and sessions, we describe two complementary types of analyses to support query understanding. First, we propose a domain-specific intent taxonomy to characterize viewer behavior: as expected, most intents revolve around watching programs---both direct navigation as well as browsing---but there is a non-trivial fraction of non-viewing intents as well. Second, we propose a domain-specific tagging scheme for labeling query tokens, that when combined with intent and program prediction, provides a multi-faceted approach to understand voice queries directed at entertainment systems.",836ca0698fca9bd9064db242273fd03714782eb8
The Evolution of Content Analysis for Personalized Recommendations at Twitter,"[{'name': 'Ajeet Grewal', 'dblp_profile': 'https://dblp.org/pid/149/5948.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"We present a broad overview of personalized content recommendations at Twitter, discussing how our approach has evolved over the years, represented by several generations of systems. Historically, content analysis of Tweets has not been a priority, and instead engineering efforts have focused on graph-based recommendation techniques that exploit structural properties of the follow graph and engagement signals from users. These represent ""low hanging fruits"" that have enabled high-quality recommendations using simple algorithms. As deployed systems have grown in maturity and our understanding of the problem space has become more refined, we have begun to look for other opportunities to further improve recommendation quality. We overview recent investments in content analysis, particularly named-entity recognition techniques built around recurrent neural networks, and discuss how they integrate with existing graph-based capabilities to open up the design space of content recommendation algorithms.",c4f64976777c94b04291a305b8e5a573605d04fa
"Robust, Scalable, Real-Time Event Time Series Aggregation at Twitter","[{'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}, {'name': 'Srikanth Thiagarajan', 'dblp_profile': 'https://dblp.org/pid/219/9682.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Twitter's data engineering team is faced with the challenge of processing billions of events every day in batch and in real time, and we have built various tools to meet these demands. In this paper, we describe TSAR (TimeSeries AggregatoR), a robust, scalable, real-time event time series aggregation framework built primarily for engagement monitoring: aggregating interactions with Tweets, segmented along a multitude of dimensions such as device, engagement type, etc. TSAR is built on top of Summingbird, an open-source framework for integrating batch and online MapReduce computations, and removes much of the tedium associated with building end-to-end aggregation pipelines---from the ingestion and processing of events to the publication of results in heterogeneous datastores. Clients are provided a query interface that powers dashboards and supports downstream ad hoc analytics.",54befeedfaa980bd40f0df8fb9e091fcb81b592a
Overview of the TREC 2018 Real-Time Summarization Track,"[{'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,,
H2oloo at TREC 2018: Cross-Collection Relevance Transfer for the Common Core Track,"[{'name': 'Ruifan Yu', 'dblp_profile': 'https://dblp.org/pid/232/4130.html'}, {'name': 'Yuhao Xie', 'dblp_profile': 'https://dblp.org/pid/238/6385.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,,dfbe3538750137fad45673a0ec84b054ee2956d8
Query Driven Algorithm Selection in Early Stage Retrieval,"[{'name': 'Joel M. Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'J. Shane Culpepper', 'dblp_profile': 'https://dblp.org/pid/03/489.html'}, {'name': 'Roi Blanco', 'dblp_profile': 'https://dblp.org/pid/30/3795.html'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Large scale retrieval systems often employ cascaded ranking architectures, in which an initial set of candidate documents are iteratively refined and re-ranked by increasingly sophisticated and expensive ranking models. In this paper, we propose a unified framework for predicting a range of performance-sensitive parameters based on minimizing end-to-end effectiveness loss. The framework does not require relevance judgments for training, is amenable to predicting a wide range of parameters, allows for fine tuned efficiency-effectiveness trade-offs, and can be easily deployed in large scale search systems with minimal overhead. As a proof of concept, we show that the framework can accurately predict a number of performance parameters on a query-by-query basis, allowing efficient and effective retrieval, while simultaneously minimizing the tail latency of an early-stage candidate generation system. On the 50 million document ClueWeb09B collection, and across 25,000 queries, our hybrid system can achieve superior early-stage efficiency to fixed parameter systems without loss of effectiveness, and allows more finely-grained efficiency-effectiveness trade-offs across the multiple stages of the retrieval system.",081d6cf68ab29a71286d8bd8fd774546b1073ef8
Summarization,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,,3bddf90599370d7b1ec8a5bb45799b7e276a3ab8
Serverless Data Analytics with Flint,"[{'name': 'Youngbin Kim', 'dblp_profile': 'https://dblp.org/pid/89/8603.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Serverless architectures organized around loosely-coupled function invocations represent an emerging design for many applications. Recent work mostly focuses on user-facing products and event-driven processing pipelines. In this paper, we explore a completely different part of the application space and examine the feasibility of analytical processing on big data using a serverless architecture. We present Flint, a prototype Spark execution engine that takes advantage of AWS Lambda to provide a pure pay-as-you-go cost model. With Flint, a developer uses PySpark exactly as before, but without needing an actual Spark cluster. We describe the design, implementation, and performance of Flint, along with the challenges associated with serverless analytics.",3e9629366f3f2ddf48122b9d6ad973bca212037e
In-Browser Split-Execution Support for Interactive Analytics in the Cloud,"[{'name': 'Kareem El Gebaly', 'dblp_profile': 'https://dblp.org/pid/53/3059.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"The canonical analytics architecture today consists of a browser connected to a backend in the cloud. In all deployments that we are aware of, the browser is simply a dumb rendering endpoint. As an alternative, this paper explores split-execution architectures that push analytics capabilities into the browser. We show that, by taking advantage of typed arrays and asm.js, it is possible to build an analytical RDBMS in JavaScript that runs in a browser, achieving performance rivaling native databases. To support interactive data exploration, our Afterburner prototype automatically generates local materialized views from a backend database that are then shipped to the browser to facilitate subsequent interactions seamlessly and efficiently. We compare this architecture to several alternative deployments, experimentally demonstrating performance parity, while at the same time providing additional advantages in terms of administrative and operational simplicity.",1d9115701fae7050e92ac46152819d403f2ea6f9
Multi-Perspective Relevance Matching with Hierarchical ConvNets for Social Media Search,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Wei Yang', 'dblp_profile': 'https://dblp.org/pid/03/1094-17.html'}, {'name': 'Yuhao Zhang', 'dblp_profile': 'https://dblp.org/pid/139/5876-4.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Despite substantial interest in applications of neural networks to information retrieval, neural ranking models have mostly been applied to “standard” ad hoc retrieval tasks over web pages and newswire articles. This paper proposes MP-HCNN (Multi-Perspective Hierarchical Convolutional Neural Network), a novel neural ranking model specifically designed for ranking short social media posts. We identify document length, informal language, and heterogeneous relevance signals as features that distinguish documents in our domain, and present a model specifically designed with these characteristics in mind. Our model uses hierarchical convolutional layers to learn latent semantic soft-match relevance signals at the character, word, and phrase levels. A poolingbased similarity measurement layer integrates evidence from multiple types of matches between the query, the social media post, as well as URLs contained in the post. Extensive experiments using Twitter data from the TREC Microblog Tracks 2011–2014 show that our model significantly outperforms prior feature-based as well as existing neural ranking models. To our best knowledge, this paper presents the first substantial work tackling search over social media posts using neural ranking models. Our code and data are publicly available.1",2f61e9f3cf90b42db2571efd19c2c7513c3be09b
Sapphire: Querying RDF Data Made Simple,"[{'name': 'Ahmed El-Roby', 'dblp_profile': 'https://dblp.org/pid/135/4659.html'}, {'name': 'Khalid Ammar', 'dblp_profile': 'https://dblp.org/pid/29/9884.html'}, {'name': 'Ashraf Aboulnaga', 'dblp_profile': 'https://dblp.org/pid/01/3996.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"There is currently a large amount of publicly accessible structured data available as RDF data sets. For example, the Linked Open Data (LOD) cloud now consists of thousands of RDF data sets with over 30 billion triples, and the number and size of the data sets is continuously growing. Many of the data sets in the LOD cloud provide public SPARQL endpoints to allow issuing queries over them. These end-points enable users to retrieve data using precise and highly expressive SPARQL queries. However, in order to do so, the user must have sufficient knowledge about the data sets that she wishes to query, that is, the structure of data, the vocabulary used within the data set, the exact values of literals, their data types, etc. Thus, while SPARQL is powerful, it is not easy to use. An alternative to SPARQL that does not require as much prior knowledge of the data is some form of keyword search over the structured data. Keyword search queries are easy to use, but inherently ambiguous in describing structured queries. 
 
This demonstration introduces Sapphire, a system for querying RDF data that strikes a middle ground between ambiguous keyword search and difficult-to-use SPARQL. Our system does not replace either, but utilizes both where they are most effective. Sapphire helps the user construct expressive SPARQL queries that represent her information needs without requiring detailed knowledge about the queried data sets. These queries are then executed over public SPARQL endpoints from the LOD cloud. Sapphire guides the user in the query writing process by showing suggestions of query terms based on the queried data, and by recommending changes to the query based on a predictive user model.",343500e0052eb1b683f32b00efbbd1331c94184a
Repeatability Corner Cases in Document Ranking: The Impact of Score Ties,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}]",2018,,
Adaptive Pruning of Neural Language Models for Mobile Devices,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Neural language models (NLMs) exist in an accuracy-efficiency tradeoff space where better perplexity typically comes at the cost of greater computation complexity. In a software keyboard application on mobile devices, this translates into higher power consumption and shorter battery life. This paper represents the first attempt, to our knowledge, in exploring accuracy-efficiency tradeoffs for NLMs. Building on quasi-recurrent neural networks (QRNNs), we apply pruning techniques to provide a ""knob"" to select different operating points. In addition, we propose a simple technique to recover some perplexity using a negligible amount of memory. Our empirical evaluations consider both perplexity as well as energy consumption on a Raspberry Pi, where we demonstrate which methods provide the best perplexity-power consumption operating point. At one operating point, one of the techniques is able to provide energy savings of 40% over the state of the art with only a 17% relative increase in perplexity.",39dd6d4e9c70a268816d18a5762bd0101cd830f0
JavaScript Convolutional Neural Networks for Keyword Spotting in the Browser: An Experimental Analysis,"[{'name': 'Jaejun Lee', 'dblp_profile': 'https://dblp.org/pid/31/7077.html'}, {'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Used for simple commands recognition on devices from smart routers to mobile phones, keyword spotting systems are everywhere. Ubiquitous as well are web applications, which have grown in popularity and complexity over the last decade with significant improvements in usability under cross-platform conditions. However, despite their obvious advantage in natural language interaction, voice-enabled web applications are still far and few between. In this work, we attempt to bridge this gap by bringing keyword spotting capabilities directly into the browser. To our knowledge, we are the first to demonstrate a fully-functional implementation of convolutional neural networks in pure JavaScript that runs in any standards-compliant browser. We also apply network slimming, a model compression technique, to explore the accuracy-efficiency tradeoffs, reporting latency measurements on a range of devices and software. Overall, our robust, cross-device implementation for keyword spotting realizes a new paradigm for serving neural network applications, and one of our slim models reduces latency by 66% with a minimal decrease in accuracy of 4% from 94% to 90%.",729b50951bac8640fa0de0bb6aec2affabc64d1a
Progress and Tradeoffs in Neural Language Models,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"In recent years, we have witnessed a dramatic shift towards techniques driven by neural networks for a variety of NLP tasks. Undoubtedly, neural language models (NLMs) have reduced perplexity by impressive amounts. This progress, however, comes at a substantial cost in performance, in terms of inference latency and energy consumption, which is particularly of concern in deployments on mobile devices. This paper, which examines the quality-performance tradeoff of various language modeling techniques, represents to our knowledge the first to make this observation. We compare state-of-the-art NLMs with ""classic"" Kneser-Ney (KN) LMs in terms of energy usage, latency, perplexity, and prediction accuracy using two standard benchmarks. On a Raspberry Pi, we find that orders of increase in latency and energy usage correspond to less change in perplexity, while the difference is much less pronounced on a desktop.",a208e6e70f47753bc6319d851f26978425ed3d24
Simple Attention-Based Representation Learning for Ranking Short Social Media Posts,"[{'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"This paper explores the problem of ranking short social media posts with respect to user queries using neural networks. Instead of starting with a complex architecture, we proceed from the bottom up and examine the effectiveness of a simple, word-level Siamese architecture augmented with attention-based mechanisms for capturing semantic “soft” matches between query and post tokens. Extensive experiments on datasets from the TREC Microblog Tracks show that our simple models not only achieve better effectiveness than existing approaches that are far more complex or exploit a more diverse set of relevance signals, but are also much faster.",6e22f27b65093cc2d24a49b281e7b782ad06c4d4
FLOPs as a Direct Optimization Objective for Learning Sparse Neural Networks,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Ashutosh Adhikari', 'dblp_profile': 'https://dblp.org/pid/230/3772.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"There exists a plethora of techniques for inducing structured sparsity in parametric models during the optimization process, with the final goal of resource-efficient inference. However, few methods target a specific number of floating-point operations (FLOPs) as part of the optimization objective, despite many reporting FLOPs as part of the results. Furthermore, a one-size-fits-all approach ignores realistic system constraints, which differ significantly between, say, a GPU and a mobile phone -- FLOPs on the former incur less latency than on the latter; thus, it is important for practitioners to be able to specify a target number of FLOPs during model compression. In this work, we extend a state-of-the-art technique to directly incorporate FLOPs as part of the optimization objective and show that, given a desired FLOPs requirement, different neural networks can be successfully trained for image classification.",309e4a8d1ac0a32832447ac0ffb09df8da9894c7
Streaming Voice Query Recognition using Causal Convolutional Recurrent Neural Networks,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Gefei Yang', 'dblp_profile': 'https://dblp.org/pid/229/5810.html'}, {'name': 'Hong Wei', 'dblp_profile': 'https://dblp.org/pid/22/441.html'}, {'name': 'Yajie Mao', 'dblp_profile': 'https://dblp.org/pid/232/2455.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2018,"Voice-enabled commercial products are ubiquitous, typically enabled by lightweight on-device keyword spotting (KWS) and full automatic speech recognition (ASR) in the cloud. ASR systems require significant computational resources in training and for inference, not to mention copious amounts of annotated speech data. KWS systems, on the other hand, are less resource-intensive but have limited capabilities. On the Comcast Xfinity X1 entertainment platform, we explore a middle ground between ASR and KWS: We introduce a novel, resource-efficient neural network for voice query recognition that is much more accurate than state-of-the-art CNNs for KWS, yet can be easily trained and deployed with limited resources. On an evaluation dataset representing the top 200 voice queries, we achieve a low false alarm rate of 1% and a query error rate of 6%. Our model performs inference 8.24x faster than the current ASR system.",87d4e7caaa5940aee1bff826bd4b702d72e189c9
In Defense of MapReduce,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,Don't throw the MapReduce baby out with the bath water!,e15541855ed6b514b62ba70e308b4865c305da35
The Lambda and the Kappa,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Whether lambda or kappa, there’s no free lunch!",f2f0befc0b8798364734be1a840eef8ed7c0ea96
The role of index compression in score-at-a-time query evaluation,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}]",2017,,3b70f47daef52168f3a6b743ad729cc8b25f5736
Warcbase: Scalable Analytics Infrastructure for Exploring Web Archives,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Jeremy Wiebe', 'dblp_profile': 'https://dblp.org/pid/209/8314.html'}, {'name': 'Alice Zhou', 'dblp_profile': 'https://dblp.org/pid/203/0140.html'}]",2017,"Web archiving initiatives around the world capture ephemeral Web content to preserve our collective digital memory. However, unlocking the potential of Web archives for humanities scholars and social scientists requires a scalable analytics infrastructure to support exploration of captured content. We present Warcbase, an open-source Web archiving platform that aims to fill this need. Our platform takes advantage of modern open-source “big data” infrastructure, namely Hadoop, HBase, and Spark, that has been widely deployed in industry. Warcbase provides two main capabilities: support for temporal browsing and a domain-specific language that allows scholars to interrogate Web archives in several different ways. This work represents a collaboration between computer scientists and historians, where we have engaged in iterative codesign to build tools for scholars with no formal computer science training. To provide guidance, we propose a process model for scholarly interactions with Web archives that begins with a question and proceeds iteratively through four main steps: filter, analyze, aggregate, and visualize. We call this the FAAV cycle for short and illustrate with three prototypical case studies. This article presents the current state of the project and discusses future directions.",3080ec6f388e2a6dfa9504ed9d0a149995099fe0
The Ubiquity of Large Graphs and Surprising Challenges of Graph Processing,"[{'name': 'Siddhartha Sahu', 'dblp_profile': 'https://dblp.org/pid/199/6334.html'}, {'name': 'Amine Mhedhbi', 'dblp_profile': 'https://dblp.org/pid/206/6790.html'}, {'name': 'Semih Salihoglu', 'dblp_profile': 'https://dblp.org/pid/55/6560.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'M. Tamer Özsu', 'dblp_profile': 'https://dblp.org/pid/o/MTamerOzsu.html'}]",2017,,
A Comparison of Nuggets and Clusters for Evaluating Timeline Summaries,"[{'name': 'Gaurav Baruah', 'dblp_profile': 'https://dblp.org/pid/147/9130.html'}, {'name': 'Richard McCreadie', 'dblp_profile': 'https://dblp.org/pid/29/7184.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"There is growing interest in systems that generate timeline summaries by filtering high-volume streams of documents to retain only those that are relevant to a particular event or topic. Continued advances in algorithms and techniques for this task depend on standardized and reproducible evaluation methodologies for comparing systems. However, timeline summary evaluation is still in its infancy, with competing methodologies currently being explored in international evaluation forums such as TREC. One area of active exploration is how to explicitly represent the units of information that should appear in a ""good"" summary. Currently, there are two main approaches, one based on identifying nuggets in an external ""ground truth"", and the other based on clustering system outputs. In this paper, by building test collections that have both nugget and cluster annotations, we are able to compare these two approaches. Specifically, we address questions related to evaluation effort, differences in the final evaluation products, and correlations between scores and rankings generated by both approaches. We summarize advantages and disadvantages of nuggets and clusters to offer recommendations for future system evaluations.",1b0a3fca046dc667f425e0375cc3e5ded76bee49
Talking to Your TV: Context-Aware Voice Search with Hierarchical Recurrent Neural Networks,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Oliver Jojic', 'dblp_profile': 'https://dblp.org/pid/96/10321.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We tackle the novel problem of navigational voice queries posed against an entertainment system, where viewers interact with a voice-enabled remote controller to specify the TV program to watch. This is a difficult problem for several reasons: such queries are short, even shorter than comparable voice queries in other domains, which offers fewer opportunities for deciphering user intent. Furthermore, ambiguity is exacerbated by underlying speech recognition errors. We address these challenges by integrating word- and character-level query representations and by modeling voice search sessions to capture the contextual dependencies in query sequences. Both are accomplished with a probabilistic framework in which recurrent and feedforward neural network modules are organized in a hierarchical manner. From a raw dataset of 32M voice queries from 2.5M viewers on the Comcast Xfinity X1 entertainment system, we extracted data to train and test our models. We demonstrate the benefits of our hybrid representation and context-aware model, which significantly outperforms competitive baselines that use learning to rank as well as neural networks.",ea2adc520181056f41419d14ed2c0a4926821ff7
An Insight Extraction System on BioMedical Literature with Deep Neural Networks,"[{'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Kris Ganjam', 'dblp_profile': 'https://dblp.org/pid/g/KrisGanjam.html'}, {'name': 'Navendu Jain', 'dblp_profile': 'https://dblp.org/pid/94/4527.html'}, {'name': 'Jessica Lundin', 'dblp_profile': 'https://dblp.org/pid/205/8991.html'}, {'name': 'Ryen White', 'dblp_profile': 'https://dblp.org/pid/w/RyenWWhite.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Mining biomedical text offers an opportunity to automatically discover important facts and infer associations among them. As new scientific findings appear across a large collection of biomedical publications, our aim is to tap into this literature to automate biomedical knowledge extraction and identify important insights from them. Towards that goal, we develop a system with novel deep neural networks to extract insights on biomedical literature. Evaluation shows our system is able to provide insights with competitive accuracy of human acceptance and its relation extraction component outperforms previous work.",d70e934197efdd5dc1da13442049c31e0790d167
Do We Need Specialized Graph Databases?: Benchmarking Real-Time Social Networking Applications,"[{'name': 'Anil Pacaci', 'dblp_profile': 'https://dblp.org/pid/203/0030.html'}, {'name': 'Alice Zhou', 'dblp_profile': 'https://dblp.org/pid/203/0140.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'M. Tamer Özsu', 'dblp_profile': 'https://dblp.org/pid/o/MTamerOzsu.html'}]",2017,"With the advent of online social networks, there is an increasing demand for storage and processing of graph-structured data. Social networking applications pose new challenges to data management systems due to demand for real-time querying and manipulation of the graph structure. Recently, several systems specialized systems for graph-structured data have been introduced. However, whether we should abandon mature RDBMS technology for graph databases remains an ongoing discussion. In this paper we present an graph database benchmarking architecture built on the existing LDBC Social Network Benchmark. Our proposed architecture stresses the systems with an interactive transactional workload to better simulate the real-time nature of social networking applications. Using this improved architecture, we evaluated a selection of specialized graph databases, RDF stores, and RDBMSes adapted for graphs. We do not find that specialized graph databases provide definitively better performance.",7549a6e03afdd42ae94c19235db076319eeb34f8
Mining the Temporal Statistics of Query Terms for Searching Social Media Posts,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Xing Niu', 'dblp_profile': 'https://dblp.org/pid/87/9555.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"There is an emerging consensus that time is an important indicator of relevance for searching streams of social media posts. In a process similar to pseudo-relevance feedback, the distribution of document timestamps from the results of an initial query can be leveraged to infer the distribution of relevant documents, for example, using kernel density estimation. In this paper, we explore an alternative approach to mining relevance signals directly from the temporal statistics of query terms in the collection, without the need to perform an initial retrieval. We propose two approaches: a linear ranking model that combines features derived from temporal collection statistics of query terms and a regression-based method that attempts to directly predict the distribution of relevant documents from query term statistics. Experiments on standard tweet test collections show that our proposed methods significantly outperform competitive baselines. Furthermore, studies of different feature combinations show the extent to which different types of temporal signals impact retrieval effectiveness.",2d851f1f5e34e915b4b7e91add282238098ee2da
An Exploration of Serverless Architectures for Information Retrieval,"[{'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Serverless architectures represent a new approach to designing applications in the cloud without having to explicitly provision or manage servers. The developer specifies functions with well-defined entry and exit points, and the cloud provider handles all other aspects of execution. In this paper, we explore a novel application of serverless architectures to information retrieval and describe a search engine built in this manner with Amazon Web Services: postings lists are stored in the DynamoDB NoSQL store and the postings traversal algorithm for query evaluation is implemented in the Lambda service. The result is a search engine that scales elastically with a pay-per-request model, in contrast to a server-based model that requires paying for running instances even if there are no requests. We empirically assess the performance and economics of our serverless architecture. While our implementation is currently too slow for interactive searching, analysis shows that the pay-per-request model is economically compelling, and future infrastructure improvements will increase the attractiveness of serverless designs over time.",60980635db288e287ae58e60f4e7b47d87768e5e
The Pareto Frontier of Utility Models as a Framework for Evaluating Push Notification Systems,"[{'name': 'Gaurav Baruah', 'dblp_profile': 'https://dblp.org/pid/147/9130.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We propose a utility-based framework for the evaluation of push notification systems that monitor document streams for users' topics of interest. Our starting point is that users derive either positive utility (i.e., ""gain"") or negative utility (i.e., ""pain"") from consuming system updates. By separately keeping track of these quantities, we can measure system effectiveness in a gain vs. pain tradeoff space. The Pareto Frontier of evaluated systems represents the state of the art: for each system on the frontier, no other system can offer more gain without more pain. Our framework has several advantages: it unifies three previous TREC evaluations, subsumes existing metrics, and provides more insightful analyses. Furthermore, our approach can easily accommodate more refined user models and is extensible to different information-seeking modalities.",346a0ff5743e36f227f1a3df0fafb43c8631c559
Quantization in Append-Only Collections,"[{'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Quantization, the pre-calculation and conversion to integers of term/document weights in an inverted index, is a well studied aspect of search engines that substantially improves retrieval efficiency. Previous work has considered the impact of quantization on effectiveness-efficiency tradeoffs in retrieval, for example, exploring the relationship between collection size and quantization range in static web collections. We extend previous work to append-only collections and examine whether quantization settings derived from prior time periods can be applied to future time periods. Experiments confirm that previous results generalize to a collection with different characteristics and with a different ranking function, and that in an append-only collection, we can use previous quantization settings in future time periods without substantial losses in either effectiveness or efficiency.",1dc4a7e9ec13929ddab26a58e105231cec3c81da
Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images,"[{'name': 'Babak Ehteshami Bejnordi', 'dblp_profile': 'https://dblp.org/pid/175/5607.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ben Glass', 'dblp_profile': 'https://dblp.org/pid/195/5892.html'}, {'name': 'Maeve Mullooly', 'dblp_profile': 'https://dblp.org/pid/195/5623.html'}, {'name': 'Gretchen L. Gierach', 'dblp_profile': 'https://dblp.org/pid/195/6114.html'}, {'name': 'Mark E. Sherman', 'dblp_profile': 'https://dblp.org/pid/195/5790.html'}, {'name': 'Nico Karssemeijer', 'dblp_profile': 'https://dblp.org/pid/09/93.html'}, {'name': 'Jeroen van der Laak', 'dblp_profile': 'https://dblp.org/pid/147/2215.html'}, {'name': 'Andrew H. Beck', 'dblp_profile': 'https://dblp.org/pid/155/2455.html'}]",2017,"Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor associated stroma as a diagnostic biomarker.",3096798a92ab0829ceb1d601d002dd47dc49e156
Online In-Situ Interleaved Evaluation of Real-Time Push Notification Systems,"[{'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Real-time push notification systems monitor continuous document streams such as social media posts and alert users to relevant content directly on their mobile devices. We describe a user study of such systems in the context of the TREC 2016 Real-Time Summarization Track, where system updates are immediately delivered as push notifications to the mobile devices of a cohort of users. Our study represents, to our knowledge, the first deployment of an interleaved evaluation framework for prospective information needs, and also provides an opportunity to examine user behavior in a realistic setting. Results of our online in-situ evaluation are correlated against the results a more traditional post-hoc batch evaluation. We observe substantial correlations between many online and batch evaluation metrics, especially for those that share the same basic design (e.g., are utility-based). For some metrics, we observe little correlation, but are able to identify the volume of messages that a system pushes as one major source of differences.",0a6d2eeaa0b75b5a21583fbd75637a1541f30487
"On the Reusability of ""Living Labs"" Test Collections: : A Case Study of Real-Time Summarization","[{'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Gaurav Baruah', 'dblp_profile': 'https://dblp.org/pid/147/9130.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,,
Automatically Extracting High-Quality Negative Examples for Answer Selection in Question Answering,"[{'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Mark D. Smucker', 'dblp_profile': 'https://dblp.org/pid/07/801.html'}]",2017,"We propose a heuristic called ""one answer per document"" for automatically extracting high-quality negative examples for answer selection in question answering. Starting with a collection of question-answer pairs from the popular TrecQA dataset, we identify the original documents from which the answers were drawn. Sentences from these source documents that contain query terms (aside from the answers) are selected as negative examples. Training on the original data plus these negative examples yields improvements in effectiveness by a margin that is comparable to successive recent publications on this dataset. Our technique is completely unsupervised, which means that the gains come essentially for free. We confirm that the improvements can be directly attributed to our heuristic, as other approaches to extracting comparable amounts of training data are not effective. Beyond the empirical validation of this heuristic, we also share our improved TrecQA dataset with the community to support further work in answer selection.",ea70aefddcd70b0f63e3588468453790cfc9e598
Experiments with Convolutional Neural Network Models for Answer Selection,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"In recent years, neural networks have been applied to many text processing problems. One example is learning a similarity function between pairs of text, which has applications to paraphrase extraction, plagiarism detection, question answering, and ad hoc retrieval. Within the information retrieval community, the convolutional neural network model proposed by Severyn and Moschitti in a SIGIR 2015 paper has gained prominence. This paper focuses on the problem of answer selection for question answering: we attempt to replicate the results of Severyn and Moschitti using their open-source code as well as to reproduce their results via a de novo (i.e., from scratch) implementation using a completely different deep learning toolkit. Our de novo implementation is instructive in ascertaining whether reported results generalize across toolkits, each of which have their idiosyncrasies. We were able to successfully replicate and reproduce the reported results of Severyn and Moschitti, albeit with minor differences in effectiveness, but affirming the overall design of their model. Additional ablation experiments break down the components of the model to show their contributions to overall effectiveness. Interestingly, we find that removing one component actually increases effectiveness and that a simplified model with only four word overlap features performs surprisingly well, even better than convolution feature maps alone.",4d7485c4837ebe75d2b94b6aa1f91e8e84f95998
"Finally, a Downloadable Test Collection of Tweets","[{'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Due to Twitter's terms of service that forbid redistribution of content, creating publicly downloadable collections of tweets for research purposes has been a perpetual problem for the research community. Some collections are distributed by making available the ids of the tweets that comprise the collection and providing tools to fetch the actual content; this approach has scalability limitations. In other cases, evaluation organizers have set up APIs that provide access to collections for specific tasks, without exposing the underlying content. This is a workable solution, but difficult to sustain over the long term since someone has to maintain the APIs. We have noticed that the non-profit Internet Archive has been making available for public download captures of the so-called Twitter ""spritzer"" stream, which is the same source as the Tweets2013 collection used in the TREC 2013 and 2014 Microblog Tracks. We analyzed both datasets in terms of content overlap and retrieval baselines to show that the Internet Archive data can serve as a drop-in replacement for the Tweets2013 collection, thereby providing the research community with, finally, a downloadable collection of tweets. Beyond this finding, we also study the impact of tweet deletions over time and how they affect the test collections.",729c23cd07b23a9dfdb1b4c733d0b710efcb7a68
Anserini: Enabling the Use of Lucene for Information Retrieval Research,"[{'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}, {'name': 'Hui Fang', 'dblp_profile': 'https://dblp.org/pid/03/2511-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Software toolkits play an essential role in information retrieval research. Most open-source toolkits developed by academics are designed to facilitate the evaluation of retrieval models over standard test collections. Efforts are generally directed toward better ranking and less attention is usually given to scalability and other operational considerations. On the other hand, Lucene has become the de facto platform in industry for building search applications (outside a small number of companies that deploy custom infrastructure). Compared to academic IR toolkits, Lucene can handle heterogeneous web collections at scale, but lacks systematic support for evaluation over standard test collections. This paper introduces Anserini, a new information retrieval toolkit that aims to provide the best of both worlds, to better align information retrieval practice and research. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial efforts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web-scale collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for many TREC test collections, providing a convenient way to replicate competitive baselines right out of the box. Experiments verify that our system is both efficient and effective, providing a solid foundation to support future research.",61344ba701960b3d16e4792216e4b2ef9bc7c570
Event Detection on Curated Tweet Streams,"[{'name': 'Nimesh Ghelani', 'dblp_profile': 'https://dblp.org/pid/180/3165.html'}, {'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Shine Wang', 'dblp_profile': 'https://dblp.org/pid/204/0147.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We present a system for identifying interesting social media posts on Twitter and delivering them to users' mobile devices in real time as push notifications. In our problem formulation, users are interested in broad topics such as politics, sports, and entertainment: our system processes tweets in real time to identify relevant, novel, and salient content. There are three interesting aspects to our work: First, instead of attempting to tame the cacophony of unfiltered tweets, we exploit a smaller, but still sizeable, collection of curated tweet streams corresponding to the Twitter accounts of different media outlets. Second, we apply distant supervision to extract topic labels from curated streams that have a specific focus, which can then be leveraged to build high-quality topic classifiers essentially ""for free"". Finally, our system delivers content via Twitter direct messages, supporting in situ interactions modeled after conversations with intelligent agents. These ideas are demonstrated in an end-to-end working prototype.",5e913bc0ae97d94b8fe099b950e0421b0f2e199d
The Lucene for Information Access and Retrieval Research (LIARR) Workshop at SIGIR 2017,"[{'name': 'Leif Azzopardi', 'dblp_profile': 'https://dblp.org/pid/29/4018.html'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Hui Fang', 'dblp_profile': 'https://dblp.org/pid/03/2511-1.html'}, {'name': 'Grant Ingersoll', 'dblp_profile': 'https://dblp.org/pid/27/3996.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Yashar Moshfeghi', 'dblp_profile': 'https://dblp.org/pid/87/6501.html'}, {'name': 'Harrisen Scells', 'dblp_profile': 'https://dblp.org/pid/194/5258.html'}, {'name': 'Peilin Yang', 'dblp_profile': 'https://dblp.org/pid/71/9062.html'}, {'name': 'Guido Zuccon', 'dblp_profile': 'https://dblp.org/pid/22/6562.html'}]",2017,"As an empirical discipline, information access and retrieval research requires substantial software infrastructure to index and search large collections. This workshop is motivated by the desire to better align information retrieval research with the practice of building search applications from the perspective of open-source information retrieval systems. Our goal is to promote the use of Lucene for information access and retrieval research.",497ebc6faeca695740a402ba5ee974f3d3a6ee38
In-Browser Interactive SQL Analytics with Afterburner,"[{'name': 'Kareem El Gebaly', 'dblp_profile': 'https://dblp.org/pid/53/3059.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"This demonstration explores the novel and unconventional idea of implementing an analytical RDBMS in pure JavaScript so that it runs completely inside a browser with no external dependencies. Our prototype, called Afterburner, generates compiled query plans that exploit two JavaScript features: typed arrays and asm.js. On the TPC-H benchmark, we show that Afterburner achieves comparable performance to MonetDB running natively on the same machine. This is an interesting finding in that it shows how far JavaScript has come as an efficient execution platform. Beyond a mere technical curiosity, we demonstrate how our techniques can support interactive data exploration by automatically generating materialized views from a backend that is then shipped to the browser to facilitate subsequent interactions seamlessly and efficiently.",0b6a659f8647053340d0505f5e3369698b5f5ce9
Overview of the TREC 2017 Real-Time Summarization Track,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Nimesh Ghelani', 'dblp_profile': 'https://dblp.org/pid/180/3165.html'}, {'name': 'Mustafa Abualsaud', 'dblp_profile': 'https://dblp.org/pid/160/1276.html'}, {'name': 'Richard McCreadie', 'dblp_profile': 'https://dblp.org/pid/29/7184.html'}, {'name': 'Dmitrijs Milajevs', 'dblp_profile': 'https://dblp.org/pid/130/0471.html'}, {'name': 'Ellen M. Voorhees', 'dblp_profile': 'https://dblp.org/pid/60/3753.html'}]",2017,"Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees, and Fernando Diaz 1 David R. Cheriton School of Computer Science, University of Waterloo, Ontario, Canada 2 School of Computing Science, University of Glasgow, Scotland, the United Kingdom 3 National Institute for Standards and Technology, Maryland, USA 4 Microsoft Research, New York, USA {jimmylin, aroegies, luchen.tan}@uwaterloo.ca richard.mccreadie@glasgow.ac.uk, ellen.voorhees@nist.gov, fdiaz@microsoft.com",489f452af0ee24a71be90556b4432138c3b155dd
Topic Shifts Between Two US Presidential Administrations,"[{'name': 'Ziquan Wang', 'dblp_profile': 'https://dblp.org/pid/297/1161.html'}, {'name': 'Borui Lin', 'dblp_profile': 'https://dblp.org/pid/329/2392.html'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,,
A Comparison of Document-at-a-Time and Score-at-a-Time Query Evaluation,"[{'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'J. Shane Culpepper', 'dblp_profile': 'https://dblp.org/pid/03/489.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Joel M. Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}]",2017,"We present an empirical comparison between document-at-a-time (DaaT) and score-at-a-time (SaaT) document ranking strategies within a common framework. Although both strategies have been extensively explored, the literature lacks a fair, direct comparison: such a study has been difficult due to vastly different query evaluation mechanics and index organizations. Our work controls for score quantization, document processing, compression, implementation language, implementation effort, and a number of details, arriving at an empirical evaluation that fairly characterizes the performance of three specific techniques: WAND (DaaT), BMW (DaaT), and JASS (SaaT). Experiments reveal a number of interesting findings. The performance gap between WAND and BMW is not as clear as the literature suggests, and both methods are susceptible to tail queries that may take orders of magnitude longer than the median query to execute. Surprisingly, approximate query evaluation in WAND and BMW does not significantly reduce the risk of these tail queries. Overall, JASS is slightly slower than either WAND or BMW, but exhibits much lower variance in query latencies and is much less susceptible to tail query effects. Furthermore, JASS query latency is not particularly sensitive to the retrieval depth, making it an appealing solution for performance-sensitive applications where bounds on query latencies are desirable.",5fbe4fe2f27d458c5f0208de18d036445d45b4a2
Partitioning and Segment Organization Strategies for Real-Time Selective Search on Document Streams,"[{'name': 'Yulu Wang', 'dblp_profile': 'https://dblp.org/pid/143/3574.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"The basic idea behind selective search is to partition a collection into topical clusters, and for each query, consider only a subset of the clusters that are likely to contain relevant documents. Previous work on web collections has shown that it is possible to retain high-quality results while considering only a small fraction of the collection. These studies, however, assume static collections where it is feasible to run batch clustering algorithms for partitioning. In this work, we consider the novel formulation of selective search on document streams (specifically, tweets), where partitioning must be performed incrementally. In our approach, documents are partitioned into temporal segments and selective search is performed within each segment: these segments can either be clustered using batch or online algorithms, and at different temporal granularities. For efficiency, we take advantage of word embeddings to reduce the dimensionality of the document vectors. Experiments with test collections from the TREC Microblog Tracks show that we are able to achieve precision indistinguishable from exhaustive search while considering only around 5% of the collection. Interestingly, we observe no significant effectiveness differences between batch vs. online clustering and between hourly vs. daily temporal segments, despite them being very different index organizations. This suggests that architectural choices should be primarily guided by efficiency considerations.",f8a45eb4737a63b774b56b378e17ae197a33d103
Ten Blue Links on Mars,"[{'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Gordon V. Cormack', 'dblp_profile': 'https://dblp.org/pid/c/GVCormack.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}]",2017,"This paper explores a simple question: How would we provide a high-quality search experience on Mars, where the fundamental physical limit is speed-of-light propagation delays on the order of tens of minutes? On Earth, users are accustomed to nearly instantaneous responses from web services. Is it possible to overcome orders-of-magnitude longer latency to provide a tolerable user experience on Mars? In this paper, we formulate the searching from Mars problem as a tradeoff between ""effort"" (waiting for responses from Earth) and ""data transfer"" (pre-fetching or caching data on Mars). The contribution of our work is articulating this design space and presenting two case studies that explore the effectiveness of baseline techniques, using publicly available data from the TREC Total Recall and Sessions Tracks. We intend for this research problem to be aspirational as well as inspirational---even if one is not convinced by the premise of Mars colonization, there are Earth-based scenarios such as searching from rural villages in India that share similar constraints, thus making the problem worthy of exploration and attention from researchers.",5d77fe775365a8a34909ddf2f155e423efe2e47b
"Comparative Assessment of Alignment Algorithms for NGS Data: Features, Considerations, Implementations, and Future","[{'name': 'Carol Shen', 'dblp_profile': 'https://dblp.org/pid/218/8708.html'}, {'name': 'Tony Shen', 'dblp_profile': 'https://dblp.org/pid/218/8626.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,,c8926ba4937a3b4a8ecb72fdec30f3ccd9431316
Deep learning-based assessment of tumor-associated stroma for diagnosing breast cancer in histopathology images,"[{'name': 'Babak Ehteshami Bejnordi', 'dblp_profile': 'https://dblp.org/pid/175/5607.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ben Glass', 'dblp_profile': 'https://dblp.org/pid/195/5892.html'}, {'name': 'Maeve Mullooly', 'dblp_profile': 'https://dblp.org/pid/195/5623.html'}, {'name': 'Gretchen L. Gierach', 'dblp_profile': 'https://dblp.org/pid/195/6114.html'}, {'name': 'Mark E. Sherman', 'dblp_profile': 'https://dblp.org/pid/195/5790.html'}, {'name': 'Nico Karssemeijer', 'dblp_profile': 'https://dblp.org/pid/09/93.html'}, {'name': 'Jeroen van der Laak', 'dblp_profile': 'https://dblp.org/pid/147/2215.html'}, {'name': 'Andrew H. Beck', 'dblp_profile': 'https://dblp.org/pid/155/2455.html'}]",2017,"Diagnosis of breast carcinomas has so far been limited to the morphological interpretation of epithelial cells and the assessment of epithelial tissue architecture. Consequently, most of the automated systems have focused on characterizing the epithelial regions of the breast to detect cancer. In this paper, we propose a system for classification of hematoxylin and eosin (H&E) stained breast specimens based on convolutional neural networks that primarily targets the assessment of tumor-associated stroma to diagnose breast cancer patients. We evaluate the performance of our proposed system using a large cohort containing 646 breast tissue biopsies. Our evaluations show that the proposed system achieves an area under ROC of 0.92, demonstrating the discriminative power of previously neglected tumor associated stroma as a diagnostic biomarker.",3096798a92ab0829ceb1d601d002dd47dc49e156
Efficient and Effective Tail Latency Minimization in Multi-Stage Retrieval Systems,"[{'name': 'Joel M. Mackenzie', 'dblp_profile': 'https://dblp.org/pid/174/0021.html'}, {'name': 'J. Shane Culpepper', 'dblp_profile': 'https://dblp.org/pid/03/489.html'}, {'name': 'Roi Blanco', 'dblp_profile': 'https://dblp.org/pid/30/3795.html'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,,2911979850e027732118bf98a6e5b0408e859bb7
Distant Supervision for Topic Classification of Tweets in Curated Streams,"[{'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Nimesh Ghelani', 'dblp_profile': 'https://dblp.org/pid/180/3165.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We tackle the challenge of topic classification of tweets in the context of analyzing a large collection of curated streams by news outlets and other organizations to deliver relevant content to users. Our approach is novel in applying distant supervision based on semi-automatically identifying curated streams that are topically focused (for example, on politics, entertainment, or sports). These streams provide a source of labeled data to train topic classifiers that can then be applied to categorize tweets from more topically-diffuse streams. Experiments on both noisy labels and human ground-truth judgments demonstrate that our approach yields good topic classifiers essentially ""for free"", and that topic classifiers trained in this manner are able to dynamically adjust for topic drift as news on Twitter evolves.",c1c350ff5dc1cec4f0336656457cd4b8d345eeb2
Talking to Your TV: Context-Aware Voice Search with Hierarchical Recurrent Neural Networks,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Oliver Jojic', 'dblp_profile': 'https://dblp.org/pid/96/10321.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We tackle the novel problem of navigational voice queries posed against an entertainment system, where viewers interact with a voice-enabled remote controller to specify the TV program to watch. This is a difficult problem for several reasons: such queries are short, even shorter than comparable voice queries in other domains, which offers fewer opportunities for deciphering user intent. Furthermore, ambiguity is exacerbated by underlying speech recognition errors. We address these challenges by integrating word- and character-level query representations and by modeling voice search sessions to capture the contextual dependencies in query sequences. Both are accomplished with a probabilistic framework in which recurrent and feedforward neural network modules are organized in a hierarchical manner. From a raw dataset of 32M voice queries from 2.5M viewers on the Comcast Xfinity X1 entertainment system, we extracted data to train and test our models. We demonstrate the benefits of our hybrid representation and context-aware model, which significantly outperforms competitive baselines that use learning to rank as well as neural networks.",ea2adc520181056f41419d14ed2c0a4926821ff7
Integrating Lexical and Temporal Signals in Neural Ranking Models for Searching Social Media Streams,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Time is an important relevance signal when searching streams of social media posts. The distribution of document timestamps from the results of an initial query can be leveraged to infer the distribution of relevant documents, which can then be used to rerank the initial results. Previous experiments have shown that kernel density estimation is a simple yet effective implementation of this idea. This paper explores an alternative approach to mining temporal signals with recurrent neural networks. Our intuition is that neural networks provide a more expressive framework to capture the temporal coherence of neighboring documents in time. To our knowledge, we are the first to integrate lexical and temporal signals in an end-to-end neural network architecture, in which existing neural ranking models are used to generate query-document similarity vectors that feed into a bidirectional LSTM layer for temporal modeling. Our results are mixed: existing neural models for document ranking alone yield limited improvements over simple baselines, but the integration of lexical and temporal signals yield significant improvements over competitive temporal baselines.",7b54c5bcd4f79e06b441dc650feb2cc581cd1f1e
Exploring the Effectiveness of Convolutional Neural Networks for Answer Selection in End-to-End Question Answering,"[{'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Gaurav Baruah', 'dblp_profile': 'https://dblp.org/pid/147/9130.html'}, {'name': 'Zhucheng Tu', 'dblp_profile': 'https://dblp.org/pid/195/7964.html'}, {'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Most work on natural language question answering today focuses on answer selection: given a candidate list of sentences, determine which contains the answer. Although important, answer selection is only one stage in a standard end-to-end question answering pipeline. This paper explores the effectiveness of convolutional neural networks (CNNs) for answer selection in an end-to-end context using the standard TrecQA dataset. We observe that a simple idf-weighted word overlap algorithm forms a very strong baseline, and that despite substantial efforts by the community in applying deep learning to tackle answer selection, the gains are modest at best on this dataset. Furthermore, it is unclear if a CNN is more effective than the baseline in an end-to-end context based on standard retrieval metrics. To further explore this finding, we conducted a manual user evaluation, which confirms that answers from the CNN are detectably better than those from idf-weighted word overlap. This result suggests that users are sensitive to relatively small differences in answer selection quality.",3c3e27872d7f1a0f783c49b809a58d3ad4b8d904
An Exploration of Approaches to Integrating Neural Reranking Models in Multi-Stage Ranking Architectures,"[{'name': 'Zhucheng Tu', 'dblp_profile': 'https://dblp.org/pid/195/7964.html'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Royal Sequiera', 'dblp_profile': 'https://dblp.org/pid/180/3136.html'}, {'name': 'Junchen Zhang', 'dblp_profile': 'https://dblp.org/pid/203/8304.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We explore different approaches to integrating a simple convolutional neural network (CNN) with the Lucene search engine in a multi-stage ranking architecture. Our models are trained using the PyTorch deep learning toolkit, which is implemented in C/C++ with a Python frontend. One obvious integration strategy is to expose the neural network directly as a service. For this, we use Apache Thrift, a software framework for building scalable cross-language services. In exploring alternative architectures, we observe that once trained, the feedforward evaluation of neural networks is quite straightforward. Therefore, we can extract the parameters of a trained CNN from PyTorch and import the model into Java, taking advantage of the Java Deeplearning4J library for feedforward evaluation. This has the advantage that the entire end-to-end system can be implemented in Java. As a third approach, we can extract the neural network from PyTorch and ""compile"" it into a C++ program that exposes a Thrift service. We evaluate these alternatives in terms of performance (latency and throughput) as well as ease of integration. Experiments show that feedforward evaluation of the convolutional neural network is significantly slower in Java, while the performance of the compiled C++ network does not consistently beat the PyTorch implementation.",714c220aa528771f39c67cbff2b5ecdba7d4182b
The Ubiquity of Large Graphs and Surprising Challenges of Graph Processing: A User Survey,"[{'name': 'Siddhartha Sahu', 'dblp_profile': 'https://dblp.org/pid/199/6334.html'}, {'name': 'Amine Mhedhbi', 'dblp_profile': 'https://dblp.org/pid/206/6790.html'}, {'name': 'Semih Salihoglu', 'dblp_profile': 'https://dblp.org/pid/55/6560.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'M. Tamer Özsu', 'dblp_profile': 'https://dblp.org/pid/o/MTamerOzsu.html'}]",2017,,
Honk: A PyTorch Reimplementation of Convolutional Neural Networks for Keyword Spotting,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We describe Honk, an open-source PyTorch reimplementation of convolutional neural networks for keyword spotting that are included as examples in TensorFlow. These models are useful for recognizing ""command triggers"" in speech-based interfaces (e.g., ""Hey Siri""), which serve as explicit cues for audio recordings of utterances that are sent to the cloud for full speech recognition. Evaluation on Google's recently released Speech Commands Dataset shows that our reimplementation is comparable in accuracy and provides a starting point for future work on the keyword spotting task.",4e9ab80136a61eb7f76fa3d41cd77cb8ceabe003
Deep Residual Learning for Small-Footprint Keyword Spotting,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We explore the application of deep residual learning and dilated convolutions to the keyword spotting task, using the recently-released Google Speech Commands Dataset as our benchmark. Our best residual network (ResNet) implementation significantly outperforms Google's previous convolutional neural networks in terms of accuracy. By varying model depth and width, we can achieve compact models that also outperform previous small-footprint variants. To our knowledge, we are the first to examine these approaches for keyword spotting, and our results establish an open-source state-of-the-art reference to support the development of future speech-based interfaces.",4ee8622f5dacb44e4af6bc9ee1c8f48a48983d9a
An Experimental Analysis of the Power Consumption of Convolutional Neural Networks for Keyword Spotting,"[{'name': 'Raphael Tang', 'dblp_profile': 'https://dblp.org/pid/207/7684.html'}, {'name': 'Weijie Wang', 'dblp_profile': 'https://dblp.org/pid/18/689.html'}, {'name': 'Zhucheng Tu', 'dblp_profile': 'https://dblp.org/pid/195/7964.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"Nearly all previous work on small-footprint keyword spotting with neural networks quantify model footprint in terms of the number of parameters and multiply operations for a feedforward inference pass. These values are, however, proxy measures since empirical performance in actual deployments is determined by many factors. In this paper, we study the power consumption of a family of convolutional neural networks for keyword spotting on a Raspberry Pi. We find that both proxies are good predictors of energy usage, although the number of multiplies is more predictive than the number of model parameters. We also confirm that models with the highest accuracies are, unsurprisingly, the most power hungry.",c95cb204a9b45a81fc8b12eec57ae6f421730d2c
Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks,"[{'name': 'Salman Mohammed', 'dblp_profile': 'https://dblp.org/pid/199/2172.html'}, {'name': 'Peng Shi', 'dblp_profile': 'https://dblp.org/pid/172/7638-10.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2017,"We examine the problem of question answering over knowledge graphs, focusing on simple questions that can be answered by the lookup of a single fact. Adopting a straightforward decomposition of the problem into entity detection, entity linking, relation prediction, and evidence combination, we explore simple yet strong baselines. On the popular SimpleQuestions dataset, we find that basic LSTMs and GRUs plus a few heuristics yield accuracies that approach the state of the art, and techniques that do not use neural networks also perform reasonably well. These results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity.",1154add9af6c19c8b264331308ec99776583c52d
Searching from Mars,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Gaurav Baruah', 'dblp_profile': 'https://dblp.org/pid/147/9130.html'}]",2016,"How would you search from Mars? It's the user model, stupid!",ddfaa2f0d88e647bff64fc4b045b73d8cdbf5829
The Future of Big Data Is ... JavaScript?,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Kareem El Gebaly', 'dblp_profile': 'https://dblp.org/pid/53/3059.html'}]",2016,"JavaScript has already made serious inroads as an integrated technology stack for building user-facing applications. In this article, the authors explore the idea of using JavaScript in Big Data platforms. Could the future be ... JavaScript everywhere?",bdd91ae391925034980618daea7c3926240fcbc9
GraphJet: Real-Time Content Recommendations at Twitter,"[{'name': 'Aneesh Sharma', 'dblp_profile': 'https://dblp.org/pid/78/6674.html'}, {'name': 'Jerry Jiang', 'dblp_profile': 'https://dblp.org/pid/185/4257.html'}, {'name': 'Praveen Bommannavar', 'dblp_profile': 'https://dblp.org/pid/12/7140.html'}, {'name': 'Brian Larson', 'dblp_profile': 'https://dblp.org/pid/16/6426.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"This paper presents GraphJet, a new graph-based system for generating content recommendations at Twitter. As motivation, we trace the evolution of our formulation and approach to the graph recommendation problem, embodied in successive generations of systems. Two trends can be identified: supplementing batch with real-time processing and a broadening of the scope of recommendations from users to content. Both of these trends come together in Graph-Jet, an in-memory graph processing engine that maintains a real-time bipartite interaction graph between users and tweets. The storage engine implements a simple API, but one that is sufficiently expressive to support a range of recommendation algorithms based on random walks that we have refined over the years. Similar to Cassovary, a previous graph recommendation engine developed at Twitter, GraphJet assumes that the entire graph can be held in memory on a single server. The system organizes the interaction graph into temporally-partitioned index segments that hold adjacency lists. GraphJet is able to support rapid ingestion of edges while concurrently serving lookup queries through a combination of compact edge encoding and a dynamic memory allocation scheme that exploits power-law characteristics of the graph. Each GraphJet server ingests up to one million graph edges per second, and in steady state, computes up to 500 recommendations per second, which translates into several million edge read operations per second.",9eb55f9a6aff2a70415cba21a23a538ee46a36fe
Sapphire: Querying RDF Data Made Simple,"[{'name': 'Ahmed El-Roby', 'dblp_profile': 'https://dblp.org/pid/135/4659.html'}, {'name': 'Khaled Ammar', 'dblp_profile': 'https://dblp.org/pid/29/9884.html'}, {'name': 'Ashraf Aboulnaga', 'dblp_profile': 'https://dblp.org/pid/01/3996.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"There is currently a large amount of publicly accessible structured data available as RDF data sets. For example, the Linked Open Data (LOD) cloud now consists of thousands of RDF data sets with over 30 billion triples, and the number and size of the data sets is continuously growing. Many of the data sets in the LOD cloud provide public SPARQL endpoints to allow issuing queries over them. These end-points enable users to retrieve data using precise and highly expressive SPARQL queries. However, in order to do so, the user must have sufficient knowledge about the data sets that she wishes to query, that is, the structure of data, the vocabulary used within the data set, the exact values of literals, their data types, etc. Thus, while SPARQL is powerful, it is not easy to use. An alternative to SPARQL that does not require as much prior knowledge of the data is some form of keyword search over the structured data. Keyword search queries are easy to use, but inherently ambiguous in describing structured queries. 
 
This demonstration introduces Sapphire, a system for querying RDF data that strikes a middle ground between ambiguous keyword search and difficult-to-use SPARQL. Our system does not replace either, but utilizes both where they are most effective. Sapphire helps the user construct expressive SPARQL queries that represent her information needs without requiring detailed knowledge about the queried data sets. These queries are then executed over public SPARQL endpoints from the LOD cloud. Sapphire guides the user in the query writing process by showing suggestions of query terms based on the queried data, and by recommending changes to the query based on a predictive user model.",343500e0052eb1b683f32b00efbbd1331c94184a
NScale: neighborhood-centric large-scale graph analytics in the cloud,"[{'name': 'Abdul Quamar', 'dblp_profile': 'https://dblp.org/pid/127/6195.html'}, {'name': 'Amol Deshpande', 'dblp_profile': 'https://dblp.org/pid/d/AmolDeshpande.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,,073fed4761d408025035cb60d7d82dddd20ebbf7
In Vacuo and In Situ Evaluation of SIMD Codecs,"[{'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"The size of a search engine index and the time to search are inextricably related through the compression codec. This investigation examines this tradeoff using several relatively unexplored SIMD-based codecs including QMX, TurboPackV, and TurboPFor. It uses (the non-SIMD) OPTPFor as a baseline. Four new variants of QMX are introduced and also compared. Those variants include optimizations for space and for time. Experiments were conducted on the TREC .gov2 collection using topics 701-850, in crawl order and in URL order. The results suggest that there is very little difference between these codecs, but that the reference implementation of QMX performs well.",d541f91aa0df5a0f012ab0bbc76b6c29664049bc
Dynamic Cutoff Prediction in Multi-Stage Retrieval Systems,"[{'name': 'J. Shane Culpepper', 'dblp_profile': 'https://dblp.org/pid/03/489.html'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"Modern multi-stage retrieval systems are comprised of a candidate generation stage followed by one or more reranking stages. In such an architecture, the quality of the final ranked list may not be sensitive to the quality of the initial candidate pool, especially in terms of early precision. This provides several opportunities to increase retrieval efficiency without significantly sacrificing effectiveness. In this paper, we explore a new approach to dynamically predicting the size of an initial result set in the candidate generation stage, which can directly affect the overall efficiency and effectiveness of the entire system. Previous work exploring this tradeoff has focused on global parameter settings that apply to all queries, even though optimal settings vary across queries. In contrast, we propose a technique that makes a parameter prediction to maximize efficiency within an effectiveness envelope on a per query basis, using only static pre-retrieval features. Experimental results show that substantial efficiency gains are achievable. In addition, our framework provides a versatile tool that can be used to estimate the effectiveness-efficiency tradeoffs that are possible before selecting and tuning algorithms to make machine-learned predictions.",f7a34579f122b240707874b6b76dc71af23ddb0f
Discovering key moments in social media streams,"[{'name': 'Cody Buntain', 'dblp_profile': 'https://dblp.org/pid/34/7214.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jennifer Golbeck', 'dblp_profile': 'https://dblp.org/pid/48/2412.html'}]",2016,"This paper introduces a general technique, called LABurst, for identifying key moments, or moments of high impact, in social media streams without the need for domain-specific information or seed keywords. We leverage machine learning to model temporal patterns around bursts in Twitter's unfiltered public sample stream and build a classifier to identify tokens experiencing these bursts. We show LABurst performs competitively with existing burst detection techniques while simultaneously providing insight into and detection of unanticipated moments. To demonstrate our approach's potential, we compare two baseline event-detection algorithms with our language-agnostic algorithm to detect key moments across three major sporting competitions: 2013 World Series, 2014 Super Bowl, and 2014 World Cup. Our results show LABurst outperforms a time series analysis baseline and is competitive with a domain-specific baseline even though we operate without any domain knowledge. We then go further by transferring LABurst's models learned in the sports domain to the task of identifying earthquakes in Japan and show our method detects large spikes in earthquake-related tokens within two minutes of the actual event.",e98eacb6be571eb9eca0df96630c9d75a0375f96
Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"We study answer selection for question answering, in which given a question and a set of candidate answer sentences, the goal is to identify the subset that contains the answer. Unlike previous work which treats this task as a straightforward pointwise classification problem, we model this problem as a ranking task and propose a pairwise ranking approach that can directly exploit existing pointwise neural network models as base components. We extend the Noise-Contrastive Estimation approach with a triplet ranking loss function to exploit interactions in triplet inputs over the question paired with positive and negative examples. Experiments on TrecQA and WikiQA datasets show that our approach achieves state-of-the-art effectiveness without the need for external knowledge sources or feature engineering.",336ce21be76879f19c01b68726558269907ea02b
Optimizing Nugget Annotations with Active Learning,"[{'name': 'Gaurav Baruah', 'dblp_profile': 'https://dblp.org/pid/147/9130.html'}, {'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Rakesh Guttikonda', 'dblp_profile': 'https://dblp.org/pid/160/7600.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Mark D. Smucker', 'dblp_profile': 'https://dblp.org/pid/07/801.html'}, {'name': 'Olga Vechtomova', 'dblp_profile': 'https://dblp.org/pid/64/3140.html'}]",2016,"Nugget-based evaluations, such as those deployed in the TREC Temporal Summarization and Question Answering tracks, require human assessors to determine whether a nugget is present in a given piece of text. This process, known as nugget annotation, is labor-intensive. In this paper, we present two active learning techniques that prioritize the sequence in which candidate nugget/sentence pairs are presented to an assessor, based on the likelihood that the sentence contains a nugget. Our approach builds on the recognition that nugget annotation is similar to high-recall retrieval, and we adapt proven existing solutions. Simulation experiments with four existing TREC test collections show that our techniques yield far more matches for a given level of effort than baselines that are typically deployed in previous nugget-based evaluations.",43520ba60eb643f21494dac152002b650d808f09
Exploring and Discovering Archive-It Collections with Warcbase,"[{'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jeremy Wiebe', 'dblp_profile': 'https://dblp.org/pid/209/8314.html'}, {'name': 'Alice Zhou', 'dblp_profile': 'https://dblp.org/pid/203/0140.html'}]",2016,,30add08eff92b58003a9e6cfe13ff5659edd36d6
Toward Reproducible Baselines: The Open-Source IR Reproducibility Challenge,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}, {'name': 'Jamie Callan', 'dblp_profile': 'https://dblp.org/pid/c/JamesPCallan.html'}, {'name': 'Ishan Chattopadhyaya', 'dblp_profile': 'https://dblp.org/pid/127/1238.html'}, {'name': 'John Foley', 'dblp_profile': 'https://dblp.org/pid/26/8735.html'}, {'name': 'Grant Ingersoll', 'dblp_profile': 'https://dblp.org/pid/27/3996.html'}, {'name': 'Craig MacDonald', 'dblp_profile': 'https://dblp.org/pid/02/2224.html'}, {'name': 'Sebastiano Vigna', 'dblp_profile': 'https://dblp.org/pid/27/6106.html'}]",2016,,184198994820da09e00ae51f954b67400ffbf985
Compressing and Decoding Term Statistics Time Series,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Xing Niu', 'dblp_profile': 'https://dblp.org/pid/87/9555.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,,9ce51ead885bf107f7725fa1d0461422e12d24c5
Total Recall: Blue Sky on Mars,"[{'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Gordon V. Cormack', 'dblp_profile': 'https://dblp.org/pid/c/GVCormack.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}]",2016,"There are presently plans to create permanent colonies on Mars so that humanity will have a second home. These colonists will need search, email, entertainment, and indeed most services provided on the modern web. The primary challenge is network latencies, since the two planets are anywhere from 4 to 24 light minutes apart. A recent article sketches out how we might develop search technologies for Mars based on physically transporting a cache of the web to Mars, to which updates are applied via predictive models. Within this general framework, we explore the problem of high-recall retrieval, such as conducting a scientific survey. We explore simple techniques for masking speed-of-light delays and find that ""priming"" the search process with a small Martian cache is sufficient to mask a moderate amount of network latency. Simulation experiments show that it is possible to engineer high-recall search from Mars to be quite similar to the experience on Earth.",e451c15fc62f8fdbf29741688b3f4f649dc4d058
"Retrievability in API-Based ""Evaluation as a Service""","[{'name': 'Jiaul H. Paik', 'dblp_profile': 'https://dblp.org/pid/27/9831.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"""Evaluation as a service"" (EaaS) refers to a family of related evaluation methodologies that enables community-wide evaluations and the construction of test collections on documents that cannot be easily distributed. In the API-based approach, the basic idea is that evaluation organizers provide a service API through which the evaluation task can be completed, without providing access to the raw collection. One concern with this evaluation approach is that the API introduces biases and limits the diversity of techniques that can be brought to bear on the problem. In this paper, we tackle the question of API bias using the concept of retrievability. The raw data for our analyses come from a naturally-occurring experiment where we observed the same groups completing the same task with the API and also with access to the raw collection. We find that the retrievability bias of runs generated in both cases are comparable. Moreover, the fraction of relevant tweets retrieved through the API by the participating groups is at least as high as when they had access to the raw collection.",2e4078ec4ee2ab3d3fb583f1b1761d8d6c576e30
Rank-at-a-Time Query Processing,"[{'name': 'Ahmed Elbagoury', 'dblp_profile': 'https://dblp.org/pid/142/1758.html'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"Query processing strategies for ranked retrieval have been studied for decades. In this paper we propose a new strategy, which we call rank-at-a-time query processing, that evaluates documents in descending order of quantized scores and is able to directly compute the final document ranking via a sequence of boolean intersections. We show that such a strategy is equivalent to a second-order restricted composition of per-term scores. Rank-at-a-time query processing has the advantage that it is anytime score-safe, which means that the retrieval algorithm can self-adapt to produce an exact ranking given an arbitrary latency constraint. Due to the combinatorial nature of compositions, however, a naive implementation is too slow to be of practical use. To address this issue, we introduce a hybrid variant that is able to reduce query latency to a point that is on par with state-of-the-art retrieval engines.",a11869915e2618af61ae3eb785badd2c9caf2950
Temporal Query Expansion Using a Continuous Hidden Markov Model,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"In standard formulations of pseudo-relevance feedback, document timestamps do not play a role in identifying expansion terms. Yet we know that when searching social media posts such as tweets, relevant documents are bursty and usually occur in temporal clusters. The main insight of our work is that term expansions should be biased to draw from documents that occur in bursty temporal clusters. This is formally captured by a continuous hidden Markov model (cHMM), for which we derive an EM algorithm for parameter estimation. Given a query, we estimate the parameters for a cHMM that best explains the observed distribution of an initial set of retrieved documents, and then use Viterbi decoding to compute the most likely state sequence. In identifying expansion terms, we only select documents from bursty states. Experiments on test collections from the TREC 2011 and 2012 Microblog tracks show that our approach is significantly more effective than the popular RM3 pseudo-relevance feedback model.",e6800e4e647dffda7ee155872e8cf50e16fd1438
Desiderata for Exploratory Search Interfaces to Web Archives in Support of Scholarly Activities,"[{'name': 'Andrew Jackson', 'dblp_profile': 'https://dblp.org/pid/27/3387.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}]",2016,"Web archiving initiatives around the world capture ephemeral web content to preserve our collective digital memory. In this paper, we describe initial experiences in providing an exploratory search interface to web archives for humanities scholars and social scientists. We describe our initial implementation and discuss our findings in terms of desiderata for such a system. It is clear that the standard organization of a search engine results page (SERP), consisting of an ordered list of hits, is inadequate to support the needs of scholars. Shneiderman's mantra for visual information seeking (“overview first, zoom and filter, then details-on-demand”) provides a nice organizing principle for interface design, to which we propose an addendum: “Make everything transparent”. We elaborate on this by highlighting the importance of the temporal dimension of web pages as well as issues surrounding metadata and veracity.",36932d8f648bb39a65ea8ac2d1d982db864136de
Content Selection and Curation for Web Archiving: The Gatekeepers vs. the Masses,"[{'name': 'Ian Milligan', 'dblp_profile': 'https://dblp.org/pid/181/0564.html'}, {'name': 'Nick Ruest', 'dblp_profile': 'https://dblp.org/pid/181/0664.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"Any preservation effort must begin with an assessment of what content to preserve, and web archiving is no different. There have historically been two answers to the question “what should we archive?” The Internet Archive's broad entire-web crawls have been supplemented by narrower domain-or topic-specific collections gathered by numerous libraries. We can characterize this as content selection and curation by “gatekeepers”. In contrast, we have witnessed the emergence of another approach driven by “the masses” - we can archive pages that are contained in social media streams such as Twitter. The interesting question, of course, is how these approaches differ. We provide an answer to this question in the context of a case study about the 2015 Canadian federal elections. Based on our analysis, we recommend a hybrid approach that combines an effort driven by social media and more traditional curatorial methods.",dffc4ae0f2b1609ec9bea071e13fec1f4997f319
Prizm: A Wireless Access Point for Proxy-Based Web Lifelogging,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Zhucheng Tu', 'dblp_profile': 'https://dblp.org/pid/195/7964.html'}, {'name': 'Michael Rose', 'dblp_profile': 'https://dblp.org/pid/05/5431.html'}, {'name': 'Patrick White', 'dblp_profile': 'https://dblp.org/pid/62/6749.html'}]",2016,"We present Prizm, a prototype lifelogging device that comprehensively records a user's web activity. Prizm is a wireless access point deployed on a Raspberry Pi that is designed to be a substitute for the user's normal wireless access point. Prizm proxies all HTTP(S) requests from devices connected to it and records all activity it observes. Although this particular design is not entirely novel, there are a few features that are unique to our approach, most notably the physical deployment as a wireless access point. Such a package allows capture of activity from multiple devices, integration with web archiving for preservation, and support for offline operation. This paper describes the design of Prizm, the current status of our project, and future plans.",3e030850b2dded6def4e9c157fc4d1c82cdcbcd1
Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement,"[{'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"Textual similarity measurement is a challenging problem, as it requires understanding the semantics of input sentences. Most previous neural network models use coarse-grained sentence modeling, which has difficulty capturing fine-grained word-level information for semantic comparisons. As an alternative, we propose to explicitly model pairwise word interactions and present a novel similarity focus mechanism to identify important correspondences for better similarity measurement. Our ideas are implemented in a novel neural network architecture that demonstrates state-ofthe-art accuracy on three SemEval tasks and two answer selection tasks.",29006d8c9c2247fca4cd3a22822c2b042e85572d
Evaluating Search Among Secrets,"[{'name': 'Douglas W. Oard', 'dblp_profile': 'https://dblp.org/pid/o/DouglasWOard.html'}, {'name': 'Katie Shilton', 'dblp_profile': 'https://dblp.org/pid/38/1450.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"Today’s search engines are designed with a single fundamental goal: to help us find the things we want to see. Paradoxically, the very fact that they do this well means that there are many collections that we are not allowed to search. Citizens are not allowed to search some government records because there may be intermixed information that needs to be protected. Scholars are not yet allowed to see much of the growing backlog of unprocessed archival collections for similar reasons. These limitations, and many more, are direct consequences of the fact that today’s search engines are not designed to protect sensitive information. We need to change that by creating a new class of search algorithms designed to effectively “search among secrets” by balancing the user’s interest in finding relevant content with the provider’s interest in protecting sensitive content. This paper describes some first thoughts on evaluation for that task.",9df5ca4a2eb574c57fefd76b5e3e25d0e4da7f06
Estimating topical volume in social media streams,"[{'name': 'Praveen Bommannavar', 'dblp_profile': 'https://dblp.org/pid/12/7140.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Anand Rajaraman', 'dblp_profile': 'https://dblp.org/pid/r/ARajaraman.html'}]",2016,"This paper tackles the problem of estimating the volume of social media posts (e.g., tweets) that pertain to a particular topic. This task differs from related filtering and event detection applications in that the filtered content isn't meant for direct human consumption, but rather we are primarily interested in estimating the cardinality of relevant posts. We present a simple yet effective technique for generating and curating keywords to create what we call ""overlap filters"", which can be applied to a stream of social media posts. Our approach leverages human labeling and thus a crucial element of the work involves minimizing the cost of human computation. On top of a ""day zero"" cold start algorithm, we describe a number of optimizations that take advantage of history to further reduce labeling costs. Experimental results show that our overlap filters produce accurate volume estimates at low costs, and our method is simple enough to deploy in practice.",9663a8d8365cbee4dd7c49b20edf11781a086c9a
UMD-TTIC-UW at SemEval-2016 Task 1: Attention-Based Multi-Perspective Convolutional Neural Networks for Textual Similarity Measurement,"[{'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'John Wieting', 'dblp_profile': 'https://dblp.org/pid/156/0158.html'}, {'name': 'Kevin Gimpel', 'dblp_profile': 'https://dblp.org/pid/47/1252.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM-PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our final model outperforms the winning entry in STS2015 when evaluated on the STS2015 data.",00b874f8346cedadc2a6366c4b72e60140f99556
Interleaved Evaluation for Retrospective Summarization and Prospective Notification on Document Streams,"[{'name': 'Xin Qian', 'dblp_profile': 'https://dblp.org/pid/49/5061.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}]",2016,"We propose and validate a novel interleaved evaluation methodology for two complementary information seeking tasks on document streams: retrospective summarization and prospective notification. In the first, the user desires relevant and non-redundant documents that capture important aspects of an information need. In the second, the user wishes to receive timely, relevant, and non-redundant update notifications for a standing information need. Despite superficial similarities, interleaved evaluation methods for web ranking cannot be directly applied to these tasks; for example, existing techniques do not account for temporality or redundancy. Our proposed evaluation methodology consists of two components: a temporal interleaving strategy and a heuristic for credit assignment to handle redundancy. By simulating user interactions with interleaved results on submitted runs to the TREC 2014 tweet timeline generation (TTG) task and the TREC 2015 real-time filtering task, we demonstrate that our methodology yields system comparisons that accurately match the result of batch evaluations. Analysis further reveals weaknesses in current batch evaluation methodologies to suggest future directions for research.",0806321b65562ea4e5ef037be5fd9c754fc23298
An Exploration of Evaluation Metrics for Mobile Push Notifications,"[{'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}]",2016,"How do we evaluate systems that filter social media streams and send users updates via push notifications on their mobile phones? Such notifications must be relevant, timely, and novel. In this paper, we explore various evaluation metrics for this task, focusing specifically on measuring relevance. We begin with an analysis of metrics deployed at the TREC 2015 Microblog evaluations. A simple change to the metrics, reflecting a different assumption, dramatically alters system rankings. Applying another metric, previously used in the TREC Microblog evaluations, again yields different system rankings. We find little correlation between a number of ""reasonable"" evaluation metrics, which suggests that system effectiveness depends on how you measure it---an undesirable state in IR evaluation. However, we argue that existing evaluation metrics can be generalized into a framework that uses the same underlying contingency table, but places different weights and penalties. Although we stop short of proposing the ""one true metric"", this framework can guide the future development of a family of metrics that more accurately models user needs.",2b1a28417592fa3091e261401d0f93c7d08597f8
Burst Detection in Social Media Streams for Tracking Interest Profiles in Real Time,"[{'name': 'Cody Buntain', 'dblp_profile': 'https://dblp.org/pid/34/7214.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"This work presents RTTBurst, an end-to-end system for ingesting descriptions of user interest profiles and discovering new and relevant tweets based on those interest profiles using a simple model for identifying bursts in token usage. Our approach differs from standard retrieval-based techniques in that it primarily focuses on identifying noteworthy moments in the tweet stream, and ?summarizes? those moments using selected tweets. We lay out the architecture of RTTBurst, our participation in and performance at the TREC 2015 Microblog track, and a method for combining and potentially improving existing TREC systems. Official results and post hoc experiments show that our simple targeted burst detection technique is competitive with existing systems. Furthermore, we demonstrate that our burst detection mechanism can be used to improve the performance of other systems for the same task.",973ee7ce764dd3045f9545211f3c7a756f4fa823
Sampling Strategies and Active Learning for Volume Estimation,"[{'name': 'Haotian Zhang', 'dblp_profile': 'https://dblp.org/pid/83/4184-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Gordon V. Cormack', 'dblp_profile': 'https://dblp.org/pid/c/GVCormack.html'}, {'name': 'Mark D. Smucker', 'dblp_profile': 'https://dblp.org/pid/07/801.html'}]",2016,"This paper tackles the challenge of accurately and efficiently estimating the number of relevant documents in a collection for a particular topic. One real-world application is estimating the volume of social media posts (e.g., tweets) pertaining to a topic, which is fundamental to tracking the popularity of politicians and brands, the potential sales of a product, etc. Our insight is to leverage active learning techniques to find all the ""easy"" documents, and then to use sampling techniques to infer the number of relevant documents in the residual collection. We propose a simple yet effective technique for determining this ""switchover"" point, which intuitively can be understood as the ""knee"" in an effort vs. recall gain curve, as well as alternative sampling strategies beyond the knee. We show on several TREC datasets and a collection of tweets that our best technique yields more accurate estimates (with the same effort) than several alternatives.",0503db9c16df658b63e9c75a50261e03c5b4bceb
Simple Dynamic Emission Strategies for Microblog Filtering,"[{'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"Push notifications from social media provide a method to keep up-to-date on topics of personal interest. To be effective, notifications must achieve a balance between pushing too much and pushing too little. Push too little and the user misses important updates; push too much and the user is overwhelmed by unwanted information. Using data from the TREC 2015 Microblog track, we explore simple dynamic emission strategies for microblog push notifications. The key to effective notifications lies in establishing and maintaining appropriate thresholds for pushing updates. We explore and evaluate multiple threshold setting strategies, including purely static thresholds, dynamic thresholds without user feedback, and dynamic thresholds with daily feedback. Our best technique takes advantage of daily feedback in a simple yet effective manner, achieving the best known result reported in the literature to date.",bb8af4ef931dbf4b1c59522c355f03746cc2c2fb
A Platform for Streaming Push Notifications to Mobile Assessors,"[{'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}]",2016,"We present an assessment platform for gathering online relevance judgments for mobile push notifications that will be deployed in the newly-created TREC 2016 Real-Time Summarization (RTS) track. There is emerging interest in building systems that filter social media streams such as tweets to identify interesting and novel content in real time, putatively for delivery to users' mobile phones. In our evaluation design, all participants subscribe to the Twitter streaming API to identify relevant tweets with respect to a set of interest profiles. As the systems generate results, they are pushed in real time to our evaluation broker via a REST API. The broker then ""routes"" the tweets to assessors who have installed a custom app on their mobile phones. We detail the design of this platform and discuss a number of challenges that need to be tackled in this type of ""Living Labs"" setup. It is our goal that such an evaluation design will mitigate any issues that have arisen in traditional batch-style evaluations of this type of task.",7fe4e82231cf7293c2706fabe3b8ed533c89f498
Overview of the TREC 2016 Real-Time Summarization Track,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}, {'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Richard McCreadie', 'dblp_profile': 'https://dblp.org/pid/29/7184.html'}, {'name': 'Ellen M. Voorhees', 'dblp_profile': 'https://dblp.org/pid/60/3753.html'}, {'name': 'Fernando Diaz', 'dblp_profile': 'https://dblp.org/pid/38/2451-1.html'}]",2016,,
Afterburner: The Case for In-Browser Analytics,"[{'name': 'Kareem El Gebaly', 'dblp_profile': 'https://dblp.org/pid/53/3059.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"This paper explores the novel and unconventional idea of implementing an analytical RDBMS in pure JavaScript so that it runs completely inside a browser with no external dependencies. Our prototype, called Afterburner, generates compiled query plans that exploit typed arrays and asm.js, two relatively recent advances in JavaScript. On a few simple queries, we show that Afterburner achieves comparable performance to MonetDB running natively on the same machine. This is an interesting finding in that it shows how far JavaScript has come as an efficient execution platform. Beyond a mere technical curiosity, we discuss how our techniques could support ubiquitous in-browser interactive analytics (potentially integrating with browser-based notebooks) and also present interesting opportunities for split-execution strategies where query operators are distributed between the browser and backend servers.",3c022b208240a711ae465454c93fad516a4e48cb
The Effects of Latency Penalties in Evaluating Push Notification Systems,"[{'name': 'Luchen Tan', 'dblp_profile': 'https://dblp.org/pid/48/7704.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}]",2016,"We examine the effects of different latency penalties in the evaluation of push notification systems, as operationalized in the TREC 2015 Microblog track evaluation. The purpose of this study is to inform the design of metrics for the TREC 2016 Real-Time Summarization track, which is largely modeled after the TREC 2015 evaluation design.",66366b320038f637befd760626cc15b1362a92ca
Dynamic Trade-Off Prediction in Multi-Stage Retrieval Systems,"[{'name': 'J. Shane Culpepper', 'dblp_profile': 'https://dblp.org/pid/03/489.html'}, {'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2016,"Modern multi-stage retrieval systems are comprised of a candidate generation stage followed by one or more reranking stages. In such an architecture, the quality of the final ranked list may not be sensitive to the quality of initial candidate pool, especially in terms of early precision. This provides several opportunities to increase retrieval efficiency without significantly sacrificing effectiveness. In this paper, we explore a new approach to dynamically predicting two different parameters in the candidate generation stage which can directly affect the overall efficiency and effectiveness of the entire system. Previous work exploring this tradeoff has focused on global parameter settings that apply to all queries, even though optimal settings vary across queries. In contrast, we propose a technique which makes a parameter prediction that maximizes efficiency within a effectiveness envelope on a per query basis, using only static pre-retrieval features. The query-specific tradeoff point between effectiveness and efficiency is decided using a classifier cascade that weighs possible efficiency gains against effectiveness losses over a range of possible parameter cutoffs to make the prediction. The interesting twist in our new approach is to train classifiers without requiring explicit relevance judgments. We show that our framework is generalizable by applying it to two different retrieval parameters - selecting k in common top-k query retrieval algorithms, and setting a quality threshold, $\rho$, for score-at-a-time approximate query evaluation algorithms. Experimental results show that substantial efficiency gains are achievable depending on the dynamic parameter choice. In addition, our framework provides a versatile tool that can be used to estimate the effectiveness-efficiency tradeoffs that are possible before selecting and tuning algorithms to make machine learned predictions.",726da0a56d118eb8e7708688d74e0a2836a8118c
Ten Blue Links on Mars,"[{'name': 'Charles L. A. Clarke', 'dblp_profile': 'https://dblp.org/pid/96/3666.html'}, {'name': 'Gordon V. Cormack', 'dblp_profile': 'https://dblp.org/pid/c/GVCormack.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Roegiest', 'dblp_profile': 'https://dblp.org/pid/122/5812.html'}]",2016,"This paper explores a simple question: How would we provide a high-quality search experience on Mars, where the fundamental physical limit is speed-of-light propagation delays on the order of tens of minutes? On Earth, users are accustomed to nearly instantaneous responses from web services. Is it possible to overcome orders-of-magnitude longer latency to provide a tolerable user experience on Mars? In this paper, we formulate the searching from Mars problem as a tradeoff between ""effort"" (waiting for responses from Earth) and ""data transfer"" (pre-fetching or caching data on Mars). The contribution of our work is articulating this design space and presenting two case studies that explore the effectiveness of baseline techniques, using publicly available data from the TREC Total Recall and Sessions Tracks. We intend for this research problem to be aspirational as well as inspirational---even if one is not convinced by the premise of Mars colonization, there are Earth-based scenarios such as searching from rural villages in India that share similar constraints, thus making the problem worthy of exploration and attention from researchers.",5d77fe775365a8a34909ddf2f155e423efe2e47b
Is Big Data a Transient Problem?,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"What does the future hold for Big Data? The author states that it could be the same qualitatively, just bigger and better, or there might be fundamentally disruptive forces that completely reshape the computing landscape. Trying to predict the future, of course, is a perilous exercise. At best, this article provides some deep insight on future developments in Big Data. At worst, it makes for an interesting cocktail conversation. Either way, it's worth the rumination.",2a31f2de7aeebffba5722d27b1f3b855ab91e61e
Report on the Evaluation-as-a-Service (EaaS) Expert Workshop,"[{'name': 'Frank Hopfgartner', 'dblp_profile': 'https://dblp.org/pid/85/5730.html'}, {'name': 'Allan Hanbury', 'dblp_profile': 'https://dblp.org/pid/55/6683.html'}, {'name': 'Henning Müller', 'dblp_profile': 'https://dblp.org/pid/93/4446.html'}, {'name': 'Noriko Kando', 'dblp_profile': 'https://dblp.org/pid/40/4968.html'}, {'name': 'Simon Mercer', 'dblp_profile': 'https://dblp.org/pid/78/4920.html'}, {'name': 'Jayashree Kalpathy-Cramer', 'dblp_profile': 'https://dblp.org/pid/50/3269.html'}, {'name': 'Martin Potthast', 'dblp_profile': 'https://dblp.org/pid/87/6573.html'}, {'name': 'Tim Gollub', 'dblp_profile': 'https://dblp.org/pid/17/10440.html'}, {'name': 'Anastasia Krithara', 'dblp_profile': 'https://dblp.org/pid/32/6354.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Krisztian Balog', 'dblp_profile': 'https://dblp.org/pid/85/4125.html'}, {'name': 'Ivan Eggel', 'dblp_profile': 'https://dblp.org/pid/27/7331.html'}]",2015,"In this report, we summarize the outcome of the ""Evaluation-as-a-Service"" workshop that was held on the 5th and 6th March 2015 in Sierre, Switzerland. The objective of the meeting was to bring together initiatives that use cloud infrastructures, virtual machines, APIs (Application Programming Interface) and related projects that provide evaluation of information retrieval or machine learning tools as a service.",b4538c7778be366a023bc8ba6a28c5fc3686e826
"Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR)","[{'name': 'Jaime Arguello', 'dblp_profile': 'https://dblp.org/pid/90/4580.html'}, {'name': 'Matt Crane', 'dblp_profile': 'https://dblp.org/pid/123/3283.html'}, {'name': 'Fernando Diaz', 'dblp_profile': 'https://dblp.org/pid/38/2451-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}]",2015,"The SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR) took place on Thursday, August 13, 2015 in Santiago, Chile. The goal of the workshop was two fold. The first to provide a venue for the publication and presentation of negative results. The second was to provide a venue through which the authors of open source search engines could compare performance of indexing and searching on the same collections and on the same machines - encouraging the sharing of ideas and discoveries in a like-to-like environment. In total three papers were presented and seven systems participated.",ffb4f43303b43c8a2b82709fa12c279c91512862
Gappy Pattern Matching on GPUs for On-Demand Extraction of Hierarchical Translation Grammars,"[{'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Lopez', 'dblp_profile': 'https://dblp.org/pid/65/5274.html'}]",2015,"Grammars for machine translation can be materialized on demand by finding source phrases in an indexed parallel corpus and extracting their translations. This approach is limited in practical applications by the computational expense of online lookup and extraction. For phrase-based models, recent work has shown that on-demand grammar extraction can be greatly accelerated by parallelization on general purpose graphics processing units (GPUs), but these algorithms do not work for hierarchical models, which require matching patterns that contain gaps. We address this limitation by presenting a novel GPU algorithm for on-demand hierarchical grammar extraction that is at least an order of magnitude faster than a comparable CPU algorithm when processing large batches of sentences. In terms of end-to-end translation, with decoding on the CPU, we increase throughput by roughly two thirds on a standard MT evaluation dataset. The GPU necessary to achieve these improvements increases the cost of a server by about a third. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput.",a3fc2700d3ddff04136f90ad21aff305af89a191
Reproducible Experiments on Lexical and Temporal Feedback for Tweet Search,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}]",2015,,a8ed33b244a94f032437fc6f0d7e52d5aad19133
Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks,"[{'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Kevin Gimpel', 'dblp_profile': 'https://dblp.org/pid/47/1252.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"Modeling sentence similarity is complicated by the ambiguity and variability of linguistic expression. To cope with these challenges, we propose a model for comparing sentences that uses a multiplicity of perspectives. We first model each sentence using a convolutional neural network that extracts features at multiple levels of granularity and uses multiple types of pooling. We then compare our sentence representations at several granularities using multiple similarity metrics. We apply our model to three tasks, including the Microsoft Research paraphrase identification task and two SemEval semantic textual similarity tasks. We obtain strong performance on all tasks, rivaling or exceeding the state of the art without using external resources such as WordNet or parsers.",ae3e2451491f7d6ea1ee0c587fd8c811b4200c07
Anytime Ranking for Impact-Ordered Indexes,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}]",2015,"The ability for a ranking function to control its own execution time is useful for managing load, reigning in outliers, and adapting to different types of queries. We propose a simple yet effective anytime algorithm for impact-ordered indexes that builds on a score-at-a-time query evaluation strategy. In our approach, postings segments are processed in decreasing order of their impact scores, and the algorithm early terminates when a specified number of postings have been processed. With a simple linear model and a few training topics, we can determine this threshold given a time budget in milliseconds. Experiments on two web test collections show that our approach can accurately control query evaluation latency and that aggressive limits on execution time lead to minimal decreases in effectiveness.",b005d880638d69a7a7d1e4ae941884d9cbccbb16
Building a Self-Contained Search Engine in the Browser,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"JavaScript engines inside modern web browsers are capable of running sophisticated multi-player games, rendering impressive 3D scenes, and supporting complex, interactive visualizations. Can this processing power be harnessed for information retrieval? This paper explores the feasibility of building a JavaScript search engine that runs completely self-contained on the client side within the browser - this includes building the inverted index, gathering terms statistics for scoring, and performing query evaluation. The design takes advantage of the IndexDB API, which is implemented by the LevelDB key{value store inside Google's Chrome browser. Experiments show that although the performance of the JavaScript prototype falls far short of the open-source Lucene search engine, it is sufficiently responsive for interactive applications. This feasibility demonstration opens the door to interesting applications and architectures.",bbf0f58fd7eb54efd8054d90546d86fa8c9d0378
The Feasibility of Brute Force Scans for Real-Time Tweet Search,"[{'name': 'Yulu Wang', 'dblp_profile': 'https://dblp.org/pid/143/3574.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"The real-time search problem requires making ingested documents immediately searchable, which presents architectural challenges for systems built around inverted indexing. In this paper, we explore a radical proposition: What if we abandon document inversion and instead adopt an architecture based on brute force scans of document representations? In such a design, ""indexing"" simply involves appending the parsed representation of an ingested document to an existing buffer, which is simple and fast. Quite surprisingly, experiments with TREC Microblog test collections show that query evaluation with brute force scans is feasible and performance compares favorably to a traditional search architecture based on an inverted index, especially if we take advantage of vectorized SIMD instructions and multiple cores in modern processor architectures. We believe that such a novel design is worth further exploration by IR researchers and practitioners.",834d577e31f5b51210f6caa84236b2db4f99dce8
Identifying Duplicate and Contradictory Information in Wikipedia,"[{'name': 'Sarah Weissman', 'dblp_profile': 'https://dblp.org/pid/147/5240.html'}, {'name': 'Samet Ayhan', 'dblp_profile': 'https://dblp.org/pid/147/5343.html'}, {'name': 'Joshua Bradley', 'dblp_profile': 'https://dblp.org/pid/147/4908.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"In this paper, we identify sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify sentences with high Jaccard similarity, followed by a pass to generate sentence clusters. Based on manual examination, we discovered that these clusters can be categorized into six different types: templates, identical sentences, copyediting, factual drift, references, and other. Two of these categories are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia.",91dbe227966e90dce6a5b5f7936cb8037ecf0e92
The Sum of All Human Knowledge in Your Pocket: Full-Text Searchable Wikipedia on a Raspberry Pi,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"We demonstrate a prototype that takes advantage of open-source software to put a full-text searchable copy of Wikipedia on a Raspberry Pi, providing nearby devices access to content via wifi or bluetooth without requiring internet connectivity. This short paper articulates the advantages of such a form factor and provides an evaluation of browsing and search capabilities. We believe that personal digital libraries on lightweight mobile computing devices represent an interesting research direction to pursue.",730b04a84f71a9abd5531ebf3ca61caa5844b3e8
Developing an Open-Source Bibliometric Ranking Website Using Google Scholar Citation Profiles for Researchers in the Field of Biomedical Informatics,"[{'name': 'Dean F. Sittig', 'dblp_profile': 'https://dblp.org/pid/04/1456.html'}, {'name': 'Allison B. McCoy', 'dblp_profile': 'https://dblp.org/pid/08/11278.html'}, {'name': 'Adam Wright', 'dblp_profile': 'https://dblp.org/pid/46/7031.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"We developed the Biomedical Informatics Researchers ranking website (rank.informatics-review.com) to overcome many of the limitations of previous scientific productivity ranking strategies. The website is composed of four key components that work together to create an automatically updating ranking website: (1) list of biomedical informatics researchers, (2) Google Scholar scraper, (3) display page, and (4) updater. The site has been useful to other groups in evaluating researchers, such as tenure and promotions committees in interpreting the various citation statistics reported by candidates. Creation of the Biomedical Informatics Researchers ranking website highlights the vast differences in scholarly productivity among members of the biomedical informatics research community.",73346a8af61eddaf442fac1f53029822273377fb
Assessor Differences and User Preferences in Tweet Timeline Generation,"[{'name': 'Yulu Wang', 'dblp_profile': 'https://dblp.org/pid/143/3574.html'}, {'name': 'Garrick Sherman', 'dblp_profile': 'https://dblp.org/pid/147/9142.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}]",2015,"In information retrieval evaluation, when presented with an effectiveness difference between two systems, there are three relevant questions one might ask. First, are the differences statistically significant? Second, is the comparison stable with respect to assessor differences? Finally, is the difference actually meaningful to a user? This paper tackles the last two questions about assessor differences and user preferences in the context of the newly-introduced tweet timeline generation task in the TREC 2014 Microblog track, where the system's goal is to construct an informative summary of non-redundant tweets that addresses the user's information need. Central to the evaluation methodology is human-generated semantic clusters of tweets that contain substantively similar information. We show that the evaluation is stable with respect to assessor differences in clustering and that user preferences generally correlate with effectiveness metrics even though users are not explicitly aware of the semantic clustering being performed by the systems. Although our analyses are limited to this particular task, we believe that lessons learned could generalize to other evaluations based on establishing semantic equivalence between information units, such as nugget-based evaluations in question answering and temporal summarization.",2dcfbab048b397b42e955982b0fff94e4f64620d
"SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR)","[{'name': 'Jaime Arguello', 'dblp_profile': 'https://dblp.org/pid/90/4580.html'}, {'name': 'Fernando Diaz', 'dblp_profile': 'https://dblp.org/pid/38/2451-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Andrew Trotman', 'dblp_profile': 'https://dblp.org/pid/t/ATrotman.html'}]",2015,"Many, if not most, published research papers in Information Retrieval (IR) describe the following process: the authors identify an opportunity to improve on a particular IR task, implement an experimental system, and compare its performance against one or more baselines (or a control condition, in the case of a user study). The quality of the research is judged based on the magnitude of the improvement and whether the methodological choices suggest external validity and generalizability, for example, whether the experimental setup is “realistic” or whether the baseline methods reflect the state of the art. Unfortunately, research demonstrating the failure to reproduce or generalize previous results does not have a similar publication venue. This sort of result—often referred to as a ‘negative result’—serves to control the quality of published research in a scientific discipline and to better understand the limits of previously published methods. Publication venues for such research exist in fields such as ecology, biomedicine, pharmacy,, and social science. The SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR) aims to provide a venue for publication and discussion of IR research that fails to reproduce a previously published result under the same or similar experimental conditions (e.g., same test collection and system configuration) and research that demonstrates the failure to generalize an existing approach to a new domain. To this end, we have developed a set of categories covering different ways in which a result may",0bd9bd0ebcd371ab8c6a3137fa5df190816193f1
Burst Detection in Social Media Streams for Tracking Interest Profiles in Real Time,"[{'name': 'Cody Buntain', 'dblp_profile': 'https://dblp.org/pid/34/7214.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"This work presents RTTBurst, an end-to-end system for ingesting descriptions of user interest profiles and discovering new and relevant tweets based on those interest profiles using a simple model for identifying bursts in token usage. Our approach differs from standard retrieval-based techniques in that it primarily focuses on identifying noteworthy moments in the tweet stream, and ?summarizes? those moments using selected tweets. We lay out the architecture of RTTBurst, our participation in and performance at the TREC 2015 Microblog track, and a method for combining and potentially improving existing TREC systems. Official results and post hoc experiments show that our simple targeted burst detection technique is competitive with existing systems. Furthermore, we demonstrate that our burst detection mechanism can be used to improve the performance of other systems for the same task.",973ee7ce764dd3045f9545211f3c7a756f4fa823
Overview of the TREC-2015 Microblog Track,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}, {'name': 'Garrick Sherman', 'dblp_profile': 'https://dblp.org/pid/147/9142.html'}, {'name': 'Yulu Wang', 'dblp_profile': 'https://dblp.org/pid/143/3574.html'}, {'name': 'Ellen M. Voorhees', 'dblp_profile': 'https://dblp.org/pid/60/3753.html'}]",2015,,860f07c970acb58fa093c6b961be05b39ab7f0fa
Scaling Down Distributed Infrastructure on Wimpy Machines for Personal Web Archiving,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2015,"Warcbase is an open-source platform for storing, managing, and analyzing web archives using modern ""big data"" infrastructure on commodity clusters---specifically, HBase for storage and Hadoop for data analytics. This paper describes an effort to scale ""down"" Warcbase onto a Raspberry Pi, an inexpensive single-board computer about the size of a deck of playing cards. Apart from an interesting technology demonstration, such a design presents new opportunities for personal web archiving, in enabling a low-cost, low-power, portable device that is able to continuously capture a user's web browsing history---not only the URLs of the pages that a user has visited, but the contents of those pages---and allowing the user to revisit any previously-encountered page, as it appeared at that time. Experiments show that data ingestion throughput and temporal browsing latency are adequate with existing hardware, which means that such capabilities are already feasible today.",4ddcbb0919833b16466a9661b0f6e9553b77cbf2
Learning to Discover Key Moments in Social Media Streams,"[{'name': 'Cody Buntain', 'dblp_profile': 'https://dblp.org/pid/34/7214.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jennifer Golbeck', 'dblp_profile': 'https://dblp.org/pid/48/2412.html'}]",2015,"This paper introduces LABurst, a general technique for identifying key moments, or moments of high impact, in social media streams without the need for domain-specific information or seed keywords. We leverage machine learning to model temporal patterns around bursts in Twitter's unfiltered public sample stream and build a classifier to identify tokens experiencing these bursts. We show LABurst performs competitively with existing burst detection techniques while simultaneously providing insight into and detection of unanticipated moments. To demonstrate our approach's potential, we compare two baseline event-detection algorithms with our language-agnostic algorithm to detect key moments across three major sporting competitions: 2013 World Series, 2014 Super Bowl, and 2014 World Cup. Our results show LABurst outperforms a time series analysis baseline and is competitive with a domain-specific baseline even though we operate without any domain knowledge. We then go further by transferring LABurst's models learned in the sports domain to the task of identifying earthquakes in Japan and show our method detects large spikes in earthquake-related tokens within two minutes of the actual event.",e9f0cb252b092ddd03c452587d9f175912937dbf
Evaluation-as-a-Service: Overview and Outlook,"[{'name': 'Allan Hanbury', 'dblp_profile': 'https://dblp.org/pid/55/6683.html'}, {'name': 'Henning Müller', 'dblp_profile': 'https://dblp.org/pid/93/4446.html'}, {'name': 'Krisztian Balog', 'dblp_profile': 'https://dblp.org/pid/85/4125.html'}, {'name': 'Torben Brodt', 'dblp_profile': 'https://dblp.org/pid/135/6607.html'}, {'name': 'Gordon V. Cormack', 'dblp_profile': 'https://dblp.org/pid/c/GVCormack.html'}, {'name': 'Ivan Eggel', 'dblp_profile': 'https://dblp.org/pid/27/7331.html'}, {'name': 'Tim Gollub', 'dblp_profile': 'https://dblp.org/pid/17/10440.html'}, {'name': 'Frank Hopfgartner', 'dblp_profile': 'https://dblp.org/pid/85/5730.html'}, {'name': 'Jayashree Kalpathy-Cramer', 'dblp_profile': 'https://dblp.org/pid/50/3269.html'}, {'name': 'Noriko Kando', 'dblp_profile': 'https://dblp.org/pid/40/4968.html'}, {'name': 'Anastasia Krithara', 'dblp_profile': 'https://dblp.org/pid/32/6354.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Simon Mercer', 'dblp_profile': 'https://dblp.org/pid/78/4920.html'}, {'name': 'Martin Potthast', 'dblp_profile': 'https://dblp.org/pid/87/6573.html'}]",2015,"Evaluation in empirical computer science is essential to show progress and assess technologies developed. Several research domains such as information retrieval have long relied on systematic evaluation to measure progress: here, the Cranfield paradigm of creating shared test collections, defining search tasks, and collecting ground truth for these tasks has persisted up until now. In recent years, however, several new challenges have emerged that do not fit this paradigm very well: extremely large data sets, confidential data sets as found in the medical domain, and rapidly changing data sets as often encountered in industry. Also, crowdsourcing has changed the way that industry approaches problem-solving with companies now organizing challenges and handing out monetary awards to incentivize people to work on their challenges, particularly in the field of machine learning. 
This white paper is based on discussions at a workshop on Evaluation-as-a-Service (EaaS). EaaS is the paradigm of not providing data sets to participants and have them work on the data locally, but keeping the data central and allowing access via Application Programming Interfaces (API), Virtual Machines (VM) or other possibilities to ship executables. The objective of this white paper are to summarize and compare the current approaches and consolidate the experiences of these approaches to outline the next steps of EaaS, particularly towards sustainable research infrastructures. 
This white paper summarizes several existing approaches to EaaS and analyzes their usage scenarios and also the advantages and disadvantages. The many factors influencing EaaS are overviewed, and the environment in terms of motivations for the various stakeholders, from funding agencies to challenge organizers, researchers and participants, to industry interested in supplying real-world problems for which they require solutions.",956698ae71e544841fa8d77a9c41c7b852058426
Real-Time Twitter Recommendation: Online Motif Detection in Large Dynamic Graphs,"[{'name': 'Pankaj Gupta', 'dblp_profile': 'https://dblp.org/pid/92/4081-2.html'}, {'name': 'Venu Satuluri', 'dblp_profile': 'https://dblp.org/pid/94/1656.html'}, {'name': 'Ajeet Grewal', 'dblp_profile': 'https://dblp.org/pid/149/5948.html'}, {'name': 'Siva Gurumurthy', 'dblp_profile': 'https://dblp.org/pid/15/4280.html'}, {'name': 'Volodymyr Zhabiuk', 'dblp_profile': 'https://dblp.org/pid/149/5917.html'}, {'name': 'Quannan Li', 'dblp_profile': 'https://dblp.org/pid/26/6832.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"We describe a production Twitter system for generating relevant, personalized, and timely recommendations based on observing the temporally-correlated actions of each user's followings. The system currently serves millions of recommendations daily to tens of millions of mobile users. The approach can be viewed as a specific instance of the novel problem of online motif detection in large dynamic graphs. Our current solution partitions the graph across a number of machines, and with the construction of appropriate data structures, motif detection can be translated into the lookup and intersection of adjacency lists in each partition. We conclude by discussing a generalization of the problem that perhaps represents a new class of data management systems.",8cca529e651867e5ac2a30ceca4e661ef0900ef7
Summingbird: A Framework for Integrating Batch and Online MapReduce Computations,"[{'name': 'P. Oscar Boykin', 'dblp_profile': 'https://dblp.org/pid/81/6893.html'}, {'name': 'Sam Ritchie', 'dblp_profile': 'https://dblp.org/pid/149/5863.html'}, {'name': ""Ian O'Connell"", 'dblp_profile': 'https://dblp.org/pid/45/3539.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"Summingbird is an open-source domain-specific language implemented in Scala and designed to integrate online and batch MapReduce computations in a single framework. Summingbird programs are written using dataflow abstractions such as sources, sinks, and stores, and can run on different execution platforms: Hadoop for batch processing (via Scalding/Cascading) and Storm for online processing. Different execution modes require different bindings for the dataflow abstractions (e.g., HDFS files or message queues for the source) but do not require any changes to the program logic. Furthermore, Summingbird can operate in a hybrid processing mode that transparently integrates batch and online results to efficiently generate up-to-date aggregations over long time spans. The language was designed to improve developer productivity and address pain points in building analytics solutions at Twitter where often, the same code needs to be written twice (once for batch processing and again for online processing) and indefinitely maintained in parallel. Our key insight is that certain algebraic structures provide the theoretical foundation for integrating batch and online processing in a seamless fashion. This means that Summingbird imposes constraints on the types of aggregations that can be performed, although in practice we have not found these constraints to be overly restrictive for a broad range of analytics tasks at Twitter.",d4fe2b0b5b5c10be2bb32e36b3c774885ce36686
NScale: Neighborhood-centric Analytics on Large Graphs,"[{'name': 'Abdul Quamar', 'dblp_profile': 'https://dblp.org/pid/127/6195.html'}, {'name': 'Amol Deshpande', 'dblp_profile': 'https://dblp.org/pid/d/AmolDeshpande.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"There is an increasing interest in executing rich and complex analysis tasks over large-scale graphs, many of which require processing and reasoning about a large number of multi-hop neighborhoods or subgraphs in the graph. Examples of such tasks include ego network analysis, motif counting in biological networks, finding social circles, personalized recommendations, link prediction, anomaly detection, analyzing influence cascades, and so on. These tasks are not well served by existing vertex-centric graph processing frameworks whose computation and execution models limit the user program to directly access the state of a single vertex, resulting in high communication, scheduling, and memory overheads in executing such tasks. Further, most existing graph processing frameworks also typically ignore the challenges in extracting the relevant portions of the graph that an analysis task is interested in, and loading it onto distributed memory. 
 
In this demonstration proposal, we describe NScale, a novel end-to-end graph processing framework that enables the distributed execution of complex neighborhood-centric analytics over large-scale graphs in the cloud. NScale enables users to write programs at the level of neighborhoods or subgraphs. NScale uses Apache YARN for efficient and fault-tolerant distribution of data and computation; it features GEL, a novel graph extraction and loading phase, that extracts the relevant portions of the graph and loads them into distributed memory using as few machines as possible. NScale utilizes novel techniques for the distributed execution of user computation that minimize memory consumption by exploiting overlap among the neighborhoods of interest. A comprehensive experimental evaluation shows orders-of-magnitude improvements in performance and total cost over vertex-centric approaches.",7bab27d62e2dd763a419de0a661a68e3cea1ebf0
Runtime Optimizations for Tree-Based Machine Learning Models,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2014,"Tree-based models have proven to be an effective solution for web ranking as well as other machine learning problems in diverse domains. This paper focuses on optimizing the runtime performance of applying such models to make predictions, specifically using gradient-boosted regression trees for learning to rank. Although exceedingly simple conceptually, most implementations of tree-based models do not efficiently utilize modern superscalar processors. By laying out data structures in memory in a more cache-conscious fashion, removing branches from the execution flow using a technique called predication, and micro-batching predictions using a technique called vectorization, we are able to better exploit modern processor architectures. Experiments on synthetic data and on three standard learning-to-rank datasets show that our approach is significantly faster than standard",1fc4a3db0443527ac78cba218f2263cd36736783
Exploiting Representations from Statistical Machine Translation for Cross-Language Information Retrieval,"[{'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"This work explores how internal representations of modern statistical machine translation systems can be exploited for cross-language information retrieval. We tackle two core issues that are central to query translation: how to exploit context to generate more accurate translations and how to preserve ambiguity that may be present in the original query, thereby retaining a diverse set of translation alternatives. These two considerations are often in tension since ambiguity in natural language is typically resolved by exploiting context, but effective retrieval requires striking the right balance. We propose two novel query translation approaches: the grammar-based approach extracts translation probabilities from translation grammars, while the decoder-based approach takes advantage of n-best translation hypotheses. Both are context-sensitive, in contrast to a baseline context-insensitive approach that uses bilingual dictionaries for word-by-word translation. Experimental results show that by “opening up” modern statistical machine translation systems, we can access intermediate representations that yield high retrieval effectiveness. By combining evidence from multiple sources, we demonstrate significant improvements over competitive baselines on standard cross-language information retrieval test collections. In addition to effectiveness, the efficiency of our techniques are explored as well.",215957f111d8c8ae8b80fc589d9fc35e3d2551b1
Do recommendations matter?: news recommendation in real life,"[{'name': 'Alan Said', 'dblp_profile': 'https://dblp.org/pid/71/7413.html'}, {'name': 'Alejandro Bellogín', 'dblp_profile': 'https://dblp.org/pid/23/1049.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2014,"We present a study of how recommendations are received in real life by users across different news domains (traditional online newspapers, hobbyist websites, forums, etc.). Our analysis shows that readers of websites centered around specific topics are generally less likely to interact with recommendations than readers of traditional news websites.",52742a33da8ba8cdbd68d41183f108affa33661d
Cumulative Citation Recommendation: A Feature-Aware Comparison of Approaches,"[{'name': 'Gebrekirstos G. Gebremeskel', 'dblp_profile': 'https://dblp.org/pid/155/4412.html'}, {'name': 'Jiyin He', 'dblp_profile': 'https://dblp.org/pid/05/5662.html'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"In this work, we conduct a feature-aware comparison of approaches to Cumulative Citation Recommendation (CCR), a task that aims to filter and rank a stream of documents according to their relevance to entities in a knowledge base. We conducted experiments starting with a big feature set, identified a powerful subset and applied it to comparing classification and learning-to-rank algorithms. With few set of powerful features, we achieve better performance than the state-of-the-art. Surprisingly, our findings challenge the previously known preference of learning-to-rank over classification: in our study, the CCR performance of the classification approach outperforms that using learning-to-rank. This indicates that comparing two approaches is problematic due to the interplay between the approaches themselves and the feature sets one chooses to use.",52284cb7be5e87f3db39a1aa940c331d5ba4c060
"Supporting ""Distant Reading"" for Web Archives","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Kari Kraus', 'dblp_profile': 'https://dblp.org/pid/55/11301.html'}, {'name': 'Ricardo L. Punzalan', 'dblp_profile': 'https://dblp.org/pid/190/7575.html'}]",2014,,6202b7df603137c1a6081d73069c394bf12e6340
The Impact of Future Term Statistics in Real-Time Tweet Search,"[{'name': 'Yulu Wang', 'dblp_profile': 'https://dblp.org/pid/143/3574.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,,8e32045659b6607831433141025e08ec07b2fa45
Column Stores as an IR Prototyping Tool,"[{'name': 'Hannes Mühleisen', 'dblp_profile': 'https://dblp.org/pid/25/9489.html'}, {'name': 'Thaer Samar', 'dblp_profile': 'https://dblp.org/pid/143/3619.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2014,,f9af1d4ba22802a57c0318f7028ed35d779c7b9e
"Optimization Techniques for ""Scaling Down"" Hadoop on Multi-Core, Shared-Memory Systems","[{'name': 'K. Ashwin Kumar', 'dblp_profile': 'https://dblp.org/pid/86/4593.html'}, {'name': 'Jonathan Gluck', 'dblp_profile': 'https://dblp.org/pid/135/4686.html'}, {'name': 'Amol Deshpande', 'dblp_profile': 'https://dblp.org/pid/d/AmolDeshpande.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"The underlying assumption behind Hadoop and, more generally, the need for distributed processing is that the data to be analyzed cannot be held in memory on a single machine. Today, this assumption needs to be re-evaluated. Although petabyte-scale datastores are increasingly common, it is unclear whether “typical” analytics tasks require more than a single high-end server. Additionally, we are seeing increased sophistication in analytics, e.g., machine learning, where we process smaller and more refined datasets. To address these trends, we propose “scaling down” Hadoop to run on multi-core, shared-memory machines. This paper presents a prototype runtime called Hone (“Hadoop One”) that is API compatible with Hadoop. With Hone, we can take an existing Hadoop application and run it efficiently on a single server. This allows us to take existing MapReduce algorithms and find the most suitable runtime environment for execution on datasets of varying sizes. For dataset sizes that fit into memory on a single machine, our experiments show that Hone is substantially faster than Hadoop running in pseudo-distributed mode. In some cases, Hone running on a single machine outperforms a 16-node Hadoop cluster.",0a4b1e6f12e4b205ae8004dd3f5b7c522c0b4950
Partitioning strategies for spatio-textual similarity join,"[{'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Hanan Samet', 'dblp_profile': 'https://dblp.org/pid/s/HananSamet.html'}]",2014,"Given a collection of geo-tagged objects with associated textual descriptors, the spatio-textual similarity join (STJoin) problem is to identify all pairs of similar objects that are close in distance. This task, which is useful in localized recommendations and other applications, is challenging since computing the join is super-linear with respect to the size of the collection. In this paper, we explore partitioning strategies for tackling STJoin. One approach is to start with a spatial data structure, traverse regions and apply a previous algorithm for identifying similar pairs of textual documents called All-Pairs. An alternative approach is to construct a global index but partition postings spatially and modify the All-Pairs algorithm to prune candidates based on distance. We evaluate these approaches on two real-world datasets and find that when running in a single thread, both approaches are comparable in terms of performance. However, a multi-threaded implementation of the global index approach is able to achieve far better speedup given its ability to parallelize at a finer granularity to avoid skewed distributions in task sizes. In addition to using All-Pairs as the underlying textual similarity join algorithm, we also explored an alternate algorithm known as PPJ: our findings are consistent, which suggests that load balancing is a fundamental issue affecting parallel implementations of STJoin algorithms.",36e9e630a8ba725b905806be89570c68f16537ab
Using visualizations to monitor changes and harvest insights from a global-scale logging infrastructure at Twitter,"[{'name': 'Krist Wongsuphasawat', 'dblp_profile': 'https://dblp.org/pid/78/7577.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"Logging user activities is essential to data analysis for internet products and services. Twitter has built a unified logging infrastructure that captures user activities across all clients it owns, making it one of the largest datasets in the organization. This paper describes challenges and opportunities in applying information visualization to log analysis at this massive scale, and shows how various visualization techniques can be adapted to help data scientists extract insights. In particular, we focus on two scenarios: (1) monitoring and exploring a large collection of log events, and (2) performing visual funnel analysis on log data with tens of thousands of event types. Two interactive visualizations were developed for these purposes: we discuss design choices and the implementation of these systems, along with case studies of how they are being used in day-to-day operations at Twitter.",3c5154d3aa120eb8d597af7db512bef1068882e6
Visual analytics of MOOCs at maryland,"[{'name': 'Zhengzheng Xu', 'dblp_profile': 'https://dblp.org/pid/10/10201.html'}, {'name': 'Dan Goldwasser', 'dblp_profile': 'https://dblp.org/pid/38/3382.html'}, {'name': 'Benjamin B. Bederson', 'dblp_profile': 'https://dblp.org/pid/b/BenBederson.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"We use visual analytics to explore participation in five MOOCs at the University of Maryland. In some of these courses, our analysis reveals interesting clustering patterns of student behavior. For other courses, visualizations provide ""color"" to help us better understand the range of student behavior.",81555e631d369b5b37d6d715c55db9f72c8589a3
Temporal feedback for tweet search with non-parametric density estimation,"[{'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jiyin He', 'dblp_profile': 'https://dblp.org/pid/05/5662.html'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2014,"This paper investigates the temporal cluster hypothesis: in search tasks where time plays an important role, do relevant documents tend to cluster together in time? We explore this question in the context of tweet search and temporal feedback: starting with an initial set of results from a baseline retrieval model, we estimate the temporal density of relevant documents, which is then used for result reranking. Our contributions lie in a method to characterize this temporal density function using kernel density estimation, with and without human relevance judgments, and an approach to integrating this information into a standard retrieval model. Experiments on TREC datasets confirm that our temporal feedback formulation improves search effectiveness, thus providing support for our hypothesis. Our approach out-performs both a standard baseline and previous temporal retrieval models. Temporal feedback improves over standard lexical feedback (with and without human judgments), illus- trating that temporal relevance signals exist independently of document content.",97c51aa284a44ddeaef78b50b2bfb90acba12500
Old dogs are great at new tricks: column stores for ir prototyping,"[{'name': 'Hannes Mühleisen', 'dblp_profile': 'https://dblp.org/pid/25/9489.html'}, {'name': 'Thaer Samar', 'dblp_profile': 'https://dblp.org/pid/143/3619.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2014,"We make the suggestion that instead of implementing custom index structures and query evaluation algorithms, IR researchers should simply store document representations in a column-oriented relational database and implement ranking models using SQL. For rapid prototyping, this is particularly advantageous since researchers can explore new scoring functions and features by simply issuing SQL queries, without needing to write imperative code. We demonstrate the feasibility of this approach by an implementation of conjunctive BM25 using two modern column stores. Experiments on a web collection show that a retrieval engine built in this manner achieves effectiveness and efficiency on par with custom-built retrieval engines, but provides many additional advantages, including cleaner query semantics, a simpler architecture, built-in support for error analysis, and the ability to exploit advances in database technology ""for free"".",1e390cf4ecfeae3c8f22ac73411792e06fb3eb66
On run diversity in Evaluation as a Service,"[{'name': 'Ellen M. Voorhees', 'dblp_profile': 'https://dblp.org/pid/60/3753.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}]",2014,"""Evaluation as a service"" (EaaS) is a new methodology that enables community-wide evaluations and the construction of test collections on documents that cannot be distributed. The basic idea is that evaluation organizers provide a service API through which the evaluation task can be completed. However, this concept violates some of the premises of traditional pool-based collection building and thus calls into question the quality of the resulting test collection. In particular, the service API might restrict the diversity of runs that contribute to the pool: this might hamper innovation by researchers and lead to incomplete judgment pools that affect the reusability of the collection. This paper shows that the distinctiveness of the retrieval runs used to construct the first test collection built using EaaS, the TREC 2013 Microblog collection, is not substantially different from that of the TREC-8 ad hoc collection, a high-quality collection built using traditional pooling. Further analysis using the `leave out uniques' test suggests that pools from the Microblog 2013 collection are less complete than those from TREC-8, although both collections benefit from the presence of distinctive and effective manual runs. Although we cannot yet generalize to all EaaS implementations, our analyses reveal no obvious flaws in the test collection built using the methodology in the TREC 2013 Microblog track.",4ec509cfcda5025fa65ed13ee033cfaf374ba8ab
Overview of the TREC-2014 Microblog Track,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Yulu Wang', 'dblp_profile': 'https://dblp.org/pid/143/3574.html'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}, {'name': 'Garrick Sherman', 'dblp_profile': 'https://dblp.org/pid/147/9142.html'}]",2014,"Abstract : This year represents the fourth iteration of the TREC Microblog track, which has been running since 2011. The track continued using the evaluation as a service model [8, 7], in which participants had access to the document collection only through an API. In addition to the temporally-anchored ad hoc retrieval task, which has been running since the inception of the track, we introduced a new task called tweet timeline generation (TTG), where the goal is to produce concise summaries about a particular topic for human consumption. Although this overview covers both tasks, more emphasis is placed on the tweet timeline generation task, which necessitated the development of a new evaluation methodology. We refer the reader to previous track overview papers [8, 12, 9] for details on the setup of the ad hoc task.",5c32734fd4a0765db5c11971f8293e4e2089e5ce
Infrastructure support for evaluation as a service,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}]",2014,"How do we conduct large-scale community-wide evaluations for information retrieval if we are unable to distribute the document collection? This was the challenge we faced in organizing a task on searching tweets at the Text Retrieval Conference (TREC), since Twitter's terms of service forbid redistribution of tweets. Our solution, which we call ""evaluation as a service"", was to provide an API through which the collection can be accessed for completing the evaluation task. This paper describes the infrastructure underlying the service and its deployment at TREC 2013. We discuss the merits of the approach and potential applicability to other evaluation scenarios.",0adc23fdd59b13afe9872d6d77b62d1318120015
Learning to efficiently rank on big data,"[{'name': 'Lidan Wang', 'dblp_profile': 'https://dblp.org/pid/03/2525.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}, {'name': 'Jiawei Han', 'dblp_profile': 'https://dblp.org/pid/h/JiaweiHan.html'}]",2014,"Ranking in response to user queries is a central problem in information retrieval, data mining, and machine learning. In the era of ""Big data"", traditional effectiveness-centric ranking techniques tend to get more and more costly (requiring additional hardware and energy costs) to sustain reasonable ranking speed on large data. The mentality of combating big data by throwing in more hardware/machines will quickly become highly expensive since data is growing at an extremely fast rate oblivious to any cost concerns from us. ""Learning to efficiently rank"" offers a cost-effective solution to ranking on large data (e.g., billions of documents). That is, it addresses a critically important question -- whether it is possible to improve ranking effectiveness on large data without incurring (too much) additional cost?",35fe34e8e08f1d301c4a25c83707d784a8292e46
Information network or social network?: the structure of the twitter follow graph,"[{'name': 'Seth A. Myers', 'dblp_profile': 'https://dblp.org/pid/62/8656.html'}, {'name': 'Aneesh Sharma', 'dblp_profile': 'https://dblp.org/pid/78/6674.html'}, {'name': 'Pankaj Gupta', 'dblp_profile': 'https://dblp.org/pid/92/4081-2.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"In this paper, we provide a characterization of the topological features of the Twitter follow graph, analyzing properties such as degree distributions, connected components, shortest path lengths, clustering coefficients, and degree assortativity. For each of these properties, we compare and contrast with available data from other social networks. These analyses provide a set of authoritative statistics that the community can reference. In addition, we use these data to investigate an often-posed question: Is Twitter a social network or an information network? The ""follow"" relationship in Twitter is primarily about information consumption, yet many follows are built on social ties. Not surprisingly, we find that the Twitter follow graph exhibits structural characteristics of both an information network and a social network. Going beyond descriptive characterizations, we hypothesize that from an individual user's perspective, Twitter starts off more like an information network, but evolves to behave more like a social network. We provide preliminary evidence that may serve as a formal model of how a hybrid network like Twitter evolves.",d055b02f7511ccf22ce6a00f3569d0a84278f5bd
Infrastructure for supporting exploration and discovery in web archives,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Milad Gholami', 'dblp_profile': 'https://dblp.org/pid/120/1847.html'}, {'name': 'Jinfeng Rao', 'dblp_profile': 'https://dblp.org/pid/134/5708.html'}]",2014,"Web archiving initiatives around the world capture ephemeral web content to preserve our collective digital memory. However, unlocking the potential of web archives requires tools that support exploration and discovery of captured content. These tools need to be scalable and responsive, and to this end we believe that modern ""big data"" infrastructure can provide a solid foundation. We present Warcbase, an open-source platform for managing web archives built on the distributed datastore HBase. Our system provides a flexible data model for storing and managing raw content as well as metadata and extracted knowledge. Tight integration with Hadoop provides powerful tools for analytics and data processing. Relying on HBase for storage infrastructure simplifies the development of scalable and responsive applications. We describe a service that provides temporal browsing and an interactive visualization based on topic models that allows users to explore archived content.",7bba909bc4f20a43e021b1f6fb36dfbf03503a8c
NScale: Neighborhood-centric Large-Scale Graph Analytics in the Cloud,"[{'name': 'Abdul Quamar', 'dblp_profile': 'https://dblp.org/pid/127/6195.html'}, {'name': 'Amol Deshpande', 'dblp_profile': 'https://dblp.org/pid/d/AmolDeshpande.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,,073fed4761d408025035cb60d7d82dddd20ebbf7
Identifying Duplicate and Contradictory Information in Wikipedia,"[{'name': 'Sarah Weissman', 'dblp_profile': 'https://dblp.org/pid/147/5240.html'}, {'name': 'Samet Ayhan', 'dblp_profile': 'https://dblp.org/pid/147/5343.html'}, {'name': 'Joshua Bradley', 'dblp_profile': 'https://dblp.org/pid/147/4908.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"In this paper, we identify sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify sentences with high Jaccard similarity, followed by a pass to generate sentence clusters. Based on manual examination, we discovered that these clusters can be categorized into six different types: templates, identical sentences, copyediting, factual drift, references, and other. Two of these categories are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia.",91dbe227966e90dce6a5b5f7936cb8037ecf0e92
On the Feasibility and Implications of Self-Contained Search Engines in the Browser,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2014,"JavaScript engines inside modern browsers are capable of running sophisticated multi-player games, rendering impressive 3D scenes, and supporting complex, interactive visualizations. Can this processing power be harnessed for information retrieval? This paper explores the feasibility of building a JavaScript search engine that runs completely self-contained on the client side within the browser---this includes building the inverted index, gathering terms statistics for scoring, and performing query evaluation. The design takes advantage of the IndexDB API, which is implemented by the LevelDB key-value store inside Google's Chrome browser. Experiments show that although the performance of the JavaScript prototype falls far short of the open-source Lucene search engine, it is sufficiently responsive for interactive applications. This feasibility demonstration opens the door to interesting applications in offline and private search across multiple platforms as well as hybrid split-execution architectures whereby clients and servers collaboratively perform query evaluation. One possible future scenario is the rise of an online search marketplace in which commercial search engine companies and individual users participate as rational economic actors, balancing privacy, resource usage, latency, and other factors based on customizable utility profiles.",058f2abb194f579408c14afc26cb12cce698ba3c
"Mapreduce is Good Enough?If All You Have is a Hammer, Throw Away Everything That's Not a Nail!","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,,
Document vector representations for feature extraction in multi-stage document ranking,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,,ccb224b95bc879b7914d0a2c2915f6694fe6160e
"Hone: ""Scaling Down"" Hadoop on Shared-Memory Systems","[{'name': 'K. Ashwin Kumar', 'dblp_profile': 'https://dblp.org/pid/86/4593.html'}, {'name': 'Jonathan Gluck', 'dblp_profile': 'https://dblp.org/pid/135/4686.html'}, {'name': 'Amol Deshpande', 'dblp_profile': 'https://dblp.org/pid/d/AmolDeshpande.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"The underlying assumption behind Hadoop and, more generally, the need for distributed processing is that the data to be analyzed cannot be held in memory on a single machine. Today, this assumption needs to be re-evaluated. Although petabyte-scale data-stores are increasingly common, it is unclear whether ""typical"" analytics tasks require more than a single high-end server. Additionally, we are seeing increased sophistication in analytics, e.g., machine learning, which generally operates over smaller and more refined datasets. To address these trends, we propose ""scaling down"" Hadoop to run on shared-memory machines. This paper presents a prototype runtime called Hone, intended to be both API and binary compatible with standard (distributed) Hadoop. That is, Hone can take an existing Hadoop jar and efficiently execute it, without modification, on a multi-core shared memory machine. This allows us to take existing Hadoop algorithms and find the most suitable run-time environment for execution on datasets of varying sizes. Our experiments show that Hone can be an order of magnitude faster than Hadoop pseudo-distributed mode (PDM); on dataset sizes that fit into memory, Hone can outperform a fully-distributed 15-node Hadoop cluster in some cases as well.",764e859f616401f7a0b2da069d7b4036e31ac33b
Evaluation as a service for information retrieval,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}]",2013,"How can we run large-scale, community-wide evaluations of information retrieval systems if we lack the ability to distribute the document collection on which the task is based? This was the challenge we faced in the TREC Microblog tracks over the past few years. In this paper, we present a novel evaluation methodology we dub ""evaluation as a service"", which was implemented at TREC 2013 to address restrictions on data redistribution. The basic idea is that instead of distributing the document collection, we (the track organizers) provided a service API ""in the cloud"" with which participants could accomplish the evaluation task. We outline advantages as well as disadvantages of this evaluation methodology, and discuss how the approach might be extended to other evaluation scenarios.",475b023e9b0914025fdc030ee343e60004a53fea
Fast candidate generation for real-time tweet search with bloom filter chains,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"The rise of social media and other forms of user-generated content have created the demand for real-time search: against a high-velocity stream of incoming documents, users desire a list of relevant results at the time the query is issued. In the context of real-time search on tweets, this work explores candidate generation in a two-stage retrieval architecture where an initial list of results is processed by a second-stage rescorer to produce the final output. We introduce Bloom filter chains, a novel extension of Bloom filters that can dynamically expand to efficiently represent an arbitrarily long and growing list of monotonically-increasing integers with a constant false positive rate. Using a collection of Bloom filter chains, a novel approximate candidate generation algorithm called BWand is able to perform both conjunctive and disjunctive retrieval. Experiments show that our algorithm is many times faster than competitive baselines and that this increased performance does not require sacrificing end-to-end effectiveness. Our results empirically characterize the trade-off space defined by output quality, query evaluation speed, and memory footprint for this particular search architecture.",4c93c4130964c55134bcbb4aa91bfa30376e57e3
Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce,"[{'name': 'Vladimir Eidelman', 'dblp_profile': 'https://dblp.org/pid/58/8159.html'}, {'name': 'Ke Wu', 'dblp_profile': 'https://dblp.org/pid/69/6116.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Philip Resnik', 'dblp_profile': 'https://dblp.org/pid/p/PhilipResnik.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"We present an open-source framework for large-scale online structured learning. Developed with the flexibility to handle cost-augmented inference problems such as statistical machine translation (SMT), our large-margin learner can be used with any decoder. Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing.",adea93b44984e95c597e8c6f3284360a5fc5f214
A month in the life of a production news recommender system,"[{'name': 'Alan Said', 'dblp_profile': 'https://dblp.org/pid/71/7413.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Alejandro Bellogín', 'dblp_profile': 'https://dblp.org/pid/23/1049.html'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2013,"During the last decade, recommender systems have become a ubiquitous feature in the online world. Research on systems and algorithms in this area has flourished, leading to novel techniques for personalization and recommendation. The evaluation of recommender systems, however, has not seen similar progress---techniques have changed little since the advent of recommender systems, when evaluation methodologies were ""borrowed"" from related research areas. As an effort to move evaluation methodology forward, this paper describes a production recommender system infrastructure that allows research systems to be evaluated in situ, by real-world metrics such as user clickthrough. We present an analysis of one month of interactions with this infrastructure and share our findings.",a1e1c97850639f8a328387df806f3654e1815502
Training Efficient Tree-Based Models for Document Ranking,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,,1a4054cfa2001290b50dfe5e4d6a599fb5b04405
"Visualizing the ""Pulse"" of World Cities on Twitter","[{'name': 'Miguel Rios', 'dblp_profile': 'https://dblp.org/pid/64/6743.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"
 
 We present a large-scale analysis of activity on Twitter in 50 majorcities around the world throughout all of 2012. Our study consists oftwo parts: First, we created heatmap visualizations, through whichperiods of comparatively intense and sparse activity are readilyapparent — these visual patterns reflect diurnal cycles, culturalnorms, and even religious practices. Second, we performed a clusteranalysis of these activity patterns to identify groupings of citiesthat are similar in the ways their inhabitants use Twitter. Notsurprisingly, cities cluster geographically, although we are able toidentify cross-cultural similarities as well.
 
",1a9019d32af08f6aa4532142bdf2d91eacc8618c
Dynamic memory allocation policies for postings in real-time Twitter search,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Michael Busch', 'dblp_profile': 'https://dblp.org/pid/116/5292.html'}]",2013,"We explore a real-time Twitter search application where tweets are arriving at a rate of several thousands per second. Real-time search demands that they be indexed and searchable immediately, which leads to a number of implementation challenges. In this paper, we focus on one aspect: dynamic postings allocation policies for index structures that are completely held in main memory. The core issue can be characterized as a ""Goldilocks Problem"". Because memory remains today a scare resource, an allocation policy that is too aggressive leads to inefficient utilization, while a policy that is too conservative is slow and leads to fragmented postings lists. We present a dynamic postings allocation policy that allocates memory in increasingly-larger ""slices"" from a small number of large, fixed pools of memory. With an analytical model and experiments, we explore different settings that balance time (query evaluation speed) and space (memory utilization).",6d646cac903bea6295844ac4f1bf61c8cb348cf7
Massively Parallel Suffix Array Queries and On-Demand Phrase Extraction for Statistical Machine Translation Using GPUs,"[{'name': 'Hua He', 'dblp_profile': 'https://dblp.org/pid/26/7270.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Adam Lopez', 'dblp_profile': 'https://dblp.org/pid/65/5274.html'}]",2013,"Translation models in statistical machine translation can be scaled to large corpora and arbitrarily-long phrases by looking up translations of source phrases “on the fly” in an indexed parallel corpus using suffix arrays. However, this can be slow because on-demand extraction of phrase tables is computationally expensive. We address this problem by developing novel algorithms for general purpose graphics processing units (GPUs), which enable suffix array queries for phrase lookup and phrase extraction to be massively parallelized. Compared to a highly-optimized, state-of-the-art serial CPU-based implementation, our techniques achieve at least an order of magnitude improvement in terms of throughput. This work demonstrates the promise of massively parallel architectures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing.",de05c034fcfffbefdab607ab4d5c729e2ffffe16
Flat vs. hierarchical phrase-based translation models for cross-language information retrieval,"[{'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"Although context-independent word-based approaches remain popular for cross-language information retrieval, many recent studies have shown that integrating insights from modern statistical machine translation systems can lead to substantial improvements in effectiveness. In this paper, we compare flat and hierarchical phrase-based translation models for query translation. Both approaches yield significantly better results than either a token-based or a one-best translation baseline on standard test collections. The choice of model manifests interesting tradeoffs in terms of effectiveness, efficiency, and model compactness.",e2c3bec0644fdf5494d740999883abff96a2c321
Effectiveness/efficiency tradeoffs for candidate generation in multi-stage retrieval architectures,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"This paper examines a multi-stage retrieval architecture consisting of a candidate generation stage, a feature extraction stage, and a reranking stage using machine-learned models. Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs with three candidate generation approaches: postings intersection with SvS, conjunctive query evaluation with WAND, and disjunctive query evaluation with WAND. We find no significant differences in end-to-end effectiveness as measured by NDCG between conjunctive and disjunctive WAND, but conjunctive query evaluation is substantially faster. Postings intersection with SvS, while fast, yields substantially lower end-to-end effectiveness, suggesting that document and term frequencies remain important in the initial ranking stage. These findings show that conjunctive WAND is the best overall candidate generation strategy of those we examined.",b40bb6c44b48dc1f38d8d993a7134fa22e7fdf91
Fast data in the era of big data: Twitter's real-time related query suggestion architecture,"[{'name': 'Gilad Mishne', 'dblp_profile': 'https://dblp.org/pid/34/5631.html'}, {'name': 'Jeff Dalton', 'dblp_profile': 'https://dblp.org/pid/05/2762-1.html'}, {'name': 'Zhenghua Li', 'dblp_profile': 'https://dblp.org/pid/72/8937.html'}, {'name': 'Aneesh Sharma', 'dblp_profile': 'https://dblp.org/pid/78/6674.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time ""twist"": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of ""big data"". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a ""big data"" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle ""big"" as well as ""fast"" data.",5edd152e84f3795e9322316bd017612357da49e3
"CWI and TU Delft Notebook TREC 2013: Contextual Suggestion, Federated Web Search, KBA, and Web Tracks","[{'name': 'Alejandro Bellogín', 'dblp_profile': 'https://dblp.org/pid/23/1049.html'}, {'name': 'Gebrekirstos G. Gebremeskel', 'dblp_profile': 'https://dblp.org/pid/155/4412.html'}, {'name': 'Jiyin He', 'dblp_profile': 'https://dblp.org/pid/05/5662.html'}, {'name': 'Alan Said', 'dblp_profile': 'https://dblp.org/pid/71/7413.html'}, {'name': 'Thaer Samar', 'dblp_profile': 'https://dblp.org/pid/143/3619.html'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jeroen B. P. Vuurens', 'dblp_profile': 'https://dblp.org/pid/119/7122.html'}]",2013,"This paper provides an overview of the work done at the Centrum Wiskunde & Informatica (CWI) and Delft University of Technology (TU Delft) for different tracks of TREC 2013. We participated in the Contextual Suggestion Track, the Federated Web Search Track, the Knowledge Base Acceleration (KBA) Track, and the Web Ad-hoc Track. In the Contextual Suggestion track, we focused on filtering the entire ClueWeb12 collection to generate recommendations according to the provided user profiles and contexts. For the Federated Web Search track, we exploited both categories from ODP and document relevance to merge result lists. In the KBA track, we focused on the Cumulative Citation Recommendation task where we exploited different features to two classification algorithms. For the Web track, we extended an ad-hoc baseline with a proximity model that promotes documents in which the query terms are positioned closer together.",eca3de8da0c8c1a4e0c9d613cb4f5c6e5a969a1e
Overview of the TREC-2013 Microblog Track,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Miles Efron', 'dblp_profile': 'https://dblp.org/pid/41/3635.html'}]",2013,,9554d7bec4480732d419c67c6d71356a596b7cee
Towards Efficient Large-Scale Feature-Rich Statistical Machine Translation,"[{'name': 'Vladimir Eidelman', 'dblp_profile': 'https://dblp.org/pid/58/8159.html'}, {'name': 'Ke Wu', 'dblp_profile': 'https://dblp.org/pid/69/6116.html'}, {'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Philip Resnik', 'dblp_profile': 'https://dblp.org/pid/p/PhilipResnik.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"We present the system we developed to provide efficient large-scale feature-rich discriminative training for machine translation. We describe how we integrate with MapReduce using Hadoop streaming to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. We report our findings on German-English and RussianEnglish translation, and discuss benefits, as well as obstacles, to tuning on larger development sets drawn from the parallel training data.",e898370545f5b7b5608019cfc692b295630dccb3
WTF: the who to follow service at Twitter,"[{'name': 'Pankaj Gupta', 'dblp_profile': 'https://dblp.org/pid/92/4081-2.html'}, {'name': 'Ashish Goel', 'dblp_profile': 'https://dblp.org/pid/g/AshishGoel.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Aneesh Sharma', 'dblp_profile': 'https://dblp.org/pid/78/6674.html'}, {'name': 'Dong Wang', 'dblp_profile': 'https://dblp.org/pid/40/3934.html'}, {'name': 'Reza Zadeh', 'dblp_profile': 'https://dblp.org/pid/61/10451.html'}]",2013,"WTF (""Who to Follow"") is Twitter's user recommendation service, which is responsible for creating millions of connections daily between users based on shared interests, common connections, and other related factors. This paper provides an architectural overview and shares lessons we learned in building and running the service over the past few years. Particularly noteworthy was our design decision to process the entire Twitter graph in memory on a single server, which significantly reduced architectural complexity and allowed us to develop and deploy the service in only a few months. At the core of our architecture is Cassovary, an open-source in-memory graph processing engine we built from scratch for WTF. Besides powering Twitter's user recommendations, Cassovary is also used for search, discovery, promoted products, and other services as well. We describe and evaluate a few graph recommendation algorithms implemented in Cassovary, including a novel approach based on a combination of random walks and SALSA. Looking into the future, we revisit the design of our architecture and comment on its limitations, which are presently being addressed in a second-generation system under development.",989d672d38de29c36bb968edfdb593038bba7b85
Dynamic Memory Allocation Policies for Postings in Real-Time Twitter Search,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Michael Busch', 'dblp_profile': 'https://dblp.org/pid/116/5292.html'}]",2013,"We explore a real-time Twitter search application where tweets are arriving at a rate of several thousands per second. Real-time search demands that they be indexed and searchable immediately, which leads to a number of implementation challenges. In this paper, we focus on one aspect: dynamic postings allocation policies for index structures that are completely held in main memory. The core issue can be characterized as a ""Goldilocks Problem"". Because memory remains today a scare resource, an allocation policy that is too aggressive leads to inefficient utilization, while a policy that is too conservative is slow and leads to fragmented postings lists. We present a dynamic postings allocation policy that allocates memory in increasingly-larger ""slices"" from a small number of large, fixed pools of memory. With an analytical model and experiments, we explore different settings that balance time (query evaluation speed) and space (memory utilization).",6d646cac903bea6295844ac4f1bf61c8cb348cf7
Monoidify! Monoids as a Design Principle for Efficient MapReduce Algorithms,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"It is well known that since the sort/shuffle stage in MapReduce is costly, local aggregation is one important principle to designing efficient algorithms. This short paper represents an attempt to more clearly articulate this design principle in terms of monoids, which generalizes the use of combiners and the in-mapper combining pattern.",ca4334c89f4223ed7d7699b7eb8b070efb7effd1
"Fast, Incremental Inverted Indexing in Main Memory for Web-Scale Collections","[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2013,"For text retrieval systems, the assumption that all data structures reside in main memory is increasingly common. In this context, we present a novel incremental inverted indexing algorithm for web-scale collections that directly constructs compressed postings lists in memory. Designing efficient in-memory algorithms requires understanding modern processor architectures and memory hierarchies: in this paper, we explore the issue of postings lists contiguity. Naturally, postings lists that occupy contiguous memory regions are preferred for retrieval, but maintaining contiguity increases complexity and slows indexing. On the other hand, allowing discontiguous index segments simplifies index construction but decreases retrieval performance. Understanding this tradeoff is our main contribution: We find that co-locating small groups of inverted list segments yields query evaluation performance that is statistically indistinguishable from fully-contiguous postings lists. In other words, it is not necessary to lay out in-memory data structures such that all postings for a term are contiguous; we can achieve ideal performance with a relatively small amount of effort.",23b09635ebb00ee8022c89894e9af2abcdbe38f2
The Unified Logging Infrastructure for Data Analytics at Twitter,"[{'name': 'George Lee', 'dblp_profile': 'https://dblp.org/pid/23/2203.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chuang Liu', 'dblp_profile': 'https://dblp.org/pid/52/1800.html'}, {'name': 'Andrew Lorek', 'dblp_profile': 'https://dblp.org/pid/117/5957.html'}, {'name': 'Dmitriy V. Ryaboy', 'dblp_profile': 'https://dblp.org/pid/48/2867.html'}]",2012,"In recent years, there has been a substantial amount of work on large-scale data analytics using Hadoop-based platforms running on large clusters of commodity machines. A less-explored topic is how those data, dominated by application logs, are collected and structured to begin with. In this paper, we present Twitter's production logging infrastructure and its evolution from application-specific logging to a unified ""client events"" log format, where messages are captured in common, well-formatted, flexible Thrift messages. Since most analytics tasks consider the user session as the basic unit of analysis, we pre-materialize ""session sequences"", which are compact summaries that can answer a large class of common queries quickly. The development of this infrastructure has streamlined log collection and data analysis, thereby improving our ability to rapidly experiment and iterate on various aspects of the service.",f696586f96d84467eecaa7663bad6cc10756df26
Scaling big data mining infrastructure: the twitter experience,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dmitriy V. Ryaboy', 'dblp_profile': 'https://dblp.org/pid/48/2867.html'}]",2012,"The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on ""big data"". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life ""in the trenches"" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall ""big picture"" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as ""plumbing"". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.",1dc8e6c4a32040c68887ef49faddc610ee25d641
Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2012,"Most modern web search engines employ a two-phase ranking strategy: a candidate list of documents is generated using a ""cheap"" but low-quality scoring function, which is then reranked by an ""expensive"" but high-quality method (usually machine-learned). This paper focuses on the problem of candidate generation for conjunctive query processing in this context. We describe and evaluate a fast, approximate postings list intersection algorithms based on Bloom filters. Due to the power of modern learning-to-rank techniques and emphasis on early precision, significant speedups can be achieved without loss of end-to-end retrieval effectiveness. Explorations reveal a rich design space where effectiveness and efficiency can be balanced in response to specific hardware configurations and application scenarios.",db6eedd560aa2c55f6c2f8ea80d19bd083192190
Combining Statistical Translation Techniques for Cross-Language Information Retrieval,"[{'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Douglas W. Oard', 'dblp_profile': 'https://dblp.org/pid/o/DouglasWOard.html'}]",2012,"Cross-language information retrieval today is dominated by techniques that rely principally on context-independent token-to-token mappings despite the fact that state-of-the-art statistical machine translation systems now have far richer translation models available in their internal representations. This paper explores combination-of-evidence techniques using three types of statistical translation models: context-independent token translation, token translation using phrase-dependent contexts, and token translation using sentence-dependent contexts. Context-independent translation is performed using statistically-aligned tokens in parallel text, phrase-dependent translation is performed using aligned statistical phrases, and sentence-dependent translation is performed using those same aligned phrases together with an n-gram language model. Experiments on retrieval of Arabic, Chinese, and French documents using English queries show that no one technique is optimal for all queries, but that statistically significant improvements in mean average precision over strong baselines can be achieved by combining translation evidence from all three techniques. The optimal combination is, however, found to be resource-dependent, indicating a need for future work on robust tuning to the characteristics of individual collections.",5b6d17e59fb710c97c066c8197648ede807d7324
Earlybird: Real-Time Search at Twitter,"[{'name': 'Michael Busch', 'dblp_profile': 'https://dblp.org/pid/116/5292.html'}, {'name': 'Krishna Gade', 'dblp_profile': 'https://dblp.org/pid/80/2611.html'}, {'name': 'Brian Larson', 'dblp_profile': 'https://dblp.org/pid/16/6426.html'}, {'name': 'Patrick Lok', 'dblp_profile': 'https://dblp.org/pid/116/5166.html'}, {'name': 'Samuel Luckenbill', 'dblp_profile': 'https://dblp.org/pid/116/5226.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2012,"The web today is increasingly characterized by social and real-time signals, which we believe represent two frontiers in information retrieval. In this paper, we present Early bird, the core retrieval engine that powers Twitter's real-time search service. Although Early bird builds and maintains inverted indexes like nearly all modern retrieval engines, its index structures differ from those built to support traditional web search. We describe these differences and present the rationale behind our design. A key requirement of real-time search is the ability to ingest content rapidly and make it searchable immediately, while concurrently supporting low-latency, high-throughput query evaluation. These demands are met with a single-writer, multiple-reader concurrency model and the targeted use of memory barriers. Early bird represents a point in the design space of real-time search engines that has worked well for Twitter's needs. By sharing our experiences, we hope to spur additional interest and innovation in this exciting space.",dc943ad0942f77e4ba6d9740ed3005e083cc9e8b
"A Study of ""Churn"" in Tweets and Real-Time Search Queries","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Gilad Mishne', 'dblp_profile': 'https://dblp.org/pid/34/5631.html'}]",2012,"
 
 The real-time nature of Twitter means that term distributions in tweets and in search queries change rapidly: the most frequent terms in one hour may look very different from those in the next. Informally, we call this phenomenon ""churn"". Our interest in analyzing churn stems from the perspective of real-time search. How do we ""correctly"" compute term statistics, considering that the underlying distributions change rapidly? In this paper, we present an analysis of tweet and query churn on Twitter, as a first step to answering this question. Analyses reveal interesting insights on the temporal dynamics of term distributions on Twitter and hold implications for the design of search systems.
 
",c21b4f3276bab3dadd1746c23166c373291a977d
Evaluating Real-Time Search over Tweets,"[{'name': 'Dean McCullough', 'dblp_profile': 'https://dblp.org/pid/00/11520.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Craig Macdonald', 'dblp_profile': 'https://dblp.org/pid/02/2224.html'}, {'name': 'Iadh Ounis', 'dblp_profile': 'https://dblp.org/pid/21/141.html'}, {'name': 'Richard McCreadie', 'dblp_profile': 'https://dblp.org/pid/29/7184.html'}]",2012,"
 
 Twitter offers a phenomenal platform for the social sharing of information. We describe new resources that have been created in the context of the Text Retrieval Conference (TREC) to support the academic study of Twitter as a real-time information source. We formalize an information seeking task — real-time search — and offer a methodology for measuring system effectiveness. At the TREC 2011 Microblog Track, 58 research groups participated in the first ever evaluation of this task. We present data from the effort to illustrate and support our methodology.
 
",b48824a9a2197048fea578d77b1db7a8cc3d0b01
Why Not Grab a Free Lunch? Mining Large Corpora for Parallel Sentences to Improve Translation Modeling,"[{'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2012,"It is well known that the output quality of statistical machine translation (SMT) systems increases with more training data. To obtain more parallel text for translation modeling, researchers have turned to the web to mine parallel sentences, but most previous approaches have avoided the difficult problem of pairwise similarity on cross-lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case).",774cac6906db877a3e8a0d37e9f9cab3754a5528
Looking inside the box: context-sensitive translation for cross-language information retrieval,"[{'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Douglas W. Oard', 'dblp_profile': 'https://dblp.org/pid/o/DouglasWOard.html'}]",2012,"Cross-language information retrieval (CLIR) today is dominated by techniques that use token-to-token mappings from bilingual dictionaries. Yet, state-of-the-art statistical translation models (e.g., using Synchronous Context-Free Grammars) are far richer, capturing multi-term phrases, term dependencies, and contextual constraints on translation choice. We present a novel CLIR framework that is able to reach inside the translation ""black box"" and exploit these sources of evidence. Experiments on the TREC-5/6 English-Chinese test collection show this approach to be promising.",c1a64dec9d0b8f4e241fcabdfff46f46a7f756f7
On building a reusable Twitter corpus,"[{'name': 'Richard McCreadie', 'dblp_profile': 'https://dblp.org/pid/29/7184.html'}, {'name': 'Ian Soboroff', 'dblp_profile': 'https://dblp.org/pid/94/2865.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Craig Macdonald', 'dblp_profile': 'https://dblp.org/pid/02/2224.html'}, {'name': 'Iadh Ounis', 'dblp_profile': 'https://dblp.org/pid/21/141.html'}, {'name': 'Dean McCullough', 'dblp_profile': 'https://dblp.org/pid/00/11520.html'}]",2012,"The Twitter real-time information network is the subject of research for information retrieval tasks such as real-time search. However, so far, reproducible experimentation on Twitter data has been impeded by restrictions imposed by the Twitter terms of service. In this paper, we detail a new methodology for legally building and distributing Twitter corpora, developed through collaboration between the Text REtrieval Conference (TREC) and Twitter. In particular, we detail how the first publicly available Twitter corpus - referred to as Tweets2011 - was distributed via lists of tweet identifiers and specialist tweet crawling software. Furthermore, we analyse whether this distribution approach remains robust over time, as tweets in the corpus are removed either by users or Twitter itself. Tweets2011 was successfully used by 58 participating groups for the TREC 2011 Microblog track, while our results attest to the robustness of the crawling methodology over time.",52786c2bdcdf6dc5ad2d8124959a4e8911be8021
Twanchor text: a preliminary study of the value of tweets as anchor text,"[{'name': 'Gilad Mishne', 'dblp_profile': 'https://dblp.org/pid/34/5631.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2012,,
Large-scale machine learning at twitter,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Alek Kolcz', 'dblp_profile': 'https://dblp.org/pid/12/3993.html'}]",2012,"The success of data-driven solutions to difficult problems, along with the dropping costs of storing and processing massive amounts of data, has led to growing interest in large-scale machine learning. This paper presents a case study of Twitter's integration of machine learning tools into its existing Hadoop-based, Pig-centric analytics platform. We begin with an overview of this platform, which handles ""traditional"" data warehousing and business intelligence tasks for the organization. The core of this work lies in recent Pig extensions to provide predictive analytics capabilities that incorporate machine learning, focused specifically on supervised classification. In particular, we have identified stochastic gradient descent techniques for online learning and ensemble methods as being highly amenable to scaling out to large amounts of data. In our deployed solution, common machine learning tasks such as data sampling, feature generation, training, and testing can be accomplished directly in Pig, via carefully crafted loaders, storage functions, and user-defined functions. This means that machine learning is just another Pig script, which allows seamless integration with existing infrastructure for data management, scheduling, and monitoring in a production environment, as well as access to rich libraries of user-defined functions and the materialized output of other scripts.",9ead7583542e55c84bb9b90260ffde7a70c88e8d
Overview of the TREC-2012 Microblog Track,"[{'name': 'Ian Soboroff', 'dblp_profile': 'https://dblp.org/pid/94/2865.html'}, {'name': 'Iadh Ounis', 'dblp_profile': 'https://dblp.org/pid/21/141.html'}, {'name': 'Craig Macdonald', 'dblp_profile': 'https://dblp.org/pid/02/2224.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2012,,90a73e6d4b72d753745020625b9e3fd43118c1fc
"A Study of ""Churn"" in Tweets and Real-Time Search Queries (Extended Version)","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Gilad Mishne', 'dblp_profile': 'https://dblp.org/pid/34/5631.html'}]",2012,,
The Unified Logging Infrastructure for Data Analytics at Twitter,"[{'name': 'George Lee', 'dblp_profile': 'https://dblp.org/pid/23/2203.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chuang Liu', 'dblp_profile': 'https://dblp.org/pid/52/1800.html'}, {'name': 'Andrew Lorek', 'dblp_profile': 'https://dblp.org/pid/117/5957.html'}, {'name': 'Dmitriy V. Ryaboy', 'dblp_profile': 'https://dblp.org/pid/48/2867.html'}]",2012,"In recent years, there has been a substantial amount of work on large-scale data analytics using Hadoop-based platforms running on large clusters of commodity machines. A less-explored topic is how those data, dominated by application logs, are collected and structured to begin with. In this paper, we present Twitter's production logging infrastructure and its evolution from application-specific logging to a unified ""client events"" log format, where messages are captured in common, well-formatted, flexible Thrift messages. Since most analytics tasks consider the user session as the basic unit of analysis, we pre-materialize ""session sequences"", which are compact summaries that can answer a large class of common queries quickly. The development of this infrastructure has streamlined log collection and data analysis, thereby improving our ability to rapidly experiment and iterate on various aspects of the service.",f696586f96d84467eecaa7663bad6cc10756df26
"MapReduce is Good Enough? If All You Have is a Hammer, Throw Away Everything That's Not a Nail!","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2012,"Hadoop is currently the large-scale data analysis ""hammer"" of choice, but there exist classes of algorithms that aren't ""nails"" in the sense that they are not particularly amenable to the MapReduce programming model. To address this, researchers have proposed MapReduce extensions or alternative programming models in which these algorithms can be elegantly expressed. This article espouses a very different position: that MapReduce is ""good enough,"" and that instead of trying to invent screwdrivers, we should simply get rid of everything that's not a nail. To be more specific, much discussion in the literature surrounds the fact that iterative algorithms are a poor fit for MapReduce. The simple solution is to find alternative, noniterative algorithms that solve the same problem. This article captures my personal experiences as an academic researcher as well as a software engineer in a ""real-world"" production analytics environment. From this combined perspective, I reflect on the current state and future of ""big data"" research.",afbc30ec8c690c52f0ada4b64a2eaf88679de233
Fast Data in the Era of Big Data: Twitter's Real-Time Related Query Suggestion Architecture,"[{'name': 'Gilad Mishne', 'dblp_profile': 'https://dblp.org/pid/34/5631.html'}, {'name': 'Jeff Dalton', 'dblp_profile': 'https://dblp.org/pid/05/2762-1.html'}, {'name': 'Zhenghua Li', 'dblp_profile': 'https://dblp.org/pid/72/8937.html'}, {'name': 'Aneesh Sharma', 'dblp_profile': 'https://dblp.org/pid/78/6674.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2012,"We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time ""twist"": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of ""big data"". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a ""big data"" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle ""big"" as well as ""fast"" data.",5edd152e84f3795e9322316bd017612357da49e3
Runtime Optimizations for Prediction with Tree-Based Models,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Arjen P. de Vries', 'dblp_profile': 'https://dblp.org/pid/v/ArjenPdeVries.html'}]",2012,"Tree-based models have proven to be an effective solution for web ranking as well as other problems in diverse domains. This paper focuses on optimizing the runtime performance of applying such models to make predictions, given an already-trained model. Although exceedingly simple conceptually, most implementations of tree-based models do not efficiently utilize modern superscalar processor architectures. By laying out data structures in memory in a more cache-conscious fashion, removing branches from the execution flow using a technique called predication, and micro-batching predictions using a technique called vectorization, we are able to better exploit modern processor architectures and significantly improve the speed of tree-based models over hard-coded if-else blocks. Our work contributes to the exploration of architecture-conscious runtime implementations of machine learning algorithms.",1f2902ced604e52d25b8fc16b74750fa042264dc
Special Issue on Cloud Computing,"[{'name': 'Gregory V. Chockler', 'dblp_profile': 'https://dblp.org/pid/c/GregoryChockler.html'}, {'name': 'Eliezer Dekel', 'dblp_profile': 'https://dblp.org/pid/61/1540.html'}, {'name': 'Joseph F. JáJá', 'dblp_profile': 'https://dblp.org/pid/j/JosephJaJa.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2011,,e6e5fcf2337da0871708608d63dfca84fe549a6b
When close enough is good enough: approximate positional indexes for efficient ranked retrieval,"[{'name': 'Tamer Elsayed', 'dblp_profile': 'https://dblp.org/pid/99/5856.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}]",2011,"Previous research has shown that features based on term proximity are important for effective retrieval. However, they incur substantial costs in terms of larger inverted indexes and slower query execution times as compared to term-based features. This paper explores whether term proximity features based on approximate term positions are as effective as those based on exact term positions. We introduce the novel notion of approximate positional indexes based on dividing documents into coarse-grained buckets and recording term positions with respect to those buckets. We propose different approaches to defining the buckets and compactly encoding bucket ids. In the context of linear ranking functions, experimental results show that features based on approximate term positions are able to achieve effectiveness comparable to exact term positions, but with smaller indexes and faster query evaluation.",3776c4dd5c00a515b4e3229d0e76a3d14d41fd32
"Automatic management of partitioned, replicated search services","[{'name': 'Florian Leibert', 'dblp_profile': 'https://dblp.org/pid/127/9206.html'}, {'name': 'Jake Mannix', 'dblp_profile': 'https://dblp.org/pid/127/9198.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Babak Hamadani', 'dblp_profile': 'https://dblp.org/pid/127/9194.html'}]",2011,"Low-latency, high-throughput web services are typically achieved through partitioning, replication, and caching. Although these strategies and the general design of large-scale distributed search systems are well known, the academic literature provides surprisingly few details on deployment and operational considerations in production environments. In this paper, we address this gap by sharing the distributed search architecture that underlies Twitter user search, a service for discovering relevant accounts on the popular microblogging service. Our design makes use of the principle that eliminates the distinction between failure and other anticipated service disruptions: as a result, most operational scenarios share exactly the same code path. This simplicity leads to greater robustness and fault-tolerance. Another salient feature of our architecture is its exclusive reliance on open-source software components, which makes it easier for the community to learn from our experiences and replicate our findings.",bc080a666b562e8b7d7059bc0cc8c9b4152f742f
In-depth accounts and passing mentions in the news: connecting readers to the context of a news event,"[{'name': 'Earl J. Wagner', 'dblp_profile': 'https://dblp.org/pid/01/6655.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2011,"Software that models how types of news events unfold can extract information about specific events and explain them to a news reader. This support can be useful when the background provided by an article is insufficient, if other news coverage exists from which an event's history can be extracted. For extended sequences of related events, it is reasonable to expect that articles published after the sequence concludes include less background coverage of the sequence. Focusing on two stereotypical types of event sequences --- kidnappings and corporate acquisitions -- we distinguish between articles providing in-depth coverage, those having multiple sentences mentioning the same event sequence, from articles making a passing mention in just one sentence. We find that, after an event sequence concludes, passing mentions become more common and there are significantly fewer mean mentions per article.",5a342a2dd281bbd0c36e3e98758506952dd35aac
Smoothing techniques for adaptive online language models: topic tracking in tweet streams,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Rion Snow', 'dblp_profile': 'https://dblp.org/pid/42/1066.html'}, {'name': 'William Morgan', 'dblp_profile': 'https://dblp.org/pid/32/9989.html'}]",2011,"We are interested in the problem of tracking broad topics such as ""baseball"" and ""fashion"" in continuous streams of short texts, exemplified by tweets from the microblogging service Twitter. The task is conceived as a language modeling problem where per-topic models are trained using hashtags in the tweet stream, which serve as proxies for topic labels. Simple perplexity-based classifiers are then applied to filter the tweet stream for topics of interest. Within this framework, we evaluate, both intrinsically and extrinsically, smoothing techniques for integrating ""foreground"" models (to capture recency) and ""background"" models (to combat sparsity), as well as different techniques for retaining history. Experiments show that unigram language models smoothed using a normalized extension of stupid backoff and a simple queue for history retention performs well on the task.",60603f300453a6c327042f76cbdba6625498946e
A cascade ranking model for efficient ranked retrieval,"[{'name': 'Lidan Wang', 'dblp_profile': 'https://dblp.org/pid/03/2525.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}]",2011,"There is a fundamental tradeoff between effectiveness and efficiency when designing retrieval models for large-scale document collections. Effectiveness tends to derive from sophisticated ranking functions, such as those constructed using learning to rank, while efficiency gains tend to arise from improvements in query evaluation and caching strategies. Given their inherently disjoint nature, it is difficult to jointly optimize effectiveness and efficiency in end-to-end systems. To address this problem, we formulate and develop a novel cascade ranking model, which unlike previous approaches, can simultaneously improve both top k ranked effectiveness and retrieval efficiency. The model constructs a cascade of increasingly complex ranking functions that progressively prunes and refines the set of candidate documents to minimize retrieval latency and maximize result set quality. We present a novel boosting algorithm for learning such cascades to directly optimize the tradeoff between effectiveness and efficiency. Experimental results show that our cascades are faster and return higher quality results than comparable ranking models.",639c5d11e675b0287342399e2094dbe47f9e4b44
No free lunch: brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity,"[{'name': 'Ferhan Türe', 'dblp_profile': 'https://dblp.org/pid/02/4537.html'}, {'name': 'Tamer Elsayed', 'dblp_profile': 'https://dblp.org/pid/99/5856.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2011,"This work explores the problem of cross-lingual pairwise similarity, where the task is to extract similar pairs of documents across two different languages. Solutions to this problem are of general interest for text mining in the multilingual context and have specific applications in statistical machine translation. Our approach takes advantage of cross-language information retrieval (CLIR) techniques to project feature vectors from one language into another, and then uses locality-sensitive hashing (LSH) to extract similar pairs. We show that effective cross-lingual pairwise similarity requires working with similarity thresholds that are much lower than in typical monolingual applications, making the problem quite challenging. We present a parallel, scalable MapReduce implementation of the sort-based sliding window algorithm, which is compared to a brute-force approach on German and English Wikipedia collections. Our central finding can be summarized as“no free lunch”: there is no single optimal solution. Instead, we characterize effectivenessefficiency tradeoffs in the solution space, which can guide the developer to locate a desirable operating point based on applicationand resource-specific constraints.",ce8975c7f186223954c6f348181b2904161c68bd
Pseudo test collections for learning web search ranking functions,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}, {'name': 'Tamer Elsayed', 'dblp_profile': 'https://dblp.org/pid/99/5856.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2011,"Test collections are the primary drivers of progress in information retrieval. They provide yardsticks for assessing the effectiveness of ranking functions in an automatic, rapid, and repeatable fashion and serve as training data for learning to rank models. However, manual construction of test collections tends to be slow, labor-intensive, and expensive. This paper examines the feasibility of constructing web search test collections in a completely unsupervised manner given only a large web corpus as input. Within our proposed framework, anchor text extracted from the web graph is treated as a pseudo query log from which pseudo queries are sampled. For each pseudo query, a set of relevant and non-relevant documents are selected using a variety of web-specific features, including spam and aggregated anchor text weights. The automatically mined queries and judgments form a pseudo test collection that can be used for training ranking functions. Experiments carried out on TREC web track data show that learning to rank models trained using pseudo test collections outperform an unsupervised ranking function and are statistically indistinguishable from a model trained using manual judgments, demonstrating the usefulness of our approach in extracting reasonable quality training data ""for free"".",35546ad33a3d877063a6a5e938c8d9e8a34badb9
Cross-corpus relevance projection,"[{'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2011,"Document corpora are key components of information retrieval test collections. However, for certain tasks, such as evaluating the effectiveness of a new retrieval technique or estimating the parameters of a learning to rank model, a corpus alone is not enough. For these tasks, queries and relevance judgments associated with the corpus are also necessary. However, researchers often find themselves in scenarios where they only have access to a corpus, in which case evaluation and learning to rank become challenging. Document corpora are relatively straightforward to gather. On the other hand, obtaining queries and relevance judgments for a given corpus is costly. In production environments, it may be possible to obtain low-cost relevance information using query and click logs. However, in more constrained research environments these options are not available, and relevance judgments are usually provided by humans. To reduce the cost of this potentially expensive process, researchers have developed low-cost evaluation strategies, including minimal test collections [2] and crowdsourcing [1]. Despite the usefulness of these strategies, the resulting relevance judgments cannot easily be “ported” to a new or different corpus. To overcome these issues, we propose a new method to reduce manual annotation costs by transferring relevance judgments across corpora. Assuming that a set of queries and relevance judgments have been manually constructed for a source document corpus Ds, our goal is to automatically construct a test collection for a target document corpus Dt by projecting the existing test collection from Ds onto Dt. The goal of projecting test collections is not to produce manual quality test collections. In fact, it is assumed that projected test collections will contain noisy relevance judgments (i.e., ones which humans are unlikely to agree with). The important question, however, is whether these noisy projected judgments are useful for training ranking models in the target corpus.",1556704649472ef0142fa588994762a958a96af2
Overview of the TREC 2011 Microblog Track,"[{'name': 'Iadh Ounis', 'dblp_profile': 'https://dblp.org/pid/21/141.html'}, {'name': 'Craig Macdonald', 'dblp_profile': 'https://dblp.org/pid/02/2224.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Ian Soboroff', 'dblp_profile': 'https://dblp.org/pid/94/2865.html'}]",2011,,f5fbee4c06709d7a192aad33c6d29a882074f73f
Ranking under temporal constraints,"[{'name': 'Lidan Wang', 'dblp_profile': 'https://dblp.org/pid/03/2525.html'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2010,"This paper introduces the notion of temporally constrained ranked retrieval, which, given a query and a time constraint, produces the best possible ranked list within the specified time limit. Naturally, more time should translate into better results, but the ranking algorithm should always produce some results. This property is desirable from a number of perspectives: to cope with diverse users and information needs, as well as to better manage system load and variance in query execution times. We propose two temporally constrained ranking algorithms based on a class of probabilistic prediction models that can naturally incorporate efficiency constraints: one that makes independent feature selection decisions, and the other that makes joint feature selection decisions. Experiments on three different test collections show that both ranking algorithms are able to satisfy imposed time constraints, although the joint model outperforms the independent model in being able to deliver more effective results, especially under tight time constraints, due to its ability to capture feature dependencies.",d679c127eea865d99f593b315c4c9927636d2a10
Scaling Populations of a Genetic Algorithm for Job Shop Scheduling Problems Using MapReduce,"[{'name': 'Di-Wei Huang', 'dblp_profile': 'https://dblp.org/pid/91/5838.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2010,"Inspired by Darwinian evolution, a genetic algorithm (GA) approach is one popular heuristic method for solving hard problems such as the Job Shop Scheduling Problem (JSSP), which is one of the hardest problems lacking efficient exact solutions today. It is intuitive that the population size of a GA may greatly affect the quality of the solution, but it is unclear what are the effects of having population sizes that are significantly greater than typical experiments. The emergence of MapReduce, a framework running on a cluster of computers that aims to provide large-scale data processing, offers great opportunities to investigate this issue. In this paper, a GA is implemented to scale the population using MapReduce. Experiments are conducted on a large cluster, and population sizes up to 10^7 are inspected. It is shown that larger population sizes not only tend to yield better solutions, but also require fewer generations. Therefore, it is clear that when dealing with a hard problem such as JSSP, an existing GA can be improved by massively scaling up populations with MapReduce, so that the solution can be parallelized and completed in reasonable time.",780606b8e4ef77856d3544bc5c259259a5968732
Design patterns for efficient graph algorithms in MapReduce,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Michael Schatz', 'dblp_profile': 'https://dblp.org/pid/28/5383.html'}]",2010,"Graphs are analyzed in many important contexts, including ranking search results based on the hyperlink structure of the world wide web, module detection of proteinprotein interaction networks, and privacy analysis of social networks. Many graphs of interest are difficult to analyze because of their large size, often spanning millions of vertices and billions of edges. As such, researchers have increasingly turned to distributed solutions. In particular, MapReduce has emerged as an enabling technology for large-scale graph processing. However, existing best practices for MapReduce graph algorithms have significant shortcomings that limit performance, especially with respect to partitioning, serializing, and distributing the graph. In this paper, we present three design patterns that address these issues and can be used to accelerate a large class of graph algorithms based on message passing, exemplified by PageRank. Experiments show that the application of our design patterns reduces the running time of PageRank on a web graph with 1.4 billion edges by 69%.",36570c056c09d4f27b37be180668bf80c5b13505
Data-Intensive Text Processing with MapReduce,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chris Dyer', 'dblp_profile': 'https://dblp.org/pid/41/6895.html'}]",2010,"This half-day tutorial introduces participants to data-intensive text processing with the MapReduce programming model [1], using the open-source Hadoop implementation. The focus will be on scalability and the tradeoffs associated with distributed processing of large datasets. Content will include general discussions about algorithm design, presentation of illustrative algorithms, case studies in HLT applications, as well as practical advice in writing Hadoop programs and running Hadoop clusters.",508732db9cbe6cbf6dc1a5451090a0cec950a7f4
Putting the User in the Loop: Interactive Maximal Marginal Relevance for Query-Focused Summarization,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Nitin Madnani', 'dblp_profile': 'https://dblp.org/pid/72/5864.html'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}]",2010,"This work represents an initial attempt to move beyond ""single-shot"" summarization to interactive summarization. We present an extension to the classic Maximal Marginal Relevance (MMR) algorithm that places a user ""in the loop"" to assist in candidate selection. Experiments in the complex interactive Question Answering (ciQA) task at TREC 2007 show that interactively-constructed responses are significantly higher in quality than automatically-generated ones. This novel algorithm provides a starting point for future work on interactive summarization.",3394edbb49705a1763102bbb45d6d47e2ec78d99
Learning to efficiently rank,"[{'name': 'Lidan Wang', 'dblp_profile': 'https://dblp.org/pid/03/2525.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}]",2010,"It has been shown that learning to rank approaches are capable of learning highly effective ranking functions. However, these approaches have mostly ignored the important issue of efficiency. Given that both efficiency and effectiveness are important for real search engines, models that are optimized for effectiveness may not meet the strict efficiency requirements necessary to deploy in a production environment. In this work, we present a unified framework for jointly optimizing effectiveness and efficiency. We propose new metrics that capture the tradeoff between these two competing forces and devise a strategy for automatically learning models that directly optimize the tradeoff metrics. Experiments indicate that models learned in this way provide a good balance between retrieval effectiveness and efficiency. With specific loss functions, learned models converge to familiar existing ones, which demonstrates the generality of our framework. Finally, we show that our approach naturally leads to a reduction in the variance of query execution times, which is important for query load balancing and user satisfaction.",d200d34af8de9d3976d6b2fc92d672a80240f29f
UMD and USC/ISI: TREC 2010 Web Track Experiments with Ivory,"[{'name': 'Tamer Elsayed', 'dblp_profile': 'https://dblp.org/pid/99/5856.html'}, {'name': 'Nima Asadi', 'dblp_profile': 'https://dblp.org/pid/62/4319-1.html'}, {'name': 'Lidan Wang', 'dblp_profile': 'https://dblp.org/pid/03/2525.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}]",2010,"Ivory is a web-scale retrieval engine we have been developing for the past two years, built around a cluster-based environment running Hadoop, the open-source implementation of the MapReduce programming model. Building on successes last year at TREC, we explored two major directions this year: more sophisticated retrieval models and large-scale graph analysis for spam detection. We describe results of ad hoc retrieval experiments with latent concept expansion and a greedily-learned linear ranking model. Although neither model is novel, our experiments provide some insight on the behavior of these two approaches at scale, on collections larger than those previously studied. We also discuss our link-based spam filtering algorithm that operated on the entire web graph of ClueWeb09. Unfortunately, results in the spam track were worse than the baseline provided by the track organizers.",e24f6d293cd423a88249ea1ec0e0e2c56a55b3bb
Is searching full text more effective than searching abstracts?,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2009,,51ca11cddeec18dc5044dfbb7951110c73a0a397
"Where is the Cloud? Geography, Economics, Environment, and Jurisdiction in Cloud Computing","[{'name': 'Paul T. Jaeger', 'dblp_profile': 'https://dblp.org/pid/70/1469.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Justin M. Grimes', 'dblp_profile': 'https://dblp.org/pid/81/7904.html'}, {'name': 'Shannon N. Simmons', 'dblp_profile': 'https://dblp.org/pid/45/8067.html'}]",2009,"Cloud computing – the creation of large data centers that can be dynamically provisioned, configured, and reconfigured to deliver services in a scalable manner – places enormous capacity and power in the hands of users. As an emerging new technology, however, cloud computing also raises significant questions about resources, economics, the environment, and the law. Many of these questions relate to geographical considerations related to the data centers that underlie the clouds: physical location, available resources, and jurisdiction. While the metaphor of the cloud evokes images of dispersion, cloud computing actually represents centralization of information and computing resources in data centers, raising the specter of the potential for corporate or government control over information if there is insufficient consideration of these geographical issues, especially jurisdiction. This paper explores the interrelationships between the geography of cloud computing, its users, its providers, and governments.",01c0f2c0cdc61e2bdba9349ba2e4411f5a5fa29e
Modeling actions of PubMed users with n-gram language models,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'W. John Wilbur', 'dblp_profile': 'https://dblp.org/pid/99/2755.html'}]",2009,,7ec456521ec3edd05ff47095ac8e8f3e6cf0e269
Elements of a computational model for multi-party discourse: The turn-taking behavior of Supreme Court justices,"[{'name': 'Timothy Hawes', 'dblp_profile': 'https://dblp.org/pid/25/7382.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Philip Resnik', 'dblp_profile': 'https://dblp.org/pid/p/PhilipResnik.html'}]",2009,"This paper explores computational models of multi-party discourse, using transcripts from U.S. Supreme Court oral arguments. The turn-taking behavior of participants is treated as a supervised sequence labeling problem and modeled using firstand secondorder Conditional Random Fields. We specifically explore the hypothesis that discourse markers and personal references provide important features in such models. Results from a sequence prediction experiment demonstrate that incorporating these two types of features yields significant improvements in performance. This work is couched in the broader context of developing tools to support legal scholarship, although we see other NLP applications as well. Publication Date: January 14, 2008",164dab5afb4688a664be6d2dd3f43bee8983ae8a
Special Issue of the Journal of Parallel and Distributed Computing: Cloud Computing,"[{'name': 'Gregory V. Chockler', 'dblp_profile': 'https://dblp.org/pid/c/GregoryChockler.html'}, {'name': 'Eliezer Dekel', 'dblp_profile': 'https://dblp.org/pid/61/1540.html'}, {'name': 'Joseph F. JáJá', 'dblp_profile': 'https://dblp.org/pid/j/JosephJaJa.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2009,,0cc26bebbac2d234e8b2e3926bca37215267bc63
A cost-effective lexical acquisition process for large-scale thesaurus translation,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}, {'name': 'Jan Hajic', 'dblp_profile': 'https://dblp.org/pid/40/1286.html'}, {'name': 'Pavel Pecina', 'dblp_profile': 'https://dblp.org/pid/59/1052.html'}]",2009,,f42e03c4300036fde350277dffceb6f823a08a78
"Computational linguistics for metadata building (CLiMB): using text mining for the automatic identification, categorization, and disambiguation of subject terms for image metadata","[{'name': 'Judith L. Klavans', 'dblp_profile': 'https://dblp.org/pid/k/JudithKlavans.html'}, {'name': 'Carolyn Sheffield', 'dblp_profile': 'https://dblp.org/pid/56/3057.html'}, {'name': 'Eileen G. Abels', 'dblp_profile': 'https://dblp.org/pid/35/5405.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Rebecca J. Passonneau', 'dblp_profile': 'https://dblp.org/pid/04/696.html'}, {'name': 'Tandeep Sidhu', 'dblp_profile': 'https://dblp.org/pid/64/955.html'}, {'name': 'Dagobert Soergel', 'dblp_profile': 'https://dblp.org/pid/s/DagobertSoergel.html'}]",2009,,08265f9dc3a4e3a83b5ad773cb3027a84ead141b
You Are Where You Edit: Locating Wikipedia Contributors through Edit Histories,"[{'name': 'Michael D. Lieberman', 'dblp_profile': 'https://dblp.org/pid/11/1141.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2009,"
 
 Whether knowingly or otherwise, Wikipedia contributors reveal their interests and expertise through their contribution patterns. An analysis of Wikipedia edit histories shows that it is often possible to associate contributors with relatively small geographic regions, usually corresponding to where they were born or where they presently live. For many contributors, the geographic coordinates of pages they have edited are tightly clustered. Results suggest that a wealth of information about contributors can be gleaned from edit histories. This illustrates the efﬁcacy of data mining on large, publicly-available datasets and raises potential privacy concerns.
 
",e108ac671e6c9252907eeb84c41bc35668e0b940
Users' adjustments to unsuccessful queries in biomedical search,"[{'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'W. John Wilbur', 'dblp_profile': 'https://dblp.org/pid/99/2755.html'}, {'name': 'Zhiyong Lu', 'dblp_profile': 'https://dblp.org/pid/66/6604.html'}]",2009,"Biomedical researchers depend on on-line databases and digital libraries for up to date information. We introduce a pilot project aimed at characterizing adjustments made to biomedical queries that improve search results. Specifically we focus on queries submitted to PubMed®, a large sophisticated search engine that facilitates Web access to abstracts of articles in over 5,200 biomedical journals. On average 2 million users search PubMed each day. During their search, nearly 20% will experience a result page from one of their queries that has zero results. In some cases there really is no document or abstract that will satisfy a particular query. However, in analyzing one month of queries submitted to PubMed, we find that more often than not, queries that retrieved no results are queries that would retrieve something relevant if they were constructed differently. This paper describes a new effort to identify some of the characteristics of a query that produces zero results, and the changes that users most often apply in constructing new, ""corrected"" queries. Zero-result queries afford us an opportunity to examine changes made to queries that we know did not return relevant data, because they did not return any data. An investigation of the changes users make under these circumstances can yield insight into users' search processes.",295bdd383af9e8efb3fcc81267834fdac5f762f0
Data Intensive Text Processing with MapReduce,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chris Dyer', 'dblp_profile': 'https://dblp.org/pid/41/6895.html'}]",2009,,e5866538429174eabc7289377e50f9899cdf6c9b
Brute force and indexed approaches to pairwise document similarity comparisons with MapReduce,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2009,"Matching documents based on similarity is a common and important use case in world wide web search and thus in Information Retrieval. When a user discovers a web page or paper which interests him or her, the user is probably interested in similar documents which contain further information to the previous searched topics. Therefore search frameworks exist, such as PubMed which offer users a top k list, containing related documents to the searched paper. This work introduces a scientific paper presented at SIGIR, 2009.",a5418c10ae858f4bab56439c773f774174fc4d68
The Curse of Zipf and Limits to Parallelization: An Look at the Stragglers Problem in MapReduce,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2009,"This paper explores the problem of \stragglers"" in MapReduce: a common phenomenon where a small number of mappers or reducers takes signicantly longer than the others to complete. The eects of these stragglers include unnecessarily long wall-clock running times and sub-optimal cluster utilization. In many cases, this problem cannot simply be attributed to hardware idiosyncrasies, but is rather caused by the Zipan distribution of input or intermediate data. I present a simple theoretical model that shows how such distributions impose a fundamental limit on the amount of parallelism that can be extracted from a large class of algorithms where all occurrences of the same element must be processed together. A case study in parallel ad hoc query evaluation highlights the severity of the stragglers problem. Fortunately, a simple modication of the input data cuts end-to-end running time in half. This example illustrates some of the issues associated with designing ecient MapReduce algorithms for real-world datasets.",ea77068c3edc0e0a9e4a5b205105f72845440c21
Of Ivory and Smurfs: Loxodontan MapReduce Experiments for Web Search,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Tamer Elsayed', 'dblp_profile': 'https://dblp.org/pid/99/5856.html'}, {'name': 'Lidan Wang', 'dblp_profile': 'https://dblp.org/pid/03/2525.html'}, {'name': 'Donald Metzler', 'dblp_profile': 'https://dblp.org/pid/95/2272.html'}]",2009,"This paper describes Ivory, an attempt to build a distributed retrieval system around the open-source Hadoop implementation of MapReduce. We focus on three noteworthy aspects of our work: a retrieval architecture built directly on the Hadoop Distributed File System (HDFS), a scalable MapReduce algorithm for inverted indexing, and webpage classification to enhance retrieval effectiveness.",0a9f61a9ff4d99b30e76193496951867a4894b66
Summarization,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2009,,3bddf90599370d7b1ec8a5bb45799b7e276a3ab8
PageRank without hyperlinks: Reranking with PubMed related article networks for biomedical text retrieval,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2008,,c8c92f5f141ac0894e2b9226fc68ea2b0d2f8492
Single-document and multi-document summarization techniques for email threads using sentence compression,"[{'name': 'David M. Zajic', 'dblp_profile': 'https://dblp.org/pid/72/3638.html'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2008,,118df6d656ec6d193fe09ceafb650ae1d35f6ec7
Navigating information spaces: A case study of related article search in PubMed,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Michael DiCuccio', 'dblp_profile': 'https://dblp.org/pid/43/4646.html'}, {'name': 'Vahan Grigoryan', 'dblp_profile': 'https://dblp.org/pid/02/6362.html'}, {'name': 'W. John Wilbur', 'dblp_profile': 'https://dblp.org/pid/99/2755.html'}]",2008,,b8738417c367b230afa8dfe6803caf7b6d527e8a
Toward automatic facet analysis and need negotiation: Lessons from mediated search,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Philip Fei Wu', 'dblp_profile': 'https://dblp.org/pid/50/7689.html'}, {'name': 'Eileen G. Abels', 'dblp_profile': 'https://dblp.org/pid/35/5405.html'}]",2008,"This work explores the hypothesis that interactions between a trained human search intermediary and an information seeker can inform the design of interactive IR systems. We discuss results from a controlled Wizard-of-Oz case study, set in the context of the TREC 2005 HARD track evaluation, in which a trained intermediary executed an integrated search and interaction strategy based on conceptual facet analysis and informed by need negotiation techniques common in reference interviews. Having a human “in the loop” yielded large improvements over fully automated systems as measured by standard ranked-retrieval metrics, demonstrating the value of mediated search. We present a detailed analysis of the intermediary's actions to gain a deeper understanding of what worked and why. One contribution is a taxonomy of clarification types informed both by empirical results and existing theories in library and information science. We discuss how these findings can guide the development of future systems. Overall, this work illustrates how studying human information-seeking processes can lead to better information retrieval applications.",747014d3853f8982fd1ab1d38a78a67e1119202b
Pairwise Document Similarity in Large Collections with MapReduce,"[{'name': 'Tamer Elsayed', 'dblp_profile': 'https://dblp.org/pid/99/5856.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Douglas W. Oard', 'dblp_profile': 'https://dblp.org/pid/o/DouglasWOard.html'}]",2008,"This paper presents a MapReduce algorithm for computing pairwise document similarity in large document collections. MapReduce is an attractive framework because it allows us to decompose the inner products involved in computing document similarity into separate multiplication and summation stages in a way that is well matched to efficient disk access patterns across several machines. On a collection consisting of approximately 900,000 newswire articles, our algorithm exhibits linear growth in running time and space in terms of the number of documents.",08fd33cb1c8837d374bc4c863a09cc792f6c52f2
Scalable Language Processing Algorithms for the Masses: A Case Study in Computing Word Co-occurrence Matrices with MapReduce,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2008,"This paper explores the challenge of scaling up language processing algorithms to increasingly large datasets. While cluster computing has been available in commercial environments for several years, academic researchers have fallen behind in their ability to work on large datasets. I discuss two barriers contributing to this problem: lack of a suitable programming model for managing concurrency and difficulty in obtaining access to hardware. Hadoop, an open-source implementation of Google's MapReduce framework, provides a compelling solution to both issues. Its simple programming model hides system-level details from the developer, and its ability to run on commodity hardware puts cluster computing within the reach of many academic research groups. This paper illustrates these points with a case study in building word cooccurrence matrices from large corpora. I conclude with an analysis of an alternative computing model based on renting instead of buying computer clusters.",eec9a50881bbefc3b72064e9e6852558f000cf04
Computational linguistics for metadata building,"[{'name': 'Judith L. Klavans', 'dblp_profile': 'https://dblp.org/pid/k/JudithKlavans.html'}, {'name': 'Carolyn Sheffield', 'dblp_profile': 'https://dblp.org/pid/56/3057.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Tandeep Sidhu', 'dblp_profile': 'https://dblp.org/pid/64/955.html'}]",2008,"In this paper, we describe a downloadable text-mining tool for enhancing subject access to image collections in digital libraries.",5df649fb733c47bdc0366244ed7c3019672b3799
How do users find things with PubMed?: towards automatic utility evaluation with user simulations,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Mark D. Smucker', 'dblp_profile': 'https://dblp.org/pid/07/801.html'}]",2008,"In the context of document retrieval in the biomedical domain, this paper explores the complex relationship between the quality of initial query results and the overall utility of an interactive retrieval system. We demonstrate that a content-similarity browsing tool can compensate for poor retrieval results, and that the relationship between retrieval performance and overall utility is non-linear. Arguments are advanced with user simulations, which characterize the relevance of documents that a user might encounter with different browsing strategies. With broader implications to IR, this work provides a case study of how user simulations can be exploited as a formative tool for automatic utility evaluation. Simulation-based studies provide researchers with an additional evaluation tool to complement interactive and Cranfield-style experiments.",d99919e402387b7a9d6a35c370fe0e8f213e318e
"Fast, Easy, and Cheap: Construction of Statistical Machine Translation Models with MapReduce","[{'name': 'Chris Dyer', 'dblp_profile': 'https://dblp.org/pid/41/6895.html'}, {'name': 'Aaron Cordova', 'dblp_profile': 'https://dblp.org/pid/204/5026.html'}, {'name': 'Alex Mont', 'dblp_profile': 'https://dblp.org/pid/204/4886.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2008,"In recent years, the quantity of parallel training data available for statistical machine translation has increased far more rapidly than the performance of individual computers, resulting in a potentially serious impediment to progress. Parallelization of the model-building algorithms that process this data on computer clusters is fraught with challenges such as synchronization, data exchange, and fault tolerance. However, the MapReduce programming paradigm has recently emerged as one solution to these issues: a powerful functional abstraction hides system-level details from the researcher, allowing programs to be transparently distributed across potentially very large clusters of commodity hardware. We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates. On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical, optimally-parallelized version of current state-of-the-art single-core tools.",5602a2426fef55c0ad527d3e2dc722fa56ecc5fb
Multiple Alternative Sentence Compressions and Word-Pair Antonymy for Automatic Text Summarization and Recognizing Textual Entailment,"[{'name': 'Saif M. Mohammad', 'dblp_profile': 'https://dblp.org/pid/58/380.html'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}, {'name': 'Melissa Egan', 'dblp_profile': 'https://dblp.org/pid/94/8158.html'}, {'name': 'Nitin Madnani', 'dblp_profile': 'https://dblp.org/pid/72/5864.html'}, {'name': 'David M. Zajic', 'dblp_profile': 'https://dblp.org/pid/72/3638.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2008,"Abstract The University of Maryland participatedin three tasks organized by the Text Anal-ysis Conference 2008 (TAC 2008): (1) theupdate task of text summarization; (2) theopinion task of text summarization; and(3) recognizing textual entailment (RTE).At the heart of our summarization sys-tem is Trimmer, which generates multi-ple alternative compressed versions of thesource sentences that act as candidate sen-tences for inclusion in the summary. Forthe ﬁrst time, we investigated the use ofautomatically generated antonym pairs forboth text summarization and recognizingtextual entailment. The UMD summariesfor the opinion task were especially effec-tive in providing non-redundant informa-tion (rank 3 out of a total 19 submissions).More coherent summaries resulted whenusing the antonymy feature as comparedto when not using it. On the RTE task,even when using only automatically gen-erated antonyms the system performed aswell as when using a manually compiledlist of antonyms. 1 Introduction",a401ed8bff736645de14d3bdc6ffbf257121da27
PubMed related articles: a probabilistic topic-based model for content similarity,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'W. John Wilbur', 'dblp_profile': 'https://dblp.org/pid/99/2755.html'}]",2007,,7c4d6befeffc85ce5abee79bf29a737f9be7efa6
Answering Clinical Questions with Knowledge-Based and Statistical Techniques,"[{'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,"The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidence-based medicine. Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians' questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians. Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline.",9e89e07ad1b5c8b47d6543dbf3795601a48b6fd0
User simulations for evaluating answers to question series,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,,ac6744082c1b16d80b4e07ac36e35b681b05204d
Multi-candidate reduction: Sentence compression as a tool for document summarization tasks,"[{'name': 'David M. Zajic', 'dblp_profile': 'https://dblp.org/pid/72/3638.html'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Richard M. Schwartz', 'dblp_profile': 'https://dblp.org/pid/57/1658.html'}]",2007,,f9ad221c2627e2b0d16edfa05e65656447e00b77
Syntactic sentence compression in the biomedical domain: facilitating access to related articles,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'W. John Wilbur', 'dblp_profile': 'https://dblp.org/pid/99/2755.html'}]",2007,,79ee7e2dd665dee9b36c54303eeccd1d62ea5ef1
Presentation schemes for component analysis in IR experiments,"[{'name': 'Paul B. Kantor', 'dblp_profile': 'https://dblp.org/pid/k/PaulBKantor.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,"Information retrieval research, at least as conceived by the SIGIR community, is fundamentally experimental in nature. As such, the presentation of results from controlled, reproducible experiments lies at the core of our work. Many reports follow the same general format: authors propose a new retrieval method, whose performance on some well-defined task is compared against a baseline. Authors also report results from alternative configurations, e.g., variations in parameters, turning off (ablation) of different components, etc. The presentation of experimental results forms an integral part of the conferences and journals that comprise the medium in which knowledge is disseminated.",17e6e5fa698cedb5b67ea23568ccd7a8402c1b21
Overview of the TREC 2006 ciQA task,"[{'name': 'Diane Kelly', 'dblp_profile': 'https://dblp.org/pid/71/3546-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,"Growing interest in interactive systems for answering complex questions lead to the development of the complex, interactive QA (ciQA) task, introduced for the first time at TREC 2006. This paper describes the rationale and design of the ciQA task and the evaluation results. Thirty complex relationship questions based on five question templates were investigated using the AQUAINT collection of newswire text. Interaction forms were the primary vehicle for defining and capturing user-system interactions. In total, six groups participated in the ciQA task and contributed ten different sets of interaction forms. There were two main findings: baseline IR techniques are competitive for complex QA and interaction, at least as defined and implemented in this evaluation, did not appear to improve performance by much.",06827350da406d8519cd6e5037902687eb7650dc
An exploration of the principles underlying redundancy-based factoid question answering,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,"The so-called “redundancy-based” approach to question answering represents a successful strategy for mining answers to factoid questions such as “Who shot Abraham Lincoln?” from the World Wide Web. Through contrastive and ablation experiments with Aranea, a system that has performed well in several TREC QA evaluations, this work examines the underlying assumptions and principles behind redundancy-based techniques. Specifically, we develop two theses: that stable characteristics of data redundancy allow factoid systems to rely on external “black box” components, and that despite embodying a data-driven approach, redundancy-based methods encode a substantial amount of knowledge in the form of heuristics. Overall, this work attempts to address the broader question of “what really matters” and to provide guidance for future researchers.",113ab714f29a4787a8f5e9c18a240b0dc5dbfce5
"Different Structures for Evaluating Answers to Complex Questions: Pyramids Won't Topple, and Neither Will Human Assessors","[{'name': 'Hoa Trang Dang', 'dblp_profile': 'https://dblp.org/pid/02/1356.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,,
Semantic Clustering of Answers to Clinical Questions,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2007,"Access to clinical evidence is a critical component of the practice of evidence-based medicine. Advanced retrieval systems can supplement precompiled secondary sources to assist physicians in making sound clinical decisions. This study explores one particular issue related to the design of such retrieval systems: the effective organization of search results to facilitate rapid understanding and synthesis of potentially relevant information. We hypothesize that grouping retrieved MEDLINE citations into semantically-coherent clusters, based on automatically-extracted interventions from the abstract text, represents an effective strategy for presenting results, compared to a traditional ranked list. Experiments with our implemented system appear to support this claim.",9084e229cdd6b653cb839bcacd75169e4a74328c
Concept Disambiguation for Improved Subject Access Using Multiple Knowledge Sources,"[{'name': 'Tandeep Sidhu', 'dblp_profile': 'https://dblp.org/pid/64/955.html'}, {'name': 'Judith Klavans', 'dblp_profile': 'https://dblp.org/pid/k/JudithKlavans.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,"We address the problem of mining text for relevant image metadata. Our work is situated in the art and architecture domain, where highly specialized technical vocabulary presents challenges for NLP techniques. To extract high quality metadata, the problem of word sense disambiguation must be addressed in order to avoid leading the searcher to the wrong image as a result of ambiguous — and thus faulty — metadata. In this paper, we present a disambiguation algorithm that attempts to select the correct sense of nouns in textual descriptions of art objects, with respect to a rich domain-specific thesaurus, the Art and Architecture Thesaurus (AAT). We performed a series of intrinsic evaluations using a data set of 600 subject terms extracted from an online National Gallery of Art (NGA) collection of images and text. Our results showed that the use of external knowledge sources shows an improvement over a baseline.",c2c3a4542713f9f514cf9c560341cf375ea159d1
Is Question Answering Better than Information Retrieval? Towards a Task-Based Evaluation Framework for Question Series,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,"This paper introduces a novel evaluation framework for question series and employs it to explore the effectiveness of QA and IR systems at addressing users’ information needs. The framework is based on the notion of recall curves, which characterize the amount of relevant information contained within a fixed-length text segment. Although it is widely assumed that QA technology provides more efficient access to information than IR systems, our experiments show that a simple IR baseline is quite competitive. These results help us better understand the role of NLP technology in QA systems and suggest directions for future research.",6539a745318a5edbdaf821dce5e555bfe7dc8a0d
Deconstructing nuggets: the stability and reliability of complex question answering evaluation,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Pengyi Zhang', 'dblp_profile': 'https://dblp.org/pid/95/902.html'}]",2007,"A methodology based on ""information nuggets"" has recently emerged as the de facto standard by which answers to complex questions are evaluated. After several implementations in the TREC question answering tracks, the community has gained a better understanding of its many characteristics. This paper focuses on one particular aspect of the evaluation: the human assignment of nuggets to answer strings, which serves as the basis of the F-score computation. As a byproduct of the TREC 2006 ciQA task, identical answer strings were independently evaluated twice, which allowed us to assess the consistency of human judgments. Based on these results, we explored simulations of assessor behavior that provide a method to quantify scoring variations. Understanding these variations in turn lets researchers be more confident in their comparisons of systems.",8de25b499a682fb389f8b51f299139df01e1fe0a
Overview of the TREC 2007 Question Answering Track,"[{'name': 'Hoa Trang Dang', 'dblp_profile': 'https://dblp.org/pid/02/1356.html'}, {'name': 'Diane Kelly', 'dblp_profile': 'https://dblp.org/pid/71/3546-1.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2007,,
TREC 2007 ciQA Task: University of Maryland,"[{'name': 'Nitin Madnani', 'dblp_profile': 'https://dblp.org/pid/72/5864.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}]",2007,"Information needs are complex, evolving, and dicult to express or capture (Taylor, 1962), a fact thatis often overlooked by modern information retrieval systems. TREC, through the HARD track, hasbeen attempting to introduce elements of interaction into large-scale evaluations in order to achievehigh accuracy document retrieval (Allan, 2005). Previous research has shown that well-constructedclari cation questions can yield a better understanding of users’ information needs and thereby improveretrieval performance (Lin et al., 2006).Interactive question answering has recently become a focus of research in the context of complex QA.The topics in the ciQA task are substantially di erent from factoid questions in that the informationneeds are complex, multi-faceted, and often not well de ned or expressed. To investigate the roleof interaction in complex QA, we experimented with two approaches. The rst approach relied onMaximum Marginal Relevance (MMR) and is described in Section 2. The second approach employedthe Multiple Alternative Sentence Compressions (MASC) framework (Zajic, 2007; Madnani et al.,2007), described in Section 3. Section 4 presents ocial results.",b26d6ea5ee7d127a9dfa61e18f8361b83acce1ef
Methods for automatically evaluating answers to complex questions,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2006,,835b7d6cc54fcd8393a266b0ed64f65a1f4d5414
Building a reusable test collection for question answering,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}]",2006,"In contrast to traditional information retrieval systems, which return ranked lists of documents that users must manually browse through, a question answering system attempts to directly answer nat...",345a098321cff1a90befe47717e033f010de2096
"Answer Extraction, Semantic Clustering, and Extractive Summarization for Clinical Question Answering","[{'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2006,"This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval. We tackle a frequently-occurring class of questions that takes the form ""What is the best drug treatment for X?"" Starting from an initial set of MEDLINE citations, our system first identifies the drugs under study. Abstracts are then clustered using semantic classes from the UMLS ontology. Finally, a short extractive summary is generated for each abstract to populate the clusters. Two evaluations---a manual one focused on short answers and an automatic one focused on the supporting abstract---demonstrate that our system compares favorably to PubMed, the search system most widely used by physicians today.",216f2cbdcc4c06943f38f8d1bde99a5746171b62
The Role of Information Retrieval in Answering Complex Questions,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2006,"This paper explores the role of information retrieval in answering ""relationship"" questions, a new class complex information needs formally introduced in TREC 2005. Since information retrieval is often an integral component of many question answering strategies, it is important to understand the impact of different term-based techniques. Within a framework of sentence retrieval, we examine three factors that contribute to question answering performance: the use of different retrieval engines, relevance (both at the document and sentence level), and redundancy. Results point out the limitations of purely term-based methods to this challenging task. Nevertheless, IR-based techniques provide a strong baseline on top of which more sophisticated language processing techniques can be deployed.",9ecee99849a6b925f1a1dac454e87e600821565e
Leveraging Reusability: Cost-Effective Lexical Acquisition for Large-Scale Ontology Translation,"[{'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jan Hajic', 'dblp_profile': 'https://dblp.org/pid/40/1286.html'}, {'name': 'Pavel Pecina', 'dblp_profile': 'https://dblp.org/pid/59/1052.html'}]",2006,"Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations when constructing domain-specific lexical resources. We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories. Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies.",800d5005e834aadf5477b5e7866a2a35f0795fbb
Evaluation of PICO as a Knowledge Representation for Clinical Questions,"[{'name': 'Xiaoli Huang', 'dblp_profile': 'https://dblp.org/pid/00/3346.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2006,"The paradigm of evidence-based medicine (EBM) recommends that physicians formulate clinical questions in terms of the problem/population, intervention, comparison, and outcome. Together, these elements comprise a PICO frame. Although this framework was developed to facilitate the formulation of clinical queries, the ability of PICO structures to represent physicians' information needs has not been empirically investigated. This paper evaluates the adequacy and suitability of PICO frames as a knowledge representation by analyzing 59 real-world primary-care clinical questions. We discovered that only two questions in our corpus contain all four PICO elements, and that 37% of questions contain both intervention and outcome. Our study reveals prevalent structural patterns for the four types of clinical questions: therapy, diagnosis, prognosis, and etiology. We found that the PICO framework is primarily centered on therapy questions, and is less suitable for representing other types of clinical information needs. Challenges in mapping natural language questions into PICO structures are also discussed. Although we point out limitations of the PICO framework, our work as a whole reaffirms its value as a tool to assist physicians practicing EBM.",1e7b4b0770567913f7c6c048af876a6589334108
Identification of user sessions with hierarchical agglomerative clustering,"[{'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Abdur Chowdhury', 'dblp_profile': 'https://dblp.org/pid/14/3331.html'}]",2006,We introduce a novel approach to identifying Web search user sessions based on the burstiness of users’ activity. Our method is user-centered rather than population-centered or system-centered and can be deployed in situations in which users choose to withhold personal content information. We adopt a hierarchical agglomerative clustering approach with a stopping criterion that is statistically motivated by users’ activities. An evaluation based on extracts from AOL Search™ logs reveals that our algorithm achieves 98% accuracy in identifying session boundaries compared to human judgments.,6d74077f5a76db02afec352b0532ac5a1284e259
Generative Content Models for Structural Analysis of Medical Abstracts,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Damianos G. Karakos', 'dblp_profile': 'https://dblp.org/pid/54/3374.html'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}, {'name': 'Sanjeev Khudanpur', 'dblp_profile': 'https://dblp.org/pid/77/4919.html'}]",2006,"The ability to accurately model the content structure of text is important for many natural language processing applications. This paper describes experiments with generative models for analyzing the discourse structure of medical abstracts, which generally follow the pattern of ""introduction"", ""methods"", ""results"", and ""conclusions"". We demonstrate that Hidden Markov Models are capable of accurately capturing the structure of such texts, and can achieve classification accuracy comparable to that of discriminative techniques. In addition, generative approaches provide advantages that may make them preferable to discriminative techniques such as Support Vector Machines under certain conditions. Our work makes two contributions: at the application level, we report good performance on an interesting task in an important domain; more generally, our results contribute to an ongoing discussion regarding the tradeoffs between generative and discriminative techniques.",33f33197434dbcb6dbe5b2f5e27d646262fcd19d
Leveraging Recurrent Phrase Structure in Large-scale Ontology Translation,"[{'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Bonnie J. Dorr', 'dblp_profile': 'https://dblp.org/pid/d/BonnieJDorr.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Jan Hajic', 'dblp_profile': 'https://dblp.org/pid/40/1286.html'}, {'name': 'Pavel Pecina', 'dblp_profile': 'https://dblp.org/pid/59/1052.html'}]",2006,"This paper presents a process for leveraging structural relationships and reusable phrases when translating large-scale ontologies. Digital libraries are becoming more and more prevalent. An important step in providing universal access to such material is to provide multi-lingual access to the underlying principles of organization via ontologies, thesauri, and controlled vocabularies. Machine translation of these resources requires high accuracy and a deep vocabulary. Human input is often required, but full manual translation can be slow and expensive. We report on a cost-effective approach to ontology translation. We describe our technique of prioritization, our process of collecting aligned translations and generating a new lexicon, and the resulting improvement to translation system output. Our preliminary evaluation indicates that this technique provides significant cost savings for human-assisted translation. The process we developed can be applied to ontologies in other domains and is easily incorporated into other translation systems.",3bbace0cf844abc6cdc5f4fd3ec53dcd54a83eb7
Will Pyramids Built of Nuggets Topple Over?,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2006,"The present methodology for evaluating complex questions at TREC analyzes answers in terms of facts called ""nuggets"". The official F-score metric represents the harmonic mean between recall and precision at the nugget level. There is an implicit assumption that some facts are more important than others, which is implemented in a binary split between ""vital"" and ""okay"" nuggets. This distinction holds important implications for the TREC scoring model---essentially, systems only receive credit for retrieving vital nuggets---and is a source of evaluation instability. The upshot is that for many questions in the TREC testsets, the median score across all submitted runs is zero. In this work, we introduce a scoring model based on judgments from multiple assessors that captures a more refined notion of nugget importance. We demonstrate on TREC 2003, 2004, and 2005 data that our ""nugget pyramids"" address many shortcomings of the present methodology, while introducing only minimal additional overhead on the evaluation flow.",739e94bae04cdd483684ec5b93cc20f393599baf
The role of knowledge in conceptual retrieval: a study in the domain of clinical medicine,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2006,"Despite its intuitive appeal, the hypothesis that retrieval at the level of ""concepts"" should outperform purely term-based approaches remains unverified empirically. In addition, the use of ""knowledge"" has not consistently resulted in performance gains. After identifying possible reasons for previous negative results, we present a novel framework for ""conceptual retrieval"" that articulates the types of knowledge that are important for information seeking. We instantiate this general framework in the domain of clinical medicine based on the principles of evidence-based medicine (EBM). Experiments show that an EBM-based scoring algorithm dramatically outperforms a state-of-the-art baseline that employs only term statistics. Ablation studies further yield a better understanding of the performance contributions of different components. Finally, we discuss how other domains can benefit from knowledge-based approaches.",a97f3440ec7b6ab5d8faad61c339f21b404e3e24
Exploring the limits of single-iteration clarification dialogs,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Philip Fei Wu', 'dblp_profile': 'https://dblp.org/pid/50/7689.html'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}, {'name': 'Eileen G. Abels', 'dblp_profile': 'https://dblp.org/pid/35/5405.html'}]",2006,"Single-iteration clarification dialogs, as implemented in the TREC HARD track, represent an attempt to introduce interaction into ad hoc retrieval, while preserving the many benefits of large-scale evaluations. Although previous experiments have not conclusively demonstrated performance gains resulting from such interactions, it is unclear whether these findings speak to the nature of clarification dialogs, or simply the limitations of current systems. To probe the limits of such interactions, we employed a human intermediary to formulate clarification questions and exploit user responses. In addition to establishing a plausible upper bound on performance, we were also able to induce an ""ontology of clarifications"" to characterize human behavior. This ontology, in turn, serves as the input to a regression model that attempts to determine which types of clarification questions are most helpful. Our work can serve to inform the design of interactive systems that initiate user dialogs.",a6aaec3c36297ac054a3e14f1fac1cae9766c10e
Action modeling: language models that predict query behavior,"[{'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Abdur Chowdhury', 'dblp_profile': 'https://dblp.org/pid/14/3331.html'}]",2006,"We present a novel language modeling approach to capturing the query reformulation behavior of Web search users. Based on a framework that categorizes eight different types of ""user moves"" (adding/removing query terms, etc.), we treat search sessions as sequence data and build n-gram language models to capture user behavior. We evaluated our models in a prediction task. The results suggest that useful patterns of activity can be extracted from user histories. Furthermore, by examining prediction performance under different order n-gram models, we gained insight into the amount of history/context that is associated with different types of user actions. Our work serves as the basis for more refined user models.",3e91aa3b57c81b3c64e103c913daaab5fb98f389
Overview of the TREC 2006 Question Answering Track 99,"[{'name': 'Hoa Trang Dang', 'dblp_profile': 'https://dblp.org/pid/02/1356.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Diane Kelly', 'dblp_profile': 'https://dblp.org/pid/71/3546-1.html'}]",2006,"The TREC 2006 question answering (QA) track contained two tasks: the main task and the complex, interactive question answering (ciQA) task. As in 2005, the main task consisted of series of factoid, list, and “Other” questions organized around a set of targets; in contrast to previous years, the evaluation of factoid and list responses distinguished between answers that were globally correct (with respect to the document collection), and those that were only locally correct (with respect to the supporting document). The ciQA task provided a framework for participants to investigate interaction in the context of complex information needs, and was a blend of the TREC 2005 QA relationship task and the TREC 2005 HARD track. Multiple assessors were used to judge the importance of information nuggets used to evaluate the responses to ciQA and “Other” questions, resulting in an evaluation that is more stable and discriminative than one that uses only a single assessor to judge nugget importance.",901e31635918310569ec0bddf23755154a6df829
"TREC 2006 at Maryland: Blog, Enterprise, Legal and QA Tracks","[{'name': 'Douglas W. Oard', 'dblp_profile': 'https://dblp.org/pid/o/DouglasWOard.html'}, {'name': 'Tamer Elsayed', 'dblp_profile': 'https://dblp.org/pid/99/5856.html'}, {'name': 'Jianqiang Wang', 'dblp_profile': 'https://dblp.org/pid/24/4505-2.html'}, {'name': 'Yejun Wu', 'dblp_profile': 'https://dblp.org/pid/85/3571.html'}, {'name': 'Pengyi Zhang', 'dblp_profile': 'https://dblp.org/pid/95/902.html'}, {'name': 'Eileen G. Abels', 'dblp_profile': 'https://dblp.org/pid/35/5405.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dagobert Soergel', 'dblp_profile': 'https://dblp.org/pid/s/DagobertSoergel.html'}]",2006,"Abstract : In TREC 2006, teams from the University of Maryland participated in the Blog track, the Expert Search task of the Enterprise track, the Complex Interactive Question Answering task of the Question Answering track, and the Legal track. This paper reports our results.",b4f92471f946e043edd0d5a416122fe447dcc632
Evaluating Summaries and Answers: Two Sides of the Same Coin?,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2005,"This paper discusses the convergence between question answering and multidocument summarization, pointing out implications and opportunities for knowledge transfer in both directions. As a case study in one direction, we discuss the recent development of an automatic method for evaluating definition questions based on n-gram overlap, a commonlyused technique in summarization evaluation. In the other direction, the move towards topic-oriented summaries requires an understanding of relevance and topicality, issues which have received attention in the question answering literature. It is our opinion that question answering and multi-document summarization represent two complementary approaches to the same problem of satisfying complex user information needs. Although this points to many exciting opportunities for systembuilding, here we primarily focus on implications for system evaluation.",2af0473e63ef0a9a77795c449620b90bb21730e2
"""Bag of Words"" is not enough for Strength of Evidence Classification","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2005,"Incorporation of evidence from clinical research requires critical appraisal of its quality. Information retrieval systems can facilitate clinicians' judgments by automatically labeling retrieved citations with their strength of evidence categories. Preliminary results of such a text classification experiment involving MEDLINE citations show that a ""bag of words"" approach is insufficient for accurate classification.",eeb6793493f4ee2676b15f1cbb4133797443603f
Automatically Evaluating Answers to Definition Questions,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}]",2005,"Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called Pourpre, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that Pourpre outperforms direct application of existing metrics.",570bfcb94270e1d565450fb6029a9f6e7c8749a2
Evaluation of resources for question answering evaluation,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2005,"Controlled and reproducible laboratory experiments, enabled by reusable test collections, represent a well-established methodology in modern information retrieval research. In order to confidently draw conclusions about the performance of different retrieval methods using test collections, their reliability and trustworthiness must first be established. Although such studies have been performed for ad hoc test collections, currently available resources for evaluating question answering systems have not been similarly analyzed. This study evaluates the quality of answer patterns and lists of relevant documents currently employed in automatic question answering evaluation, and concludes that they are not suitable for post-hoc experimentation. These resources, created from runs submitted by TREC QA track participants, do not produce fair and reliable assessments of systems that did not participate in the original evaluations. Potential solutions for addressing this evaluation gap and their shortcomings are discussed.",9851ad4065413ef541adc913c52d7023bec5bf7c
Assessing the term independence assumption in blind relevance feedback,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'G. Craig Murray', 'dblp_profile': 'https://dblp.org/pid/64/1161.html'}]",2005,"When applying blind relevance feedback for ad hoc document retrieval, is it possible to identify, a priori, the set of query terms that will most improve retrieval performance? Can this complex problem be reduced into the simpler one of making independent decisions about the performance effects of each query term? Our experiments suggest that, for the selection of terms for blind relevance feedback, the term independence assumption may be empirically justified.",12937ad4a222a9e48099f405f60487e0003e8411
Fusion of Knowledge-Intensive and Statistical Approaches for Retrieving and Annotating Textual Genomics Documents,"[{'name': 'Alan R. Aronson', 'dblp_profile': 'https://dblp.org/pid/86/6674.html'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}, {'name': 'Susanne M. Humphrey', 'dblp_profile': 'https://dblp.org/pid/13/2941.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Patrick Ruch', 'dblp_profile': 'https://dblp.org/pid/70/4767.html'}, {'name': 'Miguel E. Ruiz', 'dblp_profile': 'https://dblp.org/pid/60/1455.html'}, {'name': 'Lawrence H. Smith', 'dblp_profile': 'https://dblp.org/pid/44/1471.html'}, {'name': 'Lorraine K. Tanabe', 'dblp_profile': 'https://dblp.org/pid/09/39.html'}, {'name': 'W. John Wilbur', 'dblp_profile': 'https://dblp.org/pid/99/2755.html'}, {'name': 'Hongfang Liu', 'dblp_profile': 'https://dblp.org/pid/92/2407.html'}]",2005,"This paper represents a continuation of research into the retrieval and annotation of textual genomics documents (both MEDLINE ® citations and full text articles) for the purpose of satisfying biologists’ real information needs. The overall approach taken here for both the ad hoc retrieval and categorization tasks within the TREC genomics track in 2005 was one combining the results of several NLP, statistical and ML methods, using a fusion method for ad hoc retrieval and ensemble methods for categorization. The results show that fusion approaches can improve the final outcome for the ad hoc and the categorization tasks, but that care must be taken in order to take advantage of the strengths of the constituent methods.",726645f2e712ead485b2d37cc1010e40aef1ad25
"A Menagerie of Tracks at Maryland: HARD, Enterprise, QA, and Genomics, Oh My!","[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Eileen G. Abels', 'dblp_profile': 'https://dblp.org/pid/35/5405.html'}, {'name': 'Dina Demner-Fushman', 'dblp_profile': 'https://dblp.org/pid/59/2029.html'}, {'name': 'Douglas W. Oard', 'dblp_profile': 'https://dblp.org/pid/o/DouglasWOard.html'}, {'name': 'Philip Fei Wu', 'dblp_profile': 'https://dblp.org/pid/50/7689.html'}, {'name': 'Yejun Wu', 'dblp_profile': 'https://dblp.org/pid/85/3571.html'}]",2005,"This year, the University of Maryland participated in four separate tracks: HARD, enterprise, question answering, and genomics. Our HARD experiments involved a trained intermediary who searched for documents on behalf of the user, created clarification forms manually, and exploited user responses accordingly. The aim was to better understand the nature of single-iteration clarification dialogs and to develop an “ontology of clarifications” that can be leveraged to guide system development. For the enterprise track, we submitted ocial runs to the Known Item Search and the Discussion Search tasks. Document transformation to normalize dates and version numbers was found to be helpful, but suppression of text quoted from earlier messages and expansion of the indexed terms for a message based on subject line threading proved to not be. For the QA track, we submitted a manual run of “other” questions in an eort to quantify human performance on the task. Our genomics track participation was in collaboration with the National Library of Medicine, and is primarily reported in NLM’s overview paper.",ef1f496106b598b1484039f6b61f37fac1961949
Event structure and the encoding of arguments: the syntax of the Mandarin and English verb phrase,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2004,"This work presents a theory of linguistic representation that attempts to capture the syntactic structure of verbs and their arguments. My framework is based on the assumption that the proper representation of argument structure is event structure. Furthermore, I develop the hypothesis that event structure is syntactic structure, and argue that verb meanings are compositionally derived in the syntax from verbalizing heads, functional elements that license eventive interpretations, and verbal roots, abstract concepts drawn from encyclopedic knowledge. The overall goal of the enterprise is to develop a theory that is able to transparently relate the structure and meaning of verbal arguments. By hypothesis, languages share the same inventory of primitive building blocks and are governed by the same set of constraints—all endowed by principles of Universal Grammar and subjected to parametric variations. Support for my theory is drawn from both Mandarin Chinese and English. In particular, the organization of the Mandarin verbal system provides strong evidence for the claim that activity and state are the only two primitive verb types in Chinese— achievements and accomplishments are syntactically-derived complex categories. As a specific instance of complex event composition, I examine Mandarin resultative verb compounds and demonstrate that a broad range of variations can be perspicuously captured in my framework. I show that patterns of argument sharing in these verbal compounds can be analyzed as control, thus grounding argument structure in wellknown syntactic constraints such as the Minimum Distance Principle. Finally, I argue that cross-linguistic differences in the realization of verbal arguments can be reduced to variations in the way functional elements interact with verbal roots. Overall, my work not only contributes to our understanding of how events are syntactically represented, but also explicates interactions at the syntax-semantics interface, clarifying the relationship between surface form, syntactic structure, and logical form. A theory of argument structure grounded in independently-motivated syntactic constraints, on the one hand, and the semantic structure of events, on the other hand, is able to account for a wide range of empirical facts with few stipulations. Thesis Supervisor: Boris Katz Title: Principal Research Scientist",20823e7c5defc16555124d045d6c4b92ff10254b
Answering Definition Questions Using Multiple Knowledge Sources,"[{'name': 'Wesley Hildebrandt', 'dblp_profile': 'https://dblp.org/pid/96/2960.html'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2004,"Definition questions represent a largely unexplored area of question answering—they are different from factoid questions in that the goal is to return as many relevant “nuggets” of information about a concept as possible. We describe a multi-strategy approach to answering such questions using a database constructed offline with surface patterns, a Webbased dictionary, and an off-the-shelf document retriever. Results are presented from component-level evaluation and from an endto-end evaluation of our implemented system at the TREC 2003 Question Answering Track.",7f4ed83a712982774f805230f4c39fe485d5ea55
A Computational Framework for Non-Lexicalist Semantics,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2004,"Under a lexicalist approach to semantics, a verb completely encodes its syntactic and semantic structures, along with the relevant syntax-to-semantics mapping; polysemy is typically attributed to the existence of different lexical entries. A lexicon organized in this fashion contains much redundant information and is unable to capture cross-categorial morphological derivations. The solution is to spread the ""semantic load"" of lexical entries to other morphemes not typically taken to bear semantic content. This approach follows current trends in linguistic theory, and more perspicuously accounts for alternations in argument structure. I demonstrate how such a framework can be computationally realized with a feature-based, agenda-driven chart parser for the Minimalist Program.",1f921759e67ad6e6e85cf4d70028d4ff85de029b
Answering Questions About Moving Objects in Videos,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chris Stauffer', 'dblp_profile': 'https://dblp.org/pid/94/2169.html'}, {'name': 'W. Eric L. Grimson', 'dblp_profile': 'https://dblp.org/pid/g/WEricLGrimson.html'}]",2004,"Current question answering systems succeed in many respects regarding questions about textual documents. However, information exists in other media, which provides both opportunities and challenges for question answering. We describe our efforts in extending question answering capabilities to video data: our implemented prototype, Spot, can answer questions about moving objects in a surveillance setting. This novel application of vision and language technology is situated within a larger framework designed to integrate knowledge from multiple domains under a common representation. We believe that our framework will support the next generation of multimodal natural language information access systems.",d9892fcc7264e8f21d6ea0d946fb2b3634ed8ab2
Viewing the Web as a Virtual Database for Question Answering,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Sue Felshin', 'dblp_profile': 'https://dblp.org/pid/11/6978.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Gregory Marton', 'dblp_profile': 'https://dblp.org/pid/95/306.html'}]",2004,"Although the World Wide Web contains a tremendous amount of information, the lack of intuitive information access methods and the paucity of uniform structure make finding the right knowledge difficult. Our solution is to turn the Web into a “virtual database” and to access it through natural language. We have accomplished this by developing a stylized relational framework, called the object-property-value model, which captures the regularity found in both natural language questions and Web resources. We have adopted this framework in START and Omnibase, two components of a system that understands natural language questions and responds with answers extracted on the fly from heterogeneous and semistructured Web sources. Our system can answer millions of questions from hundreds of Web resources with high precision.",9b920be29fad71ad686c0e10990647b6942dc6fb
Answering Multiple Questions on a Topic From Heterogeneous Resources,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Matthew W. Bilotti', 'dblp_profile': 'https://dblp.org/pid/79/1236.html'}, {'name': 'Sue Felshin', 'dblp_profile': 'https://dblp.org/pid/11/6978.html'}, {'name': 'Aaron Fernandes', 'dblp_profile': 'https://dblp.org/pid/34/2286.html'}, {'name': 'Wesley Hildebrandt', 'dblp_profile': 'https://dblp.org/pid/96/2960.html'}, {'name': 'Roni Katzir', 'dblp_profile': 'https://dblp.org/pid/45/5156.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Daniel Loreto', 'dblp_profile': 'https://dblp.org/pid/43/3417.html'}, {'name': 'Gregory Marton', 'dblp_profile': 'https://dblp.org/pid/95/306.html'}, {'name': 'Federico Mora', 'dblp_profile': 'https://dblp.org/pid/82/5093.html'}, {'name': 'Özlem Uzuner', 'dblp_profile': 'https://dblp.org/pid/43/957.html'}]",2004,"MIT CSAIL’s entry into this year’s TREC Question Answering track focused on the conversational aspect of this year’s task, on improving the coverage of our list and definition systems, and on an infrastructure to generalize our TREC-specific tools for other question answering tasks. While our overall architecture remained largely unchanged from last year, we have built on our strengths for each component: our web-based factoid engine was adapted for input from a new web search engine; our list engine’s knowledge base expanded from 150 to over 3000 lists; our definitional nugget extractor now has expanded and improved patterns with improved component precision and recall. Beyond their internal improvements, these components were adapted to a larger conversational framework that passed information about the topic1 to factoids and lists. Answer selection for definitional2 questions newly took into account the prior questions and answers for duplicate removal. Our factoid engine, Aranea (Lin et al., 2002; Katz et al., 2003), used the World Wide Web to find candidate answers to the given question, and then projects its best candidates onto the corpus, choosing the one best supported. This year, instead of using only Google for web search, we integrated results from the Teoma search engine as well. Our list engine, Pauchok (Tellex et al., 2003), retrieved passage-sized chunks of text relevant to the question using information re-",234bf75f8aa0825e639febed540e23d2c8001ca6
Extracting Structural Paraphrases from Aligned Monolingual Corpora,"[{'name': 'Ali Ibrahim', 'dblp_profile': 'https://dblp.org/pid/02/1929.html'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2003,"We present an approach for automatically learning paraphrases from aligned monolingual corpora. Our algorithm works by generalizing the syntactic paths between corresponding anchors in aligned sentence pairs. Compared to previous work, structural paraphrases generated by our algorithm tend to be much longer on average, and are capable of capturing long-distance dependencies. In addition to a standalone evaluation of our paraphrases, we also describe a question answering application currently under development that could immensely benefit from automatically-learned structural paraphrases.",29a22d3eab2668d05c8fa478e8a61e146e6dca35
The role of context in question answering systems,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dennis Quan', 'dblp_profile': 'https://dblp.org/pid/95/831.html'}, {'name': 'Vineet Sinha', 'dblp_profile': 'https://dblp.org/pid/20/5880.html'}, {'name': 'Karun Bakshi', 'dblp_profile': 'https://dblp.org/pid/79/1554.html'}, {'name': 'David Huynh', 'dblp_profile': 'https://dblp.org/pid/h/DavidHuynh.html'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'David R. Karger', 'dblp_profile': 'https://dblp.org/pid/k/DavidRKarger.html'}]",2003,"Despite recent advances in natural language question an-swering technology, the problem of designing effective user interfaces has been largely unexplored. We conducted a user study to investigate the problem and discovered that overall, users prefer a paragraph-sized chunk of text over just an exact phrase as the answer to their questions. Fur-thermore, users generally prefer answers embedded in con-text, regardless of the perceived reliability of the source documents. When users research a topic, increasing the amount of text returned to users significantly decreases the number of queries that they pose to the system, suggesting that users utilize supporting text to answer related ques-tions. We believe that these results can serve to guide future developments in question answering user interfaces.",44cf0be24082e10b59c2fcf6f7ee42f93ae750eb
Question answering from the web using knowledge annotation and knowledge mining techniques,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}]",2003,"We present a strategy for answering fact-based natural language questions that is guided by a characterization of real-world user queries. Our approach, implemented in a system called Aranea, extracts answers from the Web using two different techniques: knowledge annotation and knowledge mining. Knowledge annotation is an approach to answering large classes of frequently occurring questions by utilizing semi\-structured and structured Web sources. Knowledge mining is a statistical approach that leverages massive amounts of Web data to overcome many natural language processing challenges. We have integrated these two different paradigms into a question answering system capable of providing users with concise answers that directly address their information needs.",182ab49a3d31fe468c7659ee89129c2cd8edc9de
Better Public Policy Through Natural Language Information Access,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Roger Hurwitz', 'dblp_profile': 'https://dblp.org/pid/93/3468.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Özlem Uzuner', 'dblp_profile': 'https://dblp.org/pid/43/957.html'}]",2003,"Federal agencies implement laws passed by the Congress by creating rules and regulations that can be applied in practice. During this process, staffs at the various agencies may review past and current regulations and receive comments from stakeholders and the public regarding the proposed regulations.Putting rulemaking online can increase the public's awareness of the proposed rules and its participation in the process. It can also facilitate staff work. A key factor in realizing these benefits will be the availability of simple, intuitive, and timely access to the empowering legislation, the proposed rules and information regarding them. We propose to provide such access through an information architecture that allows members of the public as well as staff and stakeholders to obtain the texts and information they desire by using everyday language. Over the past decade, we have developed the START and Omnibase systems for natural language question answering and have applied them in a variety of domains. We plan to use these systems and our experience to support online rule making.We note that besides providing information access, these systems can function more proactively, by soliciting feedback from targeted parties or by sending out notifications and information in response to standing queries submitted by the users.",30328249e7ab7e67f4a83965415d916dd5b74ac0
START: A Framework for Facilitating E-Rulemaking,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Roger Hurwitz', 'dblp_profile': 'https://dblp.org/pid/93/3468.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Özlem Uzuner', 'dblp_profile': 'https://dblp.org/pid/43/957.html'}]",2003,"Federal agencies implement laws passed by Congress by making rules and regulations that can be applied in practice. Stakeholders and members of the public usually want to know how proposed rules will affect them, so they can effectively respond to the proposals, during the comment period. While the stakeholders, like business and advocacy groups, can employ information specialists to get their answers, individuals will have to turn to the rulemaking agencies for such information. Consequently, for online rulemaking to encourage and support public participation, there will be need for information access that is simple and intuitive to use, comprehensive in the material covered, specific to the user's needs and timely.",0e9fc78de5ad149efe7670142398fc59e334cd50
What Makes a Good Answer? The Role of Context in Question Answering,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dennis Quan', 'dblp_profile': 'https://dblp.org/pid/95/831.html'}, {'name': 'Vineet Sinha', 'dblp_profile': 'https://dblp.org/pid/20/5880.html'}, {'name': 'Karun Bakshi', 'dblp_profile': 'https://dblp.org/pid/79/1554.html'}, {'name': 'David Huynh', 'dblp_profile': 'https://dblp.org/pid/h/DavidHuynh.html'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'David R. Karger', 'dblp_profile': 'https://dblp.org/pid/k/DavidRKarger.html'}]",2003,"Question answering systems have proven to be helpful to users because they can provide succinct answers that do not require users to wade through a large number of documents. However, despite recent advances in the underlying question answering technology, the problem of designing effective interfaces has been largely unexplored. We conducted a user study to investigate this area and discovered that, overall, users prefer paragraph-sized chunks of text over just an exact phrase as the answer to their questions. Furthermore, users generally prefer answers embedded in context, regardless of the perceived reliability of the source documents. When researching a topic, increasing the amount of text returned to users significantly decreases the number of queries that they pose to the system, suggesting that users utilize supporting text to answer related questions. We believe that these results can serve to guide future developments in question answering interfaces.",51882ef298fc87c73801f3dc80947328860c0a00
Sticky notes for the semantic web,"[{'name': 'David R. Karger', 'dblp_profile': 'https://dblp.org/pid/k/DavidRKarger.html'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dennis Quan', 'dblp_profile': 'https://dblp.org/pid/95/831.html'}]",2003,"Computer-based annotation is increasing in popularity as a mechanism for revising documents and sharing comments over the Internet. One reason behind this surge is that viewpoints, summaries, and notes written by others are often helpful to readers. In particular, these types of annotations can help users locate or recall relevant documents. We believe that this model can be applied to the problem of retrieval on the Semantic Web. In this paper, we propose a generalized annotation environment that supports richer forms of description such as natural language. We discuss how RDF can be used to model annotations and the connections between annotations and the documents they describe. Furthermore, we explore the idea of a question answering interface that allows retrieval based both on the text of the annotations and the annotations associated metadata. Finally, we speculate on how these features could be pervasively integrated into an information management environment, making Semantic Web annotation a first class player in terms of document management and retrieval",08b90ce3f67a082255554d3448476fd722c05f9c
Answering Questions about Moving Objects in Surveillance Videos,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Chris Stauffer', 'dblp_profile': 'https://dblp.org/pid/94/2169.html'}, {'name': 'W. Eric L. Grimson', 'dblp_profile': 'https://dblp.org/pid/g/WEricLGrimson.html'}]",2003,"Current question answering systems succeed in many respects regarding questions about textual documents. However, information exists in other media, which provides both opportunities and challenges for question answering. We present results in extending question answering capabilities to video footage captured in a surveillance setting. Our prototype system, called Spot, can answer questions about moving objects that appear within the video. We situate this novel application of vision and language technology within a larger framework designed to integrate language and vision systems under a common representation. We believe that our framework will support the next generation of multimodal natural language information access systems.",9090ef73b412013196b464cf7d82f1238e71f414
Quantitative evaluation of passage retrieval algorithms for question answering,"[{'name': 'Stefanie Tellex', 'dblp_profile': 'https://dblp.org/pid/50/3149.html'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Aaron Fernandes', 'dblp_profile': 'https://dblp.org/pid/34/2286.html'}, {'name': 'Gregory Marton', 'dblp_profile': 'https://dblp.org/pid/95/306.html'}]",2003,"Passage retrieval is an important component common to many question answering systems. Because most evaluations of question answering systems focus on end-to-end performance, comparison of common components becomes difficult. To address this shortcoming, we present a quantitative evaluation of various passage retrieval algorithms for question answering, implemented in a framework called Pauchok. We present three important findings: Boolean querying schemes perform well in the question answering task. The performance differences between various passage retrieval algorithms vary with the choice of document retriever, which suggests significant interactions between document retrieval and passage retrieval. The best algorithms in our evaluation employ density-based measures for scoring query terms. Our results reveal future directions for passage retrieval and question answering.",828c773ec37c9f3e5e245780ba37dae7466acdd4
Integrating Web-based and Corpus-based Techniques for Question Answering,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Daniel Loreto', 'dblp_profile': 'https://dblp.org/pid/43/3417.html'}, {'name': 'Wesley Hildebrandt', 'dblp_profile': 'https://dblp.org/pid/96/2960.html'}, {'name': 'Matthew W. Bilotti', 'dblp_profile': 'https://dblp.org/pid/79/1236.html'}, {'name': 'Sue Felshin', 'dblp_profile': 'https://dblp.org/pid/11/6978.html'}, {'name': 'Aaron Fernandes', 'dblp_profile': 'https://dblp.org/pid/34/2286.html'}, {'name': 'Gregory Marton', 'dblp_profile': 'https://dblp.org/pid/95/306.html'}, {'name': 'Federico Mora', 'dblp_profile': 'https://dblp.org/pid/82/5093.html'}]",2003,"MIT CSAIL’s entry in this year’s TREC Question Answering track focused on integrating Web-based techniques with more traditional strategies based on document retrieval and named-entity detection. We believe that achieving high performance in the question answering task requires a combination of multiple strategies designed to capitalize on different characteristics of various resources. The system we deployed for the TREC evaluation last year relied exclusively on the World Wide Web to answer factoid questions (Lin et al., 2002). The advantages that the Web offers are well known and have been exploited by previous systems (Brill et al., 2001; Clarke et al., 2001; Dumais et al., 2002). The immense amount of freely available unstructured text provides data redundancy, which can be leveraged with simple pattern matching techniques involving the expected answer formulations. In many ways, we can utilize huge quantities of data to overcome many thorny problems in natural language processing such as lexical ambiguity and paraphrases. Furthermore, Web search engines such as Google provide a convenient front-end for accessing and filtering enormous amounts of Web data. We have identified this class of techniques as the knowledge mining approach to question answering (Lin and Katz, 2003). In addition to viewing the Web as a repository of unstructured documents, we can also take advantage of structured and semistructured sources available on the Web using knowledge annotation techniques (Katz, 1997; Lin and Katz, 2003). Through empirical analysis of real world natural language questions, we have noticed that large classes of commonly occurring queries can be parameterized and captured using a simple object–property–value data model (Katz et al., 2002). Furthermore, such a data model is easy to impose on Web resources through a framework of wrapper scripts. These techniques allow our system to view the Web as if it were a “virtual database” and use knowledge contained therein to answer user questions. While the Web is undeniably a useful resource for question answering, it is not without drawbacks. Useful knowledge on the Web is often drowned out by the sheer amount of irrelevant material, and statistical techniques are often insufficient to separate right answers from wrong ones. Overcoming these obstacles will require addressing many outstanding issues in computational linguistics: anaphora resolution, paraphrase normalization, temporal reference calculation, and lexical disambiguation, just to name a few. Furthermore, the setup of the TREC evaluations necessitates an extra step in the question answering process for systems that extract answers from external sources, typically known as answer projection. For every Web-derived answer, a system must find a supporting document from the AQUAINT corpus, even if the corpus was not used in the answer extraction process. This year’s main task included definition and list questions in addition to factoid questions. Although Web-based techniques have proven effective in handling factoid questions, they are less applicable to tackling definition and list questions. The datadriven approach implicitly assumes that each natural language question has a unique answer. Since a single answer instance is sufficient, algorithms were designed to trade recall for precision. For list and definition questions, however, a more balanced approach is required, since multiple answers are not only desired, but necessary. We believe that the best strategy is to integrate Web-based approaches with more traditional question answering techniques driven by document retrieval and named-entity detection. Corpusand Web-based strategies should play complementary roles in an overall question answering framework.",9df60943a1eb7cc51aae644e4d1d00523ea53a8d
Annotating the Semantic Web Using Natural Language,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2002,"Because the ultimate purpose of the Semantic Web is to help users better locate, organize, and process content, we believe that it should be grounded in the information access method humans are most comfortable with---natural language. However, the Resource Description Framework (RDF), the foundation of the Semantic Web, was designed to be easily processed by computers, not humans. To render RDF more friendly to humans, we propose to augment it with natural language annotations, or metadata written in everyday language. We argue that natural language annotations, parsed into computer-readable representations, are not only intuitive and effective, but can also accelerate the pace with which the Semantic Web is being adopted. We believe that our technology can facilitate a happy marriage between natural language technology and the Semantic Web vision.",245a4fd0c0c6ecf1cae13e92c0a6fda6c2a0a5dc
Natural Language Annotations for the Semantic Web,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Dennis Quan', 'dblp_profile': 'https://dblp.org/pid/95/831.html'}]",2002,,728ce50dc5bceb9621c9ae7cbddf37d4183e9039
The Web as a Resource for Question Answering: Perspectives and Challenges,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}]",2002,"The vast amounts of information readily available on the World Wide Web can be effectively used for question answering in two fundamentally different ways. In the federated approach, techniques for handling semistructured data are applied to access Web sources as if they were databases, allowing large classes of common questions to be answered uniformly. In the distributed approach, largescale text-processing techniques are used to extract answers directly from unstructured Web documents. Because the Web is orders of magnitude larger than any human-collected corpus, question answering systems can capitalize on its unparalleled-levels of data redundancy. Analysis of real-world user questions reveals that the federated and distributed approaches complement each other nicely, suggesting a hybrid approach in future question answering systems.",bb0468ff520abe817a1456f70e9c0eb0845ddebc
The START Multimedia Information System: Current Technology and Future Directions,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Sue Felshin', 'dblp_profile': 'https://dblp.org/pid/11/6978.html'}]",2002,"To address the problem of information overload in today’s world, we have developed Start, a natural language question answering system that provides users with high-precision multimedia information access through the use of natural language annotations. To address the difficulty of accessing large amounts of heterogeneous data, we have developed Omnibase, which assists Start by integrating structured and semistructured Web databases into a single, uniformly structured “virtual database.” Our ultimate goal is to develop a computer system that acts like a “smart reference librarian,” and we believe we have laid a firm foundation for achieving our goal. This paper describes our current implemented system and discusses future research directions.",963dc23a64603527e592b5fcfa8831845b1313ea
Omnibase: Uniform Access to Heterogeneous Data for Question Answering,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Sue Felshin', 'dblp_profile': 'https://dblp.org/pid/11/6978.html'}, {'name': 'Deniz Yuret', 'dblp_profile': 'https://dblp.org/pid/84/4160.html'}, {'name': 'Ali Ibrahim', 'dblp_profile': 'https://dblp.org/pid/02/1929.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Gregory Marton', 'dblp_profile': 'https://dblp.org/pid/95/306.html'}, {'name': 'Alton Jerome McFarland', 'dblp_profile': 'https://dblp.org/pid/19/5371.html'}, {'name': 'Baris Temelkuran', 'dblp_profile': 'https://dblp.org/pid/21/4357.html'}]",2002,,dedb02a94a1e461a065fd981f53e85e54bef170d
Web question answering: is more always better?,"[{'name': 'Susan T. Dumais', 'dblp_profile': 'https://dblp.org/pid/d/SusanTDumais.html'}, {'name': 'Michele Banko', 'dblp_profile': 'https://dblp.org/pid/34/5010.html'}, {'name': 'Eric Brill', 'dblp_profile': 'https://dblp.org/pid/18/3677.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Andrew Y. Ng', 'dblp_profile': 'https://dblp.org/pid/n/AndrewYNg.html'}]",2002,"This paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online. Most question answering systems use a wide variety of linguistic resources. We focus instead on the redundancy available in large corpora as an important resource. We use this redundancy to simplify the query rewrites that we need to use, and to support answer mining from returned snippets. Our system performs quite well given the simplicity of the techniques being utilized. Experimental results show that question answering accuracy can be greatly improved by analyzing more and more matching passages. Simple passage ranking and n-gram extraction techniques work well in our system making it efficient to use with many backend retrieval engines.",e8f9537a6cfb1ba2501c1c6ac3b114c274534095
Extracting Answers from the Web Using Data Annotation and Knowledge Mining Techniques,"[{'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Aaron Fernandes', 'dblp_profile': 'https://dblp.org/pid/34/2286.html'}, {'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Gregory Marton', 'dblp_profile': 'https://dblp.org/pid/95/306.html'}, {'name': 'Stefanie Tellex', 'dblp_profile': 'https://dblp.org/pid/50/3149.html'}]",2002,"Production of strong lightweight membrane structure by applying a thin reflective coating such as aluminum to a rotating cylinder, applying a mesh material such as nylon over the aluminum coating, coating the mesh overlying the aluminum with a polymerizing material such as a para-xylylene monomer gas to polymerize as a film bound to the mesh and the aluminum, and applying an emissivity increasing material such as chromium and silicon monoxide to the polymer film to disperse such material colloidally into the growing polymer film, or applying such material to the final polymer film, and removing the resulting membrane structure from the cylinder. Alternatively, such membrane structure can be formed by etching a substrate in the form of an organic film such as a polyimide, or a metal foil, to remove material from the substrate and reduce its thickness, applying a thin reflective coating such as aluminum on one side of the substrate and applying an emissivity increasing coating such as chromium and silicon monoxide on the reverse side of the substrate.",2e2c875098ba8949f17a6d618972dddbb4db5383
The Role of a Natural Language Conversational Interface in Online Sales: A Case Study,"[{'name': 'Joyce Y. Chai', 'dblp_profile': 'https://dblp.org/pid/c/JoyceYChai.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Wlodek Zadrozny', 'dblp_profile': 'https://dblp.org/pid/40/3212.html'}, {'name': 'Yiming Ye', 'dblp_profile': 'https://dblp.org/pid/67/333.html'}, {'name': 'Margo Stys-Budzikowska', 'dblp_profile': 'https://dblp.org/pid/176/4870.html'}, {'name': 'Veronika Horvath', 'dblp_profile': 'https://dblp.org/pid/63/1041.html'}, {'name': 'Nanda Kambhatla', 'dblp_profile': 'https://dblp.org/pid/11/3084.html'}, {'name': 'Catherine G. Wolf', 'dblp_profile': 'https://dblp.org/pid/00/6929.html'}]",2001,,b4adeca45ea786f5bdf87e7f5af4684f34c46ed2
Gathering Knowledge for a Question Answering System from Heterogeneous Information Sources,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Sue Felshin', 'dblp_profile': 'https://dblp.org/pid/11/6978.html'}]",2001,"Although vast amounts of information are available electronically today, no effective information access mechanism exists to provide humans with convenient information access. A general, open-domain question answering system is a solution to this problem. We propose an architecture for a collaborative question answering system that contains four primary components: an annotations system for storing knowledge, a ternary expression representation of language, a transformational rule system for handling some complexities of language, and a collaborative mechanism by which ordinary users can contribute new knowledge by teaching the system new information. We have developed a initial prototype, called Webnotator, with which to test these ideas.",2ab5a5fce5c7b580b6c9934ba9155be59f09a63e
Data-Intensive Question Answering,"[{'name': 'Eric Brill', 'dblp_profile': 'https://dblp.org/pid/18/3677.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Michele Banko', 'dblp_profile': 'https://dblp.org/pid/34/5010.html'}, {'name': 'Susan T. Dumais', 'dblp_profile': 'https://dblp.org/pid/d/SusanTDumais.html'}, {'name': 'Andrew Y. Ng', 'dblp_profile': 'https://dblp.org/pid/n/AndrewYNg.html'}]",2001,Utilisation de la redondance des reponses elles-memes pour ameliorer le resultat final de la recherche d'information- redondance due a la tres grande quantite d'informations disponibles actuellement,4548c06e6d056191f1a699ae6868704a14cb5f9b
Comparative Evaluation of a Natural Language Dialog Based System and a Menu Driven System for Information Access: a Case Study,"[{'name': 'Joyce Yue Chai', 'dblp_profile': 'https://dblp.org/pid/c/JoyceYChai.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Wlodek Zadrozny', 'dblp_profile': 'https://dblp.org/pid/40/3212.html'}, {'name': 'Yiming Ye', 'dblp_profile': 'https://dblp.org/pid/67/333.html'}, {'name': 'Malgorzata Budzikowska', 'dblp_profile': 'https://dblp.org/pid/91/1452.html'}, {'name': 'Veronika Horvath', 'dblp_profile': 'https://dblp.org/pid/63/1041.html'}, {'name': 'Nanda Kambhatla', 'dblp_profile': 'https://dblp.org/pid/11/3084.html'}, {'name': 'Catherine G. Wolf', 'dblp_profile': 'https://dblp.org/pid/00/6929.html'}]",2000,"This paper describes the evaluation of a natural language dialog based navigation system (HappyAssistant) that helps users access e-commerce sites to find relevant information about products and services. The prototype system leverages technologies in natural language processing and human computer interaction to create a faster and more intuitive way of interacting with websites, especially for the less experienced users. The result of a comparative study shows that users prefer the natural language enabled navigation two to one over the menu driven navigation. In addition, the study confirmed the efficiency of using natural language dialog in terms of the number of clicks and the amount of time required to obtain the relevant information. In the case study, comparing to the menu driven system, the average number of clicks used in the natural language system was reduced by 63.2% and the average time was reduced by 33.3%.",bba5443bb097e99ca0f1d01f6d2c6cd59d035299
Integrating Web Resources and Lexicons into a Natural Language Query System,"[{'name': 'Boris Katz', 'dblp_profile': 'https://dblp.org/pid/k/BorisKatz.html'}, {'name': 'Deniz Yuret', 'dblp_profile': 'https://dblp.org/pid/84/4160.html'}, {'name': 'Jimmy Lin', 'dblp_profile': 'https://dblp.org/pid/00/7739'}, {'name': 'Sue Felshin', 'dblp_profile': 'https://dblp.org/pid/11/6978.html'}, {'name': 'Rebecca Schulman', 'dblp_profile': 'https://dblp.org/pid/96/6532.html'}, {'name': 'Adnan Ilik', 'dblp_profile': 'https://dblp.org/pid/53/2891.html'}, {'name': 'Ali Ibrahim', 'dblp_profile': 'https://dblp.org/pid/02/1929.html'}, {'name': 'Philip Osafo-Kwaako', 'dblp_profile': 'https://dblp.org/pid/08/1661.html'}]",1999,"The START system responds to natural language queries with answers in text, pictures, and other media. START's sentence-level natural language parsing relies on a number of mechanisms to help it process the huge, diverse resources available on the World Wide Web. Blitz, a hybrid heuristic- and corpus-based natural language preprocessor enables START to integrate a large and ever-changing lexicon of proper names, by using heuristic rules and precompiled tables of symbols to preprocess various highly regular and fixed expressions into lexical tokens. LaMeTH, a content-based system for extracting information from HTML documents, assists START by providing a uniform method of accessing information on the Web in real time. These mechanisms have considerably improved STARTS ability to analyze real-world sentences and answer queries through expansion of its lexicon and integration of Web resources.",a3664fcd675a638d8035291a54c134737ca94301
