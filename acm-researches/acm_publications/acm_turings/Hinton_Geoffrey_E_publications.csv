title,authors,year,abstract,ss_id
How to Represent Part-Whole Hierarchies in a Neural Network,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2023,"Abstract This article does not describe a working system. Instead, it presents a single idea about representation that allows advances made by several different groups to be combined into an imaginary system called GLOM.1 The advances include transformers, neural fields, contrastive representation learning, distillation, and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy that has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.",5cee90b85b88e4de1d51b2963613a48b68916ac7
Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766.html'}, {'name': 'Ruixiang Zhang', 'dblp_profile': 'https://dblp.org/pid/20/9860.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2023,"We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",b64537bdf7a103aa01972ba06ea24a9c08f7cd74
Scaling Forward Gradient With Local Losses,"[{'name': 'Mengye Ren', 'dblp_profile': 'https://dblp.org/pid/163/1952.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Renjie Liao', 'dblp_profile': 'https://dblp.org/pid/08/8180.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2023,"Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.",7a31b37b4844653e084ff096f1ec5861b95389a9
Meta-Learning Fast Weight Language Models,"[{'name': 'Kevin Clark', 'dblp_profile': 'https://dblp.org/pid/78/6661.html'}, {'name': 'Kelvin Guu', 'dblp_profile': 'https://dblp.org/pid/164/5838.html'}, {'name': 'Ming-Wei Chang', 'dblp_profile': 'https://dblp.org/pid/69/4618.html'}, {'name': 'Panupong Pasupat', 'dblp_profile': 'https://dblp.org/pid/124/9178.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}]",2022,"Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.",37ba9c33025fb31f25436010e12c65a0bafc0e1f
Pix2seq: A Language Modeling Framework for Object Detection,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766-7.html'}, {'name': 'Saurabh Saxena', 'dblp_profile': 'https://dblp.org/pid/15/8791.html'}, {'name': 'Lala Li', 'dblp_profile': 'https://dblp.org/pid/49/7563.html'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.",19b3b074d38b250d024920732ae51a8ffa0996dd
A Unified Sequence Interface for Vision Tasks,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766-7.html'}, {'name': 'Saurabh Saxena', 'dblp_profile': 'https://dblp.org/pid/15/8791.html'}, {'name': 'Lala Li', 'dblp_profile': 'https://dblp.org/pid/49/7563.html'}, {'name': 'Tsung-Yi Lin', 'dblp_profile': 'https://dblp.org/pid/47/8105.html'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"While language tasks are naturally expressed in a single, unified, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of""core""computer vision tasks can also be unified if formulated in terms of a shared pixel-to-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a unified interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-specific customization. To solve a specific task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-specific output. We show that such a model can achieve competitive performance compared to well-established task-specific models.",06761cb27e14aa55a6c3d98b949898aa26416698
Robust and Efficient Medical Imaging with Self-Supervision,"[{'name': 'Shekoofeh Azizi', 'dblp_profile': 'https://dblp.org/pid/143/1915.html'}, {'name': 'Laura Culp', 'dblp_profile': 'https://dblp.org/pid/320/7722.html'}, {'name': 'Jan Freyberg', 'dblp_profile': 'https://dblp.org/pid/262/3737.html'}, {'name': 'Basil Mustafa', 'dblp_profile': 'https://dblp.org/pid/223/4558.html'}, {'name': 'Sebastien Baur', 'dblp_profile': 'https://dblp.org/pid/280/3074.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766.html'}, {'name': 'Patricia MacWilliams', 'dblp_profile': 'https://dblp.org/pid/270/0345.html'}, {'name': 'S. Sara Mahdavi', 'dblp_profile': 'https://dblp.org/pid/320/7735.html'}, {'name': 'Ellery Wulczyn', 'dblp_profile': 'https://dblp.org/pid/61/11517.html'}, {'name': 'Boris Babenko', 'dblp_profile': 'https://dblp.org/pid/65/2779.html'}, {'name': 'Megan Wilson', 'dblp_profile': 'https://dblp.org/pid/270/1815.html'}, {'name': 'Aaron Loh', 'dblp_profile': 'https://dblp.org/pid/271/6996.html'}, {'name': 'Po-Hsuan Cameron Chen', 'dblp_profile': 'https://dblp.org/pid/232/1634.html'}, {'name': 'Yuan Liu', 'dblp_profile': 'https://dblp.org/pid/87/2948.html'}, {'name': 'Pinal Bavishi', 'dblp_profile': 'https://dblp.org/pid/228/8547.html'}, {'name': 'Scott Mayer McKinney', 'dblp_profile': 'https://dblp.org/pid/284/0700.html'}, {'name': 'Jim Winkens', 'dblp_profile': 'https://dblp.org/pid/222/1966.html'}, {'name': 'Abhijit Guha Roy', 'dblp_profile': 'https://dblp.org/pid/165/7906.html'}, {'name': 'Zachary Beaver', 'dblp_profile': 'https://dblp.org/pid/283/7401.html'}, {'name': 'Fiona Ryan', 'dblp_profile': 'https://dblp.org/pid/283/7526.html'}, {'name': 'Justin Krogue', 'dblp_profile': 'https://dblp.org/pid/248/8224.html'}, {'name': 'Mozziyar Etemadi', 'dblp_profile': 'https://dblp.org/pid/70/432.html'}, {'name': 'Umesh Telang', 'dblp_profile': 'https://dblp.org/pid/284/0751.html'}, {'name': 'Yun Liu', 'dblp_profile': 'https://dblp.org/pid/50/2482-13.html'}, {'name': 'Lily Peng', 'dblp_profile': 'https://dblp.org/pid/198/1261.html'}, {'name': 'Gregory S. Corrado', 'dblp_profile': 'https://dblp.org/pid/05/8414.html'}, {'name': 'Dale R. Webster', 'dblp_profile': 'https://dblp.org/pid/205/2994.html'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Neil Houlsby', 'dblp_profile': 'https://dblp.org/pid/91/10669.html'}, {'name': 'Alan Karthikesalingam', 'dblp_profile': 'https://dblp.org/pid/205/1826.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Vivek Natarajan', 'dblp_profile': 'https://dblp.org/pid/51/11047.html'}]",2022,"Recent progress in Medical Artificial Intelligence (AI) has delivered systems that can reach clinical expert level performance. However, such systems tend to demonstrate sub-optimal""out-of-distribution""performance when evaluated in clinical settings different from the training environment. A common mitigation strategy is to develop separate systems for each clinical setting using site-specific data [1]. However, this quickly becomes impractical as medical data is time-consuming to acquire and expensive to annotate [2]. Thus, the problem of""data-efficient generalization""presents an ongoing difficulty for Medical AI development. Although progress in representation learning shows promise, their benefits have not been rigorously studied, specifically for out-of-distribution settings. To meet these challenges, we present REMEDIS, a unified representation learning strategy to improve robustness and data-efficiency of medical imaging AI. REMEDIS uses a generic combination of large-scale supervised transfer learning with self-supervised learning and requires little task-specific customization. We study a diverse range of medical imaging tasks and simulate three realistic application scenarios using retrospective data. REMEDIS exhibits significantly improved in-distribution performance with up to 11.5% relative improvement in diagnostic accuracy over a strong supervised baseline. More importantly, our strategy leads to strong data-efficient generalization of medical imaging AI, matching strong supervised baselines using between 1% to 33% of retraining data across tasks. These results suggest that REMEDIS can significantly accelerate the life-cycle of medical imaging AI development thereby presenting an important step forward for medical imaging AI to deliver broad impact.",fa499cccb093dcb61ba5a270c5afaefc2502a241
A Unified Sequence Interface for Vision Tasks,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766-7.html'}, {'name': 'Saurabh Saxena', 'dblp_profile': 'https://dblp.org/pid/15/8791.html'}, {'name': 'Lala Li', 'dblp_profile': 'https://dblp.org/pid/49/7563.html'}, {'name': 'Tsung-Yi Lin', 'dblp_profile': 'https://dblp.org/pid/47/8105.html'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"While language tasks are naturally expressed in a single, unified, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of""core""computer vision tasks can also be unified if formulated in terms of a shared pixel-to-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a unified interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-specific customization. To solve a specific task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-specific output. We show that such a model can achieve competitive performance compared to well-established task-specific models.",06761cb27e14aa55a6c3d98b949898aa26416698
Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766.html'}, {'name': 'Ruixiang Zhang', 'dblp_profile': 'https://dblp.org/pid/20/9860.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.",b64537bdf7a103aa01972ba06ea24a9c08f7cd74
Scaling Forward Gradient With Local Losses,"[{'name': 'Mengye Ren', 'dblp_profile': 'https://dblp.org/pid/163/1952.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Renjie Liao', 'dblp_profile': 'https://dblp.org/pid/08/8180.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. However, the standard forward gradient algorithm, when applied naively, suffers from high variance when the number of parameters to be learned is large. In this paper, we propose a series of architectural and algorithmic modifications that together make forward gradient learning practical for standard deep learning benchmark tasks. We show that it is possible to substantially reduce the variance of the forward gradient estimator by applying perturbations to activations rather than weights. We further improve the scalability of forward gradient by introducing a large number of local greedy loss functions, each of which involves only a small number of learnable parameters, and a new MLPMixer-inspired architecture, LocalMixer, that is more suitable for local learning. Our approach matches backprop on MNIST and CIFAR-10 and significantly outperforms previously proposed backprop-free algorithms on ImageNet.",7a31b37b4844653e084ff096f1ec5861b95389a9
A Generalist Framework for Panoptic Segmentation of Images and Videos,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766-7.html'}, {'name': 'Lala Li', 'dblp_profile': 'https://dblp.org/pid/49/7563.html'}, {'name': 'Saurabh Saxena', 'dblp_profile': 'https://dblp.org/pid/15/8791.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}]",2022,"Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model based on analog bits is used to model panoptic masks, with a simple, generic architecture and loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our generalist approach can perform competitively to state-of-the-art specialist methods in similar settings.",614dde18483338069d482d7452900c28052aba83
Gaussian-Bernoulli RBMs Without Tears,"[{'name': 'Renjie Liao', 'dblp_profile': 'https://dblp.org/pid/08/8180.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Mengye Ren', 'dblp_profile': 'https://dblp.org/pid/163/1952.html'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"We revisit the challenging problem of training Gaussian-Bernoulli restricted Boltzmann machines (GRBMs), introducing two innovations. We propose a novel Gibbs-Langevin sampling algorithm that outperforms existing methods like Gibbs sampling. We propose a modified contrastive divergence (CD) algorithm so that one can generate images with GRBMs starting from noise. This enables direct comparison of GRBMs with deep generative models, improving evaluation protocols in the RBM literature. Moreover, we show that modified CD and gradient clipping are enough to robustly train GRBMs with large learning rates, thus removing the necessity of various tricks in the literature. Experiments on Gaussian Mixtures, MNIST, FashionMNIST, and CelebA show GRBMs can generate good samples, despite their single-hidden-layer architecture. Our code is released at: \url{https://github.com/lrjconan/GRBM}.",b68960f1d3d4849a5333c221dc65469eb18a72de
Testing GLOM's ability to infer wholes from ambiguous parts,"[{'name': 'Laura Culp', 'dblp_profile': 'https://dblp.org/pid/320/7722.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"The GLOM architecture proposed by Hinton [2021] is a recurrent neural network for parsing an image into a hierarchy of wholes and parts. When a part is ambiguous, GLOM assumes that the ambiguity can be resolved by allowing the part to make multi-modal predictions for the pose and identity of the whole to which it belongs and then using attention to similar predictions coming from other possibly ambiguous parts to settle on a common mode that is predicted by several different parts. In this study, we describe a highly simplified version of GLOM that allows us to assess the effectiveness of this way of dealing with ambiguity. Our results show that, with supervised training, GLOM is able to successfully form islands of very similar embedding vectors for all of the locations occupied by the same object and it is also robust to strong noise injections in the input and to out-of-distribution input transformations.",f198909cc32428e481043bbf3697074dc81b54ae
Meta-Learning Fast Weight Language Models,"[{'name': 'Kevin Clark', 'dblp_profile': 'https://dblp.org/pid/78/6661.html'}, {'name': 'Kelvin Guu', 'dblp_profile': 'https://dblp.org/pid/164/5838.html'}, {'name': 'Ming-Wei Chang', 'dblp_profile': 'https://dblp.org/pid/69/4618.html'}, {'name': 'Panupong Pasupat', 'dblp_profile': 'https://dblp.org/pid/124/9178.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}]",2022,"Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.",37ba9c33025fb31f25436010e12c65a0bafc0e1f
The Forward-Forward Algorithm: Some Preliminary Investigations,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2022,"The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done offline, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.",75e3475cf49caf1dbbcad526b0132b455dc88dd5
Deep learning for AI,"[{'name': 'Yoshua Bengio', 'dblp_profile': 'https://dblp.org/pid/56/953.html'}, {'name': 'Yann LeCun', 'dblp_profile': 'https://dblp.org/pid/l/YannLeCun.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2021,How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?,87d5b61f5b6fdb0a57fc66b5c5bb428c398eaa86
Teaching with Commentaries,"[{'name': 'Aniruddh Raghu', 'dblp_profile': 'https://dblp.org/pid/200/8793.html'}, {'name': 'Maithra Raghu', 'dblp_profile': 'https://dblp.org/pid/163/6374.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'David Duvenaud', 'dblp_profile': 'https://dblp.org/pid/86/9380.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2021,"Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, meta-learned information helpful for training on a particular task or dataset. We present an efficient and scalable gradient-based method to learn commentaries, leveraging recent work on implicit differentiation. We explore diverse applications of commentaries, from learning weights for individual training examples, to parameterizing label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. In these settings, we find that commentaries can improve training speed and/or performance and also provide fundamental insights about the dataset and training process.",1ac1d3eb086c42d72a0509a42f744f626e8b5711
Unsupervised Part Representation by Flow Capsules,"[{'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}, {'name': 'Soroosh Yazdani', 'dblp_profile': 'https://dblp.org/pid/87/6842.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}]",2021,"Capsule networks are designed to parse an image into a hierarchy of objects, parts and relations. While promising, they remain limited by an inability to learn effective low level part descriptions. To address this issue we propose a novel self-supervised method for learning part descriptors of an image. During training, we exploit motion as a powerful perceptual cue for part definition, using an expressive decoder for part generation and layered image formation with occlusion. Experiments demonstrate robust part discovery in the presence of multiple objects, cluttered backgrounds, and significant occlusion. The resulting part descriptors, a.k.a. part capsules, are decoded into shape masks, filling in occluded pixels, along with relative depth on single images. We also report unsupervised object classification using our capsule parts in a stacked capsule autoencoder.",01a17af53eb629878ec9757086f344bb042730c9
Neural Additive Models: Interpretable Machine Learning with Neural Nets,"[{'name': 'Rishabh Agarwal', 'dblp_profile': 'https://dblp.org/pid/210/6453.html'}, {'name': 'Levi Melnick', 'dblp_profile': 'https://dblp.org/pid/260/5426.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Xuezhou Zhang', 'dblp_profile': 'https://dblp.org/pid/213/7993.html'}, {'name': 'Benjamin J. Lengerich', 'dblp_profile': 'https://dblp.org/pid/203/8210.html'}, {'name': 'Rich Caruana', 'dblp_profile': 'https://dblp.org/pid/c/RichCaruana.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2021,"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.",598658595e887f677967769ed11ba28158f6ea8a
Canonical Capsules: Self-Supervised Capsules in Canonical Pose,"[{'name': 'Weiwei Sun', 'dblp_profile': 'https://dblp.org/pid/63/6566.html'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}, {'name': 'Boyang Deng', 'dblp_profile': 'https://dblp.org/pid/203/8282.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Soroosh Yazdani', 'dblp_profile': 'https://dblp.org/pid/87/6842.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Kwang Moo Yi', 'dblp_profile': 'https://dblp.org/pid/30/5082.html'}]",2021,"We propose a self-supervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and self-supervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. To train our neural network we require neither classification labels nor manually-aligned training datasets. Yet, by learning an object-centric representation in a self-supervised manner, our method outperforms the state-of-the-art on 3D point cloud reconstruction, canonicalization, and unsupervised classification.",31abc1a357969844a759ad0b78c842029bae2a69
How to represent part-whole hierarchies in a neural network,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2021,"Abstract This article does not describe a working system. Instead, it presents a single idea about representation that allows advances made by several different groups to be combined into an imaginary system called GLOM.1 The advances include transformers, neural fields, contrastive representation learning, distillation, and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy that has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.",5cee90b85b88e4de1d51b2963613a48b68916ac7
Pix2seq: A Language Modeling Framework for Object Detection,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766-7.html'}, {'name': 'Saurabh Saxena', 'dblp_profile': 'https://dblp.org/pid/15/8791.html'}, {'name': 'Lala Li', 'dblp_profile': 'https://dblp.org/pid/49/7563.html'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2021,"We present Pix2Seq, a simple and generic framework for object detection. Unlike existing approaches that explicitly integrate prior knowledge about the task, we cast object detection as a language modeling task conditioned on the observed pixel inputs. Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete tokens, and we train a neural network to perceive the image and generate the desired sequence. Our approach is based mainly on the intuition that if a neural network knows about where and what the objects are, we just need to teach it how to read them out. Beyond the use of task-specific data augmentations, our approach makes minimal assumptions about the task, yet it achieves competitive results on the challenging COCO dataset, compared to highly specialized and well optimized detection algorithms.",19b3b074d38b250d024920732ae51a8ffa0996dd
CvxNet: Learnable Convex Decomposition,"[{'name': 'Boyang Deng', 'dblp_profile': 'https://dblp.org/pid/203/8282.html'}, {'name': 'Kyle Genova', 'dblp_profile': 'https://dblp.org/pid/164/5882.html'}, {'name': 'Soroosh Yazdani', 'dblp_profile': 'https://dblp.org/pid/87/6842.html'}, {'name': 'Sofien Bouaziz', 'dblp_profile': 'https://dblp.org/pid/22/9075.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}]",2020,"Any solid object can be decomposed into a collection of convex polytopes (in short, convexes). When a small number of convexes are used, such a decomposition can be thought of as a piece-wise approximation of the geometry. This decomposition is fundamental in computer graphics, where it provides one of the most common ways to approximate geometry, for example, in real-time physics simulation. A convex object also has the property of being simultaneously an explicit and implicit representation: one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull, or implicitly as the collection of half-space constraints or support functions. Their implicit representation makes them particularly well suited for neural network training, as they abstract away from the topology of the geometry they need to represent. However, at testing time, convexes can also generate explicit representations – polygonal meshes – which can then be used in any downstream application. We introduce a network architecture to represent a low dimensional family of convexes. This family is automatically derived via an auto-encoding process. We investigate the applications of this architecture including automatic convex decomposition, image to 3D reconstruction, and part-based shape retrieval.",cc5ba823330e57facf8ea04c873c824874dd13fd
NASA Neural Articulated Shape Approximation,"[{'name': 'Boyang Deng', 'dblp_profile': 'https://dblp.org/pid/203/8282.html'}, {'name': 'John P. Lewis', 'dblp_profile': 'https://dblp.org/pid/83/6305.html'}, {'name': 'Timothy Jeruzalski', 'dblp_profile': 'https://dblp.org/pid/173/2643.html'}, {'name': 'Gerard Pons-Moll', 'dblp_profile': 'https://dblp.org/pid/66/8652.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}]",2020,,
Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions,"[{'name': 'Yao Qin', 'dblp_profile': 'https://dblp.org/pid/66/10420.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Colin Raffel', 'dblp_profile': 'https://dblp.org/pid/149/0082.html'}, {'name': 'Garrison W. Cottrell', 'dblp_profile': 'https://dblp.org/pid/c/GWCottrell.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.",f368de6a7f90daec66e1eef7922773390b75fb9d
Imputer: Sequence Modelling via Imputation and Dynamic Programming,"[{'name': 'William Chan', 'dblp_profile': 'https://dblp.org/pid/58/2301.html'}, {'name': 'Chitwan Saharia', 'dblp_profile': 'https://dblp.org/pid/228/8172.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Navdeep Jaitly', 'dblp_profile': 'https://dblp.org/pid/04/6137.html'}]",2020,"This paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generative model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.",bb2bc99f8220fc681320c541940c99ae30b286d6
A Simple Framework for Contrastive Learning of Visual Representations,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",34733eaf66007516347a40ad5d9bbe1cc9dacb6b
Big Self-Supervised Models are Strong Semi-Supervised Learners,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Kevin Swersky', 'dblp_profile': 'https://dblp.org/pid/35/9381.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.",3e7f5f4382ac6f9c4fef6197dd21abf74456acd1
The Next Generation of Neural Networks,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"The most important unsolved problem with artificial neural networks is how to do unsupervised learning as effectively as the brain. There are currently two main approaches to unsupervised learning. In the first approach, exemplified by BERT and Variational Autoencoders, a deep neural network is used to reconstruct its input. This is problematic for images because the deepest layers of the network need to encode the fine details of the image. An alternative approach, introduced by Becker and Hinton in 1992, is to train two copies of a deep neural network to produce output vectors that have high mutual information when given two different crops of the same image as their inputs. This approach was designed to allow the representations to be untethered from irrelevant details of the input. The method of optimizing mutual information used by Becker and Hinton was flawed (for a subtle reason that I will explain) so Pacannaro and Hinton (2001) replaced it by a discriminative objective in which one vector representation must select a corresponding vector representation from among many alternatives. With faster hardware, contrastive learning of representations has recently become very popular and is proving to be very effective, but it suffers from a major flaw: To learn pairs of representation vectors that have N bits of mutual information we need to contrast the correct corresponding vector with about 2N incorrect alternatives. I will describe a novel and effective way of dealing with this limitation. I will also show that this leads to a simple way of implementing perceptual learning in cortex.",01c304c4c731705f371e0d0024a95b136a805d41
Subclass Distillation,"[{'name': 'Rafael Müller', 'dblp_profile': 'https://dblp.org/pid/242/8396.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"After a large ""teacher"" neural network has been trained on labeled data, the probabilities that the teacher assigns to incorrect classes reveal a lot of information about the way in which the teacher generalizes. By training a small ""student"" model to match these probabilities, it is possible to transfer most of the generalization ability of the teacher to the student, often producing a much better small model than directly training the student on the training data. The transfer works best when there are many possible classes because more is then revealed about the function learned by the teacher, but in cases where there are only a few possible classes we show that we can improve the transfer by forcing the teacher to divide each class into many subclasses that it invents during the supervised training. The student is then trained to match the subclass probabilities. For datasets where there are known, natural subclasses we demonstrate that the teacher learns similar subclasses and these improve distillation. For clickthrough datasets where the subclasses are unknown we demonstrate that subclass distillation allows the student to learn faster and better.",e93a2b3b4dbbf53bae23b45cc3a39e1eb1ee9c1e
A Simple Framework for Contrastive Learning of Visual Representations,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.",34733eaf66007516347a40ad5d9bbe1cc9dacb6b
Deflecting Adversarial Attacks,"[{'name': 'Yao Qin', 'dblp_profile': 'https://dblp.org/pid/66/10420.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Colin Raffel', 'dblp_profile': 'https://dblp.org/pid/149/0082.html'}, {'name': 'Garrison W. Cottrell', 'dblp_profile': 'https://dblp.org/pid/c/GWCottrell.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"There has been an ongoing cycle where stronger defenses against adversarial attacks are subsequently broken by a more advanced defense-aware attack. We present a new approach towards ending this cycle where we ""deflect'' adversarial attacks by causing the attacker to produce an input that semantically resembles the attack's target class. To this end, we first propose a stronger defense based on Capsule Networks that combines three detection mechanisms to achieve state-of-the-art detection performance on both standard and defense-aware attacks. We then show that undetected attacks against our defense often perceptually resemble the adversarial target class by performing a human study where participants are asked to label images produced by the attack. These attack images can no longer be called ""adversarial'' because our network classifies them the same way as humans do.",ac164903c111cfdebad6c8b5e8c37b4cd3e5cb9b
Imputer: Sequence Modelling via Imputation and Dynamic Programming,"[{'name': 'William Chan', 'dblp_profile': 'https://dblp.org/pid/58/2301.html'}, {'name': 'Chitwan Saharia', 'dblp_profile': 'https://dblp.org/pid/228/8172.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Navdeep Jaitly', 'dblp_profile': 'https://dblp.org/pid/04/6137.html'}]",2020,"This paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generative model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.",bb2bc99f8220fc681320c541940c99ae30b286d6
Neural Additive Models: Interpretable Machine Learning with Neural Nets,"[{'name': 'Rishabh Agarwal', 'dblp_profile': 'https://dblp.org/pid/210/6453.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Xuezhou Zhang', 'dblp_profile': 'https://dblp.org/pid/213/7993.html'}, {'name': 'Rich Caruana', 'dblp_profile': 'https://dblp.org/pid/c/RichCaruana.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.",598658595e887f677967769ed11ba28158f6ea8a
Big Self-Supervised Models are Strong Semi-Supervised Learners,"[{'name': 'Ting Chen', 'dblp_profile': 'https://dblp.org/pid/19/1766.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Kevin Swersky', 'dblp_profile': 'https://dblp.org/pid/35/9381.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by supervised fine-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to most previous approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on ImageNet. A key ingredient of our approach is the use of a big (deep and wide) network during pretraining and fine-tuning. We find that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) benefits from a bigger network. After fine-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classification accuracy by using the unlabeled examples for a second time, but in a task-specific way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2 (a modification of SimCLR), supervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining and transferring the task-specific knowledge. This procedure achieves 73.9\% ImageNet top-1 accuracy with just 1\% of the labels ($\le$13 labeled images per class) using ResNet-50, a $10\times$ improvement in label efficiency over the previous state-of-the-art. With 10\% of labels, ResNet-50 trained with our method achieves 77.5\% top-1 accuracy, outperforming standard supervised training with all of the labels.",3e7f5f4382ac6f9c4fef6197dd21abf74456acd1
Teaching with Commentaries,"[{'name': 'Aniruddh Raghu', 'dblp_profile': 'https://dblp.org/pid/200/8793.html'}, {'name': 'Maithra Raghu', 'dblp_profile': 'https://dblp.org/pid/163/6374.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'David Duvenaud', 'dblp_profile': 'https://dblp.org/pid/86/9380.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2020,"Effective training of deep neural networks can be challenging, and there remain many open questions on how to best learn these models. Recently developed methods to improve neural network training examine teaching: providing learned information during the training process to improve downstream model performance. In this paper, we take steps towards extending the scope of teaching. We propose a flexible teaching framework using commentaries, meta-learned information helpful for training on a particular task or dataset. We present an efficient and scalable gradient-based method to learn commentaries, leveraging recent work on implicit differentiation. We explore diverse applications of commentaries, from learning weights for individual training examples, to parameterizing label-dependent data augmentation policies, to representing attention masks that highlight salient image regions. In these settings, we find that commentaries can improve training speed and/or performance and also provide fundamental insights about the dataset and training process.",1ac1d3eb086c42d72a0509a42f744f626e8b5711
Unsupervised part representation by Flow Capsules,"[{'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}, {'name': 'Soroosh Yazdani', 'dblp_profile': 'https://dblp.org/pid/87/6842.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}]",2020,"Capsule networks are designed to parse an image into a hierarchy of objects, parts and relations. While promising, they remain limited by an inability to learn effective low level part descriptions. To address this issue we propose a novel self-supervised method for learning part descriptors of an image. During training, we exploit motion as a powerful perceptual cue for part definition, using an expressive decoder for part generation and layered image formation with occlusion. Experiments demonstrate robust part discovery in the presence of multiple objects, cluttered backgrounds, and significant occlusion. The resulting part descriptors, a.k.a. part capsules, are decoded into shape masks, filling in occluded pixels, along with relative depth on single images. We also report unsupervised object classification using our capsule parts in a stacked capsule autoencoder.",01a17af53eb629878ec9757086f344bb042730c9
Canonical Capsules: Unsupervised Capsules in Canonical Pose,"[{'name': 'Weiwei Sun', 'dblp_profile': 'https://dblp.org/pid/63/6566.html'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}, {'name': 'Boyang Deng', 'dblp_profile': 'https://dblp.org/pid/203/8282.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Soroosh Yazdani', 'dblp_profile': 'https://dblp.org/pid/87/6842.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Kwang Moo Yi', 'dblp_profile': 'https://dblp.org/pid/30/5082.html'}]",2020,"We propose an unsupervised capsule architecture for 3D point clouds. We compute capsule decompositions of objects through permutation-equivariant attention, and self-supervise the process by training with pairs of randomly rotated objects. Our key idea is to aggregate the attention masks into semantic keypoints, and use these to supervise a decomposition that satisfies the capsule invariance/equivariance properties. This not only enables the training of a semantically consistent decomposition, but also allows us to learn a canonicalization operation that enables object-centric reasoning. In doing so, we require neither classification labels nor manually-aligned training datasets to train. Yet, by learning an object-centric representation in an unsupervised manner, our method outperforms the state-of-the-art on 3D point cloud reconstruction, registration, and unsupervised classification. We will release the code and dataset to reproduce our results as soon as the paper is published.",16fd3bcea628a20d273c35d40447fba3b3aa4774
Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,"[{'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Nicolas Papernot', 'dblp_profile': 'https://dblp.org/pid/162/1405.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"We explore and expand the $\textit{Soft Nearest Neighbor Loss}$ to measure the $\textit{entanglement}$ of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that $\textit{maximizing}$ the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.",1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8
Similarity of Neural Network Representations Revisited,"[{'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Honglak Lee', 'dblp_profile': 'https://dblp.org/pid/58/2562.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.",726320cdbd04804ffa8f3a78c095bd1b55a2a695
When does label smoothing help?,"[{'name': 'Rafael Müller', 'dblp_profile': 'https://dblp.org/pid/242/8396.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",f8de25118af2abc4c48afb947d6ec298e05ef1e5
"Lookahead Optimizer: k steps forward, 1 step back","[{'name': 'Michael R. Zhang', 'dblp_profile': 'https://dblp.org/pid/245/2842.html'}, {'name': 'James Lucas', 'dblp_profile': 'https://dblp.org/pid/24/2474.html'}, {'name': 'Jimmy Ba', 'dblp_profile': 'https://dblp.org/pid/78/9117.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.",600be3dde18d1059c6b56170bd04ee65ce79a848
Stacked Capsule Autoencoders,"[{'name': 'Adam R. Kosiorek', 'dblp_profile': 'https://dblp.org/pid/202/1842.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%). The code is available at this https URL",46c53faeaf2f52215adb165559c5ce056a71146b
Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,"[{'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Nicolas Papernot', 'dblp_profile': 'https://dblp.org/pid/162/1405.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"We explore and expand the $\textit{Soft Nearest Neighbor Loss}$ to measure the $\textit{entanglement}$ of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that $\textit{maximizing}$ the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class.",1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8
Similarity of Neural Network Representations Revisited,"[{'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'Honglak Lee', 'dblp_profile': 'https://dblp.org/pid/58/2562.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.",726320cdbd04804ffa8f3a78c095bd1b55a2a695
Cerberus: A Multi-headed Derenderer,"[{'name': 'Boyang Deng', 'dblp_profile': 'https://dblp.org/pid/203/8282.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"To generalize to novel visual scenes with new viewpoints and new object poses, a visual system needs representations of the shapes of the parts of an object that are invariant to changes in viewpoint or pose. 3D graphics representations disentangle visual factors such as viewpoints and lighting from object structure in a natural way. It is possible to learn to invert the process that converts 3D graphics representations into 2D images, provided the 3D graphics representations are available as labels. When only the unlabeled images are available, however, learning to derender is much harder. We consider a simple model which is just a set of free floating parts. Each part has its own relation to the camera and its own triangular mesh which can be deformed to model the shape of the part. At test time, a neural network looks at a single image and extracts the shapes of the parts and their relations to the camera. Each part can be viewed as one head of a multi-headed derenderer. During training, the extracted parts are used as input to a differentiable 3D renderer and the reconstruction error is backpropagated to train the neural net. We make the learning task easier by encouraging the deformations of the part meshes to be invariant to changes in viewpoint and invariant to the changes in the relative positions of the parts that occur when the pose of an articulated body changes. Cerberus, our multi-headed derenderer, outperforms previous methods for extracting 3D parts from single images without part annotations, and it does quite well at extracting natural parts of human figures.",2adc55831bbb3586855cd48cca55187c7382bdce
Learning Sparse Networks Using Targeted Dropout,"[{'name': 'Aidan N. Gomez', 'dblp_profile': 'https://dblp.org/pid/202/2262.html'}, {'name': 'Ivan Zhang', 'dblp_profile': 'https://dblp.org/pid/213/8257.html'}, {'name': 'Kevin Swersky', 'dblp_profile': 'https://dblp.org/pid/35/9381.html'}, {'name': 'Yarin Gal', 'dblp_profile': 'https://dblp.org/pid/67/9076.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"Neural networks are easier to optimise when they have many more weights than are required for modelling the mapping from inputs to outputs. This suggests a two-stage learning procedure that first learns a large net and then prunes away connections or hidden units. But standard training does not necessarily encourage nets to be amenable to pruning. We introduce targeted dropout, a method for training a neural network so that it is robust to subsequent pruning. Before computing the gradients for each weight update, targeted dropout stochastically selects a set of units or weights to be dropped using a simple self-reinforcing sparsity criterion and then computes the gradients for the remaining weights. The resulting network is robust to post hoc pruning of weights or units that frequently occur in the dropped sets. The method improves upon more complicated sparsifying regularisers while being simple to implement and easy to tune.",13de3c06ef6dac1c296ada45df2be590f843edb7
When Does Label Smoothing Help?,"[{'name': 'Rafael Müller', 'dblp_profile': 'https://dblp.org/pid/242/8396.html'}, {'name': 'Simon Kornblith', 'dblp_profile': 'https://dblp.org/pid/220/4059.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.",f8de25118af2abc4c48afb947d6ec298e05ef1e5
Stacked Capsule Autoencoders,"[{'name': 'Adam R. Kosiorek', 'dblp_profile': 'https://dblp.org/pid/202/1842.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55%) and MNIST (98.7%). The code is available at this https URL",46c53faeaf2f52215adb165559c5ce056a71146b
Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions,"[{'name': 'Yao Qin', 'dblp_profile': 'https://dblp.org/pid/66/10420.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Colin Raffel', 'dblp_profile': 'https://dblp.org/pid/149/0082.html'}, {'name': 'Garrison W. Cottrell', 'dblp_profile': 'https://dblp.org/pid/c/GWCottrell.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2019,"Adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans. In this paper, we first detect adversarial examples or otherwise corrupted images based on a class-conditional reconstruction of the input. To specifically attack our detection mechanism, we propose the Reconstructive Attack which seeks both to cause a misclassification and a low reconstruction error. This reconstructive attack produces undetected adversarial examples but with much smaller success rate. Among all these attacks, we find that CapsNets always perform better than convolutional networks. Then, we diagnose the adversarial examples for CapsNets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class. Additionally, the resulting perturbations can cause the input image to appear visually more like the target class and hence become non-adversarial. This suggests that CapsNets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples.",f368de6a7f90daec66e1eef7922773390b75fb9d
"Lookahead Optimizer: k steps forward, 1 step back","[{'name': 'Michael R. Zhang', 'dblp_profile': 'https://dblp.org/pid/245/2842.html'}, {'name': 'James Lucas', 'dblp_profile': 'https://dblp.org/pid/24/2474.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Jimmy Ba', 'dblp_profile': 'https://dblp.org/pid/78/9117.html'}]",2019,"The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.",600be3dde18d1059c6b56170bd04ee65ce79a848
CvxNets: Learnable Convex Decomposition,"[{'name': 'Boyang Deng', 'dblp_profile': 'https://dblp.org/pid/203/8282.html'}, {'name': 'Kyle Genova', 'dblp_profile': 'https://dblp.org/pid/164/5882.html'}, {'name': 'Soroosh Yazdani', 'dblp_profile': 'https://dblp.org/pid/87/6842.html'}, {'name': 'Sofien Bouaziz', 'dblp_profile': 'https://dblp.org/pid/22/9075.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}]",2019,,
NASA: Neural Articulated Shape Approximation,"[{'name': 'Timothy Jeruzalski', 'dblp_profile': 'https://dblp.org/pid/173/2643.html'}, {'name': 'Boyang Deng', 'dblp_profile': 'https://dblp.org/pid/203/8282.html'}, {'name': 'Mohammad Norouzi', 'dblp_profile': 'https://dblp.org/pid/12/7659-2.html'}, {'name': 'John P. Lewis', 'dblp_profile': 'https://dblp.org/pid/83/6305.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Andrea Tagliasacchi', 'dblp_profile': 'https://dblp.org/pid/46/5514.html'}]",2019,,0d4eb14da547ef3dc5c8ecfa35b14937ecd21bc5
Who Said What: Modeling Individual Labelers Improves Classification,"[{'name': 'Melody Y. Guan', 'dblp_profile': 'https://dblp.org/pid/198/0982.html'}, {'name': 'Varun Gulshan', 'dblp_profile': 'https://dblp.org/pid/02/8654.html'}, {'name': 'Andrew M. Dai', 'dblp_profile': 'https://dblp.org/pid/59/9736.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2018,"
 
 Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010); Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.
 
",14898d3fad28202dd4330165bf6ccef4f3b01d45
Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search,"[{'name': 'Jamie Ryan Kiros', 'dblp_profile': 'https://dblp.org/pid/222/9495.html'}, {'name': 'William Chan', 'dblp_profile': 'https://dblp.org/pid/58/2301.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2018,"We introduce Picturebook, a large-scale lookup operation to ground language via ‘snapshots’ of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.",35ebe95db7ab148e25904604d3b06a9412f6b4a4
Large scale distributed neural network training through online distillation,"[{'name': 'Rohan Anil', 'dblp_profile': 'https://dblp.org/pid/182/1833.html'}, {'name': 'Gabriel Pereyra', 'dblp_profile': 'https://dblp.org/pid/170/0034.html'}, {'name': 'Alexandre Passos', 'dblp_profile': 'https://dblp.org/pid/47/10827.html'}, {'name': 'Róbert Ormándi', 'dblp_profile': 'https://dblp.org/pid/35/4647.html'}, {'name': 'George E. Dahl', 'dblp_profile': 'https://dblp.org/pid/10/7998.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2018,"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.",cc59b4b1eb7d4629f753bc24f029c5cced301381
Matrix capsules with EM routing,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}]",2018,,603caed9430283db6c7f43169555c8d18e97a281
Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures,"[{'name': 'Sergey Bartunov', 'dblp_profile': 'https://dblp.org/pid/27/10154.html'}, {'name': 'Adam Santoro', 'dblp_profile': 'https://dblp.org/pid/180/5951.html'}, {'name': 'Blake A. Richards', 'dblp_profile': 'https://dblp.org/pid/70/10850.html'}, {'name': 'Luke Marris', 'dblp_profile': 'https://dblp.org/pid/223/4422.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Timothy P. Lillicrap', 'dblp_profile': 'https://dblp.org/pid/37/10849.html'}]",2018,"The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.",c898161e3691625bcef780b78100f70e8bb94154
Large scale distributed neural network training through online distillation,"[{'name': 'Rohan Anil', 'dblp_profile': 'https://dblp.org/pid/182/1833.html'}, {'name': 'Gabriel Pereyra', 'dblp_profile': 'https://dblp.org/pid/170/0034.html'}, {'name': 'Alexandre Passos', 'dblp_profile': 'https://dblp.org/pid/47/10827.html'}, {'name': 'Róbert Ormándi', 'dblp_profile': 'https://dblp.org/pid/35/4647.html'}, {'name': 'George E. Dahl', 'dblp_profile': 'https://dblp.org/pid/10/7998.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2018,"Techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model. However, due to increased test-time cost (for ensembles) and increased complexity of the training pipeline (for distillation), these techniques are challenging to use in industrial settings. In this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multi-stage setup or many new hyperparameters. Our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast. Crucially, we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent. Two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made. These predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted. Our second claim is that online distillation is a cost-effective way to make the exact predictions of a model dramatically more reproducible. We support our claims using experiments on the Criteo Display Ad Challenge dataset, ImageNet, and the largest to-date dataset used for neural language modeling, containing $6\times 10^{11}$ tokens and based on the Common Crawl repository of web data.",cc59b4b1eb7d4629f753bc24f029c5cced301381
Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures,"[{'name': 'Sergey Bartunov', 'dblp_profile': 'https://dblp.org/pid/27/10154.html'}, {'name': 'Adam Santoro', 'dblp_profile': 'https://dblp.org/pid/180/5951.html'}, {'name': 'Blake A. Richards', 'dblp_profile': 'https://dblp.org/pid/70/10850.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Timothy P. Lillicrap', 'dblp_profile': 'https://dblp.org/pid/37/10849.html'}]",2018,"The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.",c898161e3691625bcef780b78100f70e8bb94154
DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules,"[{'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2018,"We present a simple technique that allows capsule models to detect adversarial images. In addition to being trained to classify images, the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct top-level capsule. Adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the top-level capsule for that class. We show that setting a threshold on the $l2$ distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets. The same technique works quite well for CNNs that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax. We then explore a stronger, white-box attack that takes the reconstruction error into account. This attack is able to fool our detection technique but in order to make the model change its prediction to another class, the attack must typically make the ""adversarial"" image resemble images of the other class.",5bc3f4b4f976ac888ea767cc12326b4e1348e03e
ImageNet classification with deep convolutional neural networks,"[{'name': 'Alex Krizhevsky', 'dblp_profile': 'https://dblp.org/pid/64/9381.html'}, {'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",abd1c342495432171beb7ca8fd9551ef13cbd0ff
Distilling a Neural Network Into a Soft Decision Tree,"[{'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.",bbfa39ebb84d40a5e8152546213510bc597dea4d
Regularizing Neural Networks by Penalizing Confident Output Distributions,"[{'name': 'Gabriel Pereyra', 'dblp_profile': 'https://dblp.org/pid/170/0034.html'}, {'name': 'George Tucker', 'dblp_profile': 'https://dblp.org/pid/135/5748.html'}, {'name': 'Jan Chorowski', 'dblp_profile': 'https://dblp.org/pid/02/9737.html'}, {'name': 'Lukasz Kaiser', 'dblp_profile': 'https://dblp.org/pid/39/1762.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.",6ce1922802169f757bbafc6e087cc274a867c763
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,"[{'name': 'Noam Shazeer', 'dblp_profile': 'https://dblp.org/pid/15/8264.html'}, {'name': 'Azalia Mirhoseini', 'dblp_profile': 'https://dblp.org/pid/18/8314.html'}, {'name': 'Krzysztof Maziarz', 'dblp_profile': 'https://dblp.org/pid/194/2971.html'}, {'name': 'Andy Davis', 'dblp_profile': 'https://dblp.org/pid/60/920.html'}, {'name': 'Quoc V. Le', 'dblp_profile': 'https://dblp.org/pid/29/6166.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Jeff Dean', 'dblp_profile': 'https://dblp.org/pid/d/JeffreyDean.html'}]",2017,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",510e26733aaff585d65701b9f1be7ca9d5afc586
Dynamic Routing Between Capsules,"[{'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",c4c06578f4870e4b126e6837907929f3c900b99f
Boltzmann Machines,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,,4fba57584addebdcdb259816f144088361b0ca7a
Deep Belief Nets,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,,7da2fef4667cb2dcb2efb48bdab3a2e2d7870d1c
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,"[{'name': 'Noam Shazeer', 'dblp_profile': 'https://dblp.org/pid/15/8264.html'}, {'name': 'Azalia Mirhoseini', 'dblp_profile': 'https://dblp.org/pid/18/8314.html'}, {'name': 'Krzysztof Maziarz', 'dblp_profile': 'https://dblp.org/pid/194/2971.html'}, {'name': 'Andy Davis', 'dblp_profile': 'https://dblp.org/pid/60/920.html'}, {'name': 'Quoc V. Le', 'dblp_profile': 'https://dblp.org/pid/29/6166.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Jeff Dean', 'dblp_profile': 'https://dblp.org/pid/d/JeffreyDean.html'}]",2017,"The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",510e26733aaff585d65701b9f1be7ca9d5afc586
Regularizing Neural Networks by Penalizing Confident Output Distributions,"[{'name': 'Gabriel Pereyra', 'dblp_profile': 'https://dblp.org/pid/170/0034.html'}, {'name': 'George Tucker', 'dblp_profile': 'https://dblp.org/pid/135/5748.html'}, {'name': 'Jan Chorowski', 'dblp_profile': 'https://dblp.org/pid/02/9737.html'}, {'name': 'Lukasz Kaiser', 'dblp_profile': 'https://dblp.org/pid/39/1762.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.",6ce1922802169f757bbafc6e087cc274a867c763
Who Said What: Modeling Individual Labelers Improves Classification,"[{'name': 'Melody Y. Guan', 'dblp_profile': 'https://dblp.org/pid/198/0982.html'}, {'name': 'Varun Gulshan', 'dblp_profile': 'https://dblp.org/pid/02/8654.html'}, {'name': 'Andrew M. Dai', 'dblp_profile': 'https://dblp.org/pid/59/9736.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"
 
 Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010); Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training.
 
",14898d3fad28202dd4330165bf6ccef4f3b01d45
Dynamic Routing Between Capsules,"[{'name': 'Sara Sabour', 'dblp_profile': 'https://dblp.org/pid/172/1315.html'}, {'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.",c4c06578f4870e4b126e6837907929f3c900b99f
Distilling a Neural Network Into a Soft Decision Tree,"[{'name': 'Nicholas Frosst', 'dblp_profile': 'https://dblp.org/pid/207/8270.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2017,"Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.",bbfa39ebb84d40a5e8152546213510bc597dea4d
"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models","[{'name': 'S. M. Ali Eslami', 'dblp_profile': 'https://dblp.org/pid/117/4847.html'}, {'name': 'Nicolas Heess', 'dblp_profile': 'https://dblp.org/pid/76/9181.html'}, {'name': 'Theophane Weber', 'dblp_profile': 'https://dblp.org/pid/17/7225.html'}, {'name': 'Yuval Tassa', 'dblp_profile': 'https://dblp.org/pid/20/4415.html'}, {'name': 'David Szepesvari', 'dblp_profile': 'https://dblp.org/pid/191/6739.html'}, {'name': 'Koray Kavukcuoglu', 'dblp_profile': 'https://dblp.org/pid/92/5497.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2016,"We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1
Using Fast Weights to Attend to the Recent Past,"[{'name': 'Jimmy Ba', 'dblp_profile': 'https://dblp.org/pid/78/9117.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Joel Z. Leibo', 'dblp_profile': 'https://dblp.org/pid/33/11107.html'}, {'name': 'Catalin Ionescu', 'dblp_profile': 'https://dblp.org/pid/38/8484.html'}]",2016,"Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ``fast weights'' can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proven helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.",c91ae35dbcb6d479580ecd235eabf98374acdb55
"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models","[{'name': 'S. M. Ali Eslami', 'dblp_profile': 'https://dblp.org/pid/117/4847.html'}, {'name': 'Nicolas Heess', 'dblp_profile': 'https://dblp.org/pid/76/9181.html'}, {'name': 'Theophane Weber', 'dblp_profile': 'https://dblp.org/pid/17/7225.html'}, {'name': 'Yuval Tassa', 'dblp_profile': 'https://dblp.org/pid/20/4415.html'}, {'name': 'Koray Kavukcuoglu', 'dblp_profile': 'https://dblp.org/pid/92/5497.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2016,"We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1
Layer Normalization,"[{'name': 'Lei Jimmy Ba', 'dblp_profile': 'https://dblp.org/pid/78/9117.html'}, {'name': 'Jamie Ryan Kiros', 'dblp_profile': 'https://dblp.org/pid/222/9495.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2016,"Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",97fb4e3d45bb098e27e0071448b6152217bd35a5
Using Fast Weights to Attend to the Recent Past,"[{'name': 'Jimmy Ba', 'dblp_profile': 'https://dblp.org/pid/78/9117.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Joel Z. Leibo', 'dblp_profile': 'https://dblp.org/pid/33/11107.html'}, {'name': 'Catalin Ionescu', 'dblp_profile': 'https://dblp.org/pid/38/8484.html'}]",2016,"Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ``fast weights'' can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proven helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.",c91ae35dbcb6d479580ecd235eabf98374acdb55
Guest Editorial: Deep Learning,"[{'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Yann LeCun', 'dblp_profile': 'https://dblp.org/pid/l/YannLeCun.html'}]",2015,,4b4a5f62b93812150d2a5f0b69daa35fbdb57fa9
Deep learning,"[{'name': 'Yann LeCun', 'dblp_profile': 'https://dblp.org/pid/l/YannLeCun.html'}, {'name': 'Yoshua Bengio', 'dblp_profile': 'https://dblp.org/pid/56/953.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2015,,d3a8e466d6da9e61ac4f8f1a812a69933dfbfae5
Grammar as a Foreign Language,"[{'name': 'Oriol Vinyals', 'dblp_profile': 'https://dblp.org/pid/05/726.html'}, {'name': 'Lukasz Kaiser', 'dblp_profile': 'https://dblp.org/pid/39/1762.html'}, {'name': 'Terry Koo', 'dblp_profile': 'https://dblp.org/pid/22/1028.html'}, {'name': 'Slav Petrov', 'dblp_profile': 'https://dblp.org/pid/18/5906.html'}, {'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2015,"Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.",47570e7f63e296f224a0e7f9a0d08b0de3cbaf40
Distilling the Knowledge in a Neural Network,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Oriol Vinyals', 'dblp_profile': 'https://dblp.org/pid/05/726.html'}, {'name': 'Jeffrey Dean', 'dblp_profile': 'https://dblp.org/pid/d/JeffreyDean.html'}]",2015,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",0c908739fbff75f03469d13d4a1a07de3414ee19
A Simple Way to Initialize Recurrent Networks of Rectified Linear Units,"[{'name': 'Quoc V. Le', 'dblp_profile': 'https://dblp.org/pid/29/6166.html'}, {'name': 'Navdeep Jaitly', 'dblp_profile': 'https://dblp.org/pid/04/6137.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2015,"Learning long term dependencies in recurrent networks is difficult due to vanishing and exploding gradients. To overcome this difficulty, researchers have developed sophisticated optimization techniques and network architectures. In this paper, we propose a simpler solution that use recurrent neural networks composed of rectified linear units. Key to our solution is the use of the identity matrix or its scaled version to initialize the recurrent weight matrix. We find that our solution is comparable to LSTM on our four benchmarks: two toy problems involving long-range temporal structures, a large language modeling problem and a benchmark speech recognition problem.",d46b81707786d18499f911b4ab72bb10c65406ba
Where Do Features Come From?,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2014,"It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights.",11de5b70918e4a118e8e49a3ae09110bbf59ea83
Dropout: a simple way to prevent neural networks from overfitting,"[{'name': 'Nitish Srivastava', 'dblp_profile': 'https://dblp.org/pid/00/11304.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Alex Krizhevsky', 'dblp_profile': 'https://dblp.org/pid/64/9381.html'}, {'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}]",2014,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ""thinned"" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",34f25a8704614163c4095b3ee2fc969b60de4698
Application of Deep Belief Networks for Natural Language Understanding,"[{'name': 'Ruhi Sarikaya', 'dblp_profile': 'https://dblp.org/pid/68/5953.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Anoop Deoras', 'dblp_profile': 'https://dblp.org/pid/55/8761.html'}]",2014,"Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.",76a33bd1593bd3c86238ef4b7e94b8d65c663180
Autoregressive product of multi-frame predictions can improve the accuracy of hybrid models,"[{'name': 'Navdeep Jaitly', 'dblp_profile': 'https://dblp.org/pid/04/6137.html'}, {'name': 'Vincent Vanhoucke', 'dblp_profile': 'https://dblp.org/pid/69/7157.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2014,"We describe a simple but effective way of using multi-frame targets to improve the accuracy of Artificial Neural NetworkHidden Markov Model (ANN-HMM) hybrid systems. In this approach a Deep Neural Network (DNN) is trained to predict the forced-alignment state of multiple frames using a separate softmax unit for each of the frames. This is in contrast to the usual method of training a DNN to predict only the state of the central frame. By itself this is not sufficient to improve accuracy of the system significantly. However, if we average the predictions for each frame from the different contexts it is associated with we achieve state of the art results on TIMIT using a fully connected Deep Neural Network without convolutional architectures or dropout training. On a 14 hour subset of Wall Street Journal (WSJ) using a context dependent DNN-HMM system it leads to a relative improvement of 6.4% on the dev set (testdev93) and 9.3% on test set (test-eval92).",c1b05fd52dca7ff0c4f45e29ec119d22e31a9ec3
Grammar as a Foreign Language,"[{'name': 'Oriol Vinyals', 'dblp_profile': 'https://dblp.org/pid/05/726.html'}, {'name': 'Lukasz Kaiser', 'dblp_profile': 'https://dblp.org/pid/39/1762.html'}, {'name': 'Terry Koo', 'dblp_profile': 'https://dblp.org/pid/22/1028.html'}, {'name': 'Slav Petrov', 'dblp_profile': 'https://dblp.org/pid/18/5906.html'}, {'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2014,"Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation.",47570e7f63e296f224a0e7f9a0d08b0de3cbaf40
Modeling Natural Images Using Gated MRFs,"[{'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Joshua M. Susskind', 'dblp_profile': 'https://dblp.org/pid/132/7797.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"This paper describes a Markov Random Field for real-valued image modeling that has two sets of latent variables. One set is used to gate the interactions between all pairs of pixels, while the second set determines the mean intensities of each pixel. This is a powerful model with a conditional distribution over the input that is Gaussian, with both mean and covariance determined by the configuration of latent variables, which is unlike previous models that were restricted to using Gaussians with either a fixed mean or a diagonal covariance matrix. Thanks to the increased flexibility, this gated MRF can generate more realistic samples after training on an unconstrained distribution of high-resolution natural images. Furthermore, the latent variables of the model can be inferred efficiently and can be used as very effective descriptors in recognition tasks. Both generation and discrimination drastically improve as layers of binary latent variables are added to the model, yielding a hierarchical model called a Deep Belief Network.",32ef19e90e7834ec09ef19fcef7cd2aa6eff85a9
On rectified linear units for speech processing,"[{'name': 'Matthew D. Zeiler', 'dblp_profile': 'https://dblp.org/pid/09/8653.html'}, {'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Rajat Monga', 'dblp_profile': 'https://dblp.org/pid/99/10669.html'}, {'name': 'Mark Z. Mao', 'dblp_profile': 'https://dblp.org/pid/64/8010.html'}, {'name': 'K. Yang', 'dblp_profile': 'https://dblp.org/pid/85/6981.html'}, {'name': 'Quoc Viet Le', 'dblp_profile': 'https://dblp.org/pid/29/6166.html'}, {'name': 'Patrick Nguyen', 'dblp_profile': 'https://dblp.org/pid/81/657.html'}, {'name': 'Andrew W. Senior', 'dblp_profile': 'https://dblp.org/pid/88/3071.html'}, {'name': 'Vincent Vanhoucke', 'dblp_profile': 'https://dblp.org/pid/69/7157.html'}, {'name': 'Jeffrey Dean', 'dblp_profile': 'https://dblp.org/pid/d/JeffreyDean.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.",64da1980714cfc130632c5b92b9d98c2f6763de6
Speech recognition with deep recurrent neural networks,"[{'name': 'Alex Graves', 'dblp_profile': 'https://dblp.org/pid/57/5177.html'}, {'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",4177ec52d1b80ed57f2e72b0f9a42365f1a8598d
New types of deep neural network learning for speech recognition and related applications: an overview,"[{'name': 'Li Deng', 'dblp_profile': 'https://dblp.org/pid/31/1974-1.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Brian Kingsbury', 'dblp_profile': 'https://dblp.org/pid/98/4359.html'}]",2013,"In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications,” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.",eb9243a3b98a819539ad57b7b4f05b969510d075
Improving deep neural networks for LVCSR using rectified linear units and dropout,"[{'name': 'George E. Dahl', 'dblp_profile': 'https://dblp.org/pid/10/7998.html'}, {'name': 'Tara N. Sainath', 'dblp_profile': 'https://dblp.org/pid/28/7825.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random “dropout” procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2% relative improvement over a DNN trained with sigmoid units, and a 14.4% relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code.",1a3c74c7b11ad5635570932577cdde2a3f7a6a5c
Tensor Analyzers,"[{'name': 'Yichuan Tang', 'dblp_profile': 'https://dblp.org/pid/04/8412.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"Factor Analysis is a statistical method that seeks to explain linear variations in data by using unobserved latent variables. Due to its additive nature, it is not suitable for modeling data that is generated by multiple groups of latent factors which interact multiplicatively. In this paper, we introduce Tensor Analyzers which are a multilinear generalization of Factor Analyzers. We describe an efficient way of sampling from the posterior distribution over factor values and we demonstrate that these samples can be used in the EM algorithm for learning interesting mixture models of natural image patches. Tensor Analyzers can also accurately recognize a face under significant pose and illumination variations when given only one previous image of that face. We also show that Tensor Analyzers can be trained in an unsupervised, semi-supervised, or fully supervised settings.",328760482f4d33b6b1422b8312dca37dc4b62b1f
On the importance of initialization and momentum in deep learning,"[{'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'James Martens', 'dblp_profile': 'https://dblp.org/pid/12/8412.html'}, {'name': 'George E. Dahl', 'dblp_profile': 'https://dblp.org/pid/10/7998.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. 
 
Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.",aa7bfd2304201afbb19971ebde87b17e40242e91
Using an autoencoder with deformable templates to discover features for automated speech recognition,"[{'name': 'Navdeep Jaitly', 'dblp_profile': 'https://dblp.org/pid/04/6137.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"In this paper we show how we can discover non-linear features of frames of spectrograms using a novel autoencoder. The autoencoder uses a neural network encoder that predicts how a set of prototypes called templates need to be transformed to reconstruct the data, and a decoder that is a function that performs this operation of transforming prototypes and reconstructing the input. We demonstrate this method on spectrograms from the TIMIT database. The features are used in a Deep Neural Network Hidden Markov Model (DNN-HMM) hybrid system for automatic speech recognition. On the TIMIT monophone recognition task we were able to achieve gains of 0.5% over Mel log spectra, by augmenting traditional the spectra with the predicted transformation parameters. Further, using the recently discovered ‘dropout’ training, we were able to achieve a phone error rate (PER) of 17.9% on the dev set and 19.5% on the test set, which, to our knowledge is the best reported number on this task using a hybrid system.",3fcb6838c2249d3cb4b1560f48ec3907ce6ff801
Modeling Documents with Deep Boltzmann Machines,"[{'name': 'Nitish Srivastava', 'dblp_profile': 'https://dblp.org/pid/00/11304.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"We introduce a type of Deep Boltzmann Machine (DBM) that is suitable for extracting distributed semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This enables an efficient pretraining algorithm and a state initialization scheme for fast inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.",43c0b8309d05102aa75980f6cd53e2e77f222a17
Discovering Multiple Constraints that are Frequently Approximately Satisfied,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}]",2013,"Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.",66e65f81f1f76fb3a7c8ab2d813362b924e2fa9b
Speech Recognition with Deep Recurrent Neural Networks,"[{'name': 'Alex Graves', 'dblp_profile': 'https://dblp.org/pid/57/5177.html'}, {'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.",4177ec52d1b80ed57f2e72b0f9a42365f1a8598d
Modeling Documents with Deep Boltzmann Machines,"[{'name': 'Nitish Srivastava', 'dblp_profile': 'https://dblp.org/pid/00/11304.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2013,"We introduce a type of Deep Boltzmann Machine (DBM) that is suitable for extracting distributed semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This enables an efficient pretraining algorithm and a state initialization scheme for fast inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.",43c0b8309d05102aa75980f6cd53e2e77f222a17
Visualizing non-metric similarities in multiple maps,"[{'name': 'Laurens van der Maaten', 'dblp_profile': 'https://dblp.org/pid/53/2650.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,,360e57c9c6ade296327521116af53f5aaa9f6d8a
An Efficient Learning Procedure for Deep Boltzmann Machines,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned.",d4599b177559dd5ede4dda9d6d96aa149fc71942
Introduction to the Special Section on Deep Learning for Speech and Language Processing,"[{'name': 'Dong Yu', 'dblp_profile': 'https://dblp.org/pid/71/4598-1.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Nelson Morgan', 'dblp_profile': 'https://dblp.org/pid/95/5514.html'}, {'name': 'Jen-Tzung Chien', 'dblp_profile': 'https://dblp.org/pid/03/3569.html'}, {'name': 'Shigeki Sagayama', 'dblp_profile': 'https://dblp.org/pid/76/5348.html'}]",2012,"Current speech recognition systems, for example, typically use Gaussian mixture models (GMMs), to estimate the observation (or emission) probabilities of hidden Markov models (HMMs), and GMMs are generative models that have only one layer of latent variables. Instead of developing more powerful models, most of the research effort has gone into finding better ways of estimating the GMM parameters so that error rates are decreased or the margin between different classes is increased. The same observation holds for natural language processing (NLP) in which maximum entropy (MaxEnt) models and conditional random fields (CRFs) have been popular for the last decade. Both of these approaches use shallow models whose success largely depends on the use of carefully handcrafted features.",57236bcb830af797396fefb0ac26fea9f0caeeba
Acoustic Modeling Using Deep Belief Networks,"[{'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'George E. Dahl', 'dblp_profile': 'https://dblp.org/pid/10/7998.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.",d2b62f77cb2864e465aa60bca6c26bb1d2f84963
Robust Boltzmann Machines for recognition and denoising,"[{'name': 'Yichuan Tang', 'dblp_profile': 'https://dblp.org/pid/04/8412.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"While Boltzmann Machines have been successful at unsupervised learning and density modeling of images and speech data, they can be very sensitive to noise in the data. In this paper, we introduce a novel model, the Robust Boltzmann Machine (RoBM), which allows Boltzmann Machines to be robust to corruptions. In the domain of visual recognition, the RoBM is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of Gaussians over pixels. Image denoising and in-painting correspond to posterior inference in the RoBM. Our model is trained in an unsupervised fashion with unlabeled noisy data and can learn the spatial structure of the occluders. Compared to standard algorithms, the RoBM is significantly better at recognition and denoising on several face databases.",53f0982422af3901346159d6ab11523c248f08e0
Understanding how Deep Belief Networks perform acoustic modelling,"[{'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Gerald Penn', 'dblp_profile': 'https://dblp.org/pid/37/1531.html'}]",2012,Deep Belief Networks (DBNs) are a very competitive alternative to Gaussian mixture models for relating states of a hidden Markov model to frames of coefficients derived from the acoustic input. They are competitive for three reasons: DBNs can be fine-tuned as neural networks; DBNs have many non-linear hidden layers; and DBNs are generatively pre-trained. This paper illustrates how each of these three aspects contributes to the DBN's good recognition performance using both phone recognition performance on the TIMIT corpus and a dimensionally reduced visualization of the relationships between the feature vectors learned by the DBNs that preserves the similarity structure of the feature vectors at multiple scales. The same two methods are also used to investigate the most suitable type of input representation for a DBN.,06c152df89ca6a1f1b8f8e139ddda82cd4539415
Learning to Label Aerial Images from Noisy Data,"[{'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"When training a system to label images, the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels, but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider.",cdfdaccd946dc2fe4863aed048b12b5d2282f285
Deep Mixtures of Factor Analysers,"[{'name': 'Yichuan Tang', 'dblp_profile': 'https://dblp.org/pid/04/8412.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.",f50dfcc143bc1cc8ded1d88d31a59140b0a0ebd8
Deep Lambertian Networks,"[{'name': 'Yichuan Tang', 'dblp_profile': 'https://dblp.org/pid/04/8412.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"Visual perception is a challenging problem in part due to illumination variations. A possible solution is to first estimate an illumination invariant representation before using it for recognition. The object albedo and surface normals are examples of such representations. In this paper, we introduce a multilayer generative model where the latent variables include the albedo, surface normals, and the light source. Combining Deep Belief Nets with the Lambertian reectance assumption, our model can learn good priors over the albedo from 2D images. Illumination variations can be explained by changing only the lighting latent variable in our model. By transferring learned knowledge from similar objects, albedo and surface normals estimation from a single image is possible in our model. Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one-shot face recognition.",43d36a22629114e14a0952675e15c9c76f1f024c
ImageNet Classification with Deep Convolutional Neural Networks,"[{'name': 'Alex Krizhevsky', 'dblp_profile': 'https://dblp.org/pid/64/9381.html'}, {'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",abd1c342495432171beb7ca8fd9551ef13cbd0ff
A Better Way to Pretrain Deep Boltzmann Machines,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models.",9e2fd6034db8d0c733cd17f8e76372b86b0d35bf
A Practical Guide to Training Restricted Boltzmann Machines,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,,e95d3934e51107da7610acd0b1bcb6551671f9f1
Conditional Restricted Boltzmann Machines for Structured Output Prediction,"[{'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Hugo Larochelle', 'dblp_profile': 'https://dblp.org/pid/86/3862.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.",94b0e8e97c19ad0977d26e3e355d3ae09ad49365
Products of Hidden Markov Models: It Takes N>1 to Tango,"[{'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"Products of Hidden Markov Models (PoHMMs) are an interesting class of generative models which have received little attention since their introduction. This may be in part due to their more computationally expensive gradient-based learning algorithm, and the intractability of computing the log likelihood of sequences under the model. In this paper, we demonstrate how the partition function can be estimated reliably via Annealed Importance Sampling. We perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. Our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make PoHMMs worth considering for complex time-series modeling tasks.",aa7d1cd5a750f4cfcf15f642bc788d4c8411795c
Deep Mixtures of Factor Analysers,"[{'name': 'Yichuan Tang', 'dblp_profile': 'https://dblp.org/pid/04/8412.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.",f50dfcc143bc1cc8ded1d88d31a59140b0a0ebd8
Improving neural networks by preventing co-adaptation of feature detectors,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Nitish Srivastava', 'dblp_profile': 'https://dblp.org/pid/00/11304.html'}, {'name': 'Alex Krizhevsky', 'dblp_profile': 'https://dblp.org/pid/64/9381.html'}, {'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}]",2012,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This ""overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random ""dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",0060745e006c5f14ec326904119dca19c6545e51
Efficient Parametric Projection Pursuit Density Estimation,"[{'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2012,"Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the ""undercomplete product of experts"" (UPoE), where each expert models a one dimensional projection of the data. The UPoE may be interpreted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models.",bd05feae0feb756bace09d6eedcd4d5fb7edff45
A better way to learn features: technical perspective,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2011,,
Two Distributed-State Models For Generating High-Dimensional Time Series,"[{'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Sam T. Roweis', 'dblp_profile': 'https://dblp.org/pid/r/SamTRoweis.html'}]",2011,"In this paper we develop a class of nonlinear generative models for high-dimensional time series. We first propose a model based on the restricted Boltzmann machine (RBM) that uses an undirected model with binary latent variables and real-valued ""visible"" variables. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. This ""conditional"" RBM (CRBM) makes on-line inference efficient and allows us to use a simple approximate learning procedure. We demonstrate the power of our approach by synthesizing various sequences from a model trained on motion capture data and by performing on-line filling in of data lost during capture. We extend the CRBM in a way that preserves its most important computational properties and introduces multiplicative three-way interactions that allow the effective interaction weight between two variables to be modulated by the dynamic state of a third variable. We introduce a factoring of the implied three-way weight tensor to permit a more compact parameterization. The resulting model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve its ability to blend motion styles or to transition smoothly among them. Videos and source code can be found at http://www.cs.nyu.edu/~gwtaylor/publications/jmlr2011.",92e0f569d8fb17559d580d1c3b16a70e682b48b9
Discovering Binary Codes for Documents by Learning Deep Generative Models,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}]",2011,"We describe a deep generative model in which the lowest layer represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed, top-down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF-IDF we achieve higher accuracy than TF-IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses, we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document.",05c6b2a59b021f2d5e5a580ded1681f8a1ae2a50
Modeling the joint density of two images under a variety of transformations,"[{'name': 'Joshua M. Susskind', 'dblp_profile': 'https://dblp.org/pid/132/7797.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Roland Memisevic', 'dblp_profile': 'https://dblp.org/pid/98/4508.html'}, {'name': 'Marc Pollefeys', 'dblp_profile': 'https://dblp.org/pid/p/MarcPollefeys.html'}]",2011,"We describe a generative model of the relationship between two images. The model is defined as a factored three-way Boltzmann machine, in which hidden variables collaborate to define the joint correlation matrix for image pairs. Modeling the joint distribution over pairs makes it possible to efficiently match images that are the same according to a learned measure of similarity. We apply the model to several face matching tasks, and show that it learns to represent the input images using task-specific basis functions. Matching performance is superior to previous similar generative models, including recent conditional models of transformations. We also show that the model can be used as a plug-in matching score to perform invariant classification.",6c036729284a73075d4f9b2ce7b87cd05fe2bbba
On deep generative models with applications to recognition,"[{'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Joshua M. Susskind', 'dblp_profile': 'https://dblp.org/pid/132/7797.html'}, {'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2011,"The most popular way to use probabilistic models in vision is first to extract some descriptors of small image patches or object parts using well-engineered features, and then to use statistical learning tools to model the dependencies among these features and eventual labels. Learning probabilistic models directly on the raw pixel values has proved to be much more difficult and is typically only used for regularizing discriminative methods. In this work, we use one of the best, pixel-level, generative models of natural images–a gated MRF–as the lowest level of a deep belief network (DBN) that has several hidden layers. We show that the resulting DBN is very good at coping with occlusion when predicting expression categories from face images, and it can produce features that perform comparably to SIFT descriptors for discriminating different types of scene. The generative ability of the model also makes it easy to see what information is captured and what is lost at each level of representation.",f93844c68d96f2f01da973b2ed3c236c8a369e57
Using very deep autoencoders for content-based image retrieval,"[{'name': 'Alex Krizhevsky', 'dblp_profile': 'https://dblp.org/pid/64/9381.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2011,"We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple di erent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.",88080d28536f36588740737f3b7a1f6c1a409654
Transforming Auto-Encoders,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Alex Krizhevsky', 'dblp_profile': 'https://dblp.org/pid/64/9381.html'}, {'name': 'Sida D. Wang', 'dblp_profile': 'https://dblp.org/pid/71/9736.html'}]",2011,,20f0357688876fa4662f806f985779dce6e24f3c
Deep Belief Networks using discriminative features for phone recognition,"[{'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'Tara N. Sainath', 'dblp_profile': 'https://dblp.org/pid/28/7825.html'}, {'name': 'George E. Dahl', 'dblp_profile': 'https://dblp.org/pid/10/7998.html'}, {'name': 'Bhuvana Ramabhadran', 'dblp_profile': 'https://dblp.org/pid/39/1849.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Michael A. Picheny', 'dblp_profile': 'https://dblp.org/pid/73/4588.html'}]",2011,"Deep Belief Networks (DBNs) are multi-layer generative models. They can be trained to model windows of coefficients extracted from speech and they discover multiple layers of features that capture the higher-order statistical structure of the data. These features can be used to initialize the hidden units of a feed-forward neural network that is then trained to predict the HMM state for the central frame of the window. Initializing with features that are good at generating speech makes the neural network perform much better than initializing with random weights. DBNs have already been used successfully for phone recognition with input coefficients that are MFCCs or filterbank outputs [1, 2]. In this paper, we demonstrate that they work even better when their inputs are speaker adaptive, discriminative features. On the standard TIMIT corpus, they give phone error rates of 19.6% using monophone HMMs and a bigram language model and 19.4% using monophone HMMs and a trigram language model.",be53d4def5e0601f2416e9345babc7ef1b30a664
Deep belief nets for natural language call-routing,"[{'name': 'Ruhi Sarikaya', 'dblp_profile': 'https://dblp.org/pid/68/5953.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Bhuvana Ramabhadran', 'dblp_profile': 'https://dblp.org/pid/39/1849.html'}]",2011,"This paper considers application of Deep Belief Nets (DBNs) to natural language call routing. DBNs have been successfully applied to a number of tasks, including image, audio and speech classification, thanks to the recent discovery of an efficient learning technique. DBNs learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms; Support Vector machines (SVM), Boosting and Maximum Entropy (MaxEnt). The DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models even though it currently uses an impoverished representation of the input.",4ca91c58eb35395e4a5fb5ffaee925e7c4f1ae81
Learning a better representation of speech soundwaves using restricted boltzmann machines,"[{'name': 'Navdeep Jaitly', 'dblp_profile': 'https://dblp.org/pid/04/6137.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2011,"State of the art speech recognition systems rely on preprocessed speech features such as Mel cepstrum or linear predictive coding coefficients that collapse high dimensional speech sound waves into low dimensional encodings. While these have been successfully applied in speech recognition systems, such low dimensional encodings may lose some relevant information and express other information in a way that makes it difficult to use for discrimination. Higher dimensional encodings could both improve performance in recognition tasks, and also be applied to speech synthesis by better modeling the statistical structure of the sound waves. In this paper we present a novel approach for modeling speech sound waves using a Restricted Boltzmann machine (RBM) with a novel type of hidden variable and we report initial results demonstrating phoneme recognition performance better than the current state-of-the-art for methods based on Mel cepstrum coefficients.",46af78834358337447001241cd2e18828ed926f0
Generating Text with Recurrent Neural Networks,"[{'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'James Martens', 'dblp_profile': 'https://dblp.org/pid/12/8412.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2011,"Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or ""gated"") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling – a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date.",e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de
Conditional Restricted Boltzmann Machines for Structured Output Prediction,"[{'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Hugo Larochelle', 'dblp_profile': 'https://dblp.org/pid/86/3862.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2011,"Conditional Restricted Boltzmann Machines (CRBMs) are rich probabilistic models that have recently been applied to a wide range of problems, including collaborative filtering, classification, and modeling motion capture data. While much progress has been made in training non-conditional RBMs, these algorithms are not applicable to conditional models and there has been almost no work on training and generating predictions from conditional RBMs for structured output problems. We first argue that standard Contrastive Divergence-based learning may not be suitable for training CRBMs. We then identify two distinct types of structured output prediction problems and propose an improved learning algorithm for each. The first problem type is one where the output space has arbitrary structure but the set of likely output configurations is relatively small, such as in multi-label classification. The second problem is one where the output space is arbitrarily structured but where the output space variability is much greater, such as in image denoising or pixel labeling. We show that the new learning algorithms can work much better than Contrastive Divergence on both types of problems.",94b0e8e97c19ad0977d26e3e355d3ae09ad49365
Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines,"[{'name': 'Roland Memisevic', 'dblp_profile': 'https://dblp.org/pid/98/4508.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"To allow the hidden units of a restricted Boltzmann machine to model the transformation between two successive images, Memisevic and Hinton (2007) introduced three-way multiplicative interactions that use the intensity of a pixel in the first image as a multiplicative gain on a learned, symmetric weight between a pixel in the second image and a hidden unit. This creates cubically many parameters, which form a three-dimensional interaction tensor. We describe a low-rank approximation to this interaction tensor that uses a sum of factors, each of which is a three-way outer product. This approximation allows efficient learning of transformations between larger image patches. Since each factor can be viewed as an image filter, the model as a whole learns optimal filter pairs for efficiently representing transformations. We demonstrate the learning of optimal filter pairs from various synthetic and real image sequences. We also show how learning about image transformations allows the model to perform a simple visual analogy task, and we show how a completely unsupervised network trained on transformations perceives multiple motions of transparent dot patterns in the same way as humans.",0eb2e4a205a628ab059cab41d3b772f614ad29f2
Comparing Classification Methods for Longitudinal fMRI Studies,"[{'name': 'Tanya Schmah', 'dblp_profile': 'https://dblp.org/pid/13/6735.html'}, {'name': 'Grigori Yourganov', 'dblp_profile': 'https://dblp.org/pid/66/8763.html'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Steven L. Small', 'dblp_profile': 'https://dblp.org/pid/20/5855.html'}, {'name': 'Stephen C. Strother', 'dblp_profile': 'https://dblp.org/pid/47/6792.html'}]",2010,"We compare 10 methods of classifying fMRI volumes by applying them to data from a longitudinal study of stroke recovery: adaptive Fisher's linear and quadratic discriminant; gaussian naive Bayes; support vector machines with linear, quadratic, and radial basis function (RBF) kernels; logistic regression; two novel methods based on pairs of restricted Boltzmann machines (RBM); and K-nearest neighbors. All methods were tested on three binary classification tasks, and their out-of-sample classification accuracies are compared. The relative performance of the methods varies considerably across subjects and classification tasks. The best overall performers were adaptive quadratic discriminant, support vector machines with RBF kernels, and generatively trained pairs of RBMs.",773f0addad7d7c3e4733d57a0b7d9d4b729ef1c4
Temporal-Kernel Recurrent Neural Networks,"[{'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,,da5d2b6344fce4314e5f05356d06cdf1aafeef1a
Dynamical binary latent variable models for 3D human pose tracking,"[{'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}, {'name': 'Leonid Sigal', 'dblp_profile': 'https://dblp.org/pid/09/4991.html'}, {'name': 'David J. Fleet', 'dblp_profile': 'https://dblp.org/pid/07/2099.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"We introduce a new class of probabilistic latent variable model called the Implicit Mixture of Conditional Restricted Boltzmann Machines (imCRBM) for use in human pose tracking. Key properties of the imCRBM are as follows: (1) learning is linear in the number of training exemplars so it can be learned from large datasets; (2) it learns coherent models of multiple activities; (3) it automatically discovers atomic “movemes” and (4) it can infer transitions between activities, even when such transitions are not present in the training set. We describe the model and how it is learned and we demonstrate its use in the context of Bayesian filtering for multi-view and monocular pose tracking. The model handles difficult scenarios including multiple activities and transitions among activities. We report state-of-the-art results on the HumanEva dataset.",20b97fd491a05b289dfd666a87c545664b25bb67
Modeling pixel means and covariances using factorized third-order boltzmann machines,"[{'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"Learning a generative model of natural images is a useful way of extracting features that capture interesting regularities. Previous work on learning such models has focused on methods in which the latent features are used to determine the mean and variance of each pixel independently, or on methods in which the hidden units determine the covariance matrix of a zero-mean Gaussian distribution. In this work, we propose a probabilistic model that combines these two approaches into a single framework. We represent each image using one set of binary latent features that model the image-specific covariance and a separate set that model the mean. We show that this approach provides a probabilistic framework for the widely used simple-cell complex-cell architecture, it produces very realistic samples of natural images and it extracts features that yield state-of-the-art recognition accuracy on the challenging CIFAR 10 dataset.",e2c04849a3802715d5a9d89179c9f161014d6c2a
Learning to Detect Roads in High-Resolution Aerial Images,"[{'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,,8b16106e35ff9ff97ffb73eb9c84b73ce1264c67
Phone recognition using Restricted Boltzmann Machines,"[{'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"For decades, Hidden Markov Models (HMMs) have been the state-of-the-art technique for acoustic modeling despite their unrealistic independence assumptions and the very limited representational capacity of their hidden states. Conditional Restricted Boltzmann Machines (CRBMs) have recently proved to be very effective for modeling motion capture sequences and this paper investigates the application of this more powerful type of generative model to acoustic modeling. On the standard TIMIT corpus, one type of CRBM outperforms HMMs and is comparable with the best other methods, achieving a phone error rate (PER) of 26.7% on the TIMIT core test set.",1603a40b7bb56d563d9401f0d24c67d428e509f2
Rectified Linear Units Improve Restricted Boltzmann Machines,"[{'name': 'Vinod Nair', 'dblp_profile': 'https://dblp.org/pid/20/5283.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ""Stepped Sigmoid Units"" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",a538b05ebb01a40323997629e171c91aa28b8e2f
Binary coding of speech spectrograms using a deep auto-encoder,"[{'name': 'Li Deng', 'dblp_profile': 'https://dblp.org/pid/31/1974-1.html'}, {'name': 'Michael L. Seltzer', 'dblp_profile': 'https://dblp.org/pid/69/172.html'}, {'name': 'Dong Yu', 'dblp_profile': 'https://dblp.org/pid/71/4598-1.html'}, {'name': 'Alex Acero', 'dblp_profile': 'https://dblp.org/pid/a/AlexAcero.html'}, {'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"This paper reports our recent exploration of the layer-by-layer learning strategy for training a multi-layer generative model of patches of speech spectrograms. The top layer of the generative model learns binary codes that can be used for efficient compression of speech and could also be used for scalable speech recognition or rapid speech content retrieval. Each layer of the generative model is fully connected to the layer below and the weights on these connections are pretrained efficiently by using the contrastive divergence approximation to the log likelihood gradient. After layer-bylayer pre-training we “unroll” the generative model to form a deep auto-encoder, whose parameters are then fine-tuned using back-propagation. To reconstruct the full-length speech spectrogram, individual spectrogram segments predicted by their respective binary codes are combined using an overlapand-add method. Experimental results on speech spectrogram coding demonstrate that the binary codes produce a logspectral distortion that is approximately 2 dB lower than a subband vector quantization technique over the entire frequency range of wide-band speech. Index Terms: deep learning, speech feature extraction, neural networks, auto-encoder, binary codes, Boltzmann machine",e3c1bf806c325f306e5084c3bd332b83d2077e2a
Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine,"[{'name': 'George E. Dahl', 'dblp_profile': 'https://dblp.org/pid/10/7998.html'}, {'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Abdel-rahman Mohamed', 'dblp_profile': 'https://dblp.org/pid/28/8759.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5%, which is superior to all published results on speaker-independent TIMIT to date.",90b63e917d5737b06357d50aa729619e933d9614
Learning to combine foveal glimpses with a third-order Boltzmann machine,"[{'name': 'Hugo Larochelle', 'dblp_profile': 'https://dblp.org/pid/86/3862.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the ""glimpse"" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images.",0bff8898e3ebb1ab67fd20b5db00c6cb1938e6c3
Gated Softmax Classification,"[{'name': 'Roland Memisevic', 'dblp_profile': 'https://dblp.org/pid/98/4508.html'}, {'name': 'Christopher Zach', 'dblp_profile': 'https://dblp.org/pid/93/4824.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Marc Pollefeys', 'dblp_profile': 'https://dblp.org/pid/p/MarcPollefeys.html'}]",2010,"We describe a ""log-bilinear"" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs, backpropagation, and deep belief nets.",1ca4f8711e0c5ac77a7dfd8e5916d9d4e4268719
Generating more realistic images using gated MRF's,"[{'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Volodymyr Mnih', 'dblp_profile': 'https://dblp.org/pid/04/1930.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting. A more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated MRF learns more efficient internal representations, as demonstrated in several recognition tasks.",779e5beb515ed26c47dbfc08304fe49233063c1b
Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images,"[{'name': ""Marc'Aurelio Ranzato"", 'dblp_profile': 'https://dblp.org/pid/28/1732.html'}, {'name': 'Alex Krizhevsky', 'dblp_profile': 'https://dblp.org/pid/64/9381.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,"Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the “tiny images” data set. Even better features are obtained by then using standard binary RBM’s to learn a deeper model.",e7c64258997838087c9ba4e87225627b015122b2
Boltzmann Machines,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,,4fba57584addebdcdb259816f144088361b0ca7a
Deep Belief Nets,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2010,,7da2fef4667cb2dcb2efb48bdab3a2e2d7870d1c
Semantic hashing,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,,cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a
Improving a statistical language model through non-linear prediction,"[{'name': 'Andriy Mnih', 'dblp_profile': 'https://dblp.org/pid/23/1295.html'}, {'name': 'Zhang Yuecheng', 'dblp_profile': 'https://dblp.org/pid/92/6815.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,,050f5cba0979d045a300d633ab28b77756ebde1b
Deep belief networks,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"The important aspect of this layer-wise training procedure is that, provided the number of features per layer does not decrease, [6] showed that each extra layer increases a variational lower bound on the log probability of data. So layer-by-layer training can be repeated several times1 to learn a deep, hierarchical model in which each layer of features captures strong high-order correlations between the activities of features in the layer below. We will discuss three ideas based on greedily learning a hierarchy of features:",79dc4da8f131ff26046d5564e5dedb1c5ce72c6a
Learning Generative Texture Models with extended Fields-of-Experts,"[{'name': 'Nicolas Heess', 'dblp_profile': 'https://dblp.org/pid/76/9181.html'}, {'name': 'Christopher K. I. Williams', 'dblp_profile': 'https://dblp.org/pid/w/ChristopherKIWilliams.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"We evaluate the ability of the popular Field-of-Experts (FoE) to model structure in images. As a test case we focus on modeling synthetic and natural textures. We find that even for modeling single textures, the FoE provides insufficient flexibility to learn good generative models ‐ it does not perform any better than the much simpler Gaussian FoE. We propose an extended version of the FoE (allowing for bimodal potentials) and demonstrate that this novel formulation, when trained with a better approximation of the likelihood gradient, gives rise to a more powerful generative model of specific visual structure that produces significantly better results for the texture task.",d15cdfe4bb19c7da564d4f9a8c8bae41e4f146f7
Modeling pigeon behavior using a Conditional Restricted Boltzmann Machine,"[{'name': 'Matthew D. Zeiler', 'dblp_profile': 'https://dblp.org/pid/09/8653.html'}, {'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}, {'name': 'Nikolaus F. Troje', 'dblp_profile': 'https://dblp.org/pid/06/6143.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"In an effort to better understand the complex courtship be- haviour of pigeons, we have built a model learned from motion capture data. We employ a Conditional Restricted Boltzmann Machine (CRBM) with binary latent features and real-valued visible units. The units are conditioned on information from previous time steps to capture dynam- ics. We validate a trained model by quantifying the characteristic ""head- bobbing"" present in pigeons. We also show how to predict missing data by marginalizing out the hidden variables and minimizing free energy.",3470bf95fd9e77e8aa45002d5b60c6baeaff0ed3
Workshop summary: Workshop on learning feature hierarchies,"[{'name': 'Kai Yu', 'dblp_profile': 'https://dblp.org/pid/197/1322.html'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Yann LeCun', 'dblp_profile': 'https://dblp.org/pid/l/YannLeCun.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Yoshua Bengio', 'dblp_profile': 'https://dblp.org/pid/56/953.html'}]",2009,,7c488cbc4103524b27f42254e9455429b23d92ca
Factored conditional restricted Boltzmann Machines for modeling motion style,"[{'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"The Conditional Restricted Boltzmann Machine (CRBM) is a recently proposed model for time series that has a rich, distributed hidden state and permits simple, exact inference. We present a new model, based on the CRBM that preserves its most important computational properties and includes multiplicative three-way interactions that allow the effective interaction weight between two units to be modulated by the dynamic state of a third unit. We factor the three-way weight tensor implied by the multiplicative model, reducing the number of parameters from O(N3) to O(N2). The result is an efficient, compact model whose effectiveness we demonstrate by modeling human motion. Like the CRBM, our model can capture diverse styles of motion with a single set of parameters, and the three-way interactions greatly improve the model's ability to blend motion styles or to transition smoothly among them.",346fbcffe4237aa60e8bcb3d4294a8b99436f1d0
Using fast weights to improve persistent contrastive divergence,"[{'name': 'Tijmen Tieleman', 'dblp_profile': 'https://dblp.org/pid/25/6375.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent ""fantasy particles"" that are not reinitialized to data points after each weight update. With sufficiently small weight updates, the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast, and using this insight we develop an even faster mixing chain that uses an auxiliary set of ""fast weights"" to implement a temporary overlay on the energy landscape. The fast weights learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model.",03057ea57d9f2d9bbc8a141d51f76d5bbc715234
3D Object Recognition with Deep Belief Nets,"[{'name': 'Vinod Nair', 'dblp_profile': 'https://dblp.org/pid/20/5283.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error.",5a2668bf420d8509a4dfa28e1cdcdac14c649975
Zero-shot Learning with Semantic Output Codes,"[{'name': 'Mark Palatucci', 'dblp_profile': 'https://dblp.org/pid/37/2638.html'}, {'name': 'Dean Pomerleau', 'dblp_profile': 'https://dblp.org/pid/80/3380.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Tom M. Mitchell', 'dblp_profile': 'https://dblp.org/pid/81/1460.html'}]",2009,"We consider the problem of zero-shot learning, where the goal is to learn a classifier f : X → Y that must predict novel values of Y that were omitted from the training set. To achieve this, we define the notion of a semantic output code classifier (SOC) which utilizes a knowledge base of semantic properties of Y to extrapolate to novel classes. We provide a formalism for this type of classifier and study its theoretical properties in a PAC framework, showing conditions under which the classifier can accurately predict novel classes. As a case study, we build a SOC classifier for a neural decoding task and show that it can often predict words that people are thinking about from functional magnetic resonance images (fMRI) of their neural activity, even without training examples for those words.",0f6911bc1e6abee8bbf9dd3f8d54d40466429da7
Replicated Softmax: an Undirected Topic Model,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"We introduce a two-layer undirected graphical model, called a ""Replicated Softmax"", that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efficient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.",b32de117302258dd29919435cd001a8bcdfee3b3
Products of Hidden Markov Models: It Takes N>1 to Tango,"[{'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"Products of Hidden Markov Models (PoHMMs) are an interesting class of generative models which have received little attention since their introduction. This may be in part due to their more computationally expensive gradient-based learning algorithm, and the intractability of computing the log likelihood of sequences under the model. In this paper, we demonstrate how the partition function can be estimated reliably via Annealed Importance Sampling. We perform experiments using contrastive divergence learning on rainfall data and data captured from pairs of people dancing. Our results suggest that advances in learning and evaluation for undirected graphical models and recent increases in available computing power make PoHMMs worth considering for complex time-series modeling tasks.",aa7d1cd5a750f4cfcf15f642bc788d4c8411795c
Deep Boltzmann Machines,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2009,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",85021c84383d18a7a4434d76dc8135fc6bdc0aa6
"Deep, Narrow Sigmoid Belief Networks Are Universal Approximators","[{'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2008,"In this note, we show that exponentially deep belief networks can approximate any distribution over binary vectors to arbitrary accuracy, even when the width of each layer is limited to the dimensionality of the data. We further show that such networks can be greedily learned in an easy yet impractical way.",22b5988462414ad98151bf5e6e9ebb340a0bfbb8
Improving a statistical language model by modulating the effects of context words,"[{'name': 'Zhang Yuecheng', 'dblp_profile': 'https://dblp.org/pid/92/6815.html'}, {'name': 'Andriy Mnih', 'dblp_profile': 'https://dblp.org/pid/23/1295.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2008,"We show how to improve a state-of-the-art neural network language model that converts the previous ""context"" words into feature vectors and combines these feature vectors to predict the feature vector of the next word. Significant improvements in predictive accuracy are achieved by using higher-level features to modulate the effects of the con- text words. This is more effective than using the higher-level features to directly predict the feature vector of the next word, but it is also possible to combine both methods.",70c9a8972e189d3a27f55394587f6386beb47ca1
Analysis-by-Synthesis by Learning to Invert Generative Black Boxes,"[{'name': 'Vinod Nair', 'dblp_profile': 'https://dblp.org/pid/20/5283.html'}, {'name': 'Joshua M. Susskind', 'dblp_profile': 'https://dblp.org/pid/132/7797.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2008,,8b1d79fd4db235be3920be043215664e1f36754b
A Scalable Hierarchical Distributed Language Model,"[{'name': 'Andriy Mnih', 'dblp_profile': 'https://dblp.org/pid/23/1295.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2008,"Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.",a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb
Implicit Mixtures of Restricted Boltzmann Machines,"[{'name': 'Vinod Nair', 'dblp_profile': 'https://dblp.org/pid/20/5283.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2008,"We present a mixture model whose components are Restricted Boltzmann Machines (RBMs). This possibility has not been considered before because computing the partition function of an RBM is intractable, which appears to make learning a mixture of RBMs intractable as well. Surprisingly, when formulated as a third-order Boltzmann machine, such a mixture model can be learned tractably using contrastive divergence. The energy function of the model captures three-way interactions among visible units, hidden units, and a single hidden discrete variable that represents the cluster label. The distinguishing feature of this model is that, unlike other mixture models, the mixing proportions are not explicitly parameterized. Instead, they are defined implicitly via the energy function and depend on all the parameters in the model. We present results for the MNIST and NORB datasets showing that the implicit mixture of RBMs learns clusters that reflect the class structure in the data.",0b718a3f9dae8abc741411aed5fe5d423079200f
Generative versus discriminative training of RBMs for classification of fMRI images,"[{'name': 'Tanya Schmah', 'dblp_profile': 'https://dblp.org/pid/13/6735.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Steven L. Small', 'dblp_profile': 'https://dblp.org/pid/20/5855.html'}, {'name': 'Stephen C. Strother', 'dblp_profile': 'https://dblp.org/pid/47/6792.html'}]",2008,"Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overfitting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classification task for which logistic regression performs poorly, even when L1- or L2- regularized. We show that much better discrimination can be achieved by fitting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training.",4e3aaac4439825650480f9cb914aa895d55d1e13
Using matrices to model symbolic relationship,"[{'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2008,"We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn first-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has_wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, -3) ∈ inverse or (has_husband, has_wife) ∈ higher_oppsex. We further demonstrate that the system understands how higher-order propositions are related to first-order ones by showing that it can correctly answer questions about first-order propositions involving the relations +3 or has_wife even though it has not been trained on any first-order examples involving these relations.",f85bc8fb86979c8b1b5f7d145c37930c10ace7ce
The Recurrent Temporal Restricted Boltzmann Machine,"[{'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}]",2008,"The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.",0228810a988f6b8f06337e14f564e2fd3f6e1056
Boltzmann machine,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,,3f0b88c45b16aa85652b1e1cc531eec378fc32a9
Unsupervised Learning of Image Transformations,"[{'name': 'Roland Memisevic', 'dblp_profile': 'https://dblp.org/pid/98/4508.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,"We describe a probabilistic model for learning rich, distributed representations of image transformations. The basic model is defined as a gated conditional random field that is trained to predict transformations of its inputs using a factorial set of latent variables. Inference in the model consists in extracting the transformation, given a pair of images, and can be performed exactly and efficiently. We show that, when trained on natural videos, the model develops domain specific motion features, in the form of fields of locally transformed edge filters. When trained on affine, or more general, transformations of still images, the model develops codes for these transformations, and can subsequently perform recognition tasks that are invariant under these transformations. It can also fantasize new transformations on previously unseen images. We describe several variations of the basic model and provide experimental results that demonstrate its applicability to a variety of tasks.",2ac91e028cdc602695b46bd1f372c03b4d2776cf
Three new graphical models for statistical language modelling,"[{'name': 'Andriy Mnih', 'dblp_profile': 'https://dblp.org/pid/23/1295.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best n-gram models.,bd7d93193aad6c4b71cc8942e808753019e87706
Restricted Boltzmann machines for collaborative filtering,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Andriy Mnih', 'dblp_profile': 'https://dblp.org/pid/23/1295.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",1626c940a64ad96a7ed53d7d6c0df63c6696956b
Modeling image patches with a directed hierarchy of Markov random fields,"[{'name': 'Simon Osindero', 'dblp_profile': 'https://dblp.org/pid/05/5467.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,"We describe an efficient learning procedure for multilayer generative models that combine the best aspects of Markov random fields and deep, directed belief nets. The generative models can be learned one layer at a time and when learning is complete they have a very fast inference procedure for computing a good approximation to the posterior distribution in all of the hidden layers. Each hidden layer has its own MRF whose energy function is modulated by the top-down directed connections from the layer above. To generate from the model, each layer in turn must settle to equilibrium given its top-down input. We show that this type of model is good at capturing the statistics of patches of natural images.",80c330eee12decb84aaebcc85dc7ce414134ad61
Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,"We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.",f2e95236f0fccc0b70e757ac2ebbc79b7f51de0a
Visualizing Similarity Data with a Mixture of Maps,"[{'name': 'James Cook', 'dblp_profile': 'https://dblp.org/pid/58/3899.html'}, {'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Andriy Mnih', 'dblp_profile': 'https://dblp.org/pid/23/1295.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,"We show how to visualize a set of pairwise similarities between objects by using several different two-dimensional maps, each of which captures different aspects of the similarity structure. When the objects are ambiguous words, for example, different senses of a word occur in different maps, so “river” and “loan” can both be close to “bank” without being at all close to each other. Aspect maps resemble clustering because they model pair-wise similarities as a mixture of different types of similarity, but they also resemble local multi-dimensional scaling because they model each type of similarity by a twodimensional map. We demonstrate our method on a toy example, a database of human wordassociation data, a large set of images of handwritten digits, and a set of feature vectors that represent words.",1fa265cca12dc92d5f1850d47e1bd338f924adf1
Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure,"[{'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,"We show how to pretrain and ﬁne-tune a multilayer neural network to learn a nonlinear transformation from the input space to a low-dimensional feature space in which K-nearest neighbour classiﬁcation performs well. We also show how the non-linear transformation can be improved using unlabeled data. Our method achieves a much lower error rate than Support Vector Machines or standard backpropagation on a widely used version of the MNIST handwritten digit recognition task. If some of the dimensions of the low-dimensional feature space are not used for nearest neighbor classiﬁcation, our method uses these dimensions to explicitly rep-resent transformations of the digits that do not affect their identity.",ad33d1fa8628cb55c32fb52feb537f65184c3b29
Learning Multilevel Distributed Representations for High-Dimensional Sequences,"[{'name': 'Ilya Sutskever', 'dblp_profile': 'https://dblp.org/pid/60/5276.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2007,"We describe a new family of non-linear sequence models that are substantially more powerful than hidden Markov models or linear dynamical systems. Our models have simple approximate inference and learning procedures that work well in practice. Multilevel representations of sequential data can be learned one hidden layer at a time, and adding extra hidden layers improves the resulting generative models. The models can be trained with very high-dimensional, very non-linear data such as raw pixel sequences. Their performance is demonstrated using synthetic video sequences of two balls bouncing in a box.",c74e230a5a6fd5e2db6ace765ce38afe65f96214
Unsupervised Discovery of Nonlinear Structure Using Contrastive Backpropagation,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Simon Osindero', 'dblp_profile': 'https://dblp.org/pid/05/5467.html'}, {'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}]",2006,"We describe a way of modeling high-dimensional data vectors by using an unsupervised, nonlinear, multilayer neural network in which the activity of each neuron-like unit makes an additive contribution to a global energy score that indicates how surprised the network is by the data vector. The connection weights that determine how the activity of each unit depends on the activities in earlier layers are learned by minimizing the energy assigned to data vectors that are actually observed and maximizing the energy assigned to ""confabulations"" that are generated by perturbing an observed data vector in a direction that decreases its energy under the current model.",98a3c337a435553add253eb1af71eb9fc998bf5e
Topographic Product Models Applied to Natural Scene Statistics,"[{'name': 'Simon Osindero', 'dblp_profile': 'https://dblp.org/pid/05/5467.html'}, {'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2006,"We present an energy-based model that uses a product of generalized Student-t distributions to capture the statistical structure in data sets. This model is inspired by and particularly applicable to natural data sets such as images. We begin by providing the mathematical framework, where we discuss complete and overcomplete models and provide algorithms for training these models from data. Using patches of natural scenes, we demonstrate that our approach represents a viable alternative to independent component analysis as an interpretive model of biological visual systems. Although the two approaches are similar in flavor, there are also important differences, particularly when the representations are overcomplete. By constraining the interactions within our model, we are also able to study the topographic organization of Gabor-like receptive fields that our model learns. Finally, we discuss the relation of our new approach to previous workin particular, gaussian scale mixture models and variants of independent components analysis.",d8d01934cb26064b253dbd0f1627519133c3df3e
A Fast Learning Algorithm for Deep Belief Nets,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Simon Osindero', 'dblp_profile': 'https://dblp.org/pid/05/5467.html'}, {'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}]",2006,"We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.",8978cf7574ceb35f4c3096be768c7547b28a35d0
Modeling Human Motion Using Binary Latent Variables,"[{'name': 'Graham W. Taylor', 'dblp_profile': 'https://dblp.org/pid/17/1633.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Sam T. Roweis', 'dblp_profile': 'https://dblp.org/pid/r/SamTRoweis.html'}]",2006,"We propose a non-linear generative model for human motion data that uses an undirected model with binary latent variables and real-valued “visible” variables that represent joint angles. The latent and visible variables at each time step receive directed connections from the visible variables at the last few time-steps. Such an architecture makes on-line inference efficient and allows us to use a simple approximate learning procedure. After training, the model finds a single set of parameters that simultaneously capture several different kinds of motion. We demonstrate the power of our approach by synthesizing various motion sequences and by performing on-line filling in of data lost during motion capture.",576104ba976841628c67d2d794c9a64ba876eb87
Improving dimensionality reduction with spectral gradient descent,"[{'name': 'Roland Memisevic', 'dblp_profile': 'https://dblp.org/pid/98/4508.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2005,,878f90055f639711f29dd566ab341697a13f9a1b
On Contrastive Divergence Learning,"[{'name': 'Miguel Á. Carreira-Perpiñán', 'dblp_profile': 'https://dblp.org/pid/23/5257.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2005,"Maximum-likelihood (ML) learning of Markov random fields is challenging because it requires estimates of averages that have an exponential number of terms. Markov chain Monte Carlo methods typically take a long time to converge on unbiased estimates, but Hinton (2002) showed that if the Markov chain is only run for a few steps, the learning can still work well and it approximately minimizes a different function called “contrastive divergence” (CD). CD learning has been successfully applied to various types of random fields. Here, we study the properties of CD learning and show that it provides biased estimates in general, but that the bias is typically very small. Fast CD learning can therefore be used to get close to an ML solution and slow ML learning can then be used to fine-tune the CD solution. Consider a probability distribution over a vector x (assumed discrete w.l.o.g.) and with parameters W p(x;W) = 1 Z(W) e (1) where Z(W) = ∑ x e −E(x;W) is a normalisation constant and E(x;W) is an energy function. This class of random-field distributions has found many practical applications (Li, 2001; Winkler, 2002; Teh et al., 2003; He et al., 2004). Maximum-likelihood (ML) learning of the parameters W given an iid sample X = {xn}n=1 can be done by gradient ascent: W = W + η ∂L(W;X ) ∂W ∣",e270bfa5b662c531a61a5b274da636603c23a734
Learning Causally Linked Markov Random Fields,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Simon Osindero', 'dblp_profile': 'https://dblp.org/pid/05/5467.html'}, {'name': 'Kejie Bao', 'dblp_profile': 'https://dblp.org/pid/162/1868.html'}]",2005,"We describe a learning procedure for a generative model that contains a hidden Markov Random Field (MRF) which has directed connections to the observable variables. The learning procedure uses a variational approximation for the posterior distribution over the hidden variables. Despite the intractable partition function of the MRF, the weights on the directed connections and the variational approximation itself can be learned by maximizing a lower bound on the log probability of the observed data. The parameters of the MRF are learned by using the mean field version of contrastive divergence [1]. We show that this hybrid model simultaneously learns parts of objects and their inter-relationships from intensity images. We discuss the extension to multiple MRF’s linked into in a chain graph by directed connections.",3ffc193177ee33a92f3acf3f8e607a2c861461df
What kind of graphical model is the brain?,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2005,"If neurons are treated as latent variables, our visual systems are non-linear, densely-connected graphical models containing billions of variables and thousands of billions of parameters. Current algorithms would have difficulty learning a graphical model of this scale. Starting with an algorithm that has difficulty learning more than a few thousand parameters, I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. The latest member of this series can learn deep, multi-layer belief nets quite rapidly. It turns a generic network with three hidden layers and 1:7 million connections into a very good generative model of handwritten digits. After learning, the model gives classification performance that is comparable to the best discriminative methods.",a4a5bef06587350604c7a9857ca09d91bd95763e
Inferring Motor Programs from Images of Handwritten Digits,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Vinod Nair', 'dblp_profile': 'https://dblp.org/pid/20/5283.html'}]",2005,"We describe a generative model for handwritten digits that uses two pairs of opposing springs whose stiffnesses are controlled by a motor program. We show how neural networks can be trained to infer the motor programs required to accurately reconstruct the MNIST digits. The inferred motor programs can be used directly for digit classification, but they can also be used in other ways. By adding noise to the motor program inferred from an MNIST image we can generate a large set of very different images of the same class, thus enlarging the training set available to other methods. We can also use the motor programs as additional, highly informative outputs which reduce overfitting when training a feed-forward classifier.",e3d4f463823b5a50963073f71d3c8ea29d6005fb
Reinforcement Learning with Factored States and Actions,"[{'name': 'Brian Sallans', 'dblp_profile': 'https://dblp.org/pid/75/1641.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2004,A novel approximation method is presented for approximating the value function and selecting good actions for Markov decision processes with large state and action spaces. The method approximates state-action values as negative free energies in an undirected graphical model called a product of experts. The model parameters can be learned efficiently because values and derivatives can be efficiently computed for a product of experts. Actions can be found even in large factored action spaces by the use of Markov chain Monte Carlo sampling. Simulation results show that the product of experts approximation can be used to solve large problems. In one simulation it is used to find actions in action spaces of size 240.,60922d2ca51acbebff794f4c43f6daadf4b8d103
Probabilistic sequential independent components analysis,"[{'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2004,"Under-complete models, which derive lower dimensional representations of input data, are valuable in domains in which the number of input dimensions is very large, such as data consisting of a temporal sequence of images. This paper presents the under-complete product of experts (UPoE), where each expert models a one-dimensional projection of the data. Maximum-likelihood learning rules for this model constitute a tractable and exact algorithm for learning under-complete independent components. The learning rules for this model coincide with approximate learning rules proposed earlier for under-complete independent component analysis (UICA) models. This paper also derives an efficient sequential learning algorithm from this model and discusses its relationship to sequential independent component analysis (ICA), projection pursuit density estimation, and feature induction algorithms for additive random field models. This paper demonstrates the efficacy of these novel algorithms on high-dimensional continuous datasets.",064dd12847dcaa69ae758c93bfd026f54d575599
Distinguishing text from graphics in on-line handwritten ink,"[{'name': 'Christopher M. Bishop', 'dblp_profile': 'https://dblp.org/pid/b/ChristopherMBishop.html'}, {'name': 'Markus Svensén', 'dblp_profile': 'https://dblp.org/pid/s/MarkusSvensen.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2004,"We present a system that separates text from graphics strokes in handwritten digital ink. It utilizes not just the characteristics of the strokes, but also the information provided by the gaps between the strokes, as well as the temporal characteristics of the stroke sequence. It is built using machine learning techniques that infer the internal parameters of the system from real digital ink, collected using a tablet PC.",e16a64bbef0b45b38f688414872f6ef328dfb9b6
Neighbourhood Components Analysis,"[{'name': 'Jacob Goldberger', 'dblp_profile': 'https://dblp.org/pid/65/6574.html'}, {'name': 'Sam T. Roweis', 'dblp_profile': 'https://dblp.org/pid/r/SamTRoweis.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Ruslan Salakhutdinov', 'dblp_profile': 'https://dblp.org/pid/62/5884.html'}]",2004,"In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification. Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction.",24c287d97982216c8f35c8d326dc2ec2d2475f3e
Multiple Relational Embedding,"[{'name': 'Roland Memisevic', 'dblp_profile': 'https://dblp.org/pid/98/4508.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2004,"We describe a way of using multiple different types of similarity relationship to learn a low-dimensional embedding of a dataset. Our method chooses different, possibly overlapping representations of similarity by individually reweighting the dimensions of a common underlying latent space. When applied to a single similarity relation that is based on Euclidean distances between the input data points, the method reduces to simple dimensionality reduction. If additional information is available about the dataset or about subsets of it, we can use this information to clean up or otherwise improve the embedding. We demonstrate the potential usefulness of this form of semi-supervised dimensionality reduction on some simple examples.",9c8fe61598b103ef1a3bdd2bef044439ddf71ed4
Exponential Family Harmoniums with an Application to Information Retrieval,"[{'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Michal Rosen-Zvi', 'dblp_profile': 'https://dblp.org/pid/19/6540.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2004,"Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these ""exponential family harmoniums"" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords.",2184fb6d32bc46f252b940035029273563c4fc82
Energy-Based Models for Sparse Overcomplete Representations,"[{'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}, {'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Simon Osindero', 'dblp_profile': 'https://dblp.org/pid/05/5467.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2003,"We present a new way of extending independent components analysis (ICA) to overcomplete representations. In contrast to the causal generative extensions of ICA which maintain marginal independence of sources, we define features as deterministic (linear) functions of the inputs. This assumption results in marginal dependencies among the features, but conditional independence of the features given the inputs. By assigning energies to the features a probability distribution over the input states is defined through the Boltzmann distribution. Free parameters of this model are trained using the contrastive divergence objective (Hinton, 2002). When the number of features is equal to the number of input dimensions this energy-based model reduces to noiseless ICA and we show experimentally that the proposed learning algorithm is able to perform blind source separation on speech data. In additional experiments we train overcomplete energy-based models to extract features from various standard data-sets containing speech, natural images, hand-written digits and faces.",b95799a25def71b100bd12e7ebb32cbcee6590bf
Wormholes Improve Contrastive Divergence,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Andriy Mnih', 'dblp_profile': 'https://dblp.org/pid/23/1295.html'}]",2003,"In models that define probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model's distribution. If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3]. But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct, these long-range moves have a reasonable acceptance rate.",9952caa963e3afb173505b8caf232c2e0511223b
Efficient Parametric Projection Pursuit Density Estimation,"[{'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2003,"Product models of low dimensional experts are a powerful way to avoid the curse of dimensionality. We present the ""undercomplete product of experts"" (UPoE), where each expert models a one dimensional projection of the data. The UPoE may be interpreted as a parametric probabilistic model for projection pursuit. Its ML learning rules are identical to the approximate learning rules proposed before for under-complete ICA. We also derive an efficient sequential learning algorithm and discuss its relationship to projection pursuit density estimation and feature induction algorithms for additive random field models.",bd05feae0feb756bace09d6eedcd4d5fb7edff45
In Memory of Ray Reiter (1939-2002),"[{'name': 'Fiora Pirri', 'dblp_profile': 'https://dblp.org/pid/p/FioraPirri.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Hector J. Levesque', 'dblp_profile': 'https://dblp.org/pid/l/HJLevesque.html'}]",2002,"Ray dedicated his life to his research with the wonder of a child, the fearlessness of an explorer, the precision of a mathematician, and the tirelessness of a researcher who found shallowness and confusion intolerable. He leaves a legacy of groundbreaking, deep insights that have changed the course of AI.",115832475f7dda7f0b29cca0e98f26294c855669
Local Physical Models for Interactive Character Animation,"[{'name': 'Sageev Oore', 'dblp_profile': 'https://dblp.org/pid/67/4980.html'}, {'name': 'Demetri Terzopoulos', 'dblp_profile': 'https://dblp.org/pid/85/4738.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2002,"Our goal is to design and build a tool for the creation of expressive character animation. Virtual puppetry, also known as performance animation, is a technique in which the user interactively controls a character's motion. In this paper we introduce local physical models for performance animation and describe how they can augment an existing kinematic method to achieve very effective animation control. These models approximate specific physically‐generated aspects of a character's motion. They automate certain behaviours, while still letting the user override such motion via a PD‐controller if he so desires. Furthermore, they can be tuned to ignore certain undesirable effects, such as the risk of having a character fall over, by ignoring corresponding components of the force. Although local physical models are a quite simple approximation to real physical behaviour, we show that they are extremely useful for interactive character control, and contribute positively to the expressiveness of the character's motion. In this paper, we develop such models at the knees and ankles of an interactively‐animated 3D anthropomorphic character, and demonstrate a resulting animation. This approach can be applied in a straight‐forward way to other joints.",607aa02d95827fbce9b24e0d7e7006affd8abaea
Training Products of Experts by Minimizing Contrastive Divergence,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2002,"It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.",9360e5ce9c98166bb179ad479a9d2919ff13d022
Classical and Bayesian Inference in Neuroimaging: Theory,"[{'name': 'Karl J. Friston', 'dblp_profile': 'https://dblp.org/pid/92/848.html'}, {'name': 'William D. Penny', 'dblp_profile': 'https://dblp.org/pid/72/4293.html'}, {'name': 'Christophe Phillips', 'dblp_profile': 'https://dblp.org/pid/33/2601.html'}, {'name': 'Stefan J. Kiebel', 'dblp_profile': 'https://dblp.org/pid/06/10999.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'John Ashburner', 'dblp_profile': 'https://dblp.org/pid/20/3439.html'}]",2002,"This paper reviews hierarchical observation models, used in functional neuroimaging, in a Bayesian light. It emphasizes the common ground shared by classical and Bayesian methods to show that conventional analyses of neuroimaging data can be usefully extended within an empirical Bayesian framework. In particular we formulate the procedures used in conventional data analysis in terms of hierarchical linear models and establish a connection between classical inference and parametric empirical Bayes (PEB) through covariance component estimation. This estimation is based on an expectation maximization or EM algorithm. The key point is that hierarchical models not only provide for appropriate inference at the highest level but that one can revisit lower levels suitably equipped to make Bayesian inferences. Bayesian inferences eschew many of the difficulties encountered with classical inference and characterize brain responses in a way that is more directly predicated on what one is interested in. The motivation for Bayesian approaches is reviewed and the theoretical background is presented in a way that relates to conventional methods, in particular restricted maximum likelihood (ReML). This paper is a technical and theoretical prelude to subsequent papers that deal with applications of the theory to a range of important issues in neuroimaging. These issues include; (i) Estimating nonsphericity or variance components in fMRI time-series that can arise from serial correlations within subject, or are induced by multisubject (i.e., hierarchical) studies. (ii) Spatiotemporal Bayesian models for imaging data, in which voxels-specific effects are constrained by responses in other voxels. (iii) Bayesian estimation of nonlinear models of hemodynamic responses and (iv) principled ways of mixing structural and functional priors in EEG source reconstruction. Although diverse, all these estimation problems are accommodated by the PEB framework described in this paper.",fee78005a74d7da336f1979f10ed16b0f8042482
Recognizing Handwritten Digits Using Hierarchical Products of Experts,"[{'name': 'Guy Mayraz', 'dblp_profile': 'https://dblp.org/pid/39/3226.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2002,,
A Desktop Input Device and Interface for Interactive 3D Character Animation,"[{'name': 'Sageev Oore', 'dblp_profile': 'https://dblp.org/pid/67/4980.html'}, {'name': 'Demetri Terzopoulos', 'dblp_profile': 'https://dblp.org/pid/85/4738.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2002,"We present a novel input device and interface for interactively controlling the animation of graphical human character from a desktop environment. The trackers are embedded in a new physical design, which is both simple yet also provides significant benefits, and establishes a tangible interface with coordinate frames inherent to the character. A layered kinematic motion recording strategy accesses subsets of the total degrees of freedom of the character. We present the experiences of three novice users with the system, and that of a long-term user who has prior experience with other complex continuous interfaces.",2ca21ac1bd2aa1d1ce92c9fda2da11a425cc2a15
A New Learning Algorithm for Mean Field Boltzmann Machines,"[{'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2002,,b7b5bea7b4d40003a6887794652ea07196a97134
Self Supervised Boosting,"[{'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2002,"Boosting algorithms and successful applications thereof abound for classification and regression learning problems, but not for unsupervised learning. We propose a sequential approach to adding features to a random field model by training them to improve classification performance between the data and an equal-sized sample of ""negative examples"" generated from the model's current estimate of the data density. Training in each boosting round proceeds in three stages: first we sample negative examples from the model's current Boltzmann distribution. Next, a feature is trained to improve classification performance between data and negative examples. Finally, a coefficient is learned which determines the importance of this feature relative to ones already in the pool. Negative examples only need to be generated once to learn each new feature. The validity of the approach is demonstrated on binary digits and continuous synthetic data.",0f18e18d436c51868a2cba5c7df3859986d6ba40
Stochastic Neighbor Embedding,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Sam T. Roweis', 'dblp_profile': 'https://dblp.org/pid/r/SamTRoweis.html'}]",2002,"We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional ""images"" of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word ""bank"", to have versions close to the images of both ""river"" and ""finance"" without forcing the images of outdoor concepts to be located close to those of corporate concepts.",14d46c6396837986bb4b9a14024cb64797b8c6c0
Learning Sparse Topographic Representations with Products of Student-t Distributions,"[{'name': 'Max Welling', 'dblp_profile': 'https://dblp.org/pid/16/2286.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Simon Osindero', 'dblp_profile': 'https://dblp.org/pid/05/5467.html'}]",2002,"We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some filter outputs. We encourage the system to find sparse features by using a Student-t distribution to model each filter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent filters, the system learns a topographic map in which the orientation, spatial frequency and location of the filters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efficient learning procedure that works well even for highly overcomplete sets of filters. Once the model has been learned it can be used as a prior to derive the ""iterated Wiener filter"" for the purpose of denoising images.",14d2d9b2e4c29fe105bfbb31f9749b60690303a7
Learning Distributed Representations of Concepts Using Linear Relational Embedding,"[{'name': 'Alberto Paccanaro', 'dblp_profile': 'https://dblp.org/pid/94/3076.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2001,"We introduce linear relational embedding as a means of learning a distributed representation of concepts from data consisting of binary relations between these concepts. The key idea is to represent concepts as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.",8b27153da18537bd7ec7fd8205d24a34d1c64883
Products of Hidden Markov Models,"[{'name': 'Andrew D. Brown', 'dblp_profile': 'https://dblp.org/pid/44/5980.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2001,"We present products of hidden Markov models (PoHMM's), a way of combining HMM's to form a distributed state time series model. Inference in a PoHMM is tractable and eAEcient. Learning of the parameters, although intractable, can be e ectively done using the Product of Experts learning rule. The distributed state helps the model to explain data which has multiple causes, and the fact that each model need only explain part of the data means a PoHMM can capture longer range structure than an HMM is capable of. We show some results on modelling character strings, a simple language task and the symbolic family trees problem, which highlight these advantages.",b348e98f869a5b656f98688cb9d77208b8475379
Learning Hierarchical Structures with Linear Relational Embedding,"[{'name': 'Alberto Paccanaro', 'dblp_profile': 'https://dblp.org/pid/94/3076.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2001,"We present Linear Relational Embedding (LRE), a new method of learning a distributed representation of concepts from data consisting of instances of relations between given concepts. Its final goal is to be able to generalize, i.e. infer new instances of these relations among the concepts. On a task involving family relationships we show that LRE can generalize better than any previously published method. We then show how LRE can be used effectively to find compact distributed representations for variable-sized recursive data structures, such as trees and lists.",088720694feed36064044545f99f618ac620ee99
Global Coordination of Local Linear Models,"[{'name': 'Sam T. Roweis', 'dblp_profile': 'https://dblp.org/pid/r/SamTRoweis.html'}, {'name': 'Lawrence K. Saul', 'dblp_profile': 'https://dblp.org/pid/66/6611.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2001,"High dimensional data that lies on or near a low dimensional manifold can be described by a collection of local linear models. Such a description, however, does not provide a global parameterization of the manifold—arguably an important goal of unsupervised learning. In this paper, we show how to learn a collection of local linear models that solves this more difficult problem. Our local linear models are represented by a mixture of factor analyzers, and the ""global coordination"" of these models is achieved by adding a regularizing term to the standard maximum likelihood objective function. The regularizer breaks a degeneracy in the mixture model's parameter space, favoring models whose internal coordinate systems are aligned in a consistent way. As a result, the internal coordinates change smoothly and continuously as one traverses a connected path on the manifold—even when the path crosses the domains of many different local models. The regularizer takes the form of a Kullback-Leibler divergence and illustrates an unexpected application of variational methods: not to perform approximate inference in intractable probabilistic models, but to learn more useful internal representations in tractable ones.",7d49080de5eecf0dd756ed6b28743aa837fce881
Relative Density Nets: A New Way to Combine Backpropagation with HMM's,"[{'name': 'Andrew D. Brown', 'dblp_profile': 'https://dblp.org/pid/44/5980.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2001,Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the Standard method of discriminatively training HMM's.,95d002022b5a9342fd254f09957a4deff2f621df
Discovering Multiple Constraints that are Frequently Approximately Satisfied,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}]",2001,"Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.",66e65f81f1f76fb3a7c8ab2d813362b924e2fa9b
Learning Distributed Representations of Relational Data using Linear Relational Embedding,"[{'name': 'Alberto Paccanaro', 'dblp_profile': 'https://dblp.org/pid/94/3076.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2001,,e741bdb25efd158d7020d34a09c27fd2e3138c08
Variational Learning for Switching State-Space Models,"[{'name': 'Zoubin Ghahramani', 'dblp_profile': 'https://dblp.org/pid/g/ZoubinGhahramani.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"This chapter contains sections titled: Introduction, Background, The Generative Model, Learning, Simulations, Discussion, Appendix A: Notation, Appendix B: Derivation of the Variational Fixed-Point Equations, References",9f63683b975d30eeef0d563e3464431efedef5c2
SMEM Algorithm for Mixture Models,"[{'name': 'Naonori Ueda', 'dblp_profile': 'https://dblp.org/pid/87/2491.html'}, {'name': 'Ryohei Nakano', 'dblp_profile': 'https://dblp.org/pid/43/294.html'}, {'name': 'Zoubin Ghahramani', 'dblp_profile': 'https://dblp.org/pid/g/ZoubinGhahramani.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models, local maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations, we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split- and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems.",a88f04ee3dd09c2a753685cb70f7e43d478aab82
Split and Merge EM Algorithm for Improving Gaussian Mixture Density Estimates,"[{'name': 'Naonori Ueda', 'dblp_profile': 'https://dblp.org/pid/87/2491.html'}, {'name': 'Ryohei Nakano', 'dblp_profile': 'https://dblp.org/pid/43/294.html'}, {'name': 'Zoubin Ghahramani', 'dblp_profile': 'https://dblp.org/pid/g/ZoubinGhahramani.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"The EM algorithm for Gaussian mixture models often gets caught in local maxima of the likelihood which involve having too many Gaussians in one part of the space and too few in another, widely separated part of the space. We present a new EM algorithm which performs split and merge operations on the Gaussians to escape from these configurations. This algorithm uses two novel criteria for efficiently selecting the split and merge candidates. Experimental results on synthetic and real data show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held-out test data.",2925f8c37906a4c8f23be600afcffbf60bde605b
Modeling High-Dimensional Data by Combining Simple Experts,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"It is possible to combine multiple non-linear probabilistic models of the same data by multiplying the probability distributions together and then renormalizing. A “productof experts”is a very efficient way to model data that simultaneously satisfies many different constraints. It is difficult to fit a product of experts to data using maximum likelihood because the gradient of the log likelihood is intractable, but there is an efficient way of optimizing a different objective function and this produces good models of high-dimensional data.",f6766fdc16fd2bdb868af1e311f4b0afef03800d
Learning Distributed Representations by Mapping Concepts and Relations into a Linear Space,"[{'name': 'Alberto Paccanaro', 'dblp_profile': 'https://dblp.org/pid/94/3076.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"Linear Relational Embedding is a method of learning a distributed representation of concepts from data consisting of binary relations between concepts. Concepts are represented as vectors, binary relations as matrices, and the operation of applying a relation to a concept as a matrix-vector multiplication that produces an approximation to the related concept. A representation for concepts and relations is learned by maximizing an appropriate discriminative goodness function using gradient ascent. On a task involving family relationships, learning is fast and leads to good generalization.",3a70524a98cd0c8e1f9dd160cacf56d613c3832c
Extracting Distributed Representations of Concepts and Relations from Positive and Negative Propositions,"[{'name': 'Alberto Paccanaro', 'dblp_profile': 'https://dblp.org/pid/94/3076.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"Linear relational embedding (LRE) was introduced previously by the authors (1999) as a means of extracting a distributed representation of concepts from relational data. The original formulation cannot use negative information and cannot properly handle data in which there are multiple correct answers. In this paper we propose an extended formulation of LRE that solves both these problems. We present results in two simple domains, which show that learning leads to good generalization.",a9b8eb922d7530373f1e5fa6d6d2eb98cb60eda9
Rate-coded Restricted Boltzmann Machines for Face Recognition,"[{'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.",73e93d0346e8eee6c2ab45e46c26eaafb66e12a8
Recognizing Hand-written Digits Using Hierarchical Products of Experts,"[{'name': 'Guy Mayraz', 'dblp_profile': 'https://dblp.org/pid/39/3226.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"The product of experts learning procedure [1] can discover a set of stochastic binary features that constitute a non-linear generative model of handwritten images of digits. The quality of generative models learned in this way can be assessed by learning a separate model for each class of digit and then comparing the unnormalized probabilities of test images under the 10 different class-specific models. To improve discriminative performance, it is helpful to learn a hierarchy of separate models for each digit class. Each model in the hierarchy has one layer of hidden units and the nth level model is trained on data that consists of the activities of the hidden units in the already trained (n - 1)th level model. After training, each level produces a separate, unnormalized log probabilty score. With a three-level hierarchy for each of the 10 digit classes, a test image produces 30 scores which can be used as inputs to a supervised, logistic classification network that is trained on separate data. On the MNIST database, our system is comparable with current state-of-the-art discriminative methods, demonstrating that the product of experts learning procedure can produce effective generative models of high-dimensional data.",127bf1f99d9ec32833183c2c8160903151cfafcf
Using Free Energies to Represent Q-values in a Multiagent Reinforcement Learning Task,"[{'name': 'Brian Sallans', 'dblp_profile': 'https://dblp.org/pid/75/1641.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",2000,"The problem of reinforcement learning in large factored Markov decision processes is explored. The Q-value of a state-action pair is approximated by the free energy of a product of experts network. Network parameters are learned on-line using a modified SARSA algorithm which minimizes the inconsistency of the Q-values of consecutive state-action pairs. Actions are chosen based on the current value estimates by fixing the current state and sampling actions from the network using Gibbs sampling. The algorithm is tested on a co-operative multi-agent task. The product of experts model is found to perform comparably to table-based Q-learning for small instances of the task, and continues to perform well when the problem becomes too large for a table-based representation.",388b9ef14cfb67372a1d6e7b6f3f5eea19d7f1d6
Variational Learning in Nonlinear Gaussian Belief Networks,"[{'name': 'Brendan J. Frey', 'dblp_profile': 'https://dblp.org/pid/15/1159.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1999,"We view perceptual tasks such as vision and speech recognition as inference problems where the goal is to estimate the posterior distribution over latent variables (e.g., depth in stereo vision) given the sensory input. The recent flurry of research in independent component analysis exemplifies the importance of inferring the continuous-valued latent variables of input data. The latent variables found by this method are linearly related to the input, but perception requires nonlinear inferences such as classification and depth estimation. In this article, we present a unifying framework for stochastic neural networks with nonlinear latent variables. Nonlinear units are obtained by passing the outputs of linear gaussian units through various nonlinearities. We present a general variational method that maximizes a lower bound on the likelihood of a training set and give results on two visual feature extraction problems. We also show how the variational method can be used for pattern classification and compare the performance of these nonlinear networks with other methods on the problem of handwritten digit recognition.",763aa50583ed047528ba4ef471d72bfbe34471e6
Spiking Boltzmann Machines,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Andrew D. Brown', 'dblp_profile': 'https://dblp.org/pid/44/5980.html'}]",1999,We first show how to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generative model.,40c2747fea2465efd25e07143df7db68ca029412
Learning to Parse Images,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Zoubin Ghahramani', 'dblp_profile': 'https://dblp.org/pid/g/ZoubinGhahramani.html'}, {'name': 'Yee Whye Teh', 'dblp_profile': 'https://dblp.org/pid/88/2483.html'}]",1999,"We describe a class of probabilistic models that we call credibility networks. Using parse trees as internal representations of images, credibility networks are able to perform segmentation and recognition simultaneously, removing the need for ad hoc segmentation heuristics. Promising results in the problem of segmenting handwritten digits were obtained.",1a6d209dc0bb99be6d4d87f4b7abc2cde07e331e
Coaching variables for regression and classification,"[{'name': 'Robert Tibshirani', 'dblp_profile': 'https://dblp.org/pid/t/RobertTibshirani.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1998,,0bca8bb2393eea5340d4d546b75761ad0e588995
Glove-TalkII-a neural-network interface which maps gestures to parallel formant speech synthesizer controls,"[{'name': 'Sidney S. Fels', 'dblp_profile': 'https://dblp.org/pid/f/SidneyFels.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1998,"Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a Cyberglove, a ContactGlove, a three-space tracker, and a foot pedal), a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.",42ef72fe3707d67c7696435edafc035e54ac03aa
SMEM Algorithm for Mixture Models,"[{'name': 'Naonori Ueda', 'dblp_profile': 'https://dblp.org/pid/87/2491.html'}, {'name': 'Ryohei Nakano', 'dblp_profile': 'https://dblp.org/pid/43/294.html'}, {'name': 'Zoubin Ghahramani', 'dblp_profile': 'https://dblp.org/pid/g/ZoubinGhahramani.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1998,"We present a split-and-merge expectation-maximization (SMEM) algorithm to overcome the local maxima problem in parameter estimation of finite mixture models. In the case of mixture models, local maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations, we repeatedly perform simultaneous split-and-merge operations using a new criterion for efficiently selecting the split-and-merge candidates. We apply the proposed algorithm to the training of gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split- and-merge operations to improve the likelihood of both the training data and of held-out test data. We also show the practical usefulness of the proposed algorithm by applying it to image compression and pattern recognition problems.",a88f04ee3dd09c2a753685cb70f7e43d478aab82
Fast Neural Network Emulation of Dynamical Systems for Computer Animation,"[{'name': 'Radek Grzeszczuk', 'dblp_profile': 'https://dblp.org/pid/20/5464.html'}, {'name': 'Demetri Terzopoulos', 'dblp_profile': 'https://dblp.org/pid/85/4738.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1998,"Computer animation through the numerical simulation of physics-based graphics models offers unsurpassed realism, but it can be computationally demanding. This paper demonstrates the possibility of replacing the numerical simulation of nontrivial dynamic models with a dramatically more efficient ""NeuroAnimator"" that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physics-based models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. We demonstrate NeuroAnimators for a variety of physics-based models.",7bfecbb87f447ef9706d359f8220ee7194ebbf0e
NeuroAnimator: Fast Neural Network Emulation and Control of Physics-based Models,"[{'name': 'Radek Grzeszczuk', 'dblp_profile': 'https://dblp.org/pid/20/5464.html'}, {'name': 'Demetri Terzopoulos', 'dblp_profile': 'https://dblp.org/pid/85/4738.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1998,"Animation through the numerical simulation of physicsbased graphics models offers unsurpassed realism, but it can be computationally demanding. Likewise, the search for controllers that enable physics-based models to produce desired animations usually entails formidable computational cost. This paper demonstrates the possibility of replacing the numerical simulation and control of dynamic models with a dramatically more efficient alternative. In particular, we propose the NeuroAnimator, a novel approach to creating physically realistic animation that exploits neural networks. NeuroAnimators are automatically trained off-line to emulate physical dynamics through the observation of physicsbased models in action. Depending on the model, its neural network emulator can yield physically realistic animation one or two orders of magnitude faster than conventional numerical simulation. Furthermore, by exploiting the network structure of the NeuroAnimator, we introduce a fast algorithm for learning controllers that enables either physics-based models or their neural network emulators to synthesize motions satisfying prescribed animation goals. We demonstrate NeuroAnimators for a variety of physics-based models. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism—Animation; I.6.8 [Simulation and Modeling]: Types of Simulation—Animation",9e0020d2cc7d8255661063ea5fb87f2000df1b21
"A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants","[{'name': 'Radford M. Neal', 'dblp_profile': 'https://dblp.org/pid/27/666.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1998,,9f87a11a523e4680e61966e36ea2eac516096f23
A Hierarchical Community of Experts,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Brian Sallans', 'dblp_profile': 'https://dblp.org/pid/75/1641.html'}, {'name': 'Zoubin Ghahramani', 'dblp_profile': 'https://dblp.org/pid/g/ZoubinGhahramani.html'}]",1998,,31b7d44b971eee9baf62974cbdb94a6b5d62a7af
Efficient Stochastic Source Coding and an Application to a Bayesian Network Source Model,"[{'name': 'Brendan J. Frey', 'dblp_profile': 'https://dblp.org/pid/15/1159.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1997,"In this paper, we introduce a new algorithm calledbits-back coding' that makes stochastic source codes ef""cient. For a given one-to-many source code, we show that this algorithm can actually be more ef""cient than the algorithm that always picks the shortest codeword. Optimal ef""ciency is achieved when codewords are chosen according to the Boltzmann distribution based on the codeword lengths. It turns out that a commonly used technique for determining parameters— maximum-likelihood estimation—actually minimizes the bits-back coding cost when codewords are chosen according to the Boltzmann distribution. A tractable approximation to maximum-likelihood estimation—the generalized expectation-maximization algorithm—minimizes the bits-back coding cost. After presenting a binary Bayesian network model that assigns exponentially many codewords to each symbol, we show how a tractable approximation to the Boltzmann distribution can be used for bits-back coding. We illustrate the performance of bits-back coding using non-synthetic data with a binary Bayesian network source model that produces 2 60 possible codewords for each input symbol. The rate for bits-back coding is nearly one half of that obtained by picking the shortest",a3dde51dbd39dda5bc22df0d8163608d3898823b
Instantiating Deformable Models with a Neural Net,"[{'name': 'Christopher K. I. Williams', 'dblp_profile': 'https://dblp.org/pid/w/ChristopherKIWilliams.html'}, {'name': 'Michael Revow', 'dblp_profile': 'https://dblp.org/pid/26/1387.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1997,"Deformable models are an attractive approach to recognizing objects which have considerable within-class variability such as handwritten characters. However, there are severe search problems associated with fitting the models to data which could be reduced if a better starting point for the search were available. We show that by training a neural network to predict how a deformable model should be instantiated from an input image, such improved starting points can be obtained. This method has been implemented for a system that recognizes handwritten digits using deformable models, and the results show that the search time can be significantly reduced without compromising recognition performance. © 1997 Academic Press.",c938b2d67433b7777d72d2b44114a939ae0b39bb
Using Expectation-Maximization for Reinforcement Learning,"[{'name': 'Peter Dayan', 'dblp_profile': 'https://dblp.org/pid/22/522.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1997,"We discuss Hinton's (1989) relative payoff procedure (RPP), a static reinforcement learning algorithm whose foundation is not stochastic gradient ascent. We show circumstances under which applying the RPP is guaranteed to increase the mean return, even though it can make large changes in the values of the parameters. The proof is based on a mapping between the RPP and a form of the expectation-maximization procedure of Dempster, Laird, and Rubin (1977).",628b80ac7952a67155d62e10dc2854ac8c04a6e4
A Mobile Robot that Learns its Place,"[{'name': 'Sageev Oore', 'dblp_profile': 'https://dblp.org/pid/67/4980.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Gregory Dudek', 'dblp_profile': 'https://dblp.org/pid/d/GregoryDudek.html'}]",1997,"We show how a neural network can be used to allow a mobile robot to derive an accurate estimate of its location from noisy sonar sensors and noisy motion information. The robot's model of its location is in the form of a probability distribution across a grid of possible locations. This distribution is updated using both the motion information and the predictions of a neural network that maps locations into likelihood distributions across possible sonar readings. By predicting sonar readings from locations, rather than vice versa, the robot can handle the very nongaussian noise in the sonar sensors. By using the constraint provided by the noisy motion information, the robot can use previous readings to improve its estimate of its current location. By treating the resulting estimates as if they were correct, the robot can learn the relationship between location and sonar readings without requiring an external supervision signal that specifies the actual location of the robot. It can learn to locate itself in a new environment with almost no supervision, and it can maintain its location ability even when the environment is nonstationary.",775bddc3476a25cc7da5bfcf2083fca94983661a
Modeling the manifolds of images of handwritten digits,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Peter Dayan', 'dblp_profile': 'https://dblp.org/pid/22/522.html'}, {'name': 'Michael Revow', 'dblp_profile': 'https://dblp.org/pid/26/1387.html'}]",1997,"This paper describes two new methods for modeling the manifolds of digitized images of handwritten digits. The models allow a priori information about the structure of the manifolds to be combined with empirical data. Accurate modeling of the manifolds allows digits to be discriminated using the relative probability densities under the alternative models. One of the methods is grounded in principal components analysis, the other in factor analysis. Both methods are based on locally linear low-dimensional approximations to the underlying data manifold. Links with other methods that model the manifold are discussed.",62f4d89a3c1441b47170c7e1380137fb388d0799
Glove-talk II - a neural-network interface which maps gestures to parallel formant speech synthesizer controls,"[{'name': 'Sidney S. Fels', 'dblp_profile': 'https://dblp.org/pid/f/SidneyFels.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1997,"Glove-Talk II is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to ten control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-Talk II uses several input devices, a parallel formant speech synthesizer, and three neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed user-defined relationship between hand position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency, and stop consonants are produced with a fixed mapping from the input devices. With Glove-Talk II, the subject can speak slowly but with far more natural sounding pitch variations than a text-to-speech synthesizer.",1cb7b19e8b4e90c4b1328f4487d31d8afdd53b89
Hierarchical Non-linear Factor Analysis and Topographic Maps,"[{'name': 'Zoubin Ghahramani', 'dblp_profile': 'https://dblp.org/pid/g/ZoubinGhahramani.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1997,"We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topographically organised local feature detectors.",7a64be9fb3f1bce1dada1c32bfac42164cf3cdec
Learning fast neural network emulators for physics-based models,"[{'name': 'Radek Grzeszczuk', 'dblp_profile': 'https://dblp.org/pid/20/5464.html'}, {'name': 'Demetri Terzopoulos', 'dblp_profile': 'https://dblp.org/pid/85/4738.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1997,,71c7de60ab2b28921dfe5e4bd11b0437cf75fab1
Varieties of Helmholtz Machine,"[{'name': 'Peter Dayan', 'dblp_profile': 'https://dblp.org/pid/22/522.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1996,,6dad26916ac88188dae82f94f2995c80c19a9589
Using Generative Models for Handwritten Digit Recognition,"[{'name': 'Michael Revow', 'dblp_profile': 'https://dblp.org/pid/26/1387.html'}, {'name': 'Christopher K. I. Williams', 'dblp_profile': 'https://dblp.org/pid/w/ChristopherKIWilliams.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1996,"We describe a method of recognizing handwritten digits by fitting generative models that are built from deformable B-splines with Gaussian ""ink generators"" spaced along the length of the spline. The splines are adjusted using a novel elastic matching procedure based on the expectation maximization algorithm that maximizes the likelihood of the model generating the data. This approach has many advantages: 1) the system not only produces a classification of the digit but also a rich description of the instantiation parameters which can yield information such as the writing style; 2) the generative models can perform recognition driven segmentation; 3) the method involves a relatively small number of parameters and hence training is relatively easy and fast; and 4) unlike many other recognition schemes, it does not rely on some form of pre-normalization of input images, but can handle arbitrary scalings, translations and a limited degree of image rotation. We have demonstrated that our method of fitting models to images does not get trapped in poor local minima. The main disadvantage of the method is that it requires much more computation than more standard OCR techniques.",56efc84e0858f1e0a7cf052e5c4275d4c46c21c2
Free Energy Coding,"[{'name': 'Brendan J. Frey', 'dblp_profile': 'https://dblp.org/pid/15/1159.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1996,"We introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol. It may seem that the most sensible codeword to use in this case is the shortest one. However, in the proposed free energy approach, random codeword selection yields an effective codeword length that can be less than the shortest codeword length. If the random choices are Boltzmann distributed, the effective length is optimal for the given source code. The expectation-maximization parameter estimation algorithms minimize this effective codeword length. We illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method.",7ea37a9b06cca8d4a667aeb894942c27ecdc2aca
Learning Population Codes by Minimizing Description Length,"[{'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1995,"The minimum description length (MDL) principle can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center of this bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes.",53ad5627f0bacb4c1de4aa374adbaecf40336b07
The Helmholtz machine,"[{'name': 'Peter Dayan', 'dblp_profile': 'https://dblp.org/pid/22/522.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Radford M. Neal', 'dblp_profile': 'https://dblp.org/pid/27/666.html'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}]",1995,"Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.",605402e235bd62437baf3c9ebefe77fb4d92ee95
GloveTalkII: An Adaptive Gesture-to-Formant Interface,"[{'name': 'Sidney S. Fels', 'dblp_profile': 'https://dblp.org/pid/f/SidneyFels.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1995,,
Using Pairs of Data-Points to Define Splits for Decision Trees,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Michael Revow', 'dblp_profile': 'https://dblp.org/pid/26/1387.html'}]",1995,"Conventional binary classification trees such as CART either split the data using axis-aligned hyperplanes or they perform a computationally expensive search in the continuous space of hyperplanes with unrestricted orientations. We show that the limitations of the former can be overcome without resorting to the latter. For every pair of training data-points, there is one hyperplane that is orthogonal to the line joining the data-points and bisects this line. Such hyperplanes are plausible candidates for splits. In a comparison on a suite of 12 datasets we found that this method of generating candidate splits outperformed the standard methods, particularly when the training sets were small.",bbb7229bfe2b9995ee54272217156ed91494b91d
Does the Wake-sleep Algorithm Produce Good Density Estimators?,"[{'name': 'Brendan J. Frey', 'dblp_profile': 'https://dblp.org/pid/15/1159.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Peter Dayan', 'dblp_profile': 'https://dblp.org/pid/22/522.html'}]",1995,"The wake-sleep algorithm (Hinton, Dayan, Frey and Neal 1995) is a relatively efficient method of fitting a multilayer stochastic generative model to high-dimensional data. In addition to the top-down connections in the generative model, it makes use of bottom-up connections for approximating the probability distribution over the hidden units given the data, and it trains these bottom-up connections using a simple delta rule. We use a variety of synthetic and real data sets to compare the performance of the wake-sleep algorithm with Monte Carlo and mean field methods for fitting the same generative model and also compare it with other models that are less powerful but easier to fit.",cbb0362cbfef094dbed0907329e6057dc5d09714
An Alternative Model for Mixtures of Experts,"[{'name': 'Lei Xu', 'dblp_profile': 'https://dblp.org/pid/19/360-1.html'}, {'name': 'Michael I. Jordan', 'dblp_profile': 'https://dblp.org/pid/j/MichaelIJordan.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1994,We propose an alternative model for mixtures of experts which uses a different parametric form for the gating network. The modified model is trained by the EM algorithm. In comparison with earlier models--trained by either EM or gradient ascent--there is no need to select a learning stepsize. We report simulation experiments which show that the new architecture yields faster convergence. We also apply the new model to two problem domains: piecewise nonlinear function approximation and the combination of multiple previously trained classifiers.,c8eb7f54bd9cec4e2d8c866d98c76a875374b594
Glove-TalkII: Mapping Hand Gestures to Speech Using Neural Networks,"[{'name': 'Sidney S. Fels', 'dblp_profile': 'https://dblp.org/pid/f/SidneyFels.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1994,"Glove-TalkII is a system which translates hand gestures to speech through an adaptive interface. Hand gestures are mapped continuously to 10 control parameters of a parallel formant speech synthesizer. The mapping allows the hand to act as an artificial vocal tract that produces speech in real time. This gives an unlimited vocabulary in addition to direct control of fundamental frequency and volume. Currently, the best version of Glove-TalkII uses several input devices (including a CyberGlove, a ContactGlove, a 3- space tracker, and a foot-pedal), a parallel formant speech synthesizer and 3 neural networks. The gesture-to-speech task is divided into vowel and consonant production by using a gating network to weight the outputs of a vowel and a consonant neural network. The gating network and the consonant network are trained with examples from the user. The vowel network implements a fixed, user-defined relationship between hand-position and vowel sound and does not require any training examples from the user. Volume, fundamental frequency and stop consonants are produced with a fixed mapping from the input devices. One subject has trained to speak intelligibly with Glove-TalkII. He speaks slowly with speech quality similar to a text-to-speech synthesizer but with far more natural-sounding pitch variations.",0067ef94cbf90021ad27784ce97d8a2a38f643e0
Using a neural net to instantiate a deformable model,"[{'name': 'Christopher K. I. Williams', 'dblp_profile': 'https://dblp.org/pid/w/ChristopherKIWilliams.html'}, {'name': 'Michael Revow', 'dblp_profile': 'https://dblp.org/pid/26/1387.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1994,,
Recognizing Handwritten Digits Using Mixtures of Linear Models,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Michael Revow', 'dblp_profile': 'https://dblp.org/pid/26/1387.html'}, {'name': 'Peter Dayan', 'dblp_profile': 'https://dblp.org/pid/22/522.html'}]",1994,"We construct a mixture of locally linear generative models of a collection of pixel-based images of digits, and use them for recognition. Different models of a given digit are used to capture different styles of writing, and new images are classified by evaluating their log-likelihoods under each model. We use an EM-based algorithm in which the M-step is computationally straightforward principal components analysis (PCA). Incorporating tangent-plane information [12] about expected local deformations only requires adding tangent vectors into the sample covariance matrices for the PCA, and it demonstrably improves performance.",9dea20c1e5bbb1f543ff08113ffde5380c679f1f
Learning Mixture Models of Spatial Coherence,"[{'name': 'Suzanna Becker', 'dblp_profile': 'https://dblp.org/pid/82/2494.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1993,"We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton 1992b). In this paper, we propose two new models that handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities.",f065b631b2df7964d308c8a649fb05e9c6c91f67
A soft decision-directed LMS algorithm for blind equalization,"[{'name': 'Steven J. Nowlan', 'dblp_profile': 'https://dblp.org/pid/74/4802.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1993,"An adaptation algorithm for equalizers operating on very distorted channels is presented. The algorithm is based on the idea of adjusting the equalizer tap gains to maximize the likelihood that the equalizer outputs would be generated by a mixture of two Gaussians with known means. The decision-directed least-mean-square algorithm is shown to be an approximation to maximizing the likelihood that the equalizer outputs come from such an independently and identically distributed source. The algorithm is developed in the context of a binary pulse-amplitude-modulation channel, and simulations demonstrate that the algorithm converges in channels for which the decision-directed LMS algorithms does not converge. >",da11a50037b266fe6b1929b76322395716ec2029
Glove-Talk: a neural network interface between a data-glove and a speech synthesizer,"[{'name': 'Sidney S. Fels', 'dblp_profile': 'https://dblp.org/pid/f/SidneyFels.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1993,"To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single ;speaker' in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1% of the time, and no word is produced about 5% of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user.",703b3e9dc9ad014cb1baa840f5a03b33c67021d8
Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Drew van Camp', 'dblp_profile': 'https://dblp.org/pid/54/57.html'}]",1993,"Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o(cid:11) between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e(cid:14)ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights.",25c9f33aceac6dcff357727cbe2faf145b01d13c
"Autoencoders, Minimum Description Length and Helmholtz Free Energy","[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}]",1993,"An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.",3dc3a0efe58eaf8564ca1965c0ffd23ec495b83f
Developing Population Codes by Minimizing Description Length,"[{'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1993,"The Minimum Description Length principle (MDL) can be used to train the hidden units of a neural network to extract a representation that is cheap to describe but nonetheless allows the input to be reconstructed accurately. We show how MDL can be used to develop highly redundant population codes. Each hidden unit has a location in a low-dimensional implicit space. If the hidden unit activities form a bump of a standard shape in this space, they can be cheaply encoded by the center ofthis bump. So the weights from the input units to the hidden units in an autoencoder are trained to make the activities form a standard bump. The coordinates of the hidden units in the implicit space are also learned, thus allowing flexibility, as the network develops a discontinuous topography when presented with different input classes. Population-coding in a space other than the input enables a network to extract nonlinear higher-order properties of the inputs. Most existing unsupervised learning algorithms can be understood using the Minimum Description Length (MDL) principle (Rissanen, 1989). Given an ensemble of input vectors, the aim of the learning algorithm is to find a method of coding each input vector that minimizes the total cost, in bits, of communicating the input vectors to a receiver. There are three terms in the total description length: • The code-cost is the number of bits required to communicate the code that the algorithm assigns to each input vector.",2993bed6663f53a12584ea293bf8d487a91cd25a
Simplifying Neural Networks by Soft Weight-Sharing,"[{'name': 'Steven J. Nowlan', 'dblp_profile': 'https://dblp.org/pid/74/4802.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1992,"One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms.",de75e4e15e22d4376300e5c968e2db44be29ac9e
Feudal Reinforcement Learning,"[{'name': 'Peter Dayan', 'dblp_profile': 'https://dblp.org/pid/22/522.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1992,"One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. 
 
We illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map.",1678bd32846b1aded5b1e80a617170812e80f562
Adaptive Mixtures of Local Experts,"[{'name': 'Robert A. Jacobs', 'dblp_profile': 'https://dblp.org/pid/32/638.html'}, {'name': 'Michael I. Jordan', 'dblp_profile': 'https://dblp.org/pid/j/MichaelIJordan.html'}, {'name': 'Steven J. Nowlan', 'dblp_profile': 'https://dblp.org/pid/74/4802.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1991,"We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.",c8d90974c3f3b40fa05e322df2905fc16204aa56
Learning to Make Coherent Predictions in Domains with Discontinuities,"[{'name': 'Suzanna Becker', 'dblp_profile': 'https://dblp.org/pid/82/2494.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1991,"We have previously described an unsupervised learning procedure that discovers spatially coherent properties of the world by maximizing the information that parameters extracted from different parts of the sensory input convey about some common underlying cause. When given random dot stereograms of curved surfaces, this procedure learns to extract surface depth because that is the property that is coherent across space. It also learns how to interpolate the depth at one location from the depths at nearby locations (Becker and Hinton, 1992). In this paper, we propose two new models which handle surfaces with discontinuities. The first model attempts to detect cases of discontinuities and reject them. The second model develops a mixture of expert interpolators. It learns to detect the locations of discontinuities and to invoke specialized, asymmetric interpolators that do not cross the discontinuities.",c07f22297f783475d799b1ad3d69f86f89cca1d3
Adaptive Elastic Models for Hand-Printed Character Recognition,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Christopher K. I. Williams', 'dblp_profile': 'https://dblp.org/pid/w/ChristopherKIWilliams.html'}, {'name': 'Michael Revow', 'dblp_profile': 'https://dblp.org/pid/26/1387.html'}]",1991,"Hand-printed digits can be modeled as splines that are governed by about 8 control points. For each known digit, the control points have preferred ""home"" locations, and deformations of the digit are generated by moving the control points away from their home locations. Images of digits can be produced by placing Gaussian ink generators uniformly along the spline. Real images can be recognized by finding the digit model most likely to have generated the data. For each digit model we use an elastic matching algorithm to minimize an energy function that includes both the deformation energy of the digit model and the log probability that the model would generate the inked pixels in the image. The model with the lowest total energy wins. If a uniform noise process is included in the model of image generation, some of the inked pixels can be rejected as noise as a digit model is fitting a poorly segmented image. The digit models learn by modifying the home locations of the control points.",15034107f195f625922881ef197515a7997b4c0d
Adaptive Soft Weight Tying using Gaussian Mixtures,"[{'name': 'Steven J. Nowlan', 'dblp_profile': 'https://dblp.org/pid/74/4802.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1991,"Geoffrey E. Hinton Department of Computer Science . U ni versi ty of Toran to Toronto, Canada M5S lA4 One way of simplifying neural networks so they generalize better is to add an extra t.erm 10 the error fUll c tion that will penalize complexit.y. \Ve propose a new penalt.y t.erm in which the dist rihution of weight values is modelled as a mixture of multiple gaussians . C nder this model, a set of weights is simple if the weights can be clustered into subsets so that the weights in each cluster have similar values . We allow the parameters of the mixture model to adapt at t.he same time as t.he network learns. Simulations demonstrate that this complexity term is more effective than previous complexity terms.",0a41ca65a80b5644d23649043f2f625b4002a225
Connectionist Symbol Processing - Preface,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1990,,8209e1cd0ee97873345ce49584a825f8d603bcda
Mapping Part-Whole Hierarchies into Connectionist Networks,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1990,,71dd4d477ca17b4db3b270d25225822ff3a41fac
The Bootstrap Widrow-Hoff Rule as a Cluster-Formation Algorithm,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Steven J. Nowlan', 'dblp_profile': 'https://dblp.org/pid/74/4802.html'}]",1990,An algorithm that is widely used for adaptive equalization in current modems is the bootstrap or decision-directed version of the Widrow-Hoff rule. We show that this algorithm can be viewed as an unsupervised clustering algorithm in which the data points are transformed so that they form two clusters that are as tight as possible. The standard algorithm performs gradient ascent in a crude model of the log likelihood of generating the transformed data points from two gaussian distributions with fixed centers. Better convergence is achieved by using the exact gradient of the log likelihood.,af6759ecd0f6c8ba1eb7030894eff4c91a55778d
A time-delay neural network architecture for isolated word recognition,"[{'name': 'Kevin J. Lang', 'dblp_profile': 'https://dblp.org/pid/67/4988.html'}, {'name': 'Alex Waibel', 'dblp_profile': 'https://dblp.org/pid/08/2456.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1990,,e08d090d1e586610d636a46004876e9f3ded8209
Building adaptive interfaces with neural networks: The glove-talk pilot study,"[{'name': 'Sidney S. Fels', 'dblp_profile': 'https://dblp.org/pid/f/SidneyFels.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1990,,4886fa9820ada07ca905cfd6a9ee973ce070b10b
Discovering Viewpoint-Invariant Relationships That Characterize Objects,"[{'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1990,"Using an unsupervised learning procedure, a network is trained on an ensemble of images of the same two-dimensional object at different positions, orientations and sizes. Each half of the network ""sees"" one fragment of the object, and tries to produce as output a set of 4 parameters that have high mutual information with the 4 parameters output by the other half of the network. Given the ensemble of training patterns, the 4 parameters on which the two halves of the network can agree are the position, orientation, and size of the whole object, or some recoding of them. After training, the network can reject instances of other shapes by using the fact that the predictions made by its two halves disagree. If two competing networks are trained on an unlabelled mixture of images of two objects, they cluster the training cases on the basis of the objects' shapes, independently of the position, orientation, and size.",852d3055ce81ef75b51dd9a406c0f5d056d270da
Evaluation of Adaptive Mixtures of Competing Experts,"[{'name': 'Steven J. Nowlan', 'dblp_profile': 'https://dblp.org/pid/74/4802.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1990,"We compare the performance of the modular architecture, composed of competing expert networks, suggested by Jacobs, Jordan, Nowlan and Hinton (1991) to the performance of a single back-propagation network on a complex, but low-dimensional, vowel recognition task. Simulations reveal that this system is capable of uncovering interesting decompositions in a complex task. The type of decomposition is strongly influenced by the nature of the input to the gating network that decides which expert to use for each case. The modular architecture also exhibits consistently better generalization on many variations of the task.",1c3c5c56955b658a108731f5b7f65b76cd95f84e
Distributed Representations,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'James L. McClelland', 'dblp_profile': 'https://dblp.org/pid/49/5831.html'}, {'name': 'David E. Rumelhart', 'dblp_profile': 'https://dblp.org/pid/63/2480.html'}]",1990,,3106e66537a0c8f53278e553bcb38f0b0992ec0e
Connectionist Learning Procedures,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1989,,a57c6d627ffc667ae3547073876c35d6420accff
Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1989,"This chapter contains sections titled: Introduction, The Learning Procedure for Stochastic Boltzmann Machines, Mean field theory, Deterministic Boltzmann machine learning, Symmetry of the Weights, Acknowledgments, References",63c8998421f11931f78d5a4fdfc7f2c6a3c46e0c
Phoneme recognition using time-delay neural networks,"[{'name': 'Alexander Waibel', 'dblp_profile': 'https://dblp.org/pid/08/2456.html'}, {'name': 'Toshiyuki Hanazawa', 'dblp_profile': 'https://dblp.org/pid/07/3792.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Kiyohiro Shikano', 'dblp_profile': 'https://dblp.org/pid/50/2173.html'}, {'name': 'Kevin J. Lang', 'dblp_profile': 'https://dblp.org/pid/67/4988.html'}]",1989,"The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >",cd62c9976534a6a2096a38244f6cbb03635a127e
Dimensionality Reduction and Prior Knowledge in E-Set Recognition,"[{'name': 'Kevin J. Lang', 'dblp_profile': 'https://dblp.org/pid/67/4988.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1989,"It is well known that when an automatic learning algorithm is applied to a fixed corpus of data, the size of the corpus places an upper bound on the number of degrees of freedom that the model can contain if it is to generalize well. Because the amount of hardware in a neural network typically increases with the dimensionality of its inputs, it can be challenging to build a high-performance network for classifying large input patterns. In this paper, several techniques for addressing this problem are discussed in the context of an isolated word recognition task.",b5affc896bcb291bca6e3ba60d34eeac28214e2e
TRAFFIC: Recognizing Objects Using Hierarchical Reference Frame Transformations,"[{'name': 'Richard S. Zemel', 'dblp_profile': 'https://dblp.org/pid/16/6366.html'}, {'name': 'Michael Mozer', 'dblp_profile': 'https://dblp.org/pid/m/MichaelCMozer.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1989,"We describe a model that can recognize two-dimensional shapes in an unsegmented image, independent of their orientation, position, and scale. The model, called TRAFFIC, efficiently represents the structural relation between an object and each of its component features by encoding the fixed viewpoint-invariant transformation from the feature's reference frame to the object's in the weights of a connectionist network. Using a hierarchy of such transformations, with increasing complexity of features at each successive layer, the network can recognize multiple objects in parallel. An implementation of TRAFFIC is described, along with experimental results demonstrating the network's ability to recognize constellations of stars in a viewpoint-invariant manner.",5b00ab8cc5a56ee934618bd7555965185a7f83ec
Discovering High Order Features with Mean Field Modules,"[{'name': 'Conrad C. Galland', 'dblp_profile': 'https://dblp.org/pid/67/1075.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1989,"A new form of the deterministic Boltzmann machine (DBM) learning procedure is presented which can efficiently train network modules to discriminate between input vectors according to some criterion. The new technique directly utilizes the free energy of these ""mean field modules"" to represent the probability that the criterion is met, the free energy being readily manipulated by the learning procedure. Although conventional deterministic Boltzmann learning fails to extract the higher order feature of shift at a network bottleneck, combining the new mean field modules with the mutual information objective function rapidly produces modules that perfectly extract this important higher order feature without direct external supervision.",e1d592e789bbec99188b12af20e58a75ec2987a0
A Distributed Connectionist Production System,"[{'name': 'David S. Touretzky', 'dblp_profile': 'https://dblp.org/pid/45/4246.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1988,"DCPS is a connectionist production system interpreter that uses distributed representations. As a connectionist model it consists of many simple, richly interconnected neuron-like computing units that cooperate to solve problems in parallel. One motivation for constructing DCPS was to demonstrate that connectionist models are capable of representing and using explicit rules. A second motivation was to show how “coarse coding” or “distributed representations” can be used to construct a working memory that requires far fewer units than the number of different facts that can potentially be stored. The simulation we present is intended as a detailed demonstration of the feasibility of certain ideas and should not be viewed as a full implementation of production systems. Our current model only has a few of the many interesting emergent properties that we eventually hope to demonstrate: It is damage-resistant, it performs matching and variable binding by massively parallel constraint satisfaction, and the capacity of its working memory is dependent on the similarity of the items being stored.",a2641de9a59d4f176a6e088d79846576e5ff9513
Phoneme recognition: neural networks vs. hidden Markov models,"[{'name': 'Alex Waibel', 'dblp_profile': 'https://dblp.org/pid/08/2456.html'}, {'name': 'Toshiyuki Hanazawa', 'dblp_profile': 'https://dblp.org/pid/07/3792.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Kiyohiro Shikano', 'dblp_profile': 'https://dblp.org/pid/50/2173.html'}, {'name': 'Kevin J. Lang', 'dblp_profile': 'https://dblp.org/pid/67/4988.html'}]",1988,,
GEMINI: Gradient Estimation Through Matrix Inversion After Noise Injection,"[{'name': 'Yann LeCun', 'dblp_profile': 'https://dblp.org/pid/l/YannLeCun.html'}, {'name': 'Conrad C. Galland', 'dblp_profile': 'https://dblp.org/pid/67/1075.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1988,"Learning procedures that measure how random perturbations of unit activities correlate with changes in reinforcement are inefficient but simple to implement in hardware. Procedures like back-propagation (Rumelhart, Hinton and Williams, 1986) which compute how changes in activities affect the output error are much more efficient, but require more complex hardware. GEMINI is a hybrid procedure for multilayer networks, which shares many of the implementation advantages of correlational reinforcement procedures but is more efficient. GEMINI injects noise only at the first hidden layer and measures the resultant effect on the output error. A linear network associated with each hidden layer iteratively inverts the matrix which relates the noise to the error change, thereby obtaining the error-derivatives. No back-propagation is involved, thus allowing unknown non-linearities in the system. Two simulations demonstrate the effectiveness of GEMINI.",19ec5c7be1fe5e7088f3a042d3160ede757f5902
How Learning Can Guide Evolution,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Steven J. Nowlan', 'dblp_profile': 'https://dblp.org/pid/74/4802.html'}]",1987,"The assumption that acquired character istics are not in­ herited is ofte n taken to imply t hat t he adaptations t hat an organism learns dur ing its lifeti me cannot guide t he course of evolut ion . This infere nce is incor rec t (2). Learni ng alt ers the shape of t he search space in which evolu tio n operates and thereby pro vides good evolut ion ar y paths towa rds sets of co-adapted alleles. We demonst r at e t hat th is effect allows learning organisms to evolve much faster than their 000 ­ learning equivalents, even though the characteris tics acquired by t he phenotype are not communicated to the genotype.",f9197ff9fdabd2b78bfe0602365011c6699b0d66
Connectionist Architectures for Artificial Intelligence,"[{'name': 'Scott E. Fahlman', 'dblp_profile': 'https://dblp.org/pid/16/3174.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1987,"A number of researchers have begun exploring the use of massively parallel architectures in an attempt to get around the limitations of conventional symbol processing. Many of these parallel architectures are connectionist: The system's collection of permanent knowledge is stored as a pattern of connections or connection strengths among the processing elements, so the knowledge directly determines how the processing elements interact rather that sitting passively in a memory, waiting to be looked at by the CPU. Some connectionist schemes use formal, symbolic representations, while others use more analog approaches. Some even develop their own internal representations after seeing examples of the patterns they are to recognize or the relationships they are to store. Connectionism is somewhat controversial in the AI community. It is new, still unproven in large-scale practical applications, and very different in style from the traditional AI approach. The authors have only begun to explore the behavior and potential of connectionist networks. In this article, the authors describe some of the central issues and ideas of connectionism, and also some of the unsolved problems facing this approach. Part of the motivation for connectionist research is the possible similarity in function between connectionist networks and the neutral networksmore » of the human cortex, but they concentrate here on connectionism's potential as a practical technology for building intelligent systems.« less",706ae842fb2107f5940cfa39682d52235d269eb7
Learning Representations by Recirculation,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'James L. McClelland', 'dblp_profile': 'https://dblp.org/pid/49/5831.html'}]",1987,"We describe a new learning procedure for networks that contain groups of nonlinear units arranged in a closed loop. The aim of the learning is to discover codes that allow the activity vectors in a ""visible"" group to be represented by activity vectors in a ""hidden"" group. One way to test whether a code is an accurate representation is to try to reconstruct the visible vector from the hidden vector. The difference between the original and the reconstructed visible vectors is called the reconstruction error, and the learning procedure aims to minimize this error. The learning procedure has two passes. On the first pass, the original visible vector is passed around the loop, and on the second pass an average of the original vector and the reconstructed vector is passed around the loop. The learning procedure changes each weight by an amount proportional to the product of the ""presynaptic"" activity and the difference in the post-synaptic activity on the two passes. This procedure is much simpler to implement than methods like back-propagation. Simulations in simple networks show that it usually converges rapidly on a good set of codes, and analysis shows that in certain restricted cases it performs gradient descent in the squared reconstruction error.",97f7d20e1e82347d78cef335218692207b29d23f
Learning Translation Invariant Recognition in Massively Parallel Networks,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1987,,3e6bea2649298c68d17b9421fc7dd19eeacc935e
Learning in Massively Parallel Nets (Panel),"[{'name': 'Drew V. McDermott', 'dblp_profile': 'https://dblp.org/pid/83/863.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1986,,03ec7af2824c331577cb73a2a92f4e7b9d728856
A Learning Algorithm for Boltzmann Machines,"[{'name': 'David H. Ackley', 'dblp_profile': 'https://dblp.org/pid/11/4038.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Terrence J. Sejnowski', 'dblp_profile': 'https://dblp.org/pid/26/2197.html'}]",1985,,a0d16f0e99f7ce5e6fb70b1a68c685e9ad610657
Symbols Among the Neurons: Details of a Connectionist Inference Architecture,"[{'name': 'David S. Touretzky', 'dblp_profile': 'https://dblp.org/pid/45/4246.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1985,"Pattern matching and variable binding are easily implemented in conventional computer architectures, but not necessarily in all architectures. In a distributed neural network architecture each symbol is represented by activity in many units and each unit contributes to the representation of many symbols. Manipulating symbols using this type of distributed representation is not as easy as with a local representation whore each unit denotes one symbol, but there is evidence that the distributed approach is the one chosen by nature. We describe a working implementation of a production system interpreter in a neural network using distributed representations for both symbols and rules. The research provides a detailed account of two important symbolic reasoning operations, pattern matching and variable binding, as emergent properties of collections of neuron-like elements. The success of our production system implementation goes some way towards answering a common criticism of connectionist theories: that they aren't powerful enough to do symbolic reasoning.",b0cb3be87b7f4f50d62d8dbba5a2e8d78c7d90a9
Shape Recognition and Illusory Conjunctions,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Kevin J. Lang', 'dblp_profile': 'https://dblp.org/pid/67/4988.html'}]",1985,"One way to achieve viewpoint-invariant shape recognition is to impose a canonical, object-based frame of reference on a shape and to describe the positions, sizes and orientations of the shape's features relative to the imposed frame. This compulation can be implemented in a parallel network of neuron-like processors, but the network has a tendency to make errors of a peculiar kind: When presented with several shapes it sometimes perceives one shape in the position of another. The parameters can be carefully tuned to avoid these ""illusory conjunctions"" in normal circumstances, but they reappear if the visual input is replaced by a random mask before the network has settled down. Treisman and Schmidt (1982) have shown that people make similar errors.",6b88f41738085c1a2bffe6123541755b1118e5e2
"Massively Parallel Architectures for AI: NETL, Thistle, and Boltzmann Machines","[{'name': 'Scott E. Fahlman', 'dblp_profile': 'https://dblp.org/pid/16/3174.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Terrence J. Sejnowski', 'dblp_profile': 'https://dblp.org/pid/26/2197.html'}]",1983,"It is becoming increasingly apparent that some aspects of intelligent behavior require enormous computational power and that some sort of massively parallel computing architecture is the most plausible way to deliver such power. Parallelism, rather than raw speed of the computing elements, seems to be the way that the brain gets such jobs done. But even if die need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various AI tasks. 
 
In this paper we will attempt to isolate a number of basic computational tasks that an intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of diese computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number of tasks that are inefficient or impossible on the other architectures.",a1e33c9f79f993cc2f7d917d8cb6baee095c570d
A Parallel Computation that Assigns Canonical Object-Based Frames of Reference,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1981,"A viewpoint-independent description of the shape of an object can be generated by imposing a canonical frame of reference on the object and describing the spatial dispositions of the parts relative to this object-based frame. When a familiar object is in an unusual orientation, the deciding factor in the choice of the canonical object-based frame may be the fact that relative to this frame the object has a familiar shape description. This may suggest that we first hypothesise an object-based frame and then test the resultant shape description for familiarity. However, it is possible to organise the interactions between units in a parallel network so that the pattern of activity in the network simultaneously converges on a representation of the shape and a representation of the object-based frame of reference. The connections in the network are determined by the constraints inherent in the image formation process.",5b87030ad4ef55fa7c563938217d87b840c2a158
Shape Representation in Parallel Systems,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1981,"There has been a recent revival of interest in parallel systems in which computation is performed by excitatory and inhibitory interactions within a network of relatively simple, neuronlike units [1 2 3 4]. At the early stages of visual processing, individual units can represent hypotheses about how small local fragments of the visual input should be interpreted, and interactions between units can encode knowledge about the constraints between local interpretations. Higher up in the visual system, the representational issues are more complex. This paper considers the difficulties involved in representing shapes in parallel systems, and suggests ways of overcoming them. In doing so, it provides a mechanism for shape perception and visual attention which allows a novel interpretation of the Gestalt slogan that the whole is more than the sum of its parts.",272cb3db246145e13f8db4acbe1d2eae088c7677
Some Demonstrations of the Effects of Structural Descriptions in Mental Imagery,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1979,"A visual imagery task is presented which is beyond the limits of normal human ability, and some of the factors contributing to its difficulty are isolated by comparing the difficulty of related tasks. It is argued that complex objects are assigned hierarchical structural descriptions by being parsed into parts, each of which has its own local system of significant directions. Two quite different schemas for a wire-frame cube are used to illustrate this theory, and some striking perceptual differences to which they give rise are described. The difficulty of certain mental imagery tasks is shown to depend on which of the alternative structural descriptions of an object is used, and this is interpreted as evidence that structural descriptions are an important component of mental images. Finally, it is argued that analog transformations like mental folding involve changing the values of continuous variables in a structural description.",487a36ae299563f9aaea846cf669abfa4906b8fe
Representation and Control in Vision,"[{'name': 'Aaron Sloman', 'dblp_profile': 'https://dblp.org/pid/23/5327.html'}, {'name': 'David Owen', 'dblp_profile': 'https://dblp.org/pid/42/1719.html'}, {'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}, {'name': 'Frank Birch', 'dblp_profile': 'https://dblp.org/pid/70/5118.html'}, {'name': ""Frank O'Gorman"", 'dblp_profile': 'https://dblp.org/pid/25/2413.html'}]",1978,,cd5158389fdb229879c89d5f66b2f84b3f87352c
Relaxation and its role in vision,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1977,,f41872eac65a9ff218ae8a75b97532c4654f9c71
Using Relaxation to find a Puppet,"[{'name': 'Geoffrey E. Hinton', 'dblp_profile': 'https://dblp.org/pid/10/3248'}]",1976,"The problem of finding a puppet in a configuration of overlapping, transparent rectangles is used to show how a relaxation algorithm can extract the globally best figure from a network of conflicting local interpretations.",f6afa2a788e79f180a0591f441caf34a355732cb
