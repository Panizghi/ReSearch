[
{"Title": "Computing 2002", "URL": "https://dl.acm.org/doi/10.1145/543812.543816", "Full Abstract": "No abstract available."},
{"Title": "Approaching utopia", "URL": "https://dl.acm.org/doi/10.1145/2422436.2422463", "Full Abstract": "We introduce and study strongly truthful mechanisms and their applications. We use strongly truthful mechanisms as a tool for implementation in undominated strategies for several problems, including the design of externality resistant auctions and a variant of multi-dimensional scheduling."},
{"Title": "Advances in Knowledge Discovery and Data Mining", "URL": "https://dl.acm.org/doi/book/10.5555/2821229", "Full Abstract": "This two-volume set, LNAI 9077 + 9078, constitutes the refereed proceedings of the 19th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2015, held in Ho Chi Minh City, Vietnam, in May 2015.The proceedings contain 117 paper carefully reviewed and selected from 405 submissions. They have been organized in topical sections named: social networks and social media; classification; machine learning; applications; novel methods and algorithms; opinion mining and sentiment analysis; clustering; outlier and anomaly detection; mining uncertain and imprecise data; mining temporal and spatial data; feature extraction and selection; mining heterogeneous, high-dimensional and sequential data; entity resolution and topic-modeling; itemset and high-performance data mining; and recommendations."},
{"Title": "The Beckman report on database research", "URL": "https://dl.acm.org/doi/10.1145/2845915", "Full Abstract": "Database researchers paint big data as a defining challenge. To make the most of the enormous opportunities at hand will require focusing on five research areas."},
{"Title": "Competitive generalized auctions", "URL": "https://dl.acm.org/doi/10.1145/509907.509921", "Full Abstract": "We describe mechanisms for auctions that are simultaneously truthful (alternately known as strategy-proof or incentive compatible) and guarantee high \"net\" profit. We make use of appropriate variants of competitive analysis of algorithms in designing and analyzing our mechanisms. Thus, we do not require any probabilistic assumptions on bids.We present two new concepts regarding auctions, that of a cancellable auction and that of a generalized auction. We use cancellable auctions in the design of generalized auctions, but they are of independent interest as well. Cancellable auctions have the property that if the revenue collected does not meet certain predetermined criteria, then the auction can be cancelled and the resulting auction is still truthful. The trivial approach (run a truthful auction and cancel if needed) yields an auction that is not necessarily truthfu.Generalized auctions can be used to model many problems previously considered in the literature, as well as numerous new problems. In particular, we give the first truthful profit-maximizing auctions for problems such as conditional financing and multicast."},
{"Title": "RAPID", "URL": "https://dl.acm.org/doi/10.1016/S0925-7721%2898%2900008-X", "Full Abstract": "No abstract available."},
{"Title": "Implementation of an interpreter for a parallel language in Centaur", "URL": "https://dl.acm.org/doi/10.5555/92011.92016", "Full Abstract": "No abstract available."},
{"Title": "The CAP filing system", "URL": "https://dl.acm.org/doi/10.1145/800214.806542", "Full Abstract": "The filing system for the CAP is based on the idea of preservation of capabilities: if a program has been able to obtain some capability then it has an absolute right to preserve it for subsequent use. The pursuit of this principle, using capability-oriented mechanisms in preference to access control lists, has led to a filing system in which a preserved capability may be retrieved from different directories to achieve different access statuses, in which the significance of a text name depends on the directory to which it is presented, and in which filing system 'privilege' is expressed by possession of directory capabilities."},
{"Title": "An asynchronous garbage collector for the CAP filing system", "URL": "https://dl.acm.org/doi/10.1145/775332.775338", "Full Abstract": "Copyright © 1978 Authors."},
{"Title": "VeriPhy: verified controller executables from verified cyber-physical system models", "URL": "https://dl.acm.org/doi/10.1145/3192366.3192406", "Full Abstract": "We present VeriPhy, a verified pipeline which automatically transforms verified high-level models of safety-critical cyber-physical systems (CPSs) in differential dynamic logic (dL) to verified controller executables. VeriPhy proves that all safety results are preserved end-to-end as it bridges abstraction gaps, including: i) the gap between mathematical reals in physical models and machine arithmetic in the implementation, ii) the gap between real physics and its differential-equation models, and iii) the gap between nondeterministic controller models and machine code. VeriPhy reduces CPS safety to the faithfulness of the physical environment, which is checked at runtime by synthesized, verified monitors. We use three provers in this effort: KeYmaera X, HOL4, and Isabelle/HOL. To minimize the trusted base, we cross-verify KeYmaeraX in Isabelle/HOL. We evaluate the resulting controller and monitors on commodity robotics hardware."},
{"Title": "The Claremont report on database research", "URL": "https://dl.acm.org/doi/10.1145/1516046.1516062", "Full Abstract": "Database research is expanding, with major efforts in system architecture, new languages, cloud services, mobile and virtual worlds, and interplay between structure and text."},
{"Title": "Program Checkers for Probability Generation", "URL": "https://dl.acm.org/doi/10.5555/646245.684535", "Full Abstract": "No abstract available."},
{"Title": "Combining dimensionality and rate of growth arguments for establishing lower bounds on the number of multiplications", "URL": "https://dl.acm.org/doi/10.1145/800119.803912", "Full Abstract": "In this paper we describe a new method for establishing lower bounds for the number of multiplications and divisions required to compute rational functions. We shall start by reminding the reader of some standard notations."},
{"Title": "COMBINING DIMENSIONALITY AND RATE OF GROWTH ARGUMENTS FOR ESTABLISHING LOWER BOUNDS ON THE NUMBER OF MULTIPLICATIONS", "URL": "https://dl.acm.org/doi/book/10.5555/889589", "Full Abstract": "No abstract available."},
{"Title": "Functions as Processes", "URL": "https://dl.acm.org/doi/10.5555/646244.684376", "Full Abstract": "No abstract available."},
{"Title": "Interpreting one concurrent calculus in another", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2890%2990059-Q", "Full Abstract": "No abstract available."},
{"Title": "Is Your Cat Infected with a Computer Virus?", "URL": "https://dl.acm.org/doi/10.1109/PERCOM.2006.32", "Full Abstract": "RFID systems as a whole are often treated with suspicion, but the input data received from individual RFID tags is implicitly trusted. RFID attacks are currently conceived as properly formatted but fake RFID data; however no one expects an RFID tag to send a SQL injection attack or a buffer overflow. This paper is meant to serve as a warning that data from RFID tags can be used to exploit back-end software systems. RFID middleware writers must therefore build appropriate checks (bounds checking, special character filtering, etc.), to prevent RFID middleware from suffering all of the well-known vulnerabilities experienced by the Internet. Furthermore, as a proof of concept, this paper presents the first self-replicating RFID virus. This virus uses RFID tags as a vector to compromise backend RFID middleware systems, via a SQL injection attack."},
{"Title": "Can We Make Operating Systems Reliable and Secure?", "URL": "https://dl.acm.org/doi/10.1109/MC.2006.156", "Full Abstract": "Microkernels--long discarded as unacceptable because of their lower performance compared with monolithic kernels--might be making a comeback in operating systems due to their potentially higher reliability, which many researchers now regard as more important than performance."},
{"Title": "WebIQ", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2006.172", "Full Abstract": "Integrating Deep Web sources requires highly accurate semantic matches between the attributes of the source query interfaces. These matches are usually established by comparing the similarities of the attributes' labels and instances. However, attributes on query interfaces often have no or very few data instances. The pervasive lack of instances seriously reduces the accuracy of current matching techniques. To address this problem, we describe WebIQ, a solution that learns from both the Surface Web and the Deep Web to automatically discover instances for interface attributes. WebIQ extends question answering techniques commonly used in the AI community for this purpose. We describe how to incorporate WebIQ into current interface matching systems. Extensive experiments over five realworld domains show the utility ofWebIQ. In particular, the results show that acquired instances help improve matching accuracy from 89.5% F-1 to 97.5%, at only a modest runtime overhead."},
{"Title": "On Parallel Computation for the Knapsack Problem", "URL": "https://dl.acm.org/doi/10.1145/322326.322342", "Full Abstract": "Copyright © 1982 ACM."},
{"Title": "On the Average-Case Complexity of Selecting the ", "URL": "https://dl.acm.org/doi/10.1137/0211034", "Full Abstract": "Let $\\bar V_k (n)$ be the minimum average number of pairwise comparisons needed to find the"},
{"Title": "Protocols for secure computations", "URL": "https://dl.acm.org/doi/10.5555/1382436.1382751", "Full Abstract": "No abstract available."},
{"Title": "Probabilistic Game Automata", "URL": "https://dl.acm.org/doi/10.5555/648296.754863", "Full Abstract": "No abstract available."},
{"Title": "Probabilistic game automata", "URL": "https://dl.acm.org/doi/10.5555/20284.20295", "Full Abstract": "No abstract available."},
{"Title": "Firing Squad", "URL": "https://dl.acm.org/doi/10.5555/647694.731341", "Full Abstract": "No abstract available."},
{"Title": "A Security Architecture for Object-Based Distributed Systems", "URL": "https://dl.acm.org/doi/10.5555/784592.784804", "Full Abstract": "Large-scale distributed systems present numerous securityproblems not present in local systems. In this paperwe present a general security architecture for a large-scaleobject-based distributed system. Its main features includeways for servers to authenticate clients, clients to authenticateservers, new secure servers to be instantiated withoutmanual intervention, and ways to restrict which client canperform which operation on which object. All of these featuresare done in a platform- and application-independentway, so the results are quite general. The basic idea behindthe scheme is to have each object owner issue cryptographicallysealed certificates to users to prove which operationsthey may request and to servers to prove which operationsthey are authorized to execute. These certificates are usedto ensure secure binding and secure method invocation. Thepaper discusses the required certificates and security protocolsfor using them."},
{"Title": "Enforcing security policies for distributed objects applications", "URL": "https://dl.acm.org/doi/10.1007/11542322_16", "Full Abstract": "In this paper we present the design and the implementation of a policy engine for enforcing security policies for distributed applications. Such policies, represented by using the RBAC model, include both how the distributed, shared and replicated objects are used, by mean of"},
{"Title": "Building data integration systems", "URL": "https://dl.acm.org/doi/10.5555/3104278.3104314", "Full Abstract": "No abstract available."},
{"Title": "A compact data structure for storing, retrieving and manipulating line drawings", "URL": "https://dl.acm.org/doi/10.1145/1465482.1465580", "Full Abstract": "The field of graphical man/machine interaction is customarily split into hardware and software areas. The former can be considered to have come of age: there are over twenty-five brands of off-the-shelf consoles with all the requisite input devices, and new techniques and improvements are constantly being developed. Many consoles are also provided with primitive supporting software which allow one to draw points, lines, arcs, etc., in a symbolic language of some sort. Less well understood and developed, however, is that aspect of display software concerned with representing and manipulating the problem model from which these primitive point/line/arc pictures are derived. The \"data structure\" is the machine representation of the often complex and hierarchical problem model. It must be judiciously derived from the model on the one hand and, on the other, lead readily to the reduced console display file of points, lines and arcs which cause the actual visual display. Furthermore, the data structure must be efficiently stored and processed (usually contradictory requirements)."},
{"Title": "On the average-case complexity of selecting the k-th best", "URL": "https://dl.acm.org/doi/book/10.5555/892214", "Full Abstract": "Let ${\\bar{V_k$(n) be the minimum average number of pairwise comparisons needed to find the k-th largest of n numbers (k $\\leq$ 2), assuming that all n! orderings are equally likely. D. W. Matula proved that, for some absolute constant c, ${\\bar{V_k$(n)-n $\\leq$ c k log log n as n $\\rightarrow \\infty$. In the present paper, we show that there exists an absolute constant c' < 0 such that ${\\bar{V_k$(n)-n $\\leq$ c' k log log n as n $\\rightarrow \\infty$, proving a conjecture of Matula."},
{"Title": "Some complexity questions related to distributive computing(Preliminary Report)", "URL": "https://dl.acm.org/doi/10.1145/800135.804414", "Full Abstract": "Let"},
{"Title": "Should tables by sorted?", "URL": "https://dl.acm.org/doi/book/10.5555/892229", "Full Abstract": "We examine optimality questions in the following information retrieval problem: Given a set S of n keys, store them so that queries of the form \"Is x $\\in$ S?\" can be answered quickly. It is shown that, in a rather general model including al1 the commonly-used schemes, $\\lceil$ lg(n+l) $\\rceil$ probes to the table are needed in the worst case, provided the key space is sufficiently large. The effects of smaller key space and arbitrary encoding are also explored."},
{"Title": "Completing the temporal picture", "URL": "https://dl.acm.org/doi/book/10.5555/892484", "Full Abstract": "The paper presents a relatively complete proof system for proving the validity of temporal properties of reactive programs. The presented proof system improves oll previous temporal systems, such as [MP83a] and [MP83b], in that it reduces the validity of program properties into pure assertional reasoning, not involving additional temporal reasoning. The proof system is based on the classification of temporal properties according to the Borel hierarchy, providing an appropriate proof rule for each of the main classes, such as safety, response, and progress properties."},
{"Title": "The logical basis for computer programming: vol. 2, deductive systems", "URL": "https://dl.acm.org/doi/book/10.5555/78091", "Full Abstract": "No abstract available."},
{"Title": "An algebraic definition of simulation between programs", "URL": "https://dl.acm.org/doi/book/10.5555/891902", "Full Abstract": "A simulation relation between programs is defined which is quasi-ordering. Mutual simulation is then an equivalence relation, and by dividing out by it we abstract from a program such details as how the sequencing is controlled and how data is represented. The equivalence classes are approxiamtions to the algorithms which are realized, or expressed, by their member programs. A technique is given and illustrated for proving simulation and equivalence of programs; there is an analogy with Floyd's technique for proving correctness of programs. Finally, necessary and sufficient conditions for simulation are given."},
{"Title": "An algebraic definition of simulation between programs", "URL": "https://dl.acm.org/doi/10.5555/1622876.1622926", "Full Abstract": "A simulation relation between programs is defined which is a quasi-ordering. Mutual simulation is then an equivalence relation, and by dividing out by it we abstract from a program such details as how the sequencing is controlled and how data is represented. The equivalence classes are approximations to the algorithms which are realized, or expressed, by their member programs."},
{"Title": "Implementation and applications of Scott's logic for computable functions", "URL": "https://dl.acm.org/doi/10.1145/800235.807067", "Full Abstract": "The basis for this paper is a logic designed by Dana Scott [1] in 1969 for formalizing arguments about computable functions of higher type. This logic uses typed combinators, and we give a more or less direct translation into typed λ-calculus, which is an easier formalism to use, though not so easy for the metatheory because of the presence of bound variables. We then describe, by example only, a proof-checker program which has been implemented for this logic; the program is fully described in [2]. We relate the induction rule which is central to the logic to two more familiar rules - Recursion Induction and Structural Induction - showing that the former is a theorem of the logic, and that for recursively defined structures the latter is a derived rule of the logic. Finally we show how the syntax and semantics of a simple programming language may be described completely in the logic, and we give an example of a theorem which relates syntactic and semantic properties of programs and which can be stated and proved within the logic."},
{"Title": "Report on the seventh ACM SIGOPS European workshop", "URL": "https://dl.acm.org/doi/10.1145/254784.254787", "Full Abstract": "Copyright © 1997 Author."},
{"Title": "Abstracting probabilistic actions", "URL": "https://dl.acm.org/doi/10.5555/2074394.2074429", "Full Abstract": "This paper discusses the problem of abstracting conditional probabilistic actions. We identify two distinct types of abstraction: intra-action abstraction and inter-action abstraction. We define what it means for the abstraction of an action to be correct and then derive two methods of intra-action abstraction and two methods of inter-action abstraction which are correct according to this criterion. We illustrate the developed techniques by applying them to actions described with the temporal action representation used in the DRIPS decision-theoretic planner and we describe how the planner uses abstraction to reduce the complexity of planning."},
{"Title": "Efficient decision-theoretic planning", "URL": "https://dl.acm.org/doi/10.5555/2074158.2074184", "Full Abstract": "This paper discusses techniques for performing efficient decision-theoretic planning. We give an overview of the DRIPS decision-theoretic refinement planning system, which uses abstraction to efficiently identify optimal plans. We present techniques for automatically generating search control information, which can significantly improve the planner's performance. We evaluate the efficiency of DRIPS both with and without the search control rules on a complex medical planning problem and compare its performance to that of a branch-and-bound decision tree algorithm."},
{"Title": "Experience with a regular expression compiler", "URL": "https://dl.acm.org/doi/book/10.5555/892299", "Full Abstract": "The language of regular expressions is a useful one for specifying certain sequebtial processes at a very high level. They allow easy modification of designs for circuits, like controllers, that are described by patterns of events they must recognize and the responses they must make to those patterns. This paper discusses the compilation of such expressions into reasonably compact layouts. The translation of regular expressions into nondeterministic automata by two different methods is discussed, along with the advantages of each method. A major part of the compilation problem is selection of good state codes for the nondeterministic automata; one successful strategy is explained in the paper."},
{"Title": "Competitive snoopy caching", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1986.14", "Full Abstract": "In a snoopy cache multiprocessor system, each processor has a cache in which it stores blocks of data. Each cache is connected to a bus used to communicate with the other caches and with main memory. For several of the proposed models of snoopy caching, we present new on-line algorithms which decide, for each cache, which blocks to retain and which to drop in order to minimize communication over the bus. We prove that, for any sequence of operations, our algorithms' communication costs are within a constant factor of the minimum required for that sequence; for some of our algorithms we prove that no on-line algorithm has this property with a smaller constant."},
{"Title": "A study of concrete computational complexity.", "URL": "https://dl.acm.org/doi/book/10.5555/907570", "Full Abstract": "No abstract available."},
{"Title": "On computing the minima of quadratic forms (Preliminary Report)", "URL": "https://dl.acm.org/doi/10.1145/800116.803749", "Full Abstract": "The following problem was recently raised by C. William Gear [1]: Let F(x"},
{"Title": "Addition chains with multiplicative cost", "URL": "https://dl.acm.org/doi/book/10.5555/892092", "Full Abstract": "If each step in an addition chain is assigned a cost equal to the product of the numbers added at that step, \"binary\" addition chains are shown to minimize total cost."},
{"Title": "Special relations in automated deduction", "URL": "https://dl.acm.org/doi/10.1145/4904.4905", "Full Abstract": "Two deduction rules are introduced to give streamlined treatment to relations of special importance in an automated theorem-proving system. These rules, the"},
{"Title": "A timely resolution", "URL": "https://dl.acm.org/doi/book/10.5555/892378", "Full Abstract": "We present a novel proof system R for First-order (Linear) Temporal Logic. This system extends our Propositional Temporal Logic proof system ([AM]). The system R is based on nonclausal resolution; proofs are natural and generally short. Special quantifier rules, unification techniques, and a resolution rule are introduced. We relate R to other proof systems for First-order Temporal Logic and discuss completeness issues. The system R should be useful as a tool for such tasks as verification of concurrent programs and reasoning about hardware devices."},
{"Title": "Model theorem proving", "URL": "https://dl.acm.org/doi/book/10.5555/892375", "Full Abstract": "We describe resolution proof systems for several modal logics. First we present the propositional versions of the systems and prove their completeness. The first-order resolution rule for classical logic is then modified to handle quantifiers directly. This new resolution rule enables us to extend our propositional systems to complete first-order systems. The systems for the different modal logics are closely related."},
{"Title": "How to Clear a Block", "URL": "https://dl.acm.org/doi/10.5555/648227.749317", "Full Abstract": "No abstract available."},
{"Title": "Modal Theorem Proving", "URL": "https://dl.acm.org/doi/10.5555/648227.751971", "Full Abstract": "No abstract available."},
{"Title": "A deductive approach to program synthesis", "URL": "https://dl.acm.org/doi/10.5555/31870.31871", "Full Abstract": "No abstract available."},
{"Title": "Finding hidden Hamiltonian cycles", "URL": "https://dl.acm.org/doi/10.1145/103418.103442", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "Existence and construction of edge disjoint paths on expander graphs", "URL": "https://dl.acm.org/doi/10.1145/129712.129727", "Full Abstract": "Copyright © 1992 ACM."},
{"Title": "Near-perfect Token Distribution", "URL": "https://dl.acm.org/doi/10.5555/646246.684721", "Full Abstract": "No abstract available."},
{"Title": "On the satisfiability and maximum satisfiability of random 3-CNF formulas", "URL": "https://dl.acm.org/doi/10.5555/313559.313794", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Scheduling Unit-Time Tasks with Limited Resources", "URL": "https://dl.acm.org/doi/10.5555/647406.724133", "Full Abstract": "No abstract available."},
{"Title": "On-line choice of on-line algorithms", "URL": "https://dl.acm.org/doi/10.5555/313559.313847", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "On the problem of approximating the number of bases of a matroid", "URL": "https://dl.acm.org/doi/10.1016/0020-0190%2894%2990037-X", "Full Abstract": "No abstract available."},
{"Title": "Existence and Construction of Edge-Disjoint Pathson Expander Graphs", "URL": "https://dl.acm.org/doi/10.1137/S0097539792232021", "Full Abstract": "Given an expander graph $G=(V,E)$ and a set of $q$ disjoint pairs of vertices in $V$, the authors are interested in finding for each pair $(a_i, b_i)$ a path connecting $a_i$ to $b_i$ such that the set of $q$ paths so found is edge disjoint. (For general graphs the related decision problem is NP complete.)"},
{"Title": "A hardware semantics based on temporal intervals", "URL": "https://dl.acm.org/doi/book/10.5555/892293", "Full Abstract": "We present an interval-based temporal logic that permits the rigorous specification of a variety of hardware components and facilitates describing properties such as correctness of implementation. Conceptual levels of circuit operation ranging from detailed quantitative timing and signal propagation up to functional behavior are integrated in a unified way. After giving some motivation for reasoning about hardware, we present the propositional and first-order syntax and semantics of the temporal logic. In addition we illustrate techniques for describing signal transitions as well as for formally specifying and comparing a number of delay models. Throughout the discussion, the formalism provides a means for examining such concepts as device equivalence and internal states."},
{"Title": "Proving precedence properties: the temporal way", "URL": "https://dl.acm.org/doi/book/10.5555/892294", "Full Abstract": "This paper explores the three important classes of temporal properties of concurrent programs: invariance, liveness and precedence. It presents the first methodological approach to the precedence properties, while providing a review of the invariance and liveness properties. The approach is based on the 'unless' operator, which is a weak version of the 'until' operator. For each class of properties, we present a single complete proof principle. Finally, we show that the properties of each class are decidable over finite state programs."},
{"Title": "Verification of concurrent programs: a temporal proof system", "URL": "https://dl.acm.org/doi/book/10.5555/892296", "Full Abstract": "A proof system based on temporal logic is presented for proving properties of concurrent programs based on the shared-variables computation model. The system consists of three parts: the general uninterpreted part, the domain dependent part and the program dependent part. In the general part we give a complete proof system for first-order temporal logic with detailed proofs of useful theorems. This logic enables reasoning about general time sequences. The domain dependent part characterizes the special properties of the domain over which the program operates. The program dependent part introduces program axioms which restrict the time sequences considered to be execution sequences of a given program. The utility of the full system is demonstrated by proving invariance, liveness and precedence properties of several concurrent programs. Derived proof principles for these classes of properties are obtained and lead to a compact representation of proofs."},
{"Title": "Reasoning in Interval Temporal Logic", "URL": "https://dl.acm.org/doi/10.5555/648064.747582", "Full Abstract": "No abstract available."},
{"Title": "Reasoning in interval temporal logic", "URL": "https://dl.acm.org/doi/book/10.5555/892297", "Full Abstract": "Predicate logic is a powerful and general descriptive formalism with a long history of development. However, since the logic's underlying semantics have no notion of time, statements such as \"I increases by 2\" cannot be directly expressed. We discuss interval temporal logic (ITL), a formalism that augments standard predicate logic with operators for time-dependent concepts. Our earlier work used ITL to specify and reason about hardware. In this paper we show how ITL can also directly capture various control structures found in conventional programming languages. Constructs are given for treating assignment, iteration, sequential and parallel computations and scoping. The techniques used permit specification and reasoning about such algorithms as concurrent Quicksort. We compare ITL with the logic-based programming languages Lucid and Prolog."},
{"Title": "A Hardware Semantics Based on Temporal Intervals", "URL": "https://dl.acm.org/doi/10.5555/646237.683015", "Full Abstract": "No abstract available."},
{"Title": "Proving Precedence Properties", "URL": "https://dl.acm.org/doi/10.5555/646237.683020", "Full Abstract": "No abstract available."},
{"Title": "The r-Stirling numbers", "URL": "https://dl.acm.org/doi/book/10.5555/892285", "Full Abstract": "The r-Stirling numbers of the first and second kind count restricted permutations and respectively restricted partitions, the restriction being that the first r elements must be in distinct cycles and respectively distinct subsets. The combinatorial and algebraic properties of these numbers, which is most cases generalize similar properties of the regular Stirling numbers, are explored starting from the above definition."},
{"Title": "Efficient fault tolerant routings in networks", "URL": "https://dl.acm.org/doi/10.1145/800057.808724", "Full Abstract": "We analyze the problem of constructing a network which will have a fixed routing and which will be highly fault tolerant. A construction is presented which forms a “product route graph” from two or more constituent “route graphs.” The analysis involves the"},
{"Title": "A provably secure polynomial approximation scheme for the distributed lottery problem (extended abstract)", "URL": "https://dl.acm.org/doi/10.1145/323596.323608", "Full Abstract": "Copyright © 1985 ACM."},
{"Title": "Efficient fault-tolerant routings in networks", "URL": "https://dl.acm.org/doi/10.1016/0890-5401%2887%2990063-0", "Full Abstract": "We analyze the problem of constructing a network with a given number of nodes which has a fixed routing and which is highly fault tolerant. A construction is presented which forms a “product route graph” from two or more constituent “route graphs.” The analysis involves the"},
{"Title": "On the second eigenvalue of random regular graphs", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1987.45", "Full Abstract": "Expanders have many applications in Computer Science. It is known that random d-regular graphs are very efficient expanders, almost surely. However, checking whether a particular graph is a good expander is co-NP-complete. We show that the second eigenvalue of d-regular graphs, λ2, is concentrated in an interval of width O(√d) around its mean, and that its mean is O(d3/4). The result holds under various models for random d-regular graphs. As a consequence a random d-regular graph on n vertices, is, with high probability a certifiable efficient expander for n sufficiently large. The bound on the width of the interval is derived from martingale theory and the bound on E(λ2) is obtained by exploring the properties of random walks in random graphs."},
{"Title": "On Generating Solved Instances of Computational Problems", "URL": "https://dl.acm.org/doi/10.5555/646753.704892", "Full Abstract": "No abstract available."},
{"Title": "The cost distribution of clustering in random probing", "URL": "https://dl.acm.org/doi/10.1145/77600.77619", "Full Abstract": "A new approach to the analysis of random probing hashing algorithms is presented. The probability-generating function in closed form for the asymptotic cost of insertion via random probing with secondary clustering is derived. For higher-order clustering, it is shown that all the moments of the probability distribution of the insertion cost exist and are asymptotically equal to the corresponding moments of the cost distribution under uniform hashing. The method in this paper also leads to simple derivations for the expected cost of insertion for random probing with secondary and higher-order clustering."},
{"Title": "A UNIX clone with source code for operating systems courses", "URL": "https://dl.acm.org/doi/10.1145/24592.24596", "Full Abstract": "Copyright © 1987 Author."},
{"Title": "Structured programming with recursion", "URL": "https://dl.acm.org/doi/book/10.5555/892162", "Full Abstract": "No abstract available."},
{"Title": "Is “sometime” sometimes better than “always”?", "URL": "https://dl.acm.org/doi/10.1145/359340.359353", "Full Abstract": "This paper explores a technique for proving the correctness and termination of programs simultaneously. This approach, the"},
{"Title": "Proving termination and multiset orderings", "URL": "https://dl.acm.org/doi/book/10.5555/892168", "Full Abstract": "A common tool for proving the termination of programs is the well-founded set, a set ordered in such a way as to admit no infinite descending sequences. The basic approach is to find a termination function that maps the elements of the program into some well-founded set, such that the value of the termination function is continually reduced throughout the computation. All too often, the termination functions required are difficult to find and are of a complexity out of proportion to the program under consideration. However, by providing more sophisticated well-founded sets, the corresponding termination functions can be simplified. Given a well-founded set S, we consider multisets over S, \"sets\" that admit multiple occurrences of elements taken from S. We define an ordering on all finite multisets over S that is induced by the given ordering on S. This multiset ordering is shown to be well-founded. The value of the multiset ordering is that it permits the use of relatively simple and intuitive termination functions in otherwise difficult termination proofs. In particular, we apply the multiset ordering to provide simple proofs of the termination of production systems, programs defined in terms of sets of rewriting rules."},
{"Title": "Inference rules for program annotation", "URL": "https://dl.acm.org/doi/10.5555/800099.803206", "Full Abstract": "Methods are presented whereby an Algol-like program, given together with its specifications, can be documented automatically. The program is incrementally annotated with invariant relationships that hold between program variables at intermediate points in the program and explain the actual workings of the program regardless of whether the program is correct. Thus this documentation can be used for proving the correctness of the program or may serve as an aid in the debugging of an incorrect program."},
{"Title": "The synthesis of structure-changing programs", "URL": "https://dl.acm.org/doi/10.5555/800099.803208", "Full Abstract": "Deductive techniques are presented for deriving programs systematically from given specifications. The specifications express the purpose of the desired program without giving any hint of the algorithm to be employed. The desired program is intended to achieve this purpose by means of such low-level primitives as assignment statements, the conditional statements, and recursion. The basic approach is to transform the specifications repeatedly according to certain rules, until a satisfactory program is produced. The rules are guided by a number of strategic controls."},
{"Title": "A deductive approach to program synthesis", "URL": "https://dl.acm.org/doi/book/10.5555/892190", "Full Abstract": "Program synthesis is the systematic derivation of a program from a given specification. A deductive approach to program synthesis is presented for the construction of recursive programs. This approach regards program synthesis as a theorem-proving task and relies on a theorem-proving method that combines the features of transformation rules, unification, and mathematical induction within a single framework."},
{"Title": "The translation of 'go to' programs to 'while' programs", "URL": "https://dl.acm.org/doi/10.5555/1241515.1241521", "Full Abstract": "Some of the papers presented in this book already have been widely circulated; others were published in well-known journals, like IBM Systems Journal but largely were ignored when they first appeared; and then there are the obscure papers like this one by Ashcroft and Manna, which was presented at the 1971 IFIP Conference in Ljubljana, Yugoslavia. It's not that the ideas in the paper are obscure -- it's just that very few people in the mainstream EDP community attended the Conference, and precious few copies of the conference proceedings ever found their way into American libraries. It is, however, a paper that many people over the years have wanted to read, particularly since it deals with a subject also mentioned by Knuth (\"Structured Programming with go to State, ments\" [see Paper 20]), Wulf (\"A Case Against the GOTO\" [Paper 8]), and Böm and Jacopini (\"Flow Diagrams, Turing Machines and Languages with Only Two Formation Rules\" [Paper 2])."},
{"Title": "The Modal Logic of Programs", "URL": "https://dl.acm.org/doi/10.5555/646233.682234", "Full Abstract": "No abstract available."},
{"Title": "Proving termination with Multiset Orderings", "URL": "https://dl.acm.org/doi/10.5555/646233.682248", "Full Abstract": "No abstract available."},
{"Title": "Proving termination with multiset orderings", "URL": "https://dl.acm.org/doi/10.1145/359138.359142", "Full Abstract": "A common tool for proving the termination of programs is the"},
{"Title": "A deductive approach to program synthesis", "URL": "https://dl.acm.org/doi/10.5555/1624861.1624985", "Full Abstract": "Program synthesis is the systematic derivation of a program from a given specification. A deductive approach to program synthesis is presented for the construction of recursive programs. This approach regards program synthesis as a theorem-proving task and relies on a theorem-proving method that combines the features of transformation rules, unification, and mathematical induction within a single framework."},
{"Title": "A Deductive Approach to Program Synthesis", "URL": "https://dl.acm.org/doi/10.1145/357084.357090", "Full Abstract": "Program synthesis is the systematic derivation of a program from a given specification. A deductive approach to program synthesis is presented for the construction of recursive programs. This approach regards program synthesis as a theorem-proving task and relies on a theorem-proving method that combines the features of transformation rules, unification, and mathematical induction within a single framework."},
{"Title": "Synchronous schemes and their decision problems", "URL": "https://dl.acm.org/doi/10.1145/567446.567453", "Full Abstract": "A class of schemes called synchronous schemes is defined. A synchronous scheme can have several variables, but all the active ones are required to keep a synchronized rate of computation as measured by the height of their respective Herbrand values. A \"reset\" statement, which causes all the variables to restart a new computation, is admitted. It is shown that equivalence, convergence, and other properties are decidable for schemes in this class. The class of synchronous schemes contains, as special cases, the known decidable classes of Ianov schemes, one-variable schemes with resets, and progressive schemes."},
{"Title": "Problematic features of programming languages: a situational-calculus approach", "URL": "https://dl.acm.org/doi/book/10.5555/892247", "Full Abstract": "Certain features of programming languages, such as data structure operations and procedure call mechanisms, have been found to resist formalization by classical techniques. An alternate approach is presented, based on a \"situational calculus,\" which makes explicit reference to the states of a computation. For each state, a distinction is drawn between an expression, its value, and the location of the value. Within this conceptual framework, the features of a programming language can be described axiomatically. Programs in the language can then be synthesized, executed, verified, or transformed by performing deductions in this axiomatic system. Properties of entire classes of programs, and of programming languages, can also be expressed and proved in this way. The approach is amenable to machine implementation. In a situational-calculus formalism it is possible to model precisely many \"problematic\" features of programming langauges, including operations on such data structures as arrays, pointers, lists, and records, and such procedure call mechanisms as call-by-reference, call-by-value, and call-by-name. No particular obstacle is presented by aliasing between variables, by declarations, or by recursive procedures. The paper is divided into three parts, focusing respectively on the assignment statement, on data structure operations, and on procedure call mechanisms. In this first part, we introduce the conceptual framework to be applied throughout and present the axiomatic definition of the assignment statement. If suitable restrictions on the programming language are imposed, the well-known Hoare assignment axiom can then be proved as a theorem. However, our definition can also describe the assignment statement of unrestricted programming languages, for which the Hoare axiom does not hold."},
{"Title": "The temporal logic of branching time", "URL": "https://dl.acm.org/doi/10.1145/567532.567551", "Full Abstract": "A temporal language and system are presented which are based on branching time structure. By the introduction of symmetrically dual sets of temporal operators, it is possible to discuss properties which hold either along one path or along all paths. Consequently it is possible to express in this system all the properties that were previously expressible in linear time or branching time systems. We present an exponential decision procedure for satisfiability in the language based on tableaux methods, and a complete deduction system. As associated temporal semantics is illustrated for both structured and graph representation of programs."},
{"Title": "Towards automatic debugging of programs", "URL": "https://dl.acm.org/doi/10.1145/390016.808434", "Full Abstract": "We present the germ of an idea for automatically correcting logical errors in programs by manipulating the invariants of the program. An invariant tree is defined, and we show how it can be used to change the program in order to guarantee correctness."},
{"Title": "The optimal fixedpoint of recursive programs", "URL": "https://dl.acm.org/doi/10.1145/800116.803769", "Full Abstract": "In this paper a new fixedpoint approach towards the semantics of recursive programs is presented. The fixedpoint defined by a recursive program under this semantics contains, in some sense, the maximal amount of “interesting” information which can be extracted from the program. This optimal fixedpoint (which always uniquely exists) may be strictly more defined than the program's least fixedpoint. We consider both the theoretical and the computational aspects of the approach, as well as some techniques for proving properties of the optimal fixedpoint of a given recursive program."},
{"Title": "Translating Program Schemas to While-Schemas", "URL": "https://dl.acm.org/doi/10.1137/0204011", "Full Abstract": "While-schemas are defined as program schemas without goto statements, in which iteration is achieved using while statements. We present two translations of program schemas into equivalent while-schemas, the first one by adding extra program variables, and the second one by adding extra logical variables. In both cases we aim to preserve as much of the structure of the original program schemas as possible."},
{"Title": "Knowledge and reasoning in program synthesis", "URL": "https://dl.acm.org/doi/10.5555/1624626.1624670", "Full Abstract": "Prograin synthesis is the construction of a computer program from given specifications. An automatic program synthesis system must combine reasoning and programming ability with a good deal of knowledge about the subject matter of the program. This ability and knowledge must be manifested both procedurally (by programs) and structurally (by choice of representation)."},
{"Title": "A new approach to recursive programs.", "URL": "https://dl.acm.org/doi/book/10.5555/892089", "Full Abstract": "In this paper we critically evaluate the classical least-fixedpoint approach towards recursive programs. We suggest a new approach which extracts the maximal amount of valuable information embedded in the programs. The presentation is informal, with emphasis on examples."},
{"Title": "The theoretical aspects of the optimal fixedpoint", "URL": "https://dl.acm.org/doi/book/10.5555/892093", "Full Abstract": "In thls paper we define a new type of fixedpoint of recursive definitions and investigate some of its properties. This optimal fixedpoint (which always uniquely exists) contains, in some sense, the maximal amount of \"interesting\" information which can be extracted from the recursive definition, and it may be strictly more defined than the program's least fixedpoint. This fixedpoint can be the basis for assigning a new semantics to recursive programs. This is a modified and extended version of part 1 of a paper presented at the Symposium on Theory of Computing, Albuquerque, New Mexico (May 1975)."},
{"Title": "Logical analysis of programs", "URL": "https://dl.acm.org/doi/10.1145/360032.360048", "Full Abstract": "Most present systems for verification of computer programs are incomplete in that intermediate inductive assertions must be provided manually by the user, termination is not proven, and incorrect programs are not treated. As a unified solution to these problems, this paper suggests conducting a logical analysis of programs by using invariants which express what is actually occurring in the program."},
{"Title": "The Theoretical Aspects of the Optimal Fixedpoint", "URL": "https://dl.acm.org/doi/10.1137/0205033", "Full Abstract": "In this paper we define a new type of fixedpoint of recursive definitions and investigate some of its properties. This optimal fixedpoint (which always uniquely exists) contains, in some sense, the maximal amount of “interesting” information which can be extracted from the recursive definition, and it may be strictly more defined than the program’s least fixedpoint. This fixedpoint can be the basis for assigning a new semantics to recursive programs."},
{"Title": "Is “sometime” sometimes better than “always”?", "URL": "https://dl.acm.org/doi/10.5555/800253.807645", "Full Abstract": "This paper explores a technique for proving the correctness and termination of programs simultaneously. This approach, which we call the"},
{"Title": "The evolution of programs: a system for automatic program modification", "URL": "https://dl.acm.org/doi/book/10.5555/892128", "Full Abstract": "An attempt is made to formulate techniques of program modification, whereby a program that achieves one result can be transformed into a new program that uses the same principles to achieve a different goal. For example, a program that uses the binary search paradigm to calculate the square-root of a number may be modified to divide two numbers in a similar manner, or vice versa. Program debugging is considered as a special case of modification: if a program computes wrong results, it must be modified to achieve the intended results. The application of abstract program schemata to concrete problems is also viewed from the perspective of modification techniques. We have embedded this approach in a running implementation; our methods are illustrated with several examples that have been performed by it."},
{"Title": "Studies in Automatic Programming Logic", "URL": "https://dl.acm.org/doi/book/10.5555/578683", "Full Abstract": "No abstract available."},
{"Title": "Studies in Automatic Programming Logic", "URL": "https://dl.acm.org/doi/book/10.5555/578684", "Full Abstract": "No abstract available."},
{"Title": "The evolution of programs", "URL": "https://dl.acm.org/doi/10.1145/512950.512964", "Full Abstract": "A programmer spends more time modifying already existing programs than constructing original ones. An attempt is made to formulate techniques of program modification, whereby a program that achieves one result can be transformed into a new program that uses the same principles to achieve a different goal. For example, a program that uses the binary search paradigm to divide two numbers may be modified to calculate the square-root of a number in a similar manner.Program debugging is considered as a special case of modification if a program computers wrong results, it must be modified to achieve the intended results The application of abstract program schemata to concrete problems is also viewed from the perspective of modification techniques.We, have embedded this approach in a running implementation; our methods are illustrated with several examples that have been performed by it."},
{"Title": "Is \"sometime\" sometimes better than \"always\"? Intermittent assertions in proving program correctness", "URL": "https://dl.acm.org/doi/book/10.5555/892103", "Full Abstract": "This paper explores a technique for proving the correctness and termination of programs simultaneously. This approach, which we call the intermittent-assertion method, involves documenting the program with assertions that must be true at some time when control passes through the corresponding point, but that need not be true every time. The method, introduced by Burstall, promises to provide a valuable complement to the more conventional methods. We first introduce the intermittent-assertion method with a number of examples of correctness and termination proofs. Some of these proofs are markedly simpler than their conventional counterparts. On the other hand, we show that a proof of correctness or termination by any of the conventional techniques can be rephrased directly as a proof using intermittent assertions. Finally, we show how the intermittent assertion method can be applied to prove the validity of program transformations and the correctness of continuously operating programs. This is a revised and simplified version of a previous paper with the same title (AIM-281, June 1976)."},
{"Title": "The convergence of functions to fixedpoints of recursive definitions", "URL": "https://dl.acm.org/doi/book/10.5555/892143", "Full Abstract": "The classical method for constructing the least fixedpoint of a recursive definition is to generate a sequence of functions whose initial element is the totally undefined function and which converges to the desired least fixedpoint. This method, due to Kleene, cannot be generalized to allow the construction of other fixedpoints. In this paper we present an alternate definition of convergence and a new fixedpoint access method of generating sequences of functions for a given recursive definition. The initial function of the sequence can be an arbitrary function, and the sequence will always converge to a fixedpoint that is \"close\" to the initial function. This defines a monotonic mapping from the set of partial functions onto the set of all fixedpoints of the given recursive definition."},
{"Title": "The people's time sharing system", "URL": "https://dl.acm.org/doi/10.1002/spe.4380030204", "Full Abstract": "A set of programs running under a multiprogramming batch operating system on the CDC 6600 which provide remote users with a time sharing service is described. The basis for the system is the ability of a user program to create job control statements during execution, thereby tricking the operating system into treating it as an ordinary batch job. The text editor and the interactive debugging facilities are described. The performance of the system, known as the People's Time Sharing System (PTSS), and user reaction to it are also described."},
{"Title": "Formalization of Properties of Functional Programs", "URL": "https://dl.acm.org/doi/10.1145/321592.321606", "Full Abstract": "The problems of convergence, correctness, and equivalence of computer programs can be formulated by means of the satisfiability or validity of certain first-order formulas. An algorithm is presented for constructing such formulas for functional programs, i.e. programs defined by LISP-like conditional recursive expressions."},
{"Title": "Towards automatic program synthesis", "URL": "https://dl.acm.org/doi/book/10.5555/891872", "Full Abstract": "An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details. The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees. In order to construct a program satisfying certain specifications, a theorem induced by those specifications is proved, and the desired program is extracted from the proof. The same technique is applied to transform recursively defined functions into iterative programs, frequently with a major gain in efficiency. It is emphasized that in order to construct a program with loops or with recursion, the principle of mathematical induction must be applied. The relation between the version of the induction rule used and the form of the program constructed is explored in some detail."},
{"Title": "The translation of ''go to'' programs to ''while'' programs", "URL": "https://dl.acm.org/doi/book/10.5555/891881", "Full Abstract": "In this paper we show that every flowchart program can be written without go to$ statements by using while$ statements. The main idea is to introduce new variables to preserve the values of certain variables at particular points in the program; or alternatively, to introduce special boolean variables to keep information about the course of the computation. The 'while' programs produced yield the same final results as the original flowchart program but need not perform computations in exactly the same way. However, the new programs do preserve the 'topology' of the original flowchart program, and are of the same order of efficiency. We also show that this cannot be done in general without adding variables."},
{"Title": "Toward automatic program synthesis", "URL": "https://dl.acm.org/doi/10.1145/362566.362568", "Full Abstract": "An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details. The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers, lists, and trees."},
{"Title": "Decidable properties of monadic functional schemas", "URL": "https://dl.acm.org/doi/book/10.5555/891910", "Full Abstract": "We define a class of (monadic) functional schemas which properly includes 'Ianov' flowchart schemas. We show that the termination, divergence and freedom problems for functional schemas are decidable. Although it is possible to translate a large class of non-free functional schemas into equivalent free functional schemas, we show that this cannot be done in general. We show also that the equivalence problem for free functional schemas is decidable. Most of the results are obtained from well-known results in Formal Languages and Automata Theory."},
{"Title": "Computation of recursive programs", "URL": "https://dl.acm.org/doi/10.1145/1478873.1478902", "Full Abstract": "This note is actually an informal exposition of a part of a recent paper by Manna, Ness and Vuillemin. We have two main purposes in this note. First, we present some known results about computation of recursive programs, emphasizing some differences between the theoretical and practical approaches. Second, we introduce the computational induction method for proving properties of recursive programs. It turns out that most known methods for proving properties of programs are very closely related to the computational induction method. We illustrate this point by showing how Floyd's inductive assertions method for proving properties of \"flowchart programs\" can be expressed in terms of computational induction on recursive programs."},
{"Title": "Program schemas with equality", "URL": "https://dl.acm.org/doi/book/10.5555/891927", "Full Abstract": "We discuss the class of program schemas augmented with equality tests, that is, tests of equality between terms. In the first part of the paper we discuss and illustrate the \"power\" of equality tests. It turns out that the class of program schemas with equality is more powerful than the \"maximal\" classes of schemas suggested by other investigators. In the second part of the paper we discuss the decision problems of program schemas with equality. It is shown for example that while the decision problems normally considered for schemas (such as halting, divergence, equivalence, isomorphism and freedom) are solvable for Ianov schemas, they all become unsolvable if general equality tests are added. We suggest, however, limited equality tests which can be added to certain subclasses of program schemas while preserving their solvable properties."},
{"Title": "Recursive definitions of partial functions and their computations", "URL": "https://dl.acm.org/doi/10.1145/942578.807072", "Full Abstract": "The object of this paper is to present a syntactic and semantic model for recursive definitions, and to study the relation between their computed functions and their fixpoints. The recursive definitions that we consider are syntactic generalizations of those introduced in [2] by Kleene and in [5] by McCarthy."},
{"Title": "Inductive methods for proving properties of programs", "URL": "https://dl.acm.org/doi/10.1145/942580.807070", "Full Abstract": "We have two main purposes in this paper. First, we clarify and extend known results about computation of recursive programs, emphasizing the difference between the theoretical and practical approaches. Secondly, we present and examine various known methods for proving properties of recursive programs. We discuss in detail two powerful inductive methods, computational induction and structural induction, illustrating their applications by various examples. We also briefly discuss some other related methods."},
{"Title": "Fixpoint approach to the theory of computation.", "URL": "https://dl.acm.org/doi/book/10.5555/891944", "Full Abstract": "Following the fixpoint theory of Scott, we propose to define the semantics of computer programs in terms of the least fixpoints of recursive programs. This allows one not only to justify all existing verification techniques, but also to extend them to handle various properties of computer programs, including correctness, termination and equivalence, in a uniform manner."},
{"Title": "Program schemas with equality", "URL": "https://dl.acm.org/doi/10.1145/800152.804896", "Full Abstract": "We discuss the class of program schemas augmented with equality tests, that is, tests of equality between terms."},
{"Title": "Fixpoint approach to the theory of computation", "URL": "https://dl.acm.org/doi/10.1145/361454.361460", "Full Abstract": "Following the fixpoint theory of Scott, the semantics of computer programs are defined in terms of the least fixpoints of recursive programs. This allows not only the justification of all existing verification techniques, but also their extension to the handling, in a uniform manner of various properties of computer programs, including correctness, termination, and equivalence."},
{"Title": "On the power of programming features.", "URL": "https://dl.acm.org/doi/book/10.5555/891979", "Full Abstract": "We consider the power of several programming features such as counters, pushdown stacks, queues, arrays, recursion and equality. In this study program schemas are used as the model for computation. The relations between the powers of these features is completely described by a comparison diagram."},
{"Title": "A heuristic approach to program verification.", "URL": "https://dl.acm.org/doi/book/10.5555/891986", "Full Abstract": "We present various heuristic techniques for use in proving the correctness of computer programs. The techniques are designed to obtain automatically the \"inductive assertions\" attached to the loops of the program which previously required human \"understanding\" of the program's performance. We distinguish between two general approaches: one in which we obtain the inductive assertion by analyzing predicates which are known to be true at the entrances and exits of the loop (top-down$ approach), and another in which we generate the inductive assertion directly from the statements of the loop (bottom-up$ approach)."},
{"Title": "Decidable Properties of Monadic Functional Schemas", "URL": "https://dl.acm.org/doi/10.1145/321765.321780", "Full Abstract": "A class of (monadic) functional schemas which properly includes “Ianov” flowchart schemas is defined. It is shown that the termination, divergence, and freedom problems for functional schemas are decidable. Although it is possible to translate a large class of non-free functional schemas into equivalent free functional schemas, it is shown that in general this cannot be done. It is also shown that the equivalence problem for free functional schemas is decidable. Most of the results are obtained from well-known results in formal languages and automata theory."},
{"Title": "Axiomatic approach to total correctness of programs.", "URL": "https://dl.acm.org/doi/book/10.5555/891989", "Full Abstract": "We present here an axiomatic approach which enables one to prove by formal methods that his program is \"totally correct\" (i.e., it terminates and is logically correct -- does what it is supposed to do). The approach is similar to Hoare's approach for proving that a program is \"partially correct\" (i.e., that whenever it terminates it produces correct results). Our extension to Hoare's method lies in the possibility of proving correctness and$ termination at once, and in the enlarged scope of properties that can be proved by it."},
{"Title": "Termination of algorithms", "URL": "https://dl.acm.org/doi/book/10.5555/904866", "Full Abstract": "No abstract available."},
{"Title": "Properties of Programs and the First-Order Predicate Calculus", "URL": "https://dl.acm.org/doi/10.1145/321510.321516", "Full Abstract": "This paper is concerned with the relationship of the termination problem for programs and abstract programs to the validity of certain formulas in the first-order predicate calculus. By exploiting this relationship, subclasses of abstract programs for which the termination problem is decidable can be isolated. Moreover, known proof procedures for the first-order predicate calculus (e.g. resolution) can be applied to prove the termination of both programs and abstract programs. The correctness and equivalence problems of abstract programs are shown to be reducible to the termination problem."},
{"Title": "Formalization of properties of recursively defined functions", "URL": "https://dl.acm.org/doi/10.1145/800169.805434", "Full Abstract": "This paper is concerned with the relationship between the convergence, correctness and equivalence of recursively defined functions and the satisfiability (or unsatisfiability) of certain first-order formulas."},
{"Title": "Second-order mathematical theory of computation", "URL": "https://dl.acm.org/doi/10.1145/800161.805161", "Full Abstract": "In this work we show that it is possible to formalize all properties regularly observed in (deterministic and non-deterministic) algorithms in second-order predicate calculus."},
{"Title": "Inductive methods for proving properties of programs", "URL": "https://dl.acm.org/doi/10.1145/355609.362336", "Full Abstract": "There are two main purposes in this paper: first, clarification and extension of known results about computation of recursive programs, with emphasis on the difference between the theoretical and practical approaches; second, presentation and examination of various known methods for proving properties of recursive programs. Discussed in detail are two powerful inductive methods, computational induction and structural induction, including examples of their applications."},
{"Title": "A heuristic approach to program verification", "URL": "https://dl.acm.org/doi/10.5555/1624775.1624837", "Full Abstract": "We present various heuristic techniques for use in proving the correctness of computer programs. The techniques are designed to obtain automatically the \"inductive assertions\" attached to the loops of the program which previously required human \"understanding\" of the program's performance. We distinguish between two general approaches: one in which we obtain the inductive assertion by analyzing predicates which are known to be true at the entrances and exits of the loop (top-down approach), and another in which we generate the inductive assertion directly from the statements of the loop (bottom-up approach)."},
{"Title": "Knowledge and Reasoning in Program Synthesis", "URL": "https://dl.acm.org/doi/10.5555/647950.742874", "Full Abstract": "No abstract available."},
{"Title": "Introduction to Mathematical Theory of Computation", "URL": "https://dl.acm.org/doi/book/10.5555/542899", "Full Abstract": "No abstract available."},
{"Title": "Computer recreations", "URL": "https://dl.acm.org/doi/10.1002/spe.4380030412", "Full Abstract": "Jotto is a popular word game for two players. It is of interest here because it unquestionably requires some intellectual ability for people to play it well, and it is a game in which a simple program can beat most human players nearly all the time. © 1973 Wiley Periodicals, Inc."},
{"Title": "A General-Purpose Macro Processor as a Poor Man's Compiler-Compiler", "URL": "https://dl.acm.org/doi/10.1109/TSE.1976.233539", "Full Abstract": "A method for quickly producing compilers for high level languages is described. The technique consists of feeding a description of the language to be translated to a general-purpose macro processor. Used in this way, the macro processor functions as a compiler-compiler, providing automatic parsing, lexical scanning, symbol table operations, and handling of syntactic errors. A complete syntactic and semantic description of a WHILE statement (except for Boolean expression processing) is given in only seven lines, as an example. A system programming language implemented by this method is discussed in order to illustrate the main ideas. The compiler produced for this language is compared to other compilers produced by conventional methods."},
{"Title": "In defense of program testing or correctness proofs considered harmful", "URL": "https://dl.acm.org/doi/10.1145/956003.956011", "Full Abstract": "Copyright © 1976 Author."},
{"Title": "A Tutorial on Algol 68", "URL": "https://dl.acm.org/doi/10.1145/356669.356671", "Full Abstract": "Copyright © 1976 ACM."},
{"Title": "The automatic synthesis of recursive programs", "URL": "https://dl.acm.org/doi/10.1145/872734.806929", "Full Abstract": "We describe a deductive technique for the automatic construction of recursive programs to meet given input-output specifications. These specifications express what conditions the output of the desired program is expected to satisfy. The deductive technique involves transforming the specifications by a collection of rules, summoned by pattern-directed function invocation. Some of these transformation rules express the semantics of the subject domain; others represent more general programming techniques. The rules that introduce conditional expressions and recursive calls into the program are discussed in some detail."},
{"Title": "The logic of computer programming", "URL": "https://dl.acm.org/doi/book/10.5555/892142", "Full Abstract": "Techniques derived from mathematical logic promise to provide an alternative to the conventional methodology for constructing, debugging, and optimizing computer programs. Ultimately, these techniques are intended to lead to the automation of many of the facets of the programming process. This paper provides a unified tutorial exposition of the logical techniques, illustrating each with examples. The strengths and limitations of each technique as a practical programming aid are assessed and attempts to implement these methods in experimental systems are discussed."},
{"Title": "The automatic synthesis of systems of recursive programs", "URL": "https://dl.acm.org/doi/10.5555/1624435.1624526", "Full Abstract": "A technique is presented for constructing, a program from given specifications. The basic approach is to transform the specifications repeatedly, according to certain rules, until the desired program is produced. Two important transformation rules are those responsible for introducing conditional expressions and recursion into the target program. These transformations have been introduced in previous publications, and are discussed here briefly."},
{"Title": "Inference rules for program annotation", "URL": "https://dl.acm.org/doi/book/10.5555/892155", "Full Abstract": "Methods are presented whereby an Algol-like program, given together with its specifications, can be documented automatically. The program is incrementally annotated with invariant relationships that hold between program variables at intermediate points in the program and explain the acutal workings of the program regardless of whether the program is correct. Thus this documentation can be used for proving the correctness of the program or may serve as an aid in the debugging of an incorrect program. The annotation techniques are formulated as Hoare-llike inference rules which derive invariants from the assignment statements, from the control structure of the program, or, heuristically, from suggested invariants. The application of these rules is demonstrated by two examples which have run on an experimental implementation."},
{"Title": "The optimal approach to recursive programs", "URL": "https://dl.acm.org/doi/10.1145/359863.359885", "Full Abstract": "The classical fixedpoint approach toward recursive programs suggests choosing the “least defined fixedpoint” as the most appropriate solution to a recursive program. A new approach is described which introduces an “optimal fixedpoint,” which, in contrast to the least defined fixedpoint, embodies the maximal amount of valuable information embedded in the program. The practical implications of this approach are discussed and techniques for proving properties of optimal fixedpoints are given. The presentation is informal, with emphasis on examples."},
{"Title": "Ambiguous machine architecture and program efficiency", "URL": "https://dl.acm.org/doi/10.1145/859402.859404", "Full Abstract": "Copyright © 1977 Author."},
{"Title": "Corrigenda: “A Tutorial on Algol 68”", "URL": "https://dl.acm.org/doi/10.1145/356698.356706", "Full Abstract": "No abstract available."},
{"Title": "Implications of structured programming for machine architecture", "URL": "https://dl.acm.org/doi/10.1145/359361.359454", "Full Abstract": "Based on an empirical study of more than 10,000 lines of program text written in a GOTO-less language, a machine architecture specifically designed for structured programs is proposed. Since assignment, CALL, RETURN, and IF statements together account for 93 percent of all executable statements, special care is given to ensure that these statements can be implemented efficiently. A highly compact instruction encoding scheme is presented, which can reduce program size by a factor of 3. Unlike a Huffman code, which utilizes variable length fields, this method uses only fixed length (1-byte) opcode and address fields. The most frequent instructions consist of a single 1-byte field. As a consequence, instruction decoding time is minimized, and the machine is efficient with respect to both space and time."},
{"Title": "A method for implementing paged, segmented virtual memories on microprogrammable computers", "URL": "https://dl.acm.org/doi/10.1145/850657.850660", "Full Abstract": "Copyright © 1979 Author."},
{"Title": "Efficient encoding of machine instructions", "URL": "https://dl.acm.org/doi/10.1145/859470.859472", "Full Abstract": "Copyright © 1979 Authors."},
{"Title": "An overview of the Amoeba distributed operating system", "URL": "https://dl.acm.org/doi/10.1145/1041500.1041502", "Full Abstract": "As hardware prices continue to drop rapidly, building large computer systems by interconnecting substantial numbers of microcomputers becomes increasingly attractive. Many techniques for interconnecting the hardware, such as Ethernet [Metcalfe and Boggs, 1976], ring nets [Farber and Larson, 1972], packet switching, and shared memory are well understood, but the corresponding software techniques are poorly understood. The design of general purpose distributed operating systems is one of the key research issues for the 1980s."},
{"Title": "Network Protocols", "URL": "https://dl.acm.org/doi/10.1145/356859.356864", "Full Abstract": "Copyright © 1981 ACM."},
{"Title": "Using Peephole Optimization on Intermediate Code", "URL": "https://dl.acm.org/doi/10.1145/357153.357155", "Full Abstract": "Copyright © 1982 ACM."},
{"Title": "A practical tool kit for making portable compilers", "URL": "https://dl.acm.org/doi/10.1145/358172.358182", "Full Abstract": "Copyright © 1983 ACM."},
{"Title": "Structured Computer Organization", "URL": "https://dl.acm.org/doi/book/10.5555/538160", "Full Abstract": "From the Publisher:"},
{"Title": "Immediate files", "URL": "https://dl.acm.org/doi/10.1002/spe.4380140407", "Full Abstract": "No abstract available."},
{"Title": "Does anybody out there want to write <u>HALF</u> of a compiler?", "URL": "https://dl.acm.org/doi/10.1145/988241.988252", "Full Abstract": "Copyright © 1984 Authors."},
{"Title": "An overview of the Amoeba distributed operating system", "URL": "https://dl.acm.org/doi/10.5555/40489.40494", "Full Abstract": "No abstract available."},
{"Title": "Computer Networks", "URL": "https://dl.acm.org/doi/book/10.5555/536716", "Full Abstract": "From the Book: This book is now in its third edition. Each edition has corresponded to a different phase in the way computer networks were used. When the first edition appeared in 1980, networks were an academic curiosity. When the second edition appeared in 1988, networks were used by universities and large businesses. When the third edition appeared in 1996, computer networks, especially the worldwide Internet, had become a daily reality for millions of people. Furthermore, the networking hardware and software have completely changed since the second edition appeared In 1988, nearly all networks were based on copper wire. Now, many are based on fiber optics or wireless communication. Proprietary networks, such as SNA have become far less important than public networks, especially the Internet. The OSI protocols have quietly vanished,, and the TCP/IP protocol suite has become dominant. In fact, so much has changed, the book has almost been rewritten from scratch. Although Chap. 1 has the same introductory function as it did in the second edition, the contents have been completely revised and brought up to date. For example, instead of basing the hook on the seven-layer OSI model. a five-layer hybrid model (shown in Fig. 1-21) is now used and introduced in Chap. 1. While not exactly identical to the TCP/IP model, it is much closer to the TCP/IP model in spirit than it is to the OSI model used in the second edition. Also, the new running examples used throughout the book - the Internet and Al M networks are introduced here, along with some gigabit networks and other popular networks. In Chap. 2, the focus has moved from copper wire to fiber optics and wireless communication,since these arc the technologies of the future. The telephone system has become almost entirely digital in the past decade, so the material on it has been largely rewritten, with new material on broadband ISDN added. The material on cellular radio has been greatly expanded, and new material on low-orbit satellites has been added to the chapter. The order of discussion of the data link layer and the MAC sublayer has been reversed, since experience with students shows that they understand the MAC sublayer better after they have studied the data link layer. The example protocols there have been kept, as they have proven very popular, but they have been rewritten in C. New material on the Internet and ATM data link layers has been added. The MAC sublayer principles of Chap. 4. have been revised to reflect new protocols, including wavelength division multiplexing, wireless LANs, and digital radio. The discussion of bridges has been revised, and new material has been added on high-speed LANs. Most of the routing algorithms of Chap. 5 have been replaced by more modern ones, including distance vector and link state routing. The sections on congestion control have been completely redone, and material on the running examples, the Internet and ATM is all new. Chap. 6 is still about the transport layer, but here, too, major changes have occurred, primarily, the addition of a large amount of new material about the Internet, ATM, and network performance. Chap. 7, on the application layer, is now the longest chapter in the book. The material on network security has been doubled in length, and new material has been added on DNS, SNMP, email, USENET, the World Wide Web, HTML, Java, multimedia, video on demand, and the MBone. Of the 395 figures in the third edition, 276 (70 percent) are completely new and some of the others have been revised. Of the 371 references to the literature, 282 (76 percent) are to books and papers that have appeared since the second edition was published. Of these, over 100 are to works published in 1995 and 1996 alone. All in all, probably 75 percent of the entire book is brand new, and parts of the remaining 25 percent have been heavily revised. Since this is effectively a new book, the cover was redesigned to avoid confusion with the second edition. Computer books are full of acronyms. This one is no exception. By the time you are finished reading this one, all of the following should ring a bell: AAL, AMPS, ARP, ASN, ATM, BGP, CDMA, CDPD, CSMA, DQDB, DNS, FAQ, FDM, FTP, FTTC, FTTH, GSM, HDLC, HEC, HlPPl, TAB, lCMP, IDEA, IETF, 1Pv6, ISO, ITU, LATA, MAC, MACA, MAN, MIB, MIME, NAP, NNTP, NSA, NSAP, OSI, OSPF, PCM, PCN, PCS, PEM, PGP, PPP, PSTN, PTT, PVC, QAM, RARP, RFC, RSA, SABME, SAP, SAR, SDH, SDLC, SHA, SMI, SNA, SNMP, SNRME, SPX, TCP, UDP, VHF, VLF, VSAT, WARC, WDM, WWV, and WWW. But don't worry. Each one will be carefully defined before it is used. To help instructors using this book as a text for course, the author has prepared three teaching aids: A problem solutions manual. PostScript files containing all the figures (for making overhead sheets). A simulator (written in C) for the example protocols of Chap. 3. The solutions manual is available from Prentice Hall (but only to instructors). The file with the figures and the simulator are available via the World Wide Web. To get them, please see the author's home page: http://www.cs.vu.nl/~ast/. The book was typeset in Times Roman using Troff, which, after all these years, is still the only way to go. While Troff is not as trendy as WYSIWYG systems, the reader is invited to compare the typesetting quality of this book with books produced by WYSIWYG systems. My only concession to PCs and desktop publishing is that for the first time, the art was produced using Adobe Illustrator, instead of being drawn on paper. Also for the first time, the book was produced entirely electronically. The PostScript output from Troff was sent over the Internet to the printer, where the film for making the offset plates was produced. No intermediate paper copy was printed and photographed, as is normally done. Andrew S. Tanenbaum"},
{"Title": "A distributed file service based on optimistic concurrency control", "URL": "https://dl.acm.org/doi/10.1145/323647.323634", "Full Abstract": "Copyright © 1985 ACM."},
{"Title": "Distributed operating systems", "URL": "https://dl.acm.org/doi/10.1145/6041.6074", "Full Abstract": "Distributed operating systems have many aspects in common with centralized ones, but they also differ in certain ways. This paper is intended as an introduction to distributed operating systems, and especially to current university research about them. After a discussion of what constitutes a distributed operating system and how it is distinguished from a computer network, various key design issues are discussed. Then several examples of current research projects are examined in some detail, namely, the Cambridge Distributed Computing System, Amoeba, V, and Eden."},
{"Title": "Research issues in distributed operating systems", "URL": "https://dl.acm.org/doi/10.5555/16918.16921", "Full Abstract": "No abstract available."},
{"Title": "The design of a real-time distributed system", "URL": "https://dl.acm.org/doi/10.5555/16918.16951", "Full Abstract": "No abstract available."},
{"Title": "Language and machine-independent global optimization on intermediate code", "URL": "https://dl.acm.org/doi/10.1016/0096-0551%2886%2990004-4", "Full Abstract": "No abstract available."},
{"Title": "Making distributed systems palatable", "URL": "https://dl.acm.org/doi/10.1145/503956.503999", "Full Abstract": "Designing and implementing a distributed system is easy compared to the task of convincing people to use it. In a university Computer Science Dept., people generally use UNIX and are not at all interested in moving to a different environment, no matter how wonderful it may be. In this paper we report on how we have implemented a UNIX environment for the Amoeba distributed operating system [1], in order to make the transition from UNIX to Amoeba as simple as possible."},
{"Title": "Verification of Concurrent Programs", "URL": "https://dl.acm.org/doi/10.5555/648063.747433", "Full Abstract": "No abstract available."},
{"Title": "Synthesis of Communicating Processes from Temporal Logic Specifications", "URL": "https://dl.acm.org/doi/10.5555/648063.747434", "Full Abstract": "No abstract available."},
{"Title": "Verification of concurrent programs, Part I: The temporal framework", "URL": "https://dl.acm.org/doi/book/10.5555/892270", "Full Abstract": "This is the first in a series of reports describing the application of temporal logic to the specification and verification of concurrent programs. We first introduce temporal logic as a tool for reasoning about sequences of states. Models of concurrent programs based both on transition graphs and on linear-text representations are presented and the notions of concurrent and fair executions are defined. The general temporal language is then specialized to reason aboaut those execution sequences that are fair computations of a concurrent program. Subsequently, the language is used to describe properties of concurrent programs. The set of interesting properties is classified into invariance (safety), eventuality (liveness), and precedence (until) properties. Among the properties studied are: partial correctness, global invariance, clean behavior, mutual exclusion, absence of deadlock, termination, total correctness, intermittent assertions, accessibility, responsiveness, safe liveness, absence of unsolicited response, fair responsiveness, and precedence. In the following reports of this series, we will use the temporal formalism to develop proof methodologies for proving the properties discussed here."},
{"Title": "Verification of concurrent programs: proving eventualities by well-founded ranking", "URL": "https://dl.acm.org/doi/book/10.5555/892275", "Full Abstract": "In this paper, one of a series on verification of concurrent programs, we present proof methods for establishing eventuality and until properties. The methods are based on well-founded ranking and are applicable to both \"just\" and \"fair\" computations. These methods do not assume a decrcase of the rank at each computation step. It is sufficient that there exists one process which decreases the rank when activated. Fairness then ensures that the program will eventually attain its goal."},
{"Title": "How to cook a temporal proof system for your pet language", "URL": "https://dl.acm.org/doi/10.1145/567067.567082", "Full Abstract": "An abstract temporal proof system is presented whose program-dependent part has a high-level interface with the programming language actually studied. Given a new language, it is sufficient to deline the interface notions of atomic transitions, justice, and fairness in order to obtain a full temporal proof system for this language. This construction is particularly useful for the analysis of concurrent systems. We illustrate the construction on the shared-variable model and on CSP. The generic proof system is shown to be relatively complete with respect to pure first-order temporal logic."},
{"Title": "Operating systems: design and implementation", "URL": "https://dl.acm.org/doi/book/10.5555/21853", "Full Abstract": "No abstract available."},
{"Title": "Operating systems: design and implementation", "URL": "https://dl.acm.org/doi/book/10.5555/22876", "Full Abstract": "No abstract available."},
{"Title": "Computer networks: 2nd edition", "URL": "https://dl.acm.org/doi/book/10.5555/59922", "Full Abstract": "No abstract available."},
{"Title": "Computer networks", "URL": "https://dl.acm.org/doi/book/10.5555/62795", "Full Abstract": "No abstract available."},
{"Title": "Computer networks: 2nd edition", "URL": "https://dl.acm.org/doi/book/10.5555/47313", "Full Abstract": "No abstract available."},
{"Title": "Functional specialization in distributed operating systems", "URL": "https://dl.acm.org/doi/10.1145/504092.504120", "Full Abstract": "A distributed operating system provides the same functionality and interface as a monolithic operating system. That is, for both systems the goal is to make the computing and storage facilities as provided by the hardware available to the users of the system. In distributed operating system new hardware can be added to the system to increase the storage or computing power, or to increase the availability of the storage and computing services. During and after this addition, the interface to the system remains unchanged. Transparency of access is a key concept.The top-level interface consists of sophisticated command interpreters and editors, supported by a high-resolution graphical window system. This software is run by workstations. Workstations are powerful computer units, consisting of a CPU, memory, a bitmap display, keyboard, a pointing device such as a mouse, and a network interface. In addition, workstations are often equipped with a disk. The CPU is at least as powerful as those used in traditional computer systems, and the amount of memory is equivalent or even larger.A workstation is dedicated to one individual. Consequently, the workstation is idle most of the time. It is therefore tempting to use it as the main computing resource for the owner and perhaps others as well. It could also be used autonomously from the rest of the system in case of a failure. We are opposed to these uses of workstations, since we believe that workstations should only provide the top-level interface. In this paper we will outline our reasons for this, and show how this principle has been applied in the Amoeba distributed operating system."},
{"Title": "Performance of the world's fastest distributed operating system", "URL": "https://dl.acm.org/doi/10.1145/54289.54291", "Full Abstract": "Distributed operating systems have been in the experimental stage for a number of years now, but few have progressed to the point of actually being used in a production environment. It is our belief that the reason lies primarily with the performance of these systems---they tend to be fairly slow compared to traditional single computer systems. The Amoeba system has been designed with high performance in mind. In this paper some performance measurements of Amoeba are presented and comparisons are made with UNIX on the SUN, as well as with some other interesting systems. In particular, short remote procedure calls take 1.4 msec and long data transfers achieve a user-to-user bandwidth of 677 kbytes/sec. Furthermore, the file server is so fast that it is limited by the communication bandwidth to 677 kbytes/sec. The real speed of the file server is too high to measure. To the best of our knowledge, these are the best figures yet reported in the literature for the class of hardware used."},
{"Title": "The Evolution of a Distributed Operating System", "URL": "https://dl.acm.org/doi/10.5555/645793.668329", "Full Abstract": "No abstract available."},
{"Title": "Programming languages for distributed computing systems", "URL": "https://dl.acm.org/doi/10.1145/72551.72552", "Full Abstract": "When distributed systems first appeared, they were programmed in traditional sequential languages, usually with the addition of a few library procedures for sending and receiving messages. As distributed applications became more commonplace and more sophisticated, this ad hoc approach became less satisfactory. Researchers all over the world began designing new programming languages specifically for implementing distributed applications. These languages and their history, their underlying principles, their design, and their use are the subject of this paper."},
{"Title": "An efficient reliable broadcast protocol", "URL": "https://dl.acm.org/doi/10.1145/70730.70732", "Full Abstract": "Many distributed and parallel applications can make good use of broadcast communication. In this paper we present a (software) protocol that simulates reliable broadcast, even on an unreliable network. Using this protocol, application programs need not worry about lost messages. Recovery of communication failures is handled automatically and transparently by the protocol. In normal operation, our protocol is more efficient than previously published reliable broadcast protocols. An initial implementation of the protocol on 10 MC68020 CPUs connected by a 10 Mbit/sec Ethernet performs a reliable broadcast in 1.5 msec."},
{"Title": "On the design of the amoeba configuration manager", "URL": "https://dl.acm.org/doi/10.1145/72910.73340", "Full Abstract": "The program"},
{"Title": "The design of very fast portable compilers", "URL": "https://dl.acm.org/doi/10.1145/71605.71616", "Full Abstract": "The Amsterdam Compiler Kit is a widely used compiler building system. Up until now, the emphasis has been on producing good object code. In this paper we describe recent work that has focused on reducing compile time. The techniques described in this paper have resulted in C compilers for the Sun-3 and VAX that are 3 to 4 times faster than the native compilers provided by the manufacturers."},
{"Title": "Structured computer organization (3rd ed.)", "URL": "https://dl.acm.org/doi/book/10.5555/93745", "Full Abstract": "No abstract available."},
{"Title": "Amoeba", "URL": "https://dl.acm.org/doi/10.1109/2.53354", "Full Abstract": "A description is given of the Amoeba distributed operating system, which appears to users as a centralized system but has the speed, fault tolerance, security safeguards, and flexibility required for the 1990s. The Amoeba software is based on objects. Objects are managed by server processes and named using capabilities chosen randomly from a sparse name space. Amoeba has a unique, fast file system split into two parts: the bullet service stores immutable files contiguously on the disk; the directory service gives capabilities symbolic names and handles replication and atomicity, eliminating the need for a separate transaction management system. To bridge the gap with existing systems, Amoeba has a Unix emulation facility consisting of a library of Unix system call routines that make calls to the various Amoeba server processes."},
{"Title": "Fault tolerance using group communication", "URL": "https://dl.acm.org/doi/10.1145/504136.504146", "Full Abstract": "We propose group communication as an efficient mechanism to support fault tolerance. Our approach is based on an efficient reliable broadcast protocol that requires on average only two messages per broadcast. To illustrate our approach we will describe how the task bag model can be made fault-tolerant using group communication."},
{"Title": "Minix 1.5 for Macintosh Software and Reference Manual", "URL": "https://dl.acm.org/doi/book/10.5555/1202330", "Full Abstract": "No abstract available."},
{"Title": "Experiences with the Amoeba distributed operating system", "URL": "https://dl.acm.org/doi/10.1145/96267.96281", "Full Abstract": "The Amoeba project is a research effort aimed at understanding how to connect multiple computers in a seamless way [16, 17, 26, 27, 31]. The basic idea is to provide the users with the illusion of a single powerful timesharing system, when, in fact, the system is implemented on a collection of machines, potentially distributed among several countries. This research has led to the design and implementation of the Amoeba distributed operating system, which is being used as a prototype and vehicle for further research. In this article we will describe the current state of the system (Amoeba 4.0), and show some of the lessons we have learned designing and using it over the past eight years. We will also discuss how this experience has influenced our plans for the next version, Amoeba 5.0."},
{"Title": "Network protocols", "URL": "https://dl.acm.org/doi/10.5555/128960.128961", "Full Abstract": "No abstract available."},
{"Title": "Fault tolerance using group communication", "URL": "https://dl.acm.org/doi/10.1145/122120.122126", "Full Abstract": "We propose group communication as an efficient mechanism to support fault tolerance. Our approach is based on an efficient reliable broadcast protocol that requires on average only two messages per broadcast. To illustrate our approach we will describe how the task bag model can be made fault-tolerant using group communication."},
{"Title": "The temporal logic of branching time", "URL": "https://dl.acm.org/doi/10.1007/BF01257083", "Full Abstract": "A temporal logic is defined which contains both linear and branching operators. The underlying model is the tree of all possible computations. The following metatheoretical results are proven: 1) an exponential decision procedure for satisfiability; 2) a finite model property; 3) the completeness of an axiomatization."},
{"Title": "Synthesis of Communicating Processes from Temporal Logic Specifications", "URL": "https://dl.acm.org/doi/10.1145/357233.357237", "Full Abstract": "Copyright © 1984 ACM."},
{"Title": "Adequate proof principles for invariance and liveness properties of concurrent programs", "URL": "https://dl.acm.org/doi/book/10.5555/892313", "Full Abstract": "This paper presents proof principles for establishing invariance and liveness properties of concurrent programs. Invariance properties are established by systematically checking that they are preserved by every atomic instruction in the program. The methods for establishing liveness properties are based on 'well-founded asserations' and are applicable to both \"just\" and \"fair\" computations. These methods do not assume a decrease of the rank at each computation step. It is sufficient that there exists one process which decreases the rank when activated. Fairness then ensures that the program will eventually attain its goal. In the finite state case such proofs can be represented by diagrams. Several examples are given."},
{"Title": "TABLOG: the deductive-tableau programming language", "URL": "https://dl.acm.org/doi/book/10.5555/892317", "Full Abstract": "TABLOG (Tableau Logic Programming Language) is a language based on first-order predicate logic with equality that combines functional and logic programming. TABLOG incorporates advantages of LISP and PROLOG. A program in TABLOG is a list of formulas in a first-order logic (including equality, negation, and equivalence) that is more general and more expressive than PROLOG's Horn clauses. Whereas PROLOG programs must be relational, TABLOG programs may define either relations or functions. While LISP programs yield results of a computation by returning a single output value, TABLOG programs can be relations and can produce several results simultaneously through their arguments. TABLOG employs the Manna-Waldinger deductive-tableau proof system as an interpreter in the same way that PROLOG uses a resolution-based proof system. Unification is used by TABLOG to match a call with a line in the program and to bind arguments. The basic rules of deduction used for computing are nonclausal resolution and rewriting by means of equality and equivalence. A pilot interpreter for the language has been implemented."},
{"Title": "TABLOG", "URL": "https://dl.acm.org/doi/10.1145/800055.802049", "Full Abstract": "TABLOG (Tableau Logic Programming Language) is a language combining functional and logic programming using first-order (quantifier-free) predicate logic with equality. TABLOG incorporates advantages of LISP and PROLOG."},
{"Title": "Adequate proof principles for invariance and liveness properties of concurrent programs", "URL": "https://dl.acm.org/doi/10.1016/0167-6423%2884%2990003-0", "Full Abstract": "No abstract available."},
{"Title": "The logical basis for computer programming. Volume 1:  deductive reasoning", "URL": "https://dl.acm.org/doi/book/10.5555/3510", "Full Abstract": "No abstract available."},
{"Title": "Special relations in automated deduction", "URL": "https://dl.acm.org/doi/book/10.5555/892351", "Full Abstract": "Two deduction rules are introduced to give streamlined treatment to relations of special importance in an automated theorem-proving system. These rules, the relation replacement and relation matching rules, generalize to an arbitrary binary relation the paramodulation and E-resolution rules, respectively, for equality, and may operate within a nonclausal or clausal system. The new rules depend on an extension of the notion of polarity to apply to subterms as well as to subsentences, with respect to a given binary relation. The rules allow us to eliminate troublesome axioms, such as transitivity and monotonicity, from the system; proofs are shorter and more comprehensible, and the search space is correspondingly deflated."},
{"Title": "Nonclausal temporal deduction", "URL": "https://dl.acm.org/doi/book/10.5555/892354", "Full Abstract": "We present a proof system for propositional temporal logic. This system is based on nonclausal resolution; proofs are natural and generally short. Its extension to first-order temporal logic is considered. Two variants of the system are described. The first one is for a logic with $\\Box$ (\"always\"), $\\Diamond$ (\"sometime\"), and $\\bigcirc$ (\"next\"). The second variant is an extension of the first one to a logic with the additional operators U (\"until\") and P (\"precedes\"). Each of these variants is proved complete."},
{"Title": "Nonclausal Temporal Deduction", "URL": "https://dl.acm.org/doi/10.5555/648065.747731", "Full Abstract": "No abstract available."},
{"Title": "Special Relations in Automated Deduction", "URL": "https://dl.acm.org/doi/10.5555/646239.683367", "Full Abstract": "No abstract available."},
{"Title": "The origin of the binary-search paradigm", "URL": "https://dl.acm.org/doi/10.5555/1625135.1625176", "Full Abstract": "In a binary-search algorithm for the computation of a numerical function, the interval in which the desired output is sought is divided in half at each iteration. The paper considers how such algorithms might be derived from their specifications by an automatic program-synthesis system. The derivation of the binary-search concept has been found to be surprisingly straightforward. The programs obtained, though reasonably simple and efficient, are quite different from those that would have been constructed by informal means."},
{"Title": "Deduction with Relation Matching", "URL": "https://dl.acm.org/doi/10.5555/646823.706894", "Full Abstract": "No abstract available."},
{"Title": "Balanced allocations for tree-like inputs", "URL": "https://dl.acm.org/doi/10.1016/0020-0190%2895%2900123-T", "Full Abstract": "No abstract available."},
{"Title": "The worst-case running time of the random simplex algorithm is exponential in the height", "URL": "https://dl.acm.org/doi/10.1016/0020-0190%2895%2900101-H", "Full Abstract": "No abstract available."},
{"Title": "An efficient algorithm for the vertex-disjoint paths problem in random graphs", "URL": "https://dl.acm.org/doi/10.5555/313852.314072", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Static and dynamic path selection on expander graphs (preliminary version)", "URL": "https://dl.acm.org/doi/10.1145/258533.258646", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "Counting Minimum Weight Spanning Trees", "URL": "https://dl.acm.org/doi/10.1006/jagm.1996.0851", "Full Abstract": "No abstract available."},
{"Title": "Syntactic clustering of the Web", "URL": "https://dl.acm.org/doi/10.1016/S0169-7552%2897%2900031-7", "Full Abstract": "No abstract available."},
{"Title": "Syntactic clustering of the Web", "URL": "https://dl.acm.org/doi/10.5555/283554.283370", "Full Abstract": "No abstract available."},
{"Title": "Distributed programming with shared data", "URL": "https://dl.acm.org/doi/10.1016/0096-0551%2891%2990003-R", "Full Abstract": "No abstract available."},
{"Title": "The Amoeba distributed operating system—a status report", "URL": "https://dl.acm.org/doi/10.1016/0140-3664%2891%2990058-9", "Full Abstract": "No abstract available."},
{"Title": "MINIX 1.5 for the Sun Sparcstation", "URL": "https://dl.acm.org/doi/book/10.5555/574007", "Full Abstract": "No abstract available."},
{"Title": "Transparent fault-tolerance in parallel Orca programs", "URL": "https://dl.acm.org/doi/10.5555/139156.139175", "Full Abstract": "No abstract available."},
{"Title": "Orca", "URL": "https://dl.acm.org/doi/10.1109/32.126768", "Full Abstract": "A detailed description is given of the Orca language design and the design choices are discussed. Orca is intended for applications programmers rather than systems programmers. This is reflected in its design goals to provide a simple, easy-to-use language that is type-secure and provides clean semantics. Three example parallel applications in Orca, one of which is described in detail, are discussed. One of the existing implementations, which is based on reliable broadcasting, is described. Performance measurements of this system are given for three parallel applications. The measurements show that significant speedups can be obtained for all three applications. The authors compare Orca with several related languages and systems."},
{"Title": "FLIP; an Internetwork Protocol for Supporting Distributed Systems", "URL": "https://dl.acm.org/doi/10.1145/142111.993339", "Full Abstract": "No abstract available."},
{"Title": "Modern operating systems", "URL": "https://dl.acm.org/doi/book/10.5555/129206", "Full Abstract": "No abstract available."},
{"Title": "Replication techniques for speeding up parallel applications on distributed systems", "URL": "https://dl.acm.org/doi/10.1002/cpe.4330040502", "Full Abstract": "No abstract available."},
{"Title": "Parallel Programming Using Shared Objects and Broadcasting", "URL": "https://dl.acm.org/doi/10.1109/2.153276", "Full Abstract": "The two major design approaches taken to build distributed and parallel computer systems, multiprocessing and multicomputing, are discussed. A model that combines the best properties of both multiprocessor and multicomputer systems, easy-to-build hardware, and a conceptually simple programming model is presented. Using this model, a programmer defines and invokes operations on shared objects, the runtime system handles reads and writes on these objects, and the reliable broadcast layer implements indivisible updates to objects using the sequencing protocol. The resulting system is easy to program, easy to build, and has acceptable performance on problems with a moderate grain size in which reads are much more common than writes. Orca, a procedural language whose sequential constructs are roughly similar to languages like C or Modula 2 but which also supports parallel processes and shared objects and has been used to develop applications for the prototype system, is described."},
{"Title": "An experimental comparison of remote procedure call and group communication", "URL": "https://dl.acm.org/doi/10.1145/506378.506405", "Full Abstract": "This paper suggests that a distributed system should support two communication paradigms: Remote Procedure Call (RPC) and group communication. The former is used for point-to-point communication; the latter is used for one-to-many communication. We demonstrate that group communication is an important paradigm by showing that a fault-tolerant directory service is much easier to implement with groups than with RPC and is also more efficient. The directory service exemplifies distributed services that provide high reliability and availability by replicating data."},
{"Title": "A comparison of two paradigms for distributed shared memory", "URL": "https://dl.acm.org/doi/10.1002/spe.4380221105", "Full Abstract": "No abstract available."},
{"Title": "FLIP: an internetwork protocol for supporting distributed systems", "URL": "https://dl.acm.org/doi/10.1145/151250.151253", "Full Abstract": "Most modern network protocols give adequate support for traditional applications such as file transfer and remote login. Distributed applications, however, have different requirements (e.g., efficient at-most-once remote procedure call even in the face of processor failures). Instead of using ad hoc protocols to meet each of the new requirements, we have designed a new protocol, called the Fast Local Internet Protocol (FLIP), that provides a clean and simple integrated approach to these new requirements. FLIP is an unreliable message protocol that provides both point-to-point communication and multicast communication, and requires almost no network management. Furthermore, by using FLIP we have simplified higher-level protocols such as remote procedure call and group communication, and enhanced support for process migration and security. A prototype implementation of FLIP has been built as part of the new kernel for the Amoeba distributed operating system, and is in daily use. Measurements of its performance are presented."},
{"Title": "Using active messages to support shared objects", "URL": "https://dl.acm.org/doi/10.1145/504390.504421", "Full Abstract": "This paper discusses a reliable group communication system using active messages to update shared objects. We discuss the model, implementation techniques, and our preliminary performance results."},
{"Title": "Distributed operating systems", "URL": "https://dl.acm.org/doi/book/10.5555/184674", "Full Abstract": "No abstract available."},
{"Title": "Paramecium", "URL": "https://dl.acm.org/doi/10.5555/822074.822401", "Full Abstract": "We describe the design of an extensible kernel, called Paramecium. This kernel uses an object-based software architecture which together with instance naming, late binding and explicit overrides enables easy reconfiguration. Determining which components reside in the kernel protection domain is up to the user. A certification authority or one of its delegates certifies which components are trustworthy and therefore permitted to run in the kernel protection domain. These delegates may include validation programs, correctness provers, and system administrators. The main advantage of certifications is that it can handle trust and sharing in a non-cooperative environment."},
{"Title": "A comparison of three microkernels", "URL": "https://dl.acm.org/doi/10.1007/BF01245395", "Full Abstract": "No abstract available."},
{"Title": "Orca", "URL": "https://dl.acm.org/doi/10.5555/201711.201713", "Full Abstract": "No abstract available."},
{"Title": "Computer networks (3rd ed.)", "URL": "https://dl.acm.org/doi/book/10.5555/248731", "Full Abstract": "No abstract available."},
{"Title": "An architecture for a wide area distributed system", "URL": "https://dl.acm.org/doi/10.1145/504450.504465", "Full Abstract": "Distributed systems provide sharing of resources and information over a computer network. A key design issue that makes these systems attractive is that all aspects related to distribution are transparent to users. Unfortunately, general-purpose wide area distributed systems that allow users to share and manage arbitrary resources in a transparent way hardly exist. In particular, they generally do not take into account the most important properties that characterize wide area systems: 1) A very large number of users and resources, 2) an inherent latency problem caused by the distance between nodes, 3) heterogeneity due to a variety of underlying operating systems and networks, and 4) involvement of multiple administrative organizations.The research described in this paper is part of the Globe Project (Globe stands for GLobal Object Based Environment). The goal of this project is the design and implementation of a wide area distributed system that provides a convenient programming abstraction and full transparency. The main contribution of this paper is the description of a new system for distributed shared objects. In contrast to other systems, the implementation of distribution, consistency, and replication of state is completely encapsulated in a distributed shared object. This allows for object-specific solutions, and provides the right mechanism for building efficient and truly scalable systems."},
{"Title": "Communication in GLOBE", "URL": "https://dl.acm.org/doi/10.5555/851041.856924", "Full Abstract": "Current paradigms for interprocess communication are not sufficient to describe the exchange of information at an adequate level of abstraction. They are either too low-level, or their implementations cannot meet performance requirements. As an alternative, we propose distributed shared objects as a unifying concept. These objects offer user-defined operations on shared state, but allow for efficient implementations through replication and distribution of state. In contrast to other object-based models, these implementation aspects are completely hidden from applications."},
{"Title": "Modal theorem proving", "URL": "https://dl.acm.org/doi/10.5555/22289.22302", "Full Abstract": "No abstract available."},
{"Title": "How to clear a block: plan formation in situational logic", "URL": "https://dl.acm.org/doi/10.5555/22289.22337", "Full Abstract": "No abstract available."},
{"Title": "Specification and Verification of Concurrent Programs by forall-Automata", "URL": "https://dl.acm.org/doi/10.5555/647236.720395", "Full Abstract": "No abstract available."},
{"Title": "The deductive synthesis of imperative LISP programs", "URL": "https://dl.acm.org/doi/10.5555/1856670.1856698", "Full Abstract": "A framework is described for the automatic synthesis of imperative programs, which may alter data structures and produce destructive side effects as part of their intended behavior. A program meeting a given specification is extracted from the proof of a theorem in a variant of situational logic, in which the states of a computation are explicit objects. As an example, an in-place reverse program has been derived in an imperative LISP, which includes assignment and destructive list operations (rplaca and rplacd)."},
{"Title": "The deductive synthesis of imperative LISP programs", "URL": "https://dl.acm.org/doi/10.5555/1863696.1863724", "Full Abstract": "A framework is described for the automatic synthesis of imperative programs, which may alter data structures and produce destructive side effects as part of their intended behavior. A program meeting a given specification is extracted from the proof of a theorem in a variant of situational logic, in which the states of a computation are explicit objects. As an example, an in-place reverse program has been derived in an imperative LISP, which includes assignment and destructive list operations (rplaca and rplacd)."},
{"Title": "The origin of a binary-search paradigm", "URL": "https://dl.acm.org/doi/10.1016/0167-6423%2887%2990025-6", "Full Abstract": "No abstract available."},
{"Title": "A Hierarchy of Temporal Properties", "URL": "https://dl.acm.org/doi/book/10.5555/892426", "Full Abstract": "We propose a classification of temporal properties into a hierarchy which refines the known safety-liveness classification of properties. The new classification recognizes the classes of safety, guarantee, persistence, fairness, and hyper-fairness. The classification suggested here is based on the different ways a property of finite computations can be extended into a property of infinite computations. For properties that are expressible by temporal logic and predicate automata, we provide a syntactic characterization of the formulae and automata that specify properties in the different classes. We consider the verification of properties over a given program, and provide a unique proof principle for each class."},
{"Title": "Specification and verification of concurrent programs by A ∀ automata", "URL": "https://dl.acm.org/doi/10.1145/41625.41626", "Full Abstract": "∀-automata are non-deterministic finite-state automata over infinite sequences. They differ from conventional automata in that a sequence is accepted if"},
{"Title": "How to clear a block: A theory of plans", "URL": "https://dl.acm.org/doi/10.5555/41391.41392", "Full Abstract": "No abstract available."},
{"Title": "A hierarchy of temporal properties", "URL": "https://dl.acm.org/doi/10.1145/41840.41857", "Full Abstract": "No abstract available."},
{"Title": "The anchored version of the temporal framework", "URL": "https://dl.acm.org/doi/10.5555/648140.749663", "Full Abstract": "No abstract available."},
{"Title": "Specification and Verification of Concurrent Programs by For-All Automata", "URL": "https://dl.acm.org/doi/book/10.5555/892457", "Full Abstract": "For-all automata are non-deterministic finite-state automata over infinite sequences. They differ from conventional automata in that a sequence is accepted if all runs of the automaton over the sequence are accepting. These automata are suggested as a formalism for the specification and verification of temporal properties of concurrent programs. It is shown that they are as expressive as extended temporal logic (ETL), and, in some cases, provide a more compact representation of properties than temporal logic. A structured diagram notation is suggested for the graphical representation of these automata. A single sound and complete proof rule is presented for proving that all computations of a program have the property specified by a for-all automaton."},
{"Title": "Completing the Temporal Picture", "URL": "https://dl.acm.org/doi/10.5555/646243.681453", "Full Abstract": "No abstract available."},
{"Title": "Temporal logic programming", "URL": "https://dl.acm.org/doi/10.1016/S0747-7171%2889%2980070-7", "Full Abstract": "Temporal logic, often used as a specification language for programs, can serve directly as a programming language. We propose a specific programming language TEMPLOG, which extends the classical PROLOG-like languages to include temporal operators. PROLOG progams are collections of classical Horn clauses and they are efficiently interpreted by SLD-resolution. Similarly, TEMPLOG programs are collections of temporal Horn clauses and we interpret them with temporal SLD-resolution, a restricted form of a general temporal resolution method."},
{"Title": "On the average behavior of set merging algorithms (Extended Abstract)", "URL": "https://dl.acm.org/doi/10.1145/800113.803648", "Full Abstract": "In this paper we study the expected running time of a variety of algorithms that perform set merging. The set merging problem (for example, see AHU [1]) is concerned with using suitable data structures to represent partition of a set S = { 1,2, ....,n so that a sequence of instructions of the form “x Ξ y”, meaning"},
{"Title": "Lower Bounds on Merging Networks", "URL": "https://dl.acm.org/doi/10.1145/321958.321976", "Full Abstract": "Let"},
{"Title": "K + 1 heads are better than K", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1976.18", "Full Abstract": "There are languages which can be recognized by a deterministic (k + 1)-headed oneway finite automaton but which cannot be recognized by a k-headed one-way (deterministic or non-deterministic) finite automaton. Furthermore, there is a language accepted by a 2-headed nondeterministic finite automaton which is accepted by no k-headed deterministic finite automaton."},
{"Title": "The complexity of searching an ordered random table", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1976.32", "Full Abstract": "No abstract available."},
{"Title": "An Ω(n", "URL": "https://dl.acm.org/doi/10.1145/800105.803391", "Full Abstract": "Let P be a polyhedron with f"},
{"Title": "On the loop switching addressing problem", "URL": "https://dl.acm.org/doi/book/10.5555/892151", "Full Abstract": "The following graph addressing problem was studied by Graham and Pollak in devising a routing scheme for Pierce's Loop Switching Network. Let G be a graph with n vertices. It is desired to assign to each vertex $v_i$ an address in ${{0,1,*^\\ell$, such that the Hamming distance between the addresses of any two vertices agrees with their distance in G. Let N(G) be the minimum length $\\ell$ for which an assignment is possible. It was shown by Graham and Pollak that N(G) $\\leq m_G$(n-1), where $m_G$ is the diameter of G. In the present paper, we shall prove that N(G) $\\leq 1.09(lg m_G$)n + 8n by an explicit construction. This shows in particular that any graph has an addressing scheme of length O(n log n)."},
{"Title": "The complexity of pattern matching for a random string", "URL": "https://dl.acm.org/doi/book/10.5555/892154", "Full Abstract": "We study the average-case complexity of finding all occurrences of a given pattern $\\alpha$ in an input text string. Over an alphabet of q symbols, let c($\\alpha$,n) be the minimum average number of characters that need to be examined in a random text string of length n. We prove that, for large m, almost all patterns $\\alpha$ of length m satisfy c($\\alpha$,n) = $\\Theta (\\lceil \\log_q (${n-m/{ln m + 2)\\rceil )$ if $m \\leq n \\leq 2m$, and c($\\alpha$,n) = $\\Theta ({\\lceil \\log_q m\\rceil/m n)$ if n < 2m. This in particular confirms a conjecture raised in a recent paper by Knuth, Morris, and Pratt [1977]."},
{"Title": "A lower bound to palindrome recognition by probabilistic Turing machines", "URL": "https://dl.acm.org/doi/book/10.5555/892157", "Full Abstract": "We call attention to the problem of proving lower bounds on probabilistic Turing machine computations. It is shown that any probabilisitc Turing machine recognizing the language L = {w $\\phi$ w | w $\\epsilon$ ${{0,1^*$ with error $\\lambda$ > 1/2 must take $\\Omega$(n log n) time."},
{"Title": "On constructing minimum spanning trees in k-dimensional spaces and related problems", "URL": "https://dl.acm.org/doi/book/10.5555/892163", "Full Abstract": "The problem of finding a minimum spanning tree connecting n points in a k-dimensional space is discussed under three common distance metrics -- Euclidean, rectilinear, and $L_\\infty$. By employing a subroutine that solves the post office problem, we show that, for fixed k $\\geq$ 3, such a minimum spanning tree can be found in time O($n^{2-a(k) {(log n)^{1-a(k)$), where a(k) = $2^{-(k+1)$. The bound can be improved to O(${(n log n)^{1.8$) for points in the 3-dimensional Euclidean space. We also obtain o($n^2$) algorithms for finding a farthest pair in a set of n points and for other related problems."},
{"Title": "k", "URL": "https://dl.acm.org/doi/10.1145/322063.322076", "Full Abstract": "Copyright © 1978 ACM."},
{"Title": "New algorithms in bin packing", "URL": "https://dl.acm.org/doi/book/10.5555/892175", "Full Abstract": "In the bin-packing problem a list L of n numbers are to be packed into unit-capacity bins. For any algorithm S, let r(S) be the maximum ratio S(L)/$L^*$ for large $L^*$, where S(L) denotes the number of bins used by S and $L^*$ denotes the minimum number needed. In this paper we give an on-line O(n log n)-time algorithm RFF with r(RFF) = 5/3, and an off-line polynomial-time algorithm RFFD with r(RFFD) = (11/9)-$\\epsilon$ for some fixed $\\epsilon$ < 0. These are strictly better respectively than two prominent algorithms -- the First-Fit (FF) which is on-line with r(FF) = 17/10, and the First-Fit-Decreasing (FFD) with r(FFD) = 11/9. Furthermore, it is shown that any on-line algorithm S must have r(S) $\\geq$ 3/2. We also discuss the question \"how well can an O(n)-time algorithm perform?\", showing that, in the generalized d-dimensional bin-packing, any O(n)-time algorithm S must have r(S) $\\geq$ d."},
{"Title": "Information bounds are weak in the shortest distance problem", "URL": "https://dl.acm.org/doi/book/10.5555/892180", "Full Abstract": "In the all-pair shortest distance problem, one computes the matrix D = ($d_{ij$) where $d_{ij$ is the minimum weighted length of any path from vertex i to vertex j in a directed complete graph with a weight on each edge. In all the known algorithms, a shortest path $p_{ij$ achieving $d_{ij$ is also implicitly computed. In fact, $\\log_3$ f(n) is an information-theoretic lower bound where f(n) is the total number of distinct patterns ($p_{ij$) for n-vertex graphs. As f(n) potentially can be as large as $2^{n^3$, it is hopeful that a non-trivial lower bound can be derived this way in the decision tree model. We study the characterization and enumeration of realizable patterns, and show that f(n) $\\leq C^{n^2$. Thus no lower bound greater than C$n^2$ can be derived from this approach. We prove as a corollary that the Triangular polyhedron $T^{(n)$, defined in $E^{(n\\choose 2)$ by $d_{ij \\geq 0$ and the triangle inequalities $d_{ij + d_{jk \\geq d_{ik$, has at most $C^{n^2$ faces of all dimensions, thus resolving an open question in a similar information bound approach to the shortest distance problem."},
{"Title": "On the average-case complexity of selecting k-th best", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1978.29", "Full Abstract": "Let Vk (n) be the minimum average number of pairwise comparisons needed to find the k-th largest of n numbers (k≥2), assuming that all n! orderings are equally likely. D. W. Matula proved that, for some absolute constant c, Vk(n)- n ≤ ck log log n as n → ∞. In the present paper, we show that there exists an absolute constant c′ > 0 such that Vk(n) - n ≥ c′k log log n as n → ∞, proving a conjecture by Matula."},
{"Title": "An analysis of a memory allocation scheme for implementing stacks", "URL": "https://dl.acm.org/doi/book/10.5555/892201", "Full Abstract": "Consider the implementation of two stacks by letting them grow towards each other in a table of size m . Suppose a random sequence of insertions and deletions are executed, with each instruction having a fixed probability p (0 > p > 1/2) to be a deletion. Let $A_p (m) denote the expected value of max{x,y, where x and y are the stack heights when the table first becomes full. We shall prove that, as $m \\rightarrow \\infty$, $A_p (m) = \\sqrt{m/(2 \\pi (1-2p)) + O((log m)/ \\sqrt{m)$. This gives a solution to an open problem in Knuth [\"The Art of Computer Programming, Vol. 1, Exercise 2.2.2-13]."},
{"Title": "On fault-tolerant networks for sorting", "URL": "https://dl.acm.org/doi/book/10.5555/892209", "Full Abstract": "The study of constructing reliable systems from unreliable components goes back to the work of von Neumann, and of Moore and Shannon. The present paper studies the use of redundancy to enhance reliability for sorting and related networks built from unreliable comparators. Two models of fault-tolerant networks are discussed. The first model patterns after the concept of error-correcting codes in information theory, and the other follows the stochastic criterion used by von Neumann and Moore-Shannon. It is shown, for example, that an additional k(2n-3) comparators are sufficient to render a sorting network reliable, provided that no more than k of its comparators may be faulty."},
{"Title": "An analysis of (h,k,l)-shellsort", "URL": "https://dl.acm.org/doi/book/10.5555/892211", "Full Abstract": "One classical sorting algorithm, whose performance in many cases remains unanalyzed, is Shellsort. Let $\\vec{h be a t-component vector of positive integers. An $\\vec{h$-Shellsort will sort any given n elements in t passes, by means of comparisons and exchanges of elements. Let $S_j$($\\vec{h$;n) denote the average number of element exchanges in the j-th pass, assuming that all the n! initial orderings are equally likely. In this paper we derive asymptotic formulas of $S_j$($\\vec{h$;n) for any fixed $\\vec{h$ = (h,k,l), making use of a new combinatorial interpretation of $S_3$. For the special case $\\vec{h$ = (3,2,1), the analysis if further sharpened to yield exact expressions."},
{"Title": "A lower bound to finding convex hulls", "URL": "https://dl.acm.org/doi/book/10.5555/891707", "Full Abstract": "Given a set S of n distinct points {($x_i$,$y_i$) | 0 $\\leq$ i > n, the convex hull problem is to determine the vertices of the convex hull H(S). All the known algorithms for solving this problem have a worst-case running time of cn log n or higher, and employ only quadratic tests, i.e., tests of the form f($x_0$, $y_0$, $x_1$, $y_1$,...,$x_{n-1$, $y_{n-1$): 0 with f being any polynomial of degree not exceeding 2. In this paper, we show that any algorithm in the quadratic decision-tree model must make cn log n tests for some input."},
{"Title": "Modeling probabilistic actions for practical decision-theoretic planning", "URL": "https://dl.acm.org/doi/10.5555/3036846.3036855", "Full Abstract": "Most existing decision-theoretic planners represent uncertainty about the state of the world with a precisely specified probability distribution over world states. This representation is not expressive enough to model many interesting classes of practical planning problems, and renders inapplicable some abstraction-based planning approaches. In this paper we propose as a remedy a more general world and action model with a well-founded semantics based on probability intervals. We introduce the concept of"},
{"Title": "Sound abstraction of probabilistic actions in the constraint mass assignment framework", "URL": "https://dl.acm.org/doi/10.5555/2074284.2074311", "Full Abstract": "This paper provides a formal and practical framework for sound abstraction of probabilistic actions. We start by precisely defining the concept of sound abstraction within the context of finite-horizon planning (where each plan is a finite sequence of actions). Next we show that such abstraction cannot be performed within the traditional probabilistic action representation, which models a world with a single probability distribution over the state space. We then present the constraint mass assignment representation, which models the world with a set of probability distributions and is a generalization of mass assignment representations. Within this framework, we present sound abstraction procedures for three types of action abstraction. We end the paper with discussions and related work on sound and approximate abstraction. We give pointers to papers in which we discuss other sound abstraction-related issues, including applications, estimating loss due to abstraction, and automatically generating abstraction hierarchies."},
{"Title": "Geometric foundations for interval-based probabilities", "URL": "https://dl.acm.org/doi/10.1023/A%3A1018936829318", "Full Abstract": "The need to reason with imprecise probabilities arises in a wealth of situations ranging from pooling of knowledge from multiple experts to abstractionýbased probabilistic planning. Researchers have typically represented imprecise probabilities using intervals and have developed a wide array of different techniques to suit their particular requirements. In this paper we provide an analysis of some of the central issues in representing and reasoning with interval probabilities. At the focus of our analysis is the probability crossýproduct operator and its interval generalization, the ccýoperator. We perform an extensive study of these operators relative to manipulation of sets of probability distributions. This study provides insight into the sources of the strengths and weaknesses of various approaches to handling probability intervals. We demonstrate the application of our results to the problems of inference in interval Bayesian networks and projection and evaluation of abstract probabilistic plans."},
{"Title": "Efficiently ordering query plans for data integration", "URL": "https://dl.acm.org/doi/10.5555/3068476.3068482", "Full Abstract": "We describe Streamer, the query-reformulation component of a data integration system. Given a utility measure and a user query, Streamer uses abstraction-based refinement planning and exploits information on plan independence to produce, in decreasing order of utility, a set of plans that access data sources to obtain answers to the query. We then focus on plan coverage as an important utility measure. We show how to use statistic information about the domain and data sources to estimate plan coverage, and how to incorporate the plan-coverage framework into Streamer. In doing so, we provide the first method for effectively integrating the use of quantitative information into the query optimizer of a data-integration system. We present preliminary experimental results suggesting that Streamer runs an order of magnitude faster than brute-force plan-ordering methods, which are the only currently available methods to compute"},
{"Title": "Reconciling schemas of disparate data sources", "URL": "https://dl.acm.org/doi/10.1145/375663.375731", "Full Abstract": "A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy."},
{"Title": "Learning to map between structured representations of data", "URL": "https://dl.acm.org/doi/book/10.5555/936674", "Full Abstract": "This dissertation studies"},
{"Title": "Learning to map between ontologies on the semantic web", "URL": "https://dl.acm.org/doi/10.1145/511446.511532", "Full Abstract": "Ontologies play a prominent role on the Semantic Web. They make possible the widespread publication of machine understandable data, opening myriad opportunities for automated information processing. However, because of the Semantic Web's distributed nature, data on it will inevitably come from many different ontologies. Information processing across ontologies is not possible without knowing the semantic mappings between their elements. Manually finding such mappings is tedious, error-prone, and clearly not possible at the Web scale. Hence, the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web.We describe"},
{"Title": "Database research at the University of Illinois at Urbana-Champaign", "URL": "https://dl.acm.org/doi/10.1145/601858.601881", "Full Abstract": "Copyright © 2002 Authors."},
{"Title": "Learning to Match the Schemas of Data Sources", "URL": "https://dl.acm.org/doi/10.1023/A%3A1021765902788", "Full Abstract": "The problem of integrating data from multiple data sources—either on the Internet or within enterprises—has received much attention in the database and AI communities. The focus has been on building data integration systems that provide a"},
{"Title": "Object matching for information integration", "URL": "https://dl.acm.org/doi/10.5555/3104278.3104289", "Full Abstract": "No abstract available."},
{"Title": "Optimal Construction of Edge-Disjoint Paths in Random Graphs", "URL": "https://dl.acm.org/doi/10.1137/S0097539795290805", "Full Abstract": "Given a graph G=(V,E) with n vertices, m edges, and a family of $\\kappa$ pairs of vertices in"},
{"Title": "A technique for measuring the relative size and overlap of public Web search engines", "URL": "https://dl.acm.org/doi/10.1016/S0169-7552%2898%2900127-5", "Full Abstract": "No abstract available."},
{"Title": "The connectivity server", "URL": "https://dl.acm.org/doi/10.1016/S0169-7552%2898%2980047-0", "Full Abstract": "No abstract available."},
{"Title": "A technique for measuring the relative size and overlap of public Web search engines", "URL": "https://dl.acm.org/doi/10.5555/297805.297863", "Full Abstract": "No abstract available."},
{"Title": "The connectivity server", "URL": "https://dl.acm.org/doi/10.5555/297805.297941", "Full Abstract": "No abstract available."},
{"Title": "Dynamic Packet Routing on Arrays with Bounded Buffers", "URL": "https://dl.acm.org/doi/10.5555/646387.690185", "Full Abstract": "No abstract available."},
{"Title": "Min-wise independent permutations (extended abstract)", "URL": "https://dl.acm.org/doi/10.1145/276698.276781", "Full Abstract": "Copyright © 1998 ACM."},
{"Title": "A Derandomization Using Min-Wise Independent Permutations", "URL": "https://dl.acm.org/doi/10.5555/646975.711400", "Full Abstract": "Min-wise independence is a recently introduced notion of limited independence, similar in spirit to pairwise independence. The later has proven essential for the derandomization of many algorithms. Here we show that approximate min-wise independence allows similar uses, by presenting a derandomization of the RNC algorithm for approximate set cover due to S. Rajagopalan and V. Vazirani. We also discuss how to derandomize their set multi-cover and multi-set multi-cover algorithms in restricted cases. The multi-cover case leads us to discuss the concept of"},
{"Title": "Summary cache", "URL": "https://dl.acm.org/doi/10.1145/285237.285287", "Full Abstract": "The sharing of caches among Web proxies is an important technique to reduce Web traffic and alleviate network bottlenecks. Nevertheless it is not widely deployed due to the overhead of existing protocols. In this paper we propose a new protocol called \"Summary Cache\"; each proxy keeps a summary of the URLs of cached documents of each participating proxy and checks these summaries for potential hits before sending any queries. Two factors contribute to the low overhead: the summaries are updated only periodically, and the summary representations are economical --- as low as 8 bits per entry. Using trace-driven simulations and a prototype implementation, we show that compared to the existing Internet Cache Protocol (ICP), Summary Cache reduces the number of inter-cache messages by a factor of 25 to 60, reduces the bandwidth consumption by over 50%, and eliminates between 30% to 95% of the CPU overhead, while at the same time maintaining almost the same hit ratio as ICP. Hence Summary Cache enables cache sharing among a large number of proxies."},
{"Title": "Information Retrieval on the Web", "URL": "https://dl.acm.org/doi/10.5555/795664.796468", "Full Abstract": "The Web explosion offers a bonanza of algorithmic problems. In particular, information retrieval in the web context requires methods and ideas that have not been addressed in the classic IR literature. This tutorial will survey emerging techniques for IR in the web context and discuss some of the pertinent open problems.The list of topics includes search engine technology, ranking and classification methods, web measurements (usage, size, connectivity), and new graph and data structure problems arising in the web IR context."},
{"Title": "Static and dynamic path selection on expander graphs", "URL": "https://dl.acm.org/doi/10.5555/308215.308231", "Full Abstract": "No abstract available."},
{"Title": "Unscrambling address lines", "URL": "https://dl.acm.org/doi/10.5555/314500.315057", "Full Abstract": "No abstract available."},
{"Title": "Mirror, mirror on the Web", "URL": "https://dl.acm.org/doi/10.1016/S1389-1286%2899%2900021-3", "Full Abstract": "No abstract available."},
{"Title": "Finding anything in the billion page Web", "URL": "https://dl.acm.org/doi/10.5555/313009.313138", "Full Abstract": "No abstract available."},
{"Title": "Completeness and Robustness Properties of Min-Wise Independent Permutations", "URL": "https://dl.acm.org/doi/10.5555/646976.711541", "Full Abstract": "No abstract available."},
{"Title": "Min-Wise versus linear independence (extended abstract)", "URL": "https://dl.acm.org/doi/10.5555/338219.338246", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Improved classification via connectivity information", "URL": "https://dl.acm.org/doi/10.5555/338219.338610", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Min-Wise Independent Permutations", "URL": "https://dl.acm.org/doi/10.1006/jcss.1999.1690", "Full Abstract": "We define and study the notion of min-wise independent families of permutations. We say that F Sn (the symmetric group) is min-wise independent if for any set X n and any x X, when is chosen at random in F we havePr(min{ (X)= (x))=1|X| . In other words we require that all the elements of any fixed set X have an equal chance to become the minimum element of the image of X under . Our research was motivated by the fact that such a family (under some relaxations) is essential to the algorithm used in practice by the AltaVista web index software to detect and filter near-duplicate documents. However, in the course of our investigation we have discovered interesting and challenging theoretical questions related to this concept we present the solutions to some of them and we list the rest as open problems."},
{"Title": "Graph structure in the Web", "URL": "https://dl.acm.org/doi/10.1016/S1389-1286%2800%2900083-9", "Full Abstract": "The study of the Web as a graph is not only fascinating in its own right, but also yields valuable insight into Web algorithms for crawling, searching and community discovery, and the sociological phenomena which characterize its evolution. We report on experiments on local and global properties of the Web graph using two AltaVista crawls each with over 200 million pages and 1.5 billion links. Our study indicates that the macroscopic structure of the Web is considerably more intricate than suggested by earlier experiments on a smaller scale."},
{"Title": "Operating systems (2nd ed.)", "URL": "https://dl.acm.org/doi/book/10.5555/249000", "Full Abstract": "No abstract available."},
{"Title": "Locating objects in wide-area systems", "URL": "https://dl.acm.org/doi/10.1109/35.649334", "Full Abstract": "Locating mobile objects in a worldwide system requires a scalable location service. An object can be a telephone or a notebook computer, but also a software or data object, such as a file or an electronic document. Our service strictly separates an object's name from the addresses where it can be contacted. This is done by introducing a location-independent object handle. An object's name is bound to its unique object handle, which, in turn, is mapped to the addresses where the object can be contacted. To locate an object, we need only its object handle. We present a scalable location service based on a worldwide distributed search tree that adapts dynamically to an object's migration pattern to optimize lookups and updates"},
{"Title": "A Framework for Consistent, Replicated Web Objects", "URL": "https://dl.acm.org/doi/10.5555/850926.851703", "Full Abstract": "Despite the extensive use of caching techniques, the Web is overloaded. While the caching techniques currently used help some, it would be better to use different caching and replication strategies for different Web pages, depending on their characteristics. We propose a framework in which such strategies can be devised independently per Web document.A Web document is constructed as a worldwide, scalable distributed Web object. Depending on the coherence requirements for that document, the most appropriate caching or replication strategy can subsequently be implemented and encapsulated by the Web object. Coherence requirements are formulated from two different perspectives: that of the Web object, and that of clients using the Web object. We have developed a prototype in Java to demonstrate the feasibility of implementing different strategies for different Web objects."},
{"Title": "Replicated invocations in wide-area systems", "URL": "https://dl.acm.org/doi/10.1145/319195.319215", "Full Abstract": "Copyright © 1998 ACM."},
{"Title": "Structured Computer Organization", "URL": "https://dl.acm.org/doi/book/10.5555/552473", "Full Abstract": "From the Publisher:"},
{"Title": "Globe", "URL": "https://dl.acm.org/doi/10.1109/4434.749137", "Full Abstract": "Developing large-scale wide-area applications requires an infrastructure that is presently lacking. Currently, most Internet applications have to be built on top of raw communication services, such as TCP connections. All additional services, including those for naming, replication, migration, persistence, fault tolerance, and security, have to be implemented for each application anew. Not only is this a waste of effort, it also makes interoperability between different applications difficult or even impossible. The authors present a novel, object-based framework for developing wide-area distributed applications. The framework is based on the concept of a distributed shared object, which has the characteristic feature that its state can be physically distributed across multiple machines at the same time. All implementation aspects, including communication protocols, replication strategies, and distribution and migration of state, are part of each object and are hidden behind its interface. The current performance problems of the World-Wide Web are taken as an example to illustrate the benefit of encapsulating state, operations, and implementation strategies on a per-object basis. The authors describe how distributed objects can be used to implement worldwide scalable Web documents."},
{"Title": "A Security Design for a Wide-Area Distributed System", "URL": "https://dl.acm.org/doi/10.5555/646281.687819", "Full Abstract": "No abstract available."},
{"Title": "From Remote Objects to Physically Distributed Objects", "URL": "https://dl.acm.org/doi/10.5555/795674.797051", "Full Abstract": "Present-day object-oriented middleware provides little support for the distribution, replication and caching of the state of a distributed object. This makes these platforms unsuitable for the development of large-scale distributed applications. We argue that the model of distributed objects on which these middleware platforms are based hinders the addition of comprehensive distribution and replication support to these platforms. We present an alternative view of distributed objects, in which objects are not only in control of the functional aspects of their implementation but also in control of their nonfunctional aspects, in particular, the distribution and replication of their state. We claim that a middleware platform based on this view of distributed objects is better suited for developing the large-scale applications of the future."},
{"Title": "The globe distribution network", "URL": "https://dl.acm.org/doi/10.5555/1267724.1267765", "Full Abstract": "The goal of the Globe project is to design and build a middleware platform that facilitates the development of large-scale distributed applications, such as those found on the Internet. To demonstrate the feasibility of our design and to test our ideas, we are currently building a new Internet application: The Globe Distribution Network. The Globe Distribution Network, or GDN, is an application for the efficient, worldwide distribution of free software and other free data. The GDN can be seen as an improvement to anonymous FTP and the World Wide Web due to its flexibility and extensive support for replication. This paper describes the design of the GDN. We start by explaining how the replication facilities of the Globe middleware are used to make the GDN efficient, and how these facilities are implemented. Next, we present the architecture of the GDN and discuss how the Domain Name System can be used as a first approach towards a worldwide service for naming software packages and other entities. This is followed by an analysis of the security requirements for the GDN and measures taken to satisfy these requirements. We hope to make Globe and GDN itself available for free under the BSD license by 2001."},
{"Title": "A Law-Abiding Peer-to-Peer Network for Free-Software Distribution", "URL": "https://dl.acm.org/doi/10.5555/580585.883129", "Full Abstract": "The Globe Distribution Network (GDN) is an application for worldwide distribution of freely redistributable software packages. The GDN takes a novel, optimistic approach to stop the illegal distribution of copyrighted and illicit material via the network. Instead of having moderators check the software archives at upload time, illegal content is removed and its uploader's access to the network permanently revoked only when the content is discovered. An important feature of the GDN is that the objects containing the software can run on untrustworthy servers. A first version of the GDN has been implemented and has been running since October 2000 across four European sites."},
{"Title": "Scalable Human-Friendly Resource Names", "URL": "https://dl.acm.org/doi/10.1109/4236.957891", "Full Abstract": "Currently, Uniform Resource Locators (URLs) are used to name and access Web-based resources. However, URLs pose a significant scalability problem because they cannot be used to refer to replicated Web pages. The authors propose a new URI scheme called Human-Friendly Names (HFNs) to solve this scalability problem. HFNs are high-level names that are easy-to-use by humans and name Web resources in a location-independent way. This article describes a scalable HFN-to-URL resolution mechanism that is based on URNs and makes use of the Domain Name System (DNS) and the Globe Location Service."},
{"Title": "Distributed Systems", "URL": "https://dl.acm.org/doi/book/10.5555/559404", "Full Abstract": "From the Publisher: Andrew Tanenbaum and Maarten van Steen cover the principles, advanced concepts, and technologies of distributed systems in detail, including: communication, replication, fault tolerance, and security. Intended for use in a senior/graduate level distributed systems course or by professionals, this text systematically shows how distributed systems are designed and implemented in real systems. Written in the superb writing style of other Tanenbaum books, the material also features unique accessibility and a wide variety of real-world examples and case studies, such as NFS v4, CORBA, DOM, Jini, and the World Wide Web. FEATURES Detailed coverage of seven key principles. An introductory chapter followed by a chapter devoted to each key principle: communication, processes, naming, synchronization, consistency and replication, fault tolerance, and security, including unique comprehensive coverage of middleware models. Four chapters devoted to state-of-the-art real-world examples of middleware. Covers object-based systems, document-based systems, distributed file systems, and coordination-based systems including CORBA, DCOM, Globe, NFS v4, Coda, the World Wide Web, and Jini. Excellent coverage of timely, advanced, distributed systems topics: Security, payment systems, recent Internet and Web protocols, scalability, and caching and replication. NEW-The Prentice Hall Companion Website for this book contains PowerPoint slides, figures in various file formats, and other teaching aids, and a link to the author's Web site."},
{"Title": "The distributed ASCI Supercomputer project", "URL": "https://dl.acm.org/doi/10.1145/506106.506115", "Full Abstract": "The Distributed ASCI Supercomputer (DAS) is a homogeneous wide-area distributed system consisting of four cluster computers at different locations. DAS has been used for research on communication software, parallel languages and programming systems, schedulers, parallel applications, and distributed applications. The paper gives a preview of the most interesting research results obtained so far in the DAS project."},
{"Title": "Disallowing Unauthorized State Changes of Distributed Shared Objects", "URL": "https://dl.acm.org/doi/10.5555/647183.719499", "Full Abstract": "No abstract available."},
{"Title": "Modern Operating Systems", "URL": "https://dl.acm.org/doi/book/10.5555/516975", "Full Abstract": "From the Publisher: FEATURES \\ NEWNew chapters on computer security, multimedia operating systems, and multiple processor systems. NEWExtensive coverage of Linux, UNIX&#174;, and Windows 2000&#8482; as examples. NEWNow includes coverage of graphical user interfaces, multiprocessor operating systems, trusted systems, viruses, network terminals, CD-ROM file systems, power management on laptops, RAID, soft timers, stable storage, fair-share scheduling, three-level scheduling, and new paging algorithms. NEWMost chapters have a new section on current research on the chapter's topic. NEWFocus on single-processor computer systems; a new book for a follow-up course on distributed systems is also available from Prentice Hall. NEWOver 200 references to books and papers published since the first edition. NEWThe Web site for this book contains PowerPoint slides, simulators, figures in various formats, and other teaching aids."},
{"Title": "Differentiated strategies for replicating Web documents", "URL": "https://dl.acm.org/doi/10.1016/S0140-3664%2800%2900319-4", "Full Abstract": "Replicating Web documents reduces user-perceived delays and wide-area network traffic. Numerous caching and replication protocols have been proposed to manage such replication while keeping the document copies consistent. We claim, however, that no single caching or replication policy can efficiently manage all documents. Instead, we propose that each document be replicated with a policy specifically tailored to it. We have collected traces on our university's Web server and conducted simulations to determine the performance such tailored policies would produce, as opposed to using the same policy for all documents. The results show a significant performance improvement with respect to end-user delays, wide-area network traffic and document consistency. We also present how these results can be used to build adaptive replicated Web documents, capable of automatically selecting the policy that best suits them."},
{"Title": "Experiences with the amoeba distributed operating system", "URL": "https://dl.acm.org/doi/10.5555/360596.360643", "Full Abstract": "No abstract available."},
{"Title": "Dynamically Selecting Optimal Distribution Strategies for Web Documents", "URL": "https://dl.acm.org/doi/10.1109/TC.2002.1009149", "Full Abstract": "To improve the scalability of the Web, it is common practice to apply caching and replication techniques. Numerous strategies for placing and maintaining multiple copies of Web documents at several sites have been proposed. These approaches essentially apply a global strategy by which a single family of protocols is used to choose replication sites and keep copies mutually consistent. We propose a more flexible approach by allowing each distributed document to have its own associated strategy. We propose a method for assigning an optimal strategy to each document separately and prove that it generates a family of optimal results. Using trace-based simulations, we show that optimal assignments clearly outperform any global strategy. We have designed an architecture for supporting documents that can dynamically select their optimal strategy and evaluate its feasibility."},
{"Title": "Access control, reverse access control and replication control in a world wide distributed system", "URL": "https://dl.acm.org/doi/10.5555/647802.737168", "Full Abstract": "No abstract available."},
{"Title": "Logic for Computable Functions: description of a machine implementation.", "URL": "https://dl.acm.org/doi/book/10.5555/891954", "Full Abstract": "This paper is primarily a user's manual for LCF, a proof-checking program for a logic of computable functions proposed by Dana Scott in 1969 but unpublished by him. We use the name LCF also for the logic itself, which is presented at the start of the paper. The proof-checking program is designed to allow the user interactively to generate formal proofs about computable functions and functionals over a variety of domains, including those of interest to the computer scientist - for example, integers, lists and computer programs and their semantics. The user's task is alleviated by two features: a subgoaling facility and a powerful simplification mechanism. Applications include proofs of program correctness and in particular of compiler correctness; these applications are not discussed herein, but are illustrated in the papers referenced in this introduction."},
{"Title": "A calculus for the mathematical theory of computation", "URL": "https://dl.acm.org/doi/10.5555/646795.704997", "Full Abstract": "No abstract available."},
{"Title": "Models of LCF.", "URL": "https://dl.acm.org/doi/book/10.5555/891977", "Full Abstract": "LCF is a deductive system for computable functions proposed by D. Scott in 1969 in an unpublished memorandum. The purpose of the present paper is to demonstrate the soundness of the system with respect to certain models, which are partially ordered domains of continuous functions. This demonstration was supplied by Scott in his memorandum; the present paper is merely intended to make this work more accessible."},
{"Title": "A Metalanguage for interactive proof in LCF", "URL": "https://dl.acm.org/doi/10.1145/512760.512773", "Full Abstract": "Copyright © 1978 ACM."},
{"Title": "An Algebraic Theory for Synchronization", "URL": "https://dl.acm.org/doi/10.5555/647209.719878", "Full Abstract": "No abstract available."},
{"Title": "Concurrent Processes and Their Syntax", "URL": "https://dl.acm.org/doi/10.1145/322123.322134", "Full Abstract": "Copyright © 1979 ACM."},
{"Title": "Flowgraphs and Flow Algebras", "URL": "https://dl.acm.org/doi/10.1145/322154.322167", "Full Abstract": "Copyright © 1979 ACM."},
{"Title": "On Observing Nondeterminism and Concurrency", "URL": "https://dl.acm.org/doi/10.5555/646234.758793", "Full Abstract": "No abstract available."},
{"Title": "A Modal Characterisation of Observable Machine-Behaviour", "URL": "https://dl.acm.org/doi/10.5555/648216.750906", "Full Abstract": "No abstract available."},
{"Title": "Principal type-schemes for functional programs", "URL": "https://dl.acm.org/doi/10.1145/582153.582176", "Full Abstract": "Copyright © 1982 ACM."},
{"Title": "Four combinators for concurrency", "URL": "https://dl.acm.org/doi/10.1145/800220.806687", "Full Abstract": "An algebraic calculus of asynchronous parallel computation, called CCS (Calculus of Communicating Systems), was developed in [HM,Mil 1]. CCS can express both the semantics of parallel programming languages and the behaviour of data structures (mailbox, random access memory, buffer) which serve as interfaces between independent agents. The primitive notion is 'handshake' communication. The emphasis is upon (i) synthesis from components and (ii) extensionality (meaning = observable behaviour), in contrast with Petri's Net theory which emphasizes causal independence."},
{"Title": "A  Calculus of Communicating Systems", "URL": "https://dl.acm.org/doi/book/10.5555/539036", "Full Abstract": "No abstract available."},
{"Title": "Using Algebra for Concurrency", "URL": "https://dl.acm.org/doi/10.5555/647694.731201", "Full Abstract": "No abstract available."},
{"Title": "Parallel Combinator Reduction Machine", "URL": "https://dl.acm.org/doi/10.5555/647694.731340", "Full Abstract": "No abstract available."},
{"Title": "Nonclausal deduction in first-order temporal logic", "URL": "https://dl.acm.org/doi/10.1145/77600.77617", "Full Abstract": "This paper presents a proof system for first-order temporal logic. The system extends the nonclausal resolution method for ordinary first-order logic with equality, to handle quantifiers and temporal operators. Soundness and completeness issues are considered. The use of the system for verifying concurrent programs is discussed and variants of the system for other modal logics are also described."},
{"Title": "Tools and rules for the practicing verifier", "URL": "https://dl.acm.org/doi/book/10.5555/892493", "Full Abstract": "The paper presents a minimal proof theory which is adequate for proving the main important temporal properties of reactive programs. The properties we consider consist of the classes of invariance, response, and precedence properties. For each of these classes we present a small set of rules that is complete for verifying properties belonging to this class. We illustrate the application of these rules by analyzing and verifying the properties of a new algorithm for mutual exclusion."},
{"Title": "An exercise in the verification of multi-process programs", "URL": "https://dl.acm.org/doi/10.5555/119872.119905", "Full Abstract": "No abstract available."},
{"Title": "A hierarchy of temporal properties (invited paper, 1989)", "URL": "https://dl.acm.org/doi/10.1145/93385.93442", "Full Abstract": "Copyright © 1990 ACM."},
{"Title": "An interleaving model for real time.", "URL": "https://dl.acm.org/doi/book/10.5555/892496", "Full Abstract": "The interleaving model is both adequate and sufficiently abstract to allow for the practical specification and verification of many properties of concurrent systems. We incorporate real time into this model by defining the abstract notion of a real-time transition system as a conservative extension of traditional transition systems: qualitative fairness requirements are replaced (and superseded) by quantitative lower-bound and upper-bound real-time requirements for transitions. We present proof rules to establish lower and upper real-time bounds for response properties of real-time transition systems. This proof system can be used to verify bounded-invariance and bounded-response properties, such as timely terrnination of shared-variables multi-process systems, whose semantics is defined in terms of real-time transition systems."},
{"Title": "An interleaving model for real time", "URL": "https://dl.acm.org/doi/10.5555/100512.100633", "Full Abstract": "No abstract available."},
{"Title": "A temporal proof methodology for reactive systems", "URL": "https://dl.acm.org/doi/10.5555/100512.100637", "Full Abstract": "No abstract available."},
{"Title": "Temporal proof methodologies for real-time systems", "URL": "https://dl.acm.org/doi/10.1145/99583.99629", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "Timed Transition Systems", "URL": "https://dl.acm.org/doi/10.5555/648143.749987", "Full Abstract": "No abstract available."},
{"Title": "From Timed to Hybrid Systems", "URL": "https://dl.acm.org/doi/10.5555/648143.749988", "Full Abstract": "No abstract available."},
{"Title": "Completing the temporal picture", "URL": "https://dl.acm.org/doi/10.5555/111774.111780", "Full Abstract": "No abstract available."},
{"Title": "Temporal proof methodologies for real-time systems", "URL": "https://dl.acm.org/doi/book/10.5555/892514", "Full Abstract": "We extend the specification language of temporal logic, the corresponding verification framework, and the underlying computational model to deal with real-time properties of reactive systems. The abstract notion of timed transition systems generalizes traditional transition systems conservatively: qualitative fairness requirements are replaced (and superseded) by quantitative lower-bound and upper-bound timing constraints on transitions. This framework can model real-time systems that communicate either through shared variables or by message passing and real-time issues such as time-outs, process priorities (interrupts), and process scheduling. We exhibit two styles for the specification of real-time systems. While the first approach uses bounded versions of temporal operators, the second approach allows explicit references to time through a special clock variable. Corresponding to the two styles of specification, we present and compare two fundamentally different proof methodologies for the verification of timing requirements that are expressed in these styles. For the bounded-operatoT style, we provide a set of proof rules for establishing bounded-invariance and bounded-response properties of timed transition systems. This approach generalizes the standard temporal proof rules for verifying invariance and response properties conservatively. For the explicit-clock style, we exploit the observation that every time-bounded property is a safety property and use the standard temporal proof rules for establishing safety properties."},
{"Title": "Completing the temporal picture", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2891%2990041-Y", "Full Abstract": "No abstract available."},
{"Title": "Monotonicity properties in automated deduction", "URL": "https://dl.acm.org/doi/10.5555/132218.132234", "Full Abstract": "No abstract available."},
{"Title": "The temporal logic of reactive and concurrent systems", "URL": "https://dl.acm.org/doi/book/10.5555/128869", "Full Abstract": "No abstract available."},
{"Title": "The Special-Relation Rules are Incomplete", "URL": "https://dl.acm.org/doi/10.5555/648230.752615", "Full Abstract": "No abstract available."},
{"Title": "What Good Are Digital Clocks?", "URL": "https://dl.acm.org/doi/10.5555/646246.684870", "Full Abstract": "No abstract available."},
{"Title": "Characterization of Temporal Property Classes", "URL": "https://dl.acm.org/doi/10.5555/646246.684871", "Full Abstract": "No abstract available."},
{"Title": "The Complexity of Pattern Matching for a Random String", "URL": "https://dl.acm.org/doi/10.1137/0208029", "Full Abstract": "We study the average-case complexity of finding all occurrences of a given pattern $\\alpha $ in an input text string. Over an alphabet of"},
{"Title": "On the time-space tradeoff for sorting with linear queries", "URL": "https://dl.acm.org/doi/book/10.5555/892226", "Full Abstract": "Extending a result of Borodin, et al., we show that any branching program using linear queries \" $\\sum_{i {\\lambda_i {x_i: c$ \" to sort n numbers $x_1$,$x_2$,...,$x_n$ must satisfy the time-space tradeoff relation TS = $\\Omega (n_2)$. The same relation is also shown to be true for branching programs that use queries \" min R = ? \" where R is any subset of {$x_1$,$x_2$,...,$x_n$."},
{"Title": "Some monotonicity properties of partial orders", "URL": "https://dl.acm.org/doi/book/10.5555/892231", "Full Abstract": "A fundamental quantity which arises in the sorting of n numbers $a_1$, $a_2$,..., $a_n$ is Pr($a_i$ > $a_j$ | P), the probability that $a_i$ > $a_j$ assuming that all linear extensions of the partial order P are equally likely. In this paper we establish various properties of Pr($a_i$ > $a_j$ | P) and related quantities. In particular, it is shown that Pr($a_i$ > $b_j$ | P') $\\geq$ Pr($a_i$ > $b_j$ | P), if the partial order P consists of two disjoint linearly ordered sets A = {$a_1$ > $a_2$ > ... > $a_m$, B = {$b_1$ > $b_2$ > ... > $b_n and P' = P $\\cup$ {any relations of the form $a_k$ > $b_l$. These inequalities have applications in determining the complexity of certain sorting-like computations."},
{"Title": "Storing a sparse table", "URL": "https://dl.acm.org/doi/10.1145/359168.359175", "Full Abstract": "The problem of storing and searching large sparse tables is ubiquitous in computer science. The standard technique for storing such tables is hashing, but hashing has poor worst-case performance. We propose a good worst-case method for storing a static table of"},
{"Title": "External Hashing Schemes for Collections of Data Structures", "URL": "https://dl.acm.org/doi/10.1145/322169.322177", "Full Abstract": "Copyright © 1980 ACM."},
{"Title": "New Algorithms for Bin Packing", "URL": "https://dl.acm.org/doi/10.1145/322186.322187", "Full Abstract": "In the bin-packing problem a list"},
{"Title": "Information Bounds Are Weak in the Shortest Distance Problem", "URL": "https://dl.acm.org/doi/10.1145/322203.322206", "Full Abstract": "Copyright © 1980 ACM."},
{"Title": "Bounds on Selection Networks", "URL": "https://dl.acm.org/doi/10.1137/0209043", "Full Abstract": "We investigate the complexity of network selection by measuring it in terms of $U(t,N)$, the minimum number of comparators needed, and $T(t,N)$, the minimum delay time possible, for networks selecting the smallest"},
{"Title": "On the parallel computation for the knapsack problem", "URL": "https://dl.acm.org/doi/book/10.5555/892256", "Full Abstract": "We are interested in the complexity of solving the knapsack problem with n input real numbers on a parallel computer with real arithmetic and branching operations. A processor-time tradeoff constraint is derived; in particular, it is shown that an exponential number of processors have to be used if the problem is to be solved in time $t \\le {\\sqrt{n/2$."},
{"Title": "Optimal Expected-Time Algorithms for Closest Point Problems", "URL": "https://dl.acm.org/doi/10.1145/355921.355927", "Full Abstract": "Copyright © 1980 ACM."},
{"Title": "On the security of public key protocols", "URL": "https://dl.acm.org/doi/book/10.5555/891726", "Full Abstract": "Recently, the use of public key encryption to provide secure network communication has received considerable attention. Such public key systems are usually effective against passive eavesdroppers, who merely tap the lines and try to decipher the message. It has been pointed out, however, that an improperly designed protocol could be vulnerable to an active saboteur, one who may impersonate another user or alter the message being transmitted. In this paper we formulate several models in which the security of protocols can be discussed precisely. Algorithms and characterizations that can be used to determine protocol security in these models will be given."},
{"Title": "On the parallel computation for the knapsack problem", "URL": "https://dl.acm.org/doi/10.1145/800076.802465", "Full Abstract": "We are interested in the complexity of solving the knapsack problem with"},
{"Title": "The entropic limitations on VLSI computations(Extended Abstract)", "URL": "https://dl.acm.org/doi/10.1145/800076.802483", "Full Abstract": "In this paper we will explore the limitations imposed by entropic constraints, both in generality and for specific problems. We list below the main questions that we will address."},
{"Title": "Should Tables Be Sorted?", "URL": "https://dl.acm.org/doi/10.1145/322261.322274", "Full Abstract": "Copyright © 1981 ACM."},
{"Title": "A Lower Bound to Finding Convex Hulls", "URL": "https://dl.acm.org/doi/10.1145/322276.322289", "Full Abstract": "Copyright © 1981 ACM."},
{"Title": "On the security of public key protocols", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1981.32", "Full Abstract": "No abstract available."},
{"Title": "Space-time tradeoff for answering range queries (Extended Abstract)", "URL": "https://dl.acm.org/doi/10.1145/800070.802185", "Full Abstract": "In this paper, we raise and investigate the question of (storage) space- (retrieval) time tradeoff for a static database, in the general framework of Fredman's. As will be seen, such tradeoff results also lead to lower bounds on the complexity of processing a sequence of m INSERT and QUERY instructions. The latter results are incomparable to Fredman's, since the presence of DELETE instructions was crucial for his proof technique. We will present our results in detail in the next few sections. Here we will only mention three main conclusions. Firstly, circular query is shown to be intrinsically hard in the sense that, for some static database with n records, there is a space-time tradeoff TS > n"},
{"Title": "Data and storage structures for interactive graphics", "URL": "https://dl.acm.org/doi/10.1145/1115880.1115891", "Full Abstract": "This is a tutorial paper that shows the relationship between the data structure and the rest of an interactive graphics system. The distinction between data structures and storage structures is emphasized, as is the problem of data structure segmentation. The implementation of typical graphical applications is described, along with analysis of various trade-off decisions. Finally, some high-level data structure specification languages are discussed."},
{"Title": "A Microprogrammed Intelligent Graphics Terminal", "URL": "https://dl.acm.org/doi/10.1109/T-C.1971.223346", "Full Abstract": "This paper describes a small computer, Interdata Model 3, that has been microprogrammed to serve as an intelligent terminal. The Interdata is connected to a System/360 multiplexor channel with a high-speed interface, and uses an ARDS direct view storage tube as a display console. The new Interdata target machine is patterned after the /360 (including all five instruction formats), but also has instructions particularly designed for intelligent terminal programming. These include instructions for character string manipulation, code conversion, list processing, coordinate manipulation, and virtual addressing. A powerful multiplexor channel, which allows the programmer to \"overlap\" I/O to several devices with a CPU program, has also been microprogrammed."},
{"Title": "On-line Text Editing: A Survey", "URL": "https://dl.acm.org/doi/10.1145/356589.356591", "Full Abstract": "This paper is a survey of current methods for the on-line creation and editing of computer programs and of ordinary manuscripts text. The characteristics of on-line editing systems are examined and examples of various implementations are described in three categories: program editors, text editors, and terminals with local editing facilities."},
{"Title": "Language for Systems Development", "URL": "https://dl.acm.org/doi/10.1145/800234.807060", "Full Abstract": "Well-designed efficient systems programming languages are an absolute necessity if programmers are to keep pace with the demand for systems. This paper presents briefly some criteria to be applied to the design of a general purpose systems programming language and a description of the Language for Systems Development that is being implemented at Brown University for the IBM S/360. The paper is a revised and condensed version of a much larger survey paper [3]. The research and writing of this paper and the design of LSD were partially supported by a National Science Foundation grant (No. GJ-181) and by Brown University. We would like to acknowledge the help and invaluable suggestions given us by Richard Wexelblat of Bell Laboratories, Robert Balzer of the RAND Corporation, John Brackett and Douglas Ross of Softech, Inc., and Robert Rosin of the State University of New York at Buffalo. Special thanks should be given to Paul Knueven of Digital Equipment Corporation who helped initiate the entire effort and has been a source of help and encouragement through many iterations. The authors would also like to thank Frank Tompa of the University of Toronto and Diane Shecter who were among the original designers of LSD."},
{"Title": "Microprogramming for computer graphics", "URL": "https://dl.acm.org/doi/10.1145/1316535.1316536", "Full Abstract": "Interactive computer graphics (graphics) is the construction, storage, retrieval, manipulation, alteration and analysis of pictorial data, using an on-line display console with manual input (interaction) devices. Among such input devices are the alphanumeric and function keyboards for typing text and activating preprogrammed subroutines respectively, and the light pen and data tablet for identifying and entering graphic data by means of pointing and drawing."},
{"Title": "Computer assisted tracing of text evolution", "URL": "https://dl.acm.org/doi/10.1145/1479064.1479160", "Full Abstract": "Many situations exist in which convenient access to the detailed evolutionary information associated with a text's development is desirable:"},
{"Title": "Intelligent satellites for interactive graphics", "URL": "https://dl.acm.org/doi/10.1145/1499586.1499655", "Full Abstract": "In the last four or five years it has become increasingly fashionable to speak of \"intelligent,\" \"smart,\" or \"programmable\" terminals and systems. Very few mainframe or peripheral manufacturers omit such a device from their standard product line. Although \"intelligence,\" like beauty or pornography, is in the eye of the beholder, the adjective generally connotes that the device has a degree of autonomy or processing ability which allows it to perform certain (classes of) tasks without assistance from the mainframe to which it is connected. Many such devices are programmable by virtue of including a mini, microprogrammable or micro computer."},
{"Title": "Computer architecture and instruction set design", "URL": "https://dl.acm.org/doi/10.1145/1499586.1499720", "Full Abstract": "A group of computer scientists and mathematicians at Brown University has been engaged in the study of computer graphics for the past eight years. During the course of these studies a variety of topics has been investigated, in particular, during the last few years, the use of microprogramming for implementing graphics systems. In early 1971, Professor Andries van Dam and his associates submitted a threefold research proposal to the National Science Foundation."},
{"Title": "Operating system design considerations for microprogrammed mini-computer satellite systems", "URL": "https://dl.acm.org/doi/10.1145/1499586.1499724", "Full Abstract": "The operating system described in this paper was developed as part of research sponsored by The National Science Foundation and the Office of Naval Research on satellite processing and symbolic debugging of data structures. This system runs on a small (32K bytes), dual processor, microprogrammable computer equipped with a high-speed graphic display unit and attached in satellite mode to the multiplexor channel of an IBM System/360-67."},
{"Title": "A survey of introductory and advanced programming courses", "URL": "https://dl.acm.org/doi/10.1145/800183.810465", "Full Abstract": "In the process of establishing equitable and practical computer time allocations for our computer science courses this fall, we compared our seemingly high request with standards in other universities. Twenty-three private and state universities were chosen for the comparison and a questionnaire (appendix 1) was designed to elicit information about large introductory programming courses and more specialized systems programming/software engineering courses."},
{"Title": "Design considerations for microprogramming languages", "URL": "https://dl.acm.org/doi/10.1145/1217149.1217152", "Full Abstract": "The growing acceptance of user-microprogrammable computers indicates that microprogramming, as a discipline, will require development of user-oriented microprogramming support. A number of approaches (definition of sophisticated target machines, microcode assemblers, and higher level microprogramming languages) have been proposed. The issues involved in choosing support tools include the range of proposed applications, hardware parallelism (horizental or minimally encoded control vs. vertically encoded control) and constraints on performance. After reviewing some of these tradeoffs, design considerations for higher level microprogramming languages are considered. One of the most important design decisions is fixing the level of the language, defined on a continuum from symbolic assemblers, through general purpose programming languages such as PL/I. A tailored language concept is defined and illustrated, using as an example a microprogramming language for a horizontally encoded microprogrammable computer currently under development."},
{"Title": "Parallel hashing—an efficient implementation of shared memory", "URL": "https://dl.acm.org/doi/10.1145/12130.12146", "Full Abstract": "Copyright © 1986 ACM."},
{"Title": "Sharing memory in distributed systems—methods and applications", "URL": "https://dl.acm.org/doi/book/10.5555/37653", "Full Abstract": "Information exchange between processors is essential for any efficient parallel computation. One very convenient mechanism for exchanging information is for processors to utilize shared variables. In this work, we present efficient algorithms for sharing variables on two very different types of distributed systems. The first is a synchronous, bounded degree network of processors each with an associated local memory. The problem we consider in this case is how to distribute and route the shared variables among the processor's local memories. The solution combines the use of universal hash functions for distributing the variables and probabilistic two-phase routing.The second model we consider is an asynchronous broadcast model. Here each processor has a cache and the caches are connected to each other and to a main memory over a bus. Only one variable can be transmitted over the bus per bus cycle, and shared variables may reside in any number of caches. A block retention algorithm associated with each cache monitors bus and processor activity in order to determine which blocks of data to keep and which to discard. There is a tradeoff: If a certain variable is dropped from a cache, then a subsequent read for that variable requires bus communication. On the other hand, if a variable is not dropped from a certain cache, then a write by some other processor to that variable requires a bus cycle so that the first cache will obtain the updated value. For several variants of this model we present on-line block retention algorithms which incur a communication cost that is within a constant factor of the cost to the optimal off-line algorithm."},
{"Title": "Algorithms for the compilation of regular expressions into PLAs", "URL": "https://dl.acm.org/doi/10.1007/BF01840364", "Full Abstract": "The language of regular expressions is a useful one for specifying certain sequential processes at a very high level. They allow easy modification of designs for circuits, like controllers, that are described by patterns of events they must recognize and the responses they must make to those patterns. This paper discusses the compilation of such expressions into specifications for programmable logic arrays (PLAs) that will implement the required function. A regular expression is converted into a nondeterministic finite automaton, and then the automaton states are encoded as values on wires that are inputs and outputs of a PLA. The translation of regular expressions into nondeterministic automata by two different methods is discussed, along with the advantages of each method. A major part of the compilation problem is selection of good state codes for the nondeterministic automata; one successful strategy and its application to microcode compaction is explained in the paper."},
{"Title": "Parallel hashing", "URL": "https://dl.acm.org/doi/10.1145/48014.350550", "Full Abstract": "Copyright © 1988 ACM."},
{"Title": "Bounds on the cover time", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1988.21964", "Full Abstract": "A particle that moves on a connected unidirected graph G with n vertices is considered. At each step the particle goes from the current vertex to one of its neighbors, chosen uniformly at random. The cover time is the first time when the particle has visited all the vertices in the graph, starting from a given vertex. Upper and lower bounds are presented that relate the expected cover time for a graph to the eigenvalues of the Markov chain that describes the above random walk. An interesting consequence is that regular expander graphs have expected cover time theta (n log n)."},
{"Title": "Dynamic perfect hashing", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1988.21968", "Full Abstract": "A randomized algorithm is given for the dictionary problem with O(1) worst-case time for lookup and O(1) amortized expected time for insertion and deletion. An Omega (log n) lower bound is proved for the amortized worst-case time complexity of any deterministic algorithm in a class of algorithms encompassing realistic hashing-based schemes. If the worst-case lookup time is restricted to k, then the lower bound for insertion becomes Omega (kn/sup 1/k/)."},
{"Title": "Competitive snoopy caching", "URL": "https://dl.acm.org/doi/10.1007/BF01762111", "Full Abstract": "In a snoopy cache multiprocessor system, each processor has a cache in which it stores blocks of data. Each cache is connected to a bus used to communicate with the other caches and with main memory. Each cache monitors the activity on the bus and in its own processor and decides which blocks of data to keep and which to discard. For several of the proposed architectures for snoopy caching systems, we present new on-line algorithms to be used by the caches to decide which blocks to retain and which to drop in order to minimize communication over the bus. We prove that, for any sequence of operations, our algorithms' communication costs are within a constant factor of the minimum required for that sequence; for some of our algorithms we prove that no on-line algorithm has this property with a smaller constant."},
{"Title": "Trading space for time in undirected ", "URL": "https://dl.acm.org/doi/10.1145/73007.73059", "Full Abstract": "Aleliunas"},
{"Title": "Multilevel adaptive hashing", "URL": "https://dl.acm.org/doi/10.5555/320176.320181", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Competitive randomized algorithms for non-uniform problems", "URL": "https://dl.acm.org/doi/10.5555/320176.320216", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Asymptotically tight bounds for computing with faulty arrays of processors", "URL": "https://dl.acm.org/doi/10.1109/FSCS.1990.89547", "Full Abstract": "The computational power of 2-D and 3-D processor arrays that contain a potentially large number of faults is analyzed. Both a random and a worst-case fault model are considered, and it is proved that in either scenario low-dimensional arrays are surprisingly fault tolerant. It is also shown how to route, sort, and perform systolic algorithms for problems such as matrix multiplication in optimal time on faulty arrays. In many cases, the running time is the same as if there were no faults in the array (up to constant factors). On the negative side, it is shown that any constant congestion embedding of an n*n fault-free array on an n*n array with Theta (n/sup 2/) random faults (or Theta (log n) worst-case faults) requires dilation Theta (log n). For 3-D arrays, knot theory is used to prove that the required dilation is Omega ( square root log n)."},
{"Title": "On the parallel complexity of evaluating game trees", "URL": "https://dl.acm.org/doi/10.5555/127787.127858", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Empirical studies of competitve spinning for a shared-memory multiprocessor", "URL": "https://dl.acm.org/doi/10.1145/121132.286599", "Full Abstract": "A common operation in multiprocessor programs is acquiring a lock to protect access to shared data. Typically, the requesting thread is blocked if the lock it needs is held by another thread. The cost of blocking one thread and activating another can be a substantial part of program execution time. Alternatively, the thread could spin until the lock is free, or spin for a while and then block. This may avoid context-switch overhead, but processor cycles may be wasted in unproductive spinning. This paper studies seven strategies for determining whether and how long to spin before blocking. Of particular interest are"},
{"Title": "Factors in the performance of the AN1 computer network", "URL": "https://dl.acm.org/doi/10.1145/133057.133102", "Full Abstract": "AN1 (formerly known as Autonet) is a local area network composed of crossbar switches interconnected by 100Mbit/second, full-duplex links. In this paper, we evaluate the performance impact of certain choices in the AN1 design. These include the use of FIFO input buffering in the crossbar switch, the deadlock-avoidance mechanism, cut-through routing, back-pressure for flow control, and multi-path routing. AN1's performance goals were to provide low latency and high bandwidth in a lightly loaded network. In this it is successful. Under heavy load, the most serious impediment to good performance is the use of FIFO input buffers. The deadlock-avoidance technique has an adverse effect on the performance of some topologies, but it seems to be the best alternative, given the goals and constraints of the AN1 design. Cut-through switching performs well relative to store-and-forward switching, even under heavy load. Back-pressure deals adequately with congestion in a lightly-loaded network; under moderate load, performance is acceptable when coupled with end-to-end flow control for bursts. Multi-path routing successfully exploits redundant paths between hosts to improve performance in the face of congestion."},
{"Title": "Biased random walks", "URL": "https://dl.acm.org/doi/10.1145/129712.129713", "Full Abstract": "How much can an imperfect source of randomness affect an algorithm? We examine several simple questions of this type concerning the long-term behavior of a random walk on a finite graph. In our setup, each step of the random walk a “controller” can, with a certain small probability, fix the next step, thus introducing a bias. We analyze the extent to which the bias can affect the limit behavior of the walk. The controller is assumed to associate a real, nonnegative, “benefit” with each state, and to strive to maximize the long-term expected benefit. We derive tight bounds on the maximum of this objective function over all controller's strategies, and present polynomial time algorithms for computing the optimal controller strategy."},
{"Title": "Strongly competitive algorithms for paging with locality of reference", "URL": "https://dl.acm.org/doi/10.5555/139404.139455", "Full Abstract": "What is the best paging algorithm if one has partial information about the possible sequences of page requests? We give a partial answer to this question, by presenting the analysis of strongly competitive paging algorithms in the access graph model. This model restricts page requests so that they conform to a notion of locality of reference, given by an arbitrary access graph."},
{"Title": "On-line load balancing", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1992.267770", "Full Abstract": "The setup for the authors' problem consists of n servers that must complete a set of tasks. Each task can be handled only by a subset of the servers, requires a different level of service, and once assigned can not be re-assigned. They make the natural assumption that the level of service is known at arrival time, but that the duration of service is not. The on-line load balancing problem is to assign each task to an appropriate server in such a way that the maximum load on the servers is minimized. The authors derive matching upper and lower bounds for the competitive ratio of the on-line greedy algorithm for this problem, namely /sup (3n)2/3///sub 2/(1+o(1)), and derive a lower bound, Omega ( square root n), for any other deterministic or randomized on-line algorithm."},
{"Title": "Markov paging", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1992.267771", "Full Abstract": "This paper considers the problem of paging under the assumption that the sequence of pages accessed is generated by a Markov chain. The authors use this model to study the fault-rate of paging algorithms, a quantity of interest to practitioners. They first draw on the theory of Markov decision processes to characterize the paging algorithm that achieves optimal fault-rate on any Markov chain. They address the problem of efficiently devising a paging strategy with low fault-rate for a given Markov chain. They show that a number of intuitively good approaches fail. Their main result is an efficient procedure that, on any Markov chain, will give a paging algorithm with fault-rate at most a constant times optimal. Their techniques also show that some algorithms that do poorly in practice fail in the Markov setting, despite known (good) performance guarantees when the requests are generated independently from a probability distribution."},
{"Title": "Trading Space for Time in Undirected $s-t$ Connectivity", "URL": "https://dl.acm.org/doi/10.1137/S0097539790190144", "Full Abstract": "Aleliunas et al. [20th Annual Symposium on Foundations of Computer Science, IEEE Computer Society Press, Los Alamitos, CA, 1979, pp. 218--223] posed the following question: \"The reachability problem for undirected graphs can be solved in log space and $O(mn)$ time [$m$ is the number of edges and $n$ is the number of vertices] by a probabilistic algorithm that simulates a random walk, or in linear time and space by a conventional deterministic graph traversal algorithm. Is there a spectrum of time-space trade-offs between these extremes?\" This question is answered in the affirmative for sparse graphs by presentation of an algorithm that is faster than the random walk by a factor essentially proportional to the size of its workspace. For denser graphs, this algorithm is faster than the random walk but the speed-up factor is smaller."},
{"Title": "On the fault tolerance of the butterfly", "URL": "https://dl.acm.org/doi/10.1145/195058.195117", "Full Abstract": "Copyright © 1994 ACM."},
{"Title": "Profile-Based Object Matching for Information Integration", "URL": "https://dl.acm.org/doi/10.1109/MIS.2003.1234770", "Full Abstract": "Object matching is a fundamental problem that arises in numerous information integration scenarios. Virtually all existing solutions assume that the objects to be matched share the same attribute set and that systems can match them by comparing attribute similarities. Our work addresses the more general problem in which objects also have disjoint attributes-for example, matching tuples from relational tables that have different schemas, such as (age, name) and (name, salary). Profile-Based Object Matching, which applies this idea, exploits disjoint attributes to improve matching accuracy. PROM first matches any two tuples based on a shared attribute, such as name. It then applies a set of profilers, each of which contains some knowledge about what constitutes a typical person. The profilers examine the tuple pair to see if it plausibly describes a person. A profiler might state, for example, that if the pair produces a person with an age of 6 and a salary of $100,000, the pair doesn't describe a real person, so the tuples don't match. Profilers can be manually specified by domain experts, trained on training data, transferred from other matching tasks, or built from external data. PROM is thus distinct in that it not only exploits disjoint attributes to improve matching accuracy but also facilitates knowledge reuse from previous object-matching tasks."},
{"Title": "Learning to match ontologies on the Semantic Web", "URL": "https://dl.acm.org/doi/10.1007/s00778-003-0104-2", "Full Abstract": "On the Semantic Web, data will inevitably come from many different ontologies, and information processing across ontologies is not possible without knowing the semantic mappings between them. Manually finding such mappings is tedious, error-prone, and clearly not possible on the Web scale. Hence the development of tools to assist in the ontology mapping process is crucial to the success of the Semantic Web. We describe"},
{"Title": "Semantic Integration Workshop at the Second International Semantic Web Conference (ISWC‐2003)", "URL": "https://dl.acm.org/doi/10.1609/aimag.v25i1.1753", "Full Abstract": "In numerous distributed environments, including today's World Wide Web, enterprise data management systems, large science projects, and the emerging semantic web, applications will inevitably use the information described by multiple ontologies and schemas. We organized the Workshop on Semantic Integration at the Second International Semantic Web Conference to bring together different communities working on the issues of enabling integration among different resources. The workshop generated a lot of interest and attracted more than 70 participants."},
{"Title": "Semantic integration workshop at the second international semantic web conference (ISWC-2003)", "URL": "https://dl.acm.org/doi/10.5555/996917.996930", "Full Abstract": "In numerous distributed environments, including today's World Wide Web, enterprise data management systems, large science projects, and the emerging semantic web, applications will inevitably use the information described by multiple ontologies and schemas. We organized the Workshop on Semantic Integration at the Second International Semantic Web Conference to bring together different communities working on the issues of enabling integration among different resources. The workshop generated a lot of interest and attracted more than 70 participants."},
{"Title": "An interactive clustering-based approach to integrating source query interfaces on the deep Web", "URL": "https://dl.acm.org/doi/10.1145/1007568.1007582", "Full Abstract": "An increasing number of data sources now become available on the Web, but often their contents are only accessible through query interfaces. For a domain of interest, there often exist many such sources with varied coverage or querying capabilities. As an important step to the integration of these sources, we consider the integration of their query interfaces. More specifically, we focus on the crucial step of the integration: accurately matching the interfaces. While the integration of query interfaces has received more attentions recently, current approaches are not sufficiently general: (a) they all model interfaces with flat schemas; (b) most of them only consider 1:1 mappings of fields over the interfaces; (c) they all perform the integration in a blackbox-like fashion and the whole process has to be restarted from scratch if anything goes wrong; and (d) they often require laborious parameter tuning. In this paper, we propose an interactive, clustering-based approach to matching query interfaces. The hierarchical nature of interfaces is captured with ordered trees. Varied types of complex mappings of fields are examined and several approaches are proposed to effectively identify these mappings. We put the human integrator back in the loop and propose several novel approaches to the interactive learning of parameters and the resolution of uncertain mappings. Extensive experiments are conducted and results show that our approach is highly effective."},
{"Title": "iMAP", "URL": "https://dl.acm.org/doi/10.1145/1007568.1007612", "Full Abstract": "Creating semantic matches between disparate data sources is fundamental to numerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. To date, however, virtually all of these works deal only with one-to-one (1-1) matches, such as"},
{"Title": "Privacy-preserving data integration and sharing", "URL": "https://dl.acm.org/doi/10.1145/1008694.1008698", "Full Abstract": "Integrating data from multiple sources has been a longstanding challenge in the database community. Techniques such as privacy-preserving data mining promises privacy, but assume data has integration has been accomplished. Data integration methods are seriously hampered by inability to share the data to be integrated. This paper lays out a privacy framework for data integration. Challenges for data integration in the context of this framework are discussed, in the context of existing accomplishments in data integration. Many of these challenges are opportunities for the data mining community."},
{"Title": "Introduction to the special issue on semantic integration", "URL": "https://dl.acm.org/doi/10.1145/1041410.1041412", "Full Abstract": "Semantic heterogeneity is one of the key challenges in integrating and sharing data across disparate sources, data exchange and migration, data warehousing, model management, the Semantic Web and peer-to-peer databases. Semantic heterogeneity can arise at the schema level and at the data level. At the schema level, sources can differ in relations, attribute and tag names, data normalization, levels of detail, and the coverage of a particular domain. The problem of reconciling schema-level heterogeneity is often referred to as"},
{"Title": "Semantic Integration", "URL": "https://dl.acm.org/doi/10.1609/aimag.v26i1.1794", "Full Abstract": "Sharing data across disparate sources requires solving many problems of semantic integration, such as matching ontologies or schemas, detecting duplicate tuples, reconciling inconsistent data values, modeling complex relations between concepts in different sources, and reasoning with semantic mappings. This issue of"},
{"Title": "Semantic‐Integration Research in the Database Community", "URL": "https://dl.acm.org/doi/10.1609/aimag.v26i1.1801", "Full Abstract": "Semantic integration has been a long‐standing challenge for the database community. It has received steady attention over the past two decades, and has now become a prominent area of database research. In this article, we first review database applications that require semantic integration and discuss the difficulties underlying the integration process. We then describe recent progress and identify open research issues. We focus in particular on schema matching, a topic that has received much attention in the database community, but also discuss data matching (for example, tuple deduplication) and open issues beyond the match discovery context (for example, reasoning with matches, match verification and repair, and reconciling inconsistent data values). For previous surveys of database research on semantic integration, see Rahm and Bernstein (2001); Ouksel and Seth (1999); and Batini, Lenzerini, and Navathe (1986)."},
{"Title": "Semantic integration", "URL": "https://dl.acm.org/doi/10.5555/1090488.1090490", "Full Abstract": "Sharing data across disparate sources requires solving many problems of semantic integration, such as matching ontologies or schemas, detecting duplicate tuples, reconciling inconsistent data values, modeling complex relations between concepts in different sources, and reasoning with semantic mappings. This issue of AI Magazine includes papers that discuss various methods on establishing mappings between ontology elements or data fragments. The collection includes papers that discuss semantic-integration issues in such contexts as data integration and web services. The issue also includes a brief survey of semantic-integration research in the database community."},
{"Title": "Semantic-integration research in the database community", "URL": "https://dl.acm.org/doi/10.5555/1090488.1090497", "Full Abstract": "Semantic integration has been a long-standing challenge for the database community. It has received steady attention over the past two decades, and has now become a prominent area of database research. In this article, we first review database applications that require semantic integration and discuss the difficulties underlying the integration process. We then describe recent progress and identify open research issues. We focus in particular on schema matching, a topic that has received much attention in the database community, but also discuss data matching (for example, tuple deduplication) and open issues beyond the match discovery context (for example, reasoning with matches, match verification and repair, and reconciling inconsistent data values). For previous surveys of database research on semantic integration, see Rahm and Bernstein (2001); Ouksel and Seth (1999); and Batini, Lenzerini, and Navathe (1986)."},
{"Title": "Corpus-Based Schema Matching", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2005.39", "Full Abstract": "Schema Matching is the problem of identifying corresponding elements in different schemas. Discovering these correspondences or matches is inherently difficult to automate. Past solutions have proposed a principled combination of multiple algorithms. However, these solutions sometimes perform rather poorly due to the lack ofsufficient evidence in the schemas being matched. In this paper we show how a corpus of schemas and mappings can be used to augment the evidence about the schemas being matched, so they can be matched better. Such a corpus typically contains multiple schemas that model similar concepts and hence enables us to learn variations in the elements and their properties. We exploit such a corpus in two ways. First, we increase the evidence about each element being matched by including evidence from similar elements in the corpus. Second, we learn statistics about elements and their relationships and use them to infer constraints that we use to prune candidate mappings. We also describe how to use known mappings to learn the importance of domain and generic constraints. We present experimental results that demonstrate corpus-based matching outperforms direct matching (without the benefit of a corpus) in multiple domains."},
{"Title": "Integrating Data from Disparate Sources", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2005.81", "Full Abstract": "No abstract available."},
{"Title": "Constraint-based entity matching", "URL": "https://dl.acm.org/doi/10.5555/1619410.1619471", "Full Abstract": "Entity matching is the problem of deciding if two given mentions in the data, such as \"Helen Hunt\" and \"H. M. Hunt\", refer to the same real-world entity. Numerous solutions have been developed, but they have not considered in depth the problem of exploiting integrity constraints that frequently exist in the domains. Examples of such constraints include \"a mention with age two cannot match a mention with salary 200K\" and \"if two paper citations match, then their authors are likely to match in the same order\". In this paper we describe a probabilistic solution to entity matching that exploits such constraints to improve matching accuracy. At the heart of the solution is a generative model that takes into account the constraints during the generation process, and provides well-defined interpretations of the constraints. We describe a novel combination of EM and relaxation labeling algorithms that efficiently learns the model, thereby matching mentions in an unsupervised way, without the need for annotated training data. Experiments on several real-world domains show that our solution can exploit constraints to significantly improve matching accuracy, by 3-12% F-1, and that the solution scales up to large data sets."},
{"Title": "Tuning schema matching software using synthetic scenarios", "URL": "https://dl.acm.org/doi/10.5555/1083592.1083707", "Full Abstract": "Most recent schema matching systems assemble"},
{"Title": "Mapping maintenance for data integration systems", "URL": "https://dl.acm.org/doi/10.5555/1083592.1083709", "Full Abstract": "To answer user queries, a data integration system employs a set of semantic mappings between the mediated schema and the schemas of data sources. In dynamic environments sources often undergo changes that invalidate the mappings. Hence, once the system is deployed, the administrator must monitor it over time, to detect and repair broken mappings. Today such continuous monitoring is extremely labor intensive, and poses a key bottleneck to the widespread deployment of data integration systems in practice.We describe MAVERIC, an automatic solution to detecting broken mappings. At the heart of MAVERIC is a set of computationally inexpensive modules called"},
{"Title": "Bootstrapping domain ontology for semantic web services from source web sites", "URL": "https://dl.acm.org/doi/10.1007/11607380_2", "Full Abstract": "The vision of Semantic Web services promises a network of interoperable Web services over different sources. A major challenge to the realization of this vision is the lack of automated means of acquiring domain ontologies necessary for marking up the Web services. In this paper, we propose the DeepMiner system which learns domain ontologies from the source Web sites. Given a set of sources in a domain of interest, DeepMiner first learns a base ontology from their query interfaces. It then grows the current ontology by probing the sources and discovering additional concepts and instances from the data pages retrieved from the sources. We have evaluated DeepMiner in several real-world domains. Preliminary results indicate that DeepMiner discovers concepts and instances with high accuracy."},
{"Title": "Merging Interface Schemas on the Deep Web via Clustering Aggregation", "URL": "https://dl.acm.org/doi/10.1109/ICDM.2005.92", "Full Abstract": "We consider the problem of integrating a large number of interface schemas over the Deep Web, The scale of the problem and the diversity of the sources present serious challenges to the conventional manual or rule-based approaches to schema integration. To address these challenges, we propose a novel formulation of schema integration as an optimization problem, with the objective of maximally satisfying the constraints given by individual schemas. Since the optimization problem can be shown to be NP-complete, we develop a novel approximation algorithm LMax, which builds the unified schema via recursive applications of clustering aggregation. We further extend LMax to handle the irregularities frequently occurring among the interface schemas. Extensive evaluation on real-world data sets shows the effectiveness of our approach."},
{"Title": "Summary cache", "URL": "https://dl.acm.org/doi/10.1109/90.851975", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Graph structure in the Web", "URL": "https://dl.acm.org/doi/10.5555/347319.346290", "Full Abstract": "No abstract available."},
{"Title": "Identifying and Filtering Near-Duplicate Documents", "URL": "https://dl.acm.org/doi/10.5555/647819.736184", "Full Abstract": "The mathematical concept of document resemblance captures well the informal notion of syntactic similarity. The resemblance can be estimated using a fixed size \"sketch\" for each document. For a large collection of documents (say hundreds of millions) the size of this sketch is of the order of a few hundred bytes per document."},
{"Title": "Min-wise Independent Permutations", "URL": "https://dl.acm.org/doi/10.5555/646253.686321", "Full Abstract": "A family of permutations F ⊆"},
{"Title": "A comparison of techniques to find mirrored hosts on the WWW", "URL": "https://dl.acm.org/doi/10.5555/359203.359211", "Full Abstract": "No abstract available."},
{"Title": "Completeness and robustness properties of min-wise independent permutations", "URL": "https://dl.acm.org/doi/10.5555/370982.370985", "Full Abstract": "No abstract available."},
{"Title": "A general approach to dynamic packet routing with bounded buffers", "URL": "https://dl.acm.org/doi/10.1145/375827.375849", "Full Abstract": "We prove a sufficient condition for the stability of dynamic packet routing algorithms. Our approach reduces the problem of steady state analysis to the easier and better understood question of static routing. We show that certain high probability and worst case bounds on the quasi-static (finite past) performance of a routing algorithm imply bounds on the performance of the dynamic version of that algorithm. Our technique is particularly useful in analyzing routing on networks with bounded buffers where complicated dependices make standard queuing techniques inapplicable."},
{"Title": "Algorithmic aspects of information retrieval on the web", "URL": "https://dl.acm.org/doi/10.5555/779232.779234", "Full Abstract": "The Web explosion offers a bonanza of novel problems. In particular, information retrieval in the Web context requires methods and ideas that have not been addressed in the classic information retrieval literature. This chapter will survey emerging techniques for information retrieval in the Web context and discuss some of the pertinent open problems."},
{"Title": "Optimal plans for aggregation", "URL": "https://dl.acm.org/doi/10.1145/571825.571852", "Full Abstract": "We consider the following problem, which arises in the context of distributed Web computations. An"},
{"Title": "A taxonomy of web search", "URL": "https://dl.acm.org/doi/10.1145/792550.792552", "Full Abstract": "Classic IR (information retrieval) is inherently predicated on users searching for information, the so-called \"information need\". But the need behind a web search is often not informational -- it might be navigational (give me the url of the site I want to reach) or transactional (show me sites where I can perform a certain transaction, e.g. shop, download a file, or find a map). We explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs."},
{"Title": "A derandomization using min-wise independent permutations", "URL": "https://dl.acm.org/doi/10.1016/S1570-8667%2803%2900003-0", "Full Abstract": "Min-wise independence is a recently introduced notion of limited independence, similar in spirit to pairwise independence. The latter has proven essential for the derandomization of many algorithms. Here we show that approximate min-wise independence allows similar uses, by presenting a derandomization of the RNC algorithm for approximate set cover due to S. Rajagopalan and V. Vazirani. We also discuss how to derandomize their set multi-cover and multi-set multi-cover algorithms in restricted cases. The multi-cover case leads us to discuss the concept of"},
{"Title": "Efficient URL caching for world wide web crawling", "URL": "https://dl.acm.org/doi/10.1145/775152.775247", "Full Abstract": "Crawling the web is deceptively simple: the basic algorithm is (a) Fetch a page (b) Parse it to extract all linked URLs (c) For all the URLs not seen before, repeat (a)-(c). However, the size of the web (estimated at over 4 billion pages) and its rate of change (estimated at 7% per week) move this plan from a trivial programming exercise to a serious algorithmic and system design challenge. Indeed, these two factors alone imply that for a reasonably fresh and complete crawl of the web, step (a) must be executed about a thousand times per second, and thus the membership test (c) must be done well over ten thousand times per second against a set too large to store in main memory. This requires a distributed architecture, which further complicates the membership test.A crucial way to speed up the test is to cache, that is, to store in main memory a (dynamic) subset of the \"seen\" URLs. The main goal of this paper is to carefully investigate several URL caching techniques for web crawling. We consider both practical algorithms: random replacement, static cache, LRU, and CLOCK, and theoretical limits: clairvoyant caching and infinite cache. We performed about 1,800 simulations using these algorithms with various cache sizes, using actual log data extracted from a massive 33 day web crawl that issued over one billion HTTP requests.Our main conclusion is that caching is very effective - in our setup, a cache of roughly 50,000 entries can achieve a hit rate of almost 80%. Interestingly, this cache size falls at a critical point: a substantially smaller cache is much less effective while a substantially larger cache brings little additional benefit. We conjecture that such critical points are inherent to our problem and venture an explanation for this phenomenon."},
{"Title": "Keynote Address - exploring, modeling, and using the web graph", "URL": "https://dl.acm.org/doi/10.1145/860435.860436", "Full Abstract": "The Web graph, meaning the graph induced by Web pages as nodes and their hyperlinks as directed edges, has become a fascinating object of study for many people: physicists, sociologists, mathematicians, computer scientists, and information retrieval specialists.Recent results range from theoretical (e.g.: models for the graph, semi-external algorithms), to experimental (e.g.: new insights regarding the rate of change of pages, new data on the distribution of degrees), to practical (e.g.: improvements in crawling technology).Recent results range from theoretical (e.g.: models for the graph, semi-external algorithms), to experimental (e.g.: new insights regarding the rate of change of pages, new data on the distribution of degrees), to practical (e.g.: improvements in crawling technology).The goal of this talk is to convey an introduction to the state of the art in this area and to sketch the current issues in collecting, representing, analyzing, and modeling this graph. Although graph analytic methods are essential tools in the Web IR arsenal, they are well known to the SIGIR community and will not be discussed here in any detail; instead, we will explore some challenges and opportunities for using IR methods and techniques in the exploration of the Web graph, in particular in dealing with legitimate and \"spam\" perturbations of the \"natural\" process of birth and death of nodes and links, and conversely, the challenges and opportunities of using graph methods in support of IR on the Web and in the enterprise."},
{"Title": "Efficient query evaluation using a two-level retrieval process", "URL": "https://dl.acm.org/doi/10.1145/956863.956944", "Full Abstract": "We present an efficient query evaluation method based on a two level approach: at the first level, our method iterates in parallel over query term postings and identifies candidate documents using an"},
{"Title": "Using XML to query XML", "URL": "https://dl.acm.org/doi/10.5555/2816272.2816325", "Full Abstract": "A cornerstone concept in classical Information Retrieval is the vector space model, whereby both documents and queries are viewed as vectors in a multidimensional space. Relevance of a given document to a given query is determined by evaluating the similarity between these vectors, using a measure like the cosine measure of similarity, for instance. The vector space model has been highly successful for dealing with plain text collections, in both theoretical and practical terms. In prior work, we extended this classic approach to the search of XML collections by requiring queries to be presented as XML Fragments, which allows for a very simple extension of the cosine similarity measure to the XML framework."},
{"Title": "Sic transit gloria telae", "URL": "https://dl.acm.org/doi/10.1145/988672.988716", "Full Abstract": "The rapid growth of the web has been noted and tracked extensively. Recent studies have however documented the dual phenomenon: web pages have small half lives, and thus the web exhibits rapid death as well. Consequently, page creators are faced with an increasingly burdensome task of keeping links up-to-date, and many are falling behind. In addition to just individual pages, collections of pages or even entire neighborhoods of the web exhibit significant"},
{"Title": "Efficient pagerank approximation via graph aggregation", "URL": "https://dl.acm.org/doi/10.1145/1013367.1013537", "Full Abstract": "We present a framework for approximating random-walk based probability distributions over Web pages using graph aggregation. We (1) partition the Web's graph into classes of quasi-equivalent vertices, (2) project the page-based random walk to be approximated onto those classes, and (3) compute the stationary probability distribution of the resulting class-based random walk. From this distribution we can quickly reconstruct a distribution on pages. Inparticular, our framework can approximate the well-known PageRank distribution by setting the classes according to the set of pages on each Web host. We experimented on a Web-graph containing over 1.4 billion pages, and were able to produce a ranking that has Spearman rank-order correlation of 0.95 with respect to PageRank. A simplistic implementation of our method required less than half the running time of a highly optimized implementation of PageRank, implying that larger speedup factors are probably possible."},
{"Title": "Towards the next generation of enterprise search technology", "URL": "https://dl.acm.org/doi/10.1147/sj.433.0451", "Full Abstract": "Unstructured information represents the vast majority of data collected and accessible to enterprises. Exploiting this information requires systems for managing and extracting knowledge from large collections of unstructured data and applications for discovering patterns and relationships. This paper elucidates the differences between search systems for the Web and those for enterprises, with an emphasis on the future of enterprise search systems. It also introduces the Unstructured Information Management Architecture (UIMA) and provides the context for the unstructured information management (UIM) papers that follow."},
{"Title": "Invited talk", "URL": "https://dl.acm.org/doi/10.1007/11527954_14", "Full Abstract": "The Web graph, meaning the graph induced by Web pages as nodes and their hyperlinks as directed edges, has become a fascinating object of study for many people: physicists, sociologists, mathematicians, computer scientists, and information retrieval specialists."},
{"Title": "Multidimensional balanced allocations", "URL": "https://dl.acm.org/doi/10.5555/1070432.1070460", "Full Abstract": "We consider a multidimensional variant of the balls-and-bins problem, where balls correspond to random"},
{"Title": "Secure data replication over untrusted hosts", "URL": "https://dl.acm.org/doi/10.5555/1251054.1251075", "Full Abstract": "Data replication is a widely used technique for achieving fault tolerance and improved performance. With the advent of content delivery networks, it is becoming more and more frequent that data content is placed on hosts that are not directly controlled by the content owner, and because of this, security mechanisms to protect data integrity are necessary. In this paper we present a system architecture that allows arbitrary queries to be supported on data content replicated on untrusted servers. To prevent these servers from returning erroneous answers to client queries, we make use of a small number of trusted hosts that randomly check these answers and take corrective action whenever necessary. Additionally, our system employs an audit mechanism that guarantees that any untrusted server acting maliciously will eventually be detected and excluded from the system."},
{"Title": "A Certificate Revocation Scheme for a Large-Scale Highly Replicated Distributed System", "URL": "https://dl.acm.org/doi/10.5555/839294.843438", "Full Abstract": "A common way to protect objects in distributed systemsis to issue authorization certificates to users, which theypresent to gain access. In some situations a way is needed torevoke existing certificates. Current methods, such as havinga master revocation list, have been designed to workefficiently with identity certificates, and do not take into accountthe delegation of certificate-issuing rights requiredwhen implementing complex administrative hierarchies forlarge distributed applications. In this paper we presenta novel mechanism for revoking authorization certificatesbased on clustering users and servers, and present argumentsshowing that it is more efficient than other methods.We also discuss a way for probabilistically auditingthe use of the revocation mechanism proposed to reduce thechances of any component behaving maliciously."},
{"Title": "Redes de Computadoras", "URL": "https://dl.acm.org/doi/book/10.5555/996192", "Full Abstract": "No abstract available."},
{"Title": "Safe and private data sharing with turtle", "URL": "https://dl.acm.org/doi/10.1007/11861386_24", "Full Abstract": "In this paper we describe Turtle, a peer-to-peer architecture for safe sharing of sensitive data. The truly revolutionary aspect of Turtle rests in its novel way of dealing with trust issues: while existing peer-to-peer architectures with similar aims attempt to build trust relationships on top of the basic, trust-agnostic, peer-to-peer overlay, Turtle takes the opposite approach, and builds its overlay on top of pre-existent trust relationships among its users. This allows both data sender and receiver anonymity, while also protecting"},
{"Title": "Sistemas Operativos Modernos", "URL": "https://dl.acm.org/doi/book/10.5555/1205752", "Full Abstract": "No abstract available."},
{"Title": "Support for multi-level security policies in DRM architectures", "URL": "https://dl.acm.org/doi/10.1145/1065907.1065909", "Full Abstract": "Digital rights management systems allow copyrighted content to be commercialized in digital format without the risk of revenue loss due to piracy. Making such systems secure is no easy task, given that content needs to be protected while accessed through electronic devices in the hands of potentially malicious end-users; in this context, intrusion tolerance becomes a very useful system property. In this paper we point out a limitation shared by all current DRM architectures, namely their weakness in reacting to possible device compromise and confining the damage caused by such a compromise. As a solution, we propose a paradigm shift - moving from the original DRM system model where all devices are equally trustworthy and have discretionary control over all protected content, to a new model where information flow is controlled through a multi-level security policy that differentiates between devices based on their tamper-resistance properties. We show that besides improved intrusion-tolerance, supporting such policies has other advantages, such as the ability to define more flexible business models for supplying content. We also show that for a given DRM architecture, the type authentication protocol used when accepting new devices in the system has a big impact on how well multi-level security policies can be supported, and that a number of protocols currently being considered are not very well suited for this job."},
{"Title": "A DRM security architecture for home networks", "URL": "https://dl.acm.org/doi/10.1145/1029146.1029150", "Full Abstract": "This paper describes a security architecture allowing digital rights management in home networks consisting of consumer electronic devices. The idea is to allow devices to establish dynamic groups, so called \"Authorized Domains\", where legally acquired copyrighted content can seamlessly move from device to device. This greatly improves the end-user experience, preserves \"fair use\" expectations, and enables the development of new business models by content providers. Key to our design is a hybrid compliance checking and group establishment protocol, based on pre-distributed symmetric keys, with minimal reliance on public key cryptographic operations. Our architecture does not require continuous network connectivity between devices, and allows for efficient and flexible key updating and revocation."},
{"Title": "How to incorporate revocation status information into the trust metrics for public-key certification", "URL": "https://dl.acm.org/doi/10.1145/1066677.1067037", "Full Abstract": "In a traditional PKI, the trust associated with a public key is expressed in binary either by 0 or 1. Alternatively, several authors have proposed trust metrics to evaluate the confidence afforded by a public key. However their work has a static point of view and does not take into account the issue of public key revocation. In this paper, we make the first attempt to incorporate the revocation status information into the trust metrics for public key certification. To achieve our goal, we use a tailored form of a vector of trust model recently proposed. This would allow us to reason formally about when there is a need to check revocation status and how reliable the revocation mechanism should be in a given security application."},
{"Title": "Securely Replicated Web Documents", "URL": "https://dl.acm.org/doi/10.1109/IPDPS.2005.395", "Full Abstract": "In order to achieve better scalability and reduce latency in handling user requests, many Web applications make extensive use of data replication through caches and Content Delivery Networks. However, in such scenarios data is often placed on untrusted hosts. As a result, existing replication mechanisms open a wide class vulnerabilities, ranging from denial of service to content masquerading. In this paper we present an architecture that combines data content, replication strategies and security in one unified object model and offers integrity guarantees for Web documents replicated on non secure servers."},
{"Title": "Keep on blockin' in the free world", "URL": "https://dl.acm.org/doi/10.5555/1802438.1802444", "Full Abstract": "This paper introduces an off-tag RFID access control mechanism called \"Selective RFID Jamming\". Selective RFID Jamming protects low-cost RFID tags by enforcing access control on their behalf, in a similar manner to the RFID Blocker Tag. However, Selective RFID Jamming is novel because it uses an active mobile device to enforce centralized ACL-based access control policies. Selective RFID Jamming also solves a Differential Signal Analysis attack to which the RFID Blocker Tag is susceptible."},
{"Title": "Structured Computer Organization (5th Edition)", "URL": "https://dl.acm.org/doi/book/10.5555/1215402", "Full Abstract": "No abstract available."},
{"Title": "RFID guardian", "URL": "https://dl.acm.org/doi/10.1007/11506157_16", "Full Abstract": "RFID tags are tiny, inexpensive, inductively powered computers that are going to replace bar codes on many products, but which have many other uses as well. For example, they will allow smart washing machines to check for incompatible clothes (e.g., white shirts and red socks) and smart refrigerators to check for milk that is too old to be consumed. Subdermal tags with medical information are already being implanted in animals and people. However, a world in which practically everything is tagged and can be read at a modest distance by anyone who wants to buy an RFID reader introduces serious security and privacy issues. For example, women walking down the street may be effectively broadcasting the sizes of their RFID-tagged bras and medical data without realizing it. To protect people in this environment, we propose developing a compact, portable, electronic device called an RFID Guardian, which people can carry with them. In the future, it could be integrated into PDAs or cell phones. The RFID Guardian looks for, records, and displays all RFID tags and scans in the vicinity, manages RFID keys, authenticates nearby RFID readers, and blocks attempted accesses to the user’s RFID tags from unauthorized readers. In this way, people can find out what RFID activity is occuring around them and take corrective action if need be."},
{"Title": "Counting abuses using flexible off-line credentials", "URL": "https://dl.acm.org/doi/10.1007/11506157_46", "Full Abstract": "Mobile and ad-hoc networks allow businesses to provide a new range of applications and services and at the same time they introduce new constraints that have important effects on the way in which security primitives must be designed. This is challenging because it translates to a demand of richer and more flexible security primitives that often need to satisfy stricter requirements than traditional wired network scenarios. In this paper we focus on one of this primitive, namely security credentials. We present a solution that extends the existing protocols used to implement off-line credentials such that, not only abuses can be detected but they can also be counted. Our solution addresses the problem of 1-time and 2-times credentials and we will conclude by discussing the challenges that need to be solved to generalize the primitive to"},
{"Title": "One-Time sensors", "URL": "https://dl.acm.org/doi/10.1007/11601494_7", "Full Abstract": "Dealing with captured nodes is generally accepted as the most difficult challenge to wireless sensor network security. By utilizing the low-cost property of sensor nodes, we introduce the novel concept of one-time sensors to mitigate node-capture attacks. The basic idea is to load each sensor with only one cryptographic token so that the captured node can inject only a single malicious message into the network. In addition, sybil attacks are avoided and explicit revocation is not necessary using one-time sensors. By using public key techniques, one-way hash functions and Merkle’s hash tree, we also show efficient implementations and interesting tradeoffs for one-time sensors."},
{"Title": "Operating Systems Design and Implementation (3rd Edition)", "URL": "https://dl.acm.org/doi/book/10.5555/1076555", "Full Abstract": "No abstract available."},
{"Title": "Enabling DRM-Preserving Digital Content Redistribution", "URL": "https://dl.acm.org/doi/10.1109/ICECT.2005.43", "Full Abstract": "Traditionally, the process of online digital content distribution has involved a limited number of centralised distributors selling protected contents and licenses authorising the use of these contents, to consumers. In this paper, we extend this model by introducing a security scheme that enables DRM preserving digital content redistribution. Essentially consumers can not only buy the rights to use digital content but also the rights to redistribute it to other consumers in a DRM controlled fashion. We examine the threats associated with such a redistribution model and explain how our scheme addresses them."},
{"Title": "The Evolution of RFID Security", "URL": "https://dl.acm.org/doi/10.1109/MPRV.2006.17", "Full Abstract": "RFID is a contactless identification technology that augments real-world objects with remotely-powered computing capabilities. Even though RFID-tagging of objects such as passports, pets, prisoners, and even the elderly seems revolutionary and unprecedented, RFID technology itself has a rich history that began with Identification Friend or Foe (IFF) systems during World War II. This article charts the parallel evolution of RFID technology and its security and privacy threats and solutions. Historical RFID-based systems faced constant security threats, and many of these attacks (and their defenses) have since been canonized as classical signal warfare and countermeasures. Examining RFID and its threats from a historical perspective lets us learn from past experiences and enables us to reuse old solutions. More importantly, looking back provides us as researchers with inspiration to devise new solutions, so we can lead RFID security research into the future.This article is part of a special issue on RFID Technology."},
{"Title": "File size distribution on UNIX systems", "URL": "https://dl.acm.org/doi/10.1145/1113361.1113364", "Full Abstract": "Knowledge of the file size distribution is needed to optimize file system design. In particular, if all the files are small, the disk block size should be small, too, to avoid wasting too large a fraction of the disk. On the other hand, if files are generally large, choosing a large block size is good since it leads to more efficient transfers. Only by knowing the file size distribution can reasonable choices be made. In 1984, we published the file size distribution for a university computer science department. We have now made the same measurements 20 years later to see how file sizes have changed. In short, the median file size has more than doubled (from 1080 bytes to 2475 bytes), but large files still dominate the storage requirements."},
{"Title": "Lectures on a Calculus for Communicating Systems", "URL": "https://dl.acm.org/doi/10.5555/646723.702732", "Full Abstract": "No abstract available."},
{"Title": "A proposal for standard ML", "URL": "https://dl.acm.org/doi/10.1145/800055.802035", "Full Abstract": "Copyright © 1984 ACM."},
{"Title": "Algebraic laws for nondeterminism and concurrency", "URL": "https://dl.acm.org/doi/10.1145/2455.2460", "Full Abstract": "Since a nondeterministic and concurrent program may, in general, communicate repeatedly with its environment, its meaning cannot be presented naturally as an input/output function (as is often done in the denotational approach to semantics). In this paper, an alternative is put forth. First, a definition is given of what it is for two programs or program parts to be equivalent for all observers; then two program parts are said to be"},
{"Title": "The use of machines to assist in rigorous proof", "URL": "https://dl.acm.org/doi/10.5555/3721.3725", "Full Abstract": "No abstract available."},
{"Title": "Using algebra for concurrency: some approaches", "URL": "https://dl.acm.org/doi/10.5555/5010.5011", "Full Abstract": "No abstract available."},
{"Title": "Lectures on a calculus for communicating systems", "URL": "https://dl.acm.org/doi/10.5555/22086.22090", "Full Abstract": "No abstract available."},
{"Title": "Dialogue with a proof system", "URL": "https://dl.acm.org/doi/10.5555/29580.29599", "Full Abstract": "No abstract available."},
{"Title": "Lectures on a calculus for communicating systems", "URL": "https://dl.acm.org/doi/10.5555/58776.58781", "Full Abstract": "No abstract available."},
{"Title": "A type discipline for Program modules", "URL": "https://dl.acm.org/doi/10.5555/67683.67703", "Full Abstract": "No abstract available."},
{"Title": "Dialogue with a Proof System", "URL": "https://dl.acm.org/doi/10.5555/646622.759379", "Full Abstract": "No abstract available."},
{"Title": "A Type Discipline for Program Modules", "URL": "https://dl.acm.org/doi/10.5555/646623.698042", "Full Abstract": "No abstract available."},
{"Title": "Verifying a protocol using relativized bisimulation", "URL": "https://dl.acm.org/doi/10.5555/29601.29612", "Full Abstract": "No abstract available."},
{"Title": "Verifying a Protocol Using Relativized Bisimulation", "URL": "https://dl.acm.org/doi/10.5555/646241.681247", "Full Abstract": "No abstract available."},
{"Title": "Communication and Concurrency", "URL": "https://dl.acm.org/doi/book/10.5555/534666", "Full Abstract": "No abstract available."},
{"Title": "Communication and concurrency", "URL": "https://dl.acm.org/doi/book/10.5555/63446", "Full Abstract": "No abstract available."},
{"Title": "A complete axiomatisation for observational congruence of finite-state behaviours", "URL": "https://dl.acm.org/doi/10.1016/0890-5401%2889%2990070-9", "Full Abstract": "No abstract available."},
{"Title": "The  Definition of Standard ML", "URL": "https://dl.acm.org/doi/book/10.5555/575336", "Full Abstract": "From the Publisher:"},
{"Title": "The definition of Standard ML", "URL": "https://dl.acm.org/doi/book/10.5555/77325", "Full Abstract": "No abstract available."},
{"Title": "Functions as processes", "URL": "https://dl.acm.org/doi/10.5555/90397.90426", "Full Abstract": "No abstract available."},
{"Title": "Computational models of games", "URL": "https://dl.acm.org/doi/book/10.5555/913750", "Full Abstract": "Because games and game-like phenomena occur naturally in a computational setting, it is natural to formulate many problems in Computer Science in terms of games. In order to understand their complexity, various models of computation have been developed which reflect the game-like properties of such problems. These models include the alternating Turing machines of Chandra, Kozen, and Stockmeyer (CKS81), the games against nature of Papadimitriou (PAP83), the Arthur-Merlin games of Babai (BAB85), and the interactive proof systems of Goldwasser, Micali, and Rackoff (GMR85)."},
{"Title": "Probabilistic games automata", "URL": "https://dl.acm.org/doi/10.1016/0022-0000%2888%2990038-4", "Full Abstract": "No abstract available."},
{"Title": "On the complexity of space bounded interactive proofs", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1989.63519", "Full Abstract": "Two results on interactive proof systems with two-way probabilistic finite-state verifiers are proved. The first is a lower bound on the power of such proof systems if they are not required to halt with high probability on rejected inputs: it is shown that they can accept any recursively enumerable language. The second is an upper bound on the power of interactive proof systems that halt with high probability on all inputs. The proof method for the lower bound also shows that the emptiness problem for one-way probabilistic finite-state machines is undecidable. In the proof of the upper bound some results of independent interest on the rate of convergence of time-varying Markov chains to their halting states are obtained."},
{"Title": "Playing games of incomplete information", "URL": "https://dl.acm.org/doi/10.5555/90326.90344", "Full Abstract": "No abstract available."},
{"Title": "Playing Games of Incomplete Information", "URL": "https://dl.acm.org/doi/10.5555/646506.694175", "Full Abstract": "No abstract available."},
{"Title": "The complexity of the max word problem and the power of one-way interactive proof systems", "URL": "https://dl.acm.org/doi/10.5555/112102.112139", "Full Abstract": "No abstract available."},
{"Title": "Space-bounded probabilistic game automata", "URL": "https://dl.acm.org/doi/10.1145/103516.128681", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "Random Walks in Colored Graphs", "URL": "https://dl.acm.org/doi/book/10.5555/894573", "Full Abstract": "We give tight upper and lower bounds on the expected cover time of a random walk in an undirected graph with colored edges. We show that for graphs with two colors the expected cover time is exponential, and that for three or more colors it is double exponential. In addition, we give polynomial bounds in a number of interesting special cases. We describe applications of these results to understanding the eigenvalues of products and weighted averages of matrices, and to problems on time-inhomogeneous Markov chains."},
{"Title": "The complexity of stochastic games", "URL": "https://dl.acm.org/doi/10.1016/0890-5401%2892%2990048-K", "Full Abstract": "No abstract available."},
{"Title": "A Theory of Strict P-completeness", "URL": "https://dl.acm.org/doi/10.5555/646508.694496", "Full Abstract": "No abstract available."},
{"Title": "Asynchronous analysis of parallel dynamic programming", "URL": "https://dl.acm.org/doi/10.1145/166955.167035", "Full Abstract": "We examine a very simple asynchronous model of parallel computation that assumes the time to compute a task is random, following some probability distribution. The goal of this model is to capture the effects of unexpected delays on processors."},
{"Title": "Probabilistically checkable debate systems and approximation algorithms for PSPACE-hard functions", "URL": "https://dl.acm.org/doi/10.1145/167088.167190", "Full Abstract": "Copyright © 1993 ACM."},
{"Title": "On games of incomplete information", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2892%2990085-T", "Full Abstract": "No abstract available."},
{"Title": "The complexity of the max word problem and the power of one-way interactive proof systems", "URL": "https://dl.acm.org/doi/10.1007/BF01271372", "Full Abstract": "No abstract available."},
{"Title": "The complexity of space bounded interactive proof systems", "URL": "https://dl.acm.org/doi/10.5555/183589.183728", "Full Abstract": "No abstract available."},
{"Title": "PSPACE is provable by two provers in one round", "URL": "https://dl.acm.org/doi/10.1016/S0022-0000%2805%2980026-1", "Full Abstract": "We show that every language in PSPACE, or equivalently, every language accepted by an unbounded round interactive proof system, ha a 1-round, 2-prover interactive proof system with exponentially small error probability. To obtain this result, we prove the correctness of a simple but powerful method for parallelizing 2-prover interactive proof systems to reduce their error."},
{"Title": "Random walks on colored graphs", "URL": "https://dl.acm.org/doi/10.1002/rsa.3240050204", "Full Abstract": "We initiate a study of random walks on undirected graphs with colored edges. In our model, a sequence of colors is specified before the walk begins, and it dictates the color of edge to be followed at each step. We give tight upper and lower bounds on the expected cover time of a random walk on an undirected graph with colored edges. We show that, in general, graphs with two colors have exponential expected cover time, and graphs with three or more colors have doubly‐exponential expected cover time. We also give polynomial bounds on the expected cover time in a number of interesting special cases. We described applications of our results to understanding the dominant eigenvectors of products and weighted averages of stochastic matrices, and to problems on time‐inhomogeneous Markov chains. © 1994 John Wiley & Sons, Inc."},
{"Title": "On the power of finite automata with both nondeterministic and probabilistic states (preliminary version)", "URL": "https://dl.acm.org/doi/10.1145/195058.195431", "Full Abstract": "Copyright © 1994 ACM."},
{"Title": "Verifying Hybrid Systems", "URL": "https://dl.acm.org/doi/10.5555/646874.709969", "Full Abstract": "No abstract available."},
{"Title": "Time for Concurrency", "URL": "https://dl.acm.org/doi/10.5555/646324.756790", "Full Abstract": "No abstract available."},
{"Title": "Fundamentals of Deductive Program Synthesis", "URL": "https://dl.acm.org/doi/10.1109/32.153379", "Full Abstract": "An informal tutorial for program synthesis is presented, with an emphasis on deductive methods. According to this approach, to construct a program meeting a given specification, the authors prove the existence of an object meeting the specified conditions. The proof is restricted to be sufficiently constructive, in the sense that, in establishing the existence of the desired output, the proof is forced to indicate a computational method for finding it. That method becomes the basis for a program that can be extracted from the proof. The exposition is based on the deductive-tableau system, a theorem-proving framework particularly suitable for program synthesis. The system includes a nonclausal resolution rule, facilities for reasoning about equality, and a well-founded induction rule."},
{"Title": "Towards Refining Temporal Specifications into Hybrid Systems", "URL": "https://dl.acm.org/doi/10.5555/646874.709981", "Full Abstract": "No abstract available."},
{"Title": "The deductive foundations of computer programming", "URL": "https://dl.acm.org/doi/book/10.5555/174817", "Full Abstract": "No abstract available."},
{"Title": "Temporal Verification of Simulation and Refinement", "URL": "https://dl.acm.org/doi/10.5555/648145.750144", "Full Abstract": "No abstract available."},
{"Title": "A Decision Algorithm for Full Propositional Temporal Logic", "URL": "https://dl.acm.org/doi/10.5555/647762.735505", "Full Abstract": "No abstract available."},
{"Title": "Models for reactivity", "URL": "https://dl.acm.org/doi/10.5555/2697441.2697671", "Full Abstract": "A hierarchy of models that capture realistic aspects of reactive, real-time, and hybrid systems is introduced. On the most abstract level, the qualitative (non-quantitative) model of"},
{"Title": "Temporal Verification Diagrams", "URL": "https://dl.acm.org/doi/10.5555/645868.670943", "Full Abstract": "No abstract available."},
{"Title": "STeP: The Stanford Temporal Prover", "URL": "https://dl.acm.org/doi/book/10.5555/891757", "Full Abstract": "We describe the Stanford Temporal Prover (STeP), a system being developed to support the computer-aided formal verification of concurrent and reactive systems based on temporal specifications. Unlike systems based on model-checking, STeP is not restricted to finite-state systems. It combines model checking and deductive methods to allow the verification of a broad class of systems, including programs with infinite data domains, N-process programs, and N-component circuit designs, for arbitrary N. In short, STeP has been designed with the objective of combining the expressiveness of deductive methods with the simplicity of model checking. The verification process is for the most part automatic. User interaction occurs mostly at the highest, most intuitive level, primarily through a graphical proof language of verification diagrams. Efficient simplification methods, decision procedures, and invariant generation techniques are then invoked automatically to prove resulting first-order verification conditions with minimal assistance. We describe the performance of the system when applied to several examples, including the N-process dining philosopher's program, Szymanski's N-process mutual exclusion algorithm, and a distributed N-way arbiter circuit."},
{"Title": "Beyond Model Checking", "URL": "https://dl.acm.org/doi/10.5555/647763.735670", "Full Abstract": "No abstract available."},
{"Title": "Realizability and Synthesis of Reactive Modules", "URL": "https://dl.acm.org/doi/10.5555/647763.760718", "Full Abstract": "No abstract available."},
{"Title": "Annotation-Based Deduction in Temporal Logic", "URL": "https://dl.acm.org/doi/10.5555/645548.659006", "Full Abstract": "No abstract available."},
{"Title": "Temporal Proof Methodologies for Timed Transition-Systems", "URL": "https://dl.acm.org/doi/10.1006/inco.1994.1060", "Full Abstract": "We extend the specification language of temporal logic, the corresponding verification framework, and the underlying computational model to deal with real-;time properties of reactive systems. The abstract notion of timed transition systems generalizes traditional transition systems conservatively: qualitative fairness requirements are replaced (and superseded) by quantitative lower-bound and upper-bound timing constraints on transitions. This framework can model real-time systems that communicate either through shared variables or by message passing and real-time issues such as timeouts, process priorities (interrupts), and process scheduling. We exhibit two styles for the specification of real-time systems. While the first approach uses time-bounded versions of the temporal operators, the second approach allows explicit references to time through a special clock variable. Corresponding to the two styles of specification, we present and compare two different proof methodologies for the verification of timing requirements that are expressed in these styles. For the bounded-operator style, we provide a set of proof rules for establishing bounded-invariance and bounded-responce properties of timed transition systems. This approach generalizes the standard temporal proof rules for verifying invariance and response properties conservatively. For the explicit-clock style, we exploit the observation that every time-bounded property is a safety property and use the standard temporal proof rules for establishing safety properties."},
{"Title": "Continuous Verification by Discrete Reasoning", "URL": "https://dl.acm.org/doi/book/10.5555/891763", "Full Abstract": "Two semantics are commonly used for the behavior of real-time and hybrid systems: a discrete semantics, in which the temporal evolution is represented as a sequence of snapshots describing the state of the system at certain times, and a continuous semantics, in which the temporal evolution is represented by a series of time intervals, and therefore corresponds more closely to the physical reality. Powerful verification rules are known for temporal logic formulas based on the discrete semantics. This paper shows how to transfer the verification techniques of the discrete semantics to the continuous one. We show that if a temporal logic formula has the property of finite variability, its validity in the discrete semantics implies its validity in the continuous one. This leads to a verification method based on three components: verification rules for the discrete semantics, axioms about time, and some temporal reasoning to bring the results together. This approach enables the verification of properties of real-time and hybrid systems with respect to the continuous semantics."},
{"Title": "Differential BDDs", "URL": "https://dl.acm.org/doi/book/10.5555/891764", "Full Abstract": "In this paper, we introduce a class of Binary Decision Diagrams (BDDs) which we call Differential BDDs (DBDDs), and two transformations over DBDDs, called Push-up and Delta transformations. In DBDDs and its derived classes such as Push-up DBDDs or Delta DBDDs, in addition to the ordinary node-sharing in the normal Ordered Binary Decision Diagrams (OBDDs), some isomorphic substructures are collapsed together forming an even more compact representation of boolean functions. The elimination of isomorphic substructures coincides with the repetitive occurrences of the same or similar small components in many applications of BDDs such as in the representation of hardware circuits. The reduction in the number of nodes, from OBDDs to DBDDs, is potentially exponential while boolean manipulations on DBDDs remain efficient."},
{"Title": "Prooving Safety Properties of Hybrid Systems", "URL": "https://dl.acm.org/doi/10.5555/646843.706648", "Full Abstract": "No abstract available."},
{"Title": "Specification and Verification of Controlled Systems", "URL": "https://dl.acm.org/doi/10.5555/646843.706772", "Full Abstract": "No abstract available."},
{"Title": "STeP", "URL": "https://dl.acm.org/doi/10.5555/646619.697420", "Full Abstract": "No abstract available."},
{"Title": "Verification in Continuous Time by Discrete Reasoning", "URL": "https://dl.acm.org/doi/10.5555/646056.678059", "Full Abstract": "No abstract available."},
{"Title": "Strong signature schemes", "URL": "https://dl.acm.org/doi/10.1145/800061.808774", "Full Abstract": "The notion of digital signature based on trapdoor functions has been introduced by Diffie and Hellman[3]. Rivest, Shamir and Adleman[8] gave the first number theoretic implementation of a signature scheme based on a trapdoor function. If"},
{"Title": "Uniform hashing is optimal", "URL": "https://dl.acm.org/doi/book/10.5555/892340", "Full Abstract": "It was conjectured by J. Ullman that uniform hashing is optimal in its expected retrieval cost among all open-address hashing schemes (JACM 19 (1972), 569-575). In this paper we show that, for any open-address hashing scheme, the expected cost of retrieving a record from a large table which is alpha-fraction full is at least 1/alpha log 1/1-alpha + o(1). This proves Ullman's conjecture to be true in the asymptotic sense."},
{"Title": "On the expected performance of path compression algorithms", "URL": "https://dl.acm.org/doi/10.1137/0214010", "Full Abstract": "No abstract available."},
{"Title": "On the Complexity of Maintaining Partial Sums", "URL": "https://dl.acm.org/doi/10.1137/0214022", "Full Abstract": "Let $F = \\{ ({\\bf r_i,s_i )|0 \\leqq i < n\\ $ be a file of"},
{"Title": "On optimal arrangements of keys with double hashing", "URL": "https://dl.acm.org/doi/10.1016/0196-6774%2885%2990042-2", "Full Abstract": "No abstract available."},
{"Title": "Uniform hashing is optimal", "URL": "https://dl.acm.org/doi/10.1145/3828.3836", "Full Abstract": "It was conjectured by J. Ullman that uniform hashing is optimal in its expected retrieval cost among all open-address hashing schemes [4]. In this paper, we show that, for any open-address hashing scheme, the expected cost of retrieving a record from a large table that is α-fraction full is at least (1/α) log (1/(1 - α)) +"},
{"Title": "Separating the polynomial-time hierarchy by oracles", "URL": "https://dl.acm.org/doi/10.5555/4479.4487", "Full Abstract": "No abstract available."},
{"Title": "A general approach to d-dimensional geometric queries", "URL": "https://dl.acm.org/doi/10.1145/22145.22163", "Full Abstract": "It is shown that any bounded region in"},
{"Title": "Lower bounds to randomized algorithms for graph properties", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1987.39", "Full Abstract": "For any property P on n-vertex graphs, let C(P) be the minimum number of edges that need to be examined by any decision tree algorithm for determining P. In 1975 Rivest and Vuillemin settled the Aanderra-Rosenberg Conjecture, proving that C(P) = Ω(n2) for every nontrivial monotone graph property P. An intriguing open question is whether the theorem remains true when randomized algorithms are allowed. In this paper we report progress on this problem, showing that Ω(n(log n)1/12) edges must be examined by a randomized algorithm for determining any nontrivial monotone graph property."},
{"Title": "Monotone bipartite graph properties are evasive", "URL": "https://dl.acm.org/doi/10.1137/0217031", "Full Abstract": "No abstract available."},
{"Title": "Computational information theory", "URL": "https://dl.acm.org/doi/10.5555/60364.60365", "Full Abstract": "No abstract available."},
{"Title": "Circuits and local computation", "URL": "https://dl.acm.org/doi/10.1145/73007.73025", "Full Abstract": "This paper contains two parts. In Part I, we show that polynomial-size monotone threshold circuits of depth"},
{"Title": "On the improbability of reaching Byzantine agreements", "URL": "https://dl.acm.org/doi/10.1145/73007.73052", "Full Abstract": "It is well known that for the Byzantine Generals Problem, no deterministic protocol can exist for an"},
{"Title": "On the complexity of partial order productions", "URL": "https://dl.acm.org/doi/10.1137/0218047", "Full Abstract": "No abstract available."},
{"Title": "Coherent functions and program checkers", "URL": "https://dl.acm.org/doi/10.1145/100216.100226", "Full Abstract": "Copyright © 1990 ACM."},
{"Title": "Lower bounds to randomized algorithms for graph properties", "URL": "https://dl.acm.org/doi/10.1016/0022-0000%2891%2990003-N", "Full Abstract": "No abstract available."},
{"Title": "Program checkers for probability generation", "URL": "https://dl.acm.org/doi/10.5555/111713.111724", "Full Abstract": "No abstract available."},
{"Title": "Design considerations for microprogramming languages", "URL": "https://dl.acm.org/doi/10.1145/1500175.1500283", "Full Abstract": "Historically, microprograms have been developed using tools which are appropriate to logic designers (block diagrams, register transfer languages), or systems programmers (microcode assemblers). With the growth of user microprogramming, and the increased demands placed upon computer manufacturers for firmware support, improved tools and techniques have been suggested. In particular, microprogram compilers, i.e., compilers which translate high level source statements into sequences of microprogram control words, have been proposed and implemented. The larger issue to be faced is the nebulous task of supporting the needs of a community which includes:"},
{"Title": "Toward the development of machine", "URL": "https://dl.acm.org/doi/10.1145/1500175.1500301", "Full Abstract": "One of the reasons for developing high level languages has been the desire for program portability from one type of machine to another. To achieve the high degree of machine independence necessary for program portability, these languages have included general features such as arithmetic expressions, arrays, and subroutine calls which can be implemented on many machines. Facilities such as the interrupt mechanism, program status word, and device dependent input/output which are available to the assembly language programmer are hidden from the high level programmer. Unfortunately, the code generated by compilers for these high level languages is often very inefficient compared to that produced by experienced assembly language programmers. However, the added expressiveness and the ability to leave details to the compiler usually offset the inefficiency of generated code, particularly for non-systems applications. For such applications, the facilities which the high level programmer cannot use are not needed anyway."},
{"Title": "Intelligent satellites for interactive graphics", "URL": "https://dl.acm.org/doi/10.1145/988026.988039", "Full Abstract": "The spectrum of remote user stations with local processing ability, ranging from simple \"smart\" terminals to nearly self-sufficient intelligent systems called satellites, is considered. The emphasis is on the latter category, drawing on the authors' research on intelligent satellites for interactive graphics. The necessity of meeting a \"critical intelligence threshold\" criterion for the satellite is stressed, and some difficult problems and potential solutions in the division of labor between mainframe and satellite are briefly examined. It is hoped that many of the problems and solutions in satellite graphics apply to other areas such as network or satellite process control, experiment control, and signal processing."},
{"Title": "STRUCT programming analysis system", "URL": "https://dl.acm.org/doi/10.1109/TSE.1975.6312869", "Full Abstract": "The STRUCT system utilizes the flexibility of a powerful graphics display system to provide a set of tools for program analysis. These tools allow the analysis of the static prograin structure and the dynamic execution behavior. of programs within the entire operating system/user program environment of the Brown University Graphics System (BUGS). Information is collected and presented in a manner which fully exploits two aspects of this environment. First, the operating system has been developed in a well-structured hierarcal manner following principles laid down by other researchers (2), (3). Second the programs under analysis have been written in a structured programming language following coding conventions which make available, at the source code level, valuable program control information. A new set of pictorial constructs is introduced for presenting a. program structure (static or dynamic) for inspection. These constructs combine the best features of an indented structured source code listing and the box odented nature of traditional flow charts. The graphical tools available are USed to provide for swift changes in. the desired level of detail displayed within a program structure, for traveling linearly through a program structure, for traveling through a complex program structure (following subroutine or system calls), for concurrently viewing multiple related program structures, and for presenting dynamic program behavior data using three-dimensional projections, The volume of a three-dimensional box representing a program block is proportional to the block's resource utilization. The scope of this paper is limited to a description of the STRUCT system. This system is currently being used to predict and analyze the performance advantages available through the migration of function (program modules) between levels of software and between software and firmware within BUGS. The results of this research on migration will be included in a doctoral dissertation currently being written."},
{"Title": "Experience with distributed processing on a host/satellite graphics system", "URL": "https://dl.acm.org/doi/10.1145/563274.563310", "Full Abstract": "The problem cf distributing an application between two processors has been investigated by studying an interactive graphics application that is divided between a time-shared host computer and a dedicated satellite system. The division of labor in the application is determined by a network flow assignment algorithm. The effect of variation in availability of the host computer on the distribution of the application is also studied. In particular, host availability is an important factor in determining the task distribution. As host availability decreased, procedures miqrate from the host to the satellite."},
{"Title": "Structured programming in assembly language", "URL": "https://dl.acm.org/doi/10.1145/382222.382464", "Full Abstract": "Structured design and programming techniques can be extended from high-level languages to assembly language. Over the past three years at Brown University, beginning assembly language programmers have been successfully taught these techniques using clearly defined standards. These standards and the solutions to several of the typical problems that arise in structured assembly language programming are discussed in this paper."},
{"Title": "A multi-microprocessor implementation of a general purpose pipelined CPU", "URL": "https://dl.acm.org/doi/10.1145/800255.810649", "Full Abstract": "This paper discusses and shows by example the potential of a network of microprogrammable microprocessors as a cost-effective alternative to traditional hardwired medium- and large-scale mainframes. While biased towards vector processing, this system is not intended to compete with multi-million dollar supercomputers such as the 360/195, CDC STAR, Illiac IV, CRAY-1, TI ASC, etc., which use special algorithms and the fastest circuitry available."},
{"Title": "GPGS", "URL": "https://dl.acm.org/doi/10.1145/563858.563878", "Full Abstract": "GPGS is a subroutine package offering powerful and versatile support for passive and interactive vector graphics, for time-sharing, batch, and stand-alone minicomputer systems. The package is computer, language, and operating system, as well as display device independent. Its key purpose is to allow for transportabiliit of programs and programmers by providing easy to learn, high level features. The applications programmer writes his program once and then executes it on any supported graphics equipment without recompiling or relinking it. Device-independence was implemented by dividing GPGS into a device-independent part invoked by the applications programmer, and internal, \"device drivers\", one per display device. Like the GSPC \"Core System\" whose design it influenced, GPGS is a general purpose package. It has a subset of graphics facilities to handle output of line and character primitives with attributes such as line style and character size, and input from interaction tools such as lightpens, keyboards, valuators, and function keys. It also supports 2D and 3D viewin transformationss for clipping and window to viewport mapping, and coordinate transformations.Unlike the GSPC Core System, GPGS also includes a set of basic features for modelling objects which allows definition of device independent masters called seudo picture segment. These are distinguished from normal, device (DPU) dependent pictur segments into which primitives and their attribute-value settings are ordinarily compiled. These masters may be instanced subject to affine transformations (translate, rotate, and scale) to create a typical master-instance hierarchy. The hierarchy may be stored in a disk based library or compiled into a normal picture segment for output to a display device.The images of objects stored in device dependent picture segments may be transformed on the display surface by v port (image) transformations. These typically allow use of hardware transformation capabilities for dragging or tumbling object images.Host/satellite graphics is accommodated by having the device independent part of GPGS in the host and splitting the device drivers across host and satellite. At the source code level it therefore makes no difference on which.configuration a program will be executed.Among the existing implementations are versions written in assembler for the IB 360/370 and the PDP 11, in both stand-alone and satellite mode, and under a variety of operating systems. They support plotters, storage tubes, and high performance refresh displays. FORTRAN based implementations exist for the Univac 1108, the PDP 10, and a Harris minicomputer."},
{"Title": "Distributed Processing", "URL": "https://dl.acm.org/doi/10.1109/C-M.1978.217899", "Full Abstract": "This issue of Computer is based on two workshops in distributed processing held at Brown University August 17-19, 1976, and August 3-5, 1977. Sponsored by the Army Research Office, the National Science Foundation, and the Office of Naval Research, the workshops attempted to define what distributed processing means and to develop a taxonomy of distributed processing applications and techniques. Achievements to date and outstanding research problems were examined in an attempt to find either commonality of problems and solutions or substantial differences."},
{"Title": "Issues in Distributed Processing - an Overview of Two Workshops", "URL": "https://dl.acm.org/doi/10.1109/C-M.1978.217902", "Full Abstract": "Two workshops on distributed processing were held at Brown University in 1976 and 1977, sponsored by the Army Research Office, the National Science Foundation, and the Office of Naval Research. The workshops had three goals:"},
{"Title": "Vertical Migration for Performance Enhancement in Layered Hardware/Firmware/Software Systems", "URL": "https://dl.acm.org/doi/10.1109/C-M.1978.218182", "Full Abstract": "Vertical migration is a technique which improves system performance by moving software primitives through layers of application program and operating system software and microcode."},
{"Title": "Recent Efforts Towards Graphics Standardization", "URL": "https://dl.acm.org/doi/10.1145/356744.356746", "Full Abstract": "Copyright © 1978 ACM."},
{"Title": "Functional Overview of the Core System with Glossary", "URL": "https://dl.acm.org/doi/10.1145/356744.356747", "Full Abstract": "Copyright © 1978 ACM."},
{"Title": "Architectural considerations for a microprogrammable emulating engine using bit-slices", "URL": "https://dl.acm.org/doi/10.1145/800053.801936", "Full Abstract": "This paper describes architectural considerations which led to the design of a fast programmable processor made from ECL bit-slioes. The processor will be used as an on-line data filtering engine for high energy physics experiments. Unlike prior designs of such engines, the processor supports both user (horizontal) microcode and emulation of the PDP-11 fixed point instruction set (without memory management and multiple interrupt levels). In addition to an overview of the techniques used to achieve an execution speed of roughly three times that of the PDP-11/70 CPU, strengths and weaknesses of bit- slices are discussed, as are the use of a Signetics meta assembler and the ISPS Architecture simulation system."},
{"Title": "BUMPS", "URL": "https://dl.acm.org/doi/10.1145/800250.807498", "Full Abstract": "BUMPS (Brown University Multiple Projection System) is a program that illustrates the implementation of viewing transformations using animation. The program uses the viewing model defined in the Core Graphics System. BUMPS employs interactive computer graphics to demonstrate how planar geometric projections are generated, what the effects of different projections and projection parameters are on the projected object, and how the viewing functions of the Core Graphics System work. After presenting background material on projections, the features of BUMPS are described, followed by a pictorial user scenario of BUMPS in action. The paper concludes with a discussion of the merits of user controlled animation for teaching and possible improvements to the program."},
{"Title": "MIDAS", "URL": "https://dl.acm.org/doi/10.1109/TE.1981.4321464", "Full Abstract": "An interactive graphics program has been developed to simulate and animate the operation of a typical microcomputer system. MIDAS, a microprocessor interpreter display and animation system, allows the user full control over the simulation and the display and provides several auxiliary functions that enhance its capabilities as an instructional tool. The illustration of the activity of the computer, based on the Intel 8080 microprocessor, takes the form of an animated block diagram of the CPU and its peripherals. It shows the operation of the system at various levels of detail, down to the level of the devices' internal registers, buffers, control lines, and buses. This paper describes the design, implementation, and use of MIDAS. It discusses its effectiveness as a tool for teaching the complex, asynchronous interaction between devices of a computer system (known as \"handshaking\"). It also discusses a strategy for developing a generalized tool for simulating and animating arbitrary computer systems."},
{"Title": "Vertical and outboard migration", "URL": "https://dl.acm.org/doi/10.1145/1500412.1500422", "Full Abstract": "The primary method for gaining performance improvement on a fixed-hardware architecture is to tailor the soft components, i.e. the application program, the operating system, or the firmware, to the performance requirements. This paper deals with two specific forms of performance tuning called"},
{"Title": "Simulation of a Horizontal Bit-Sliced Processor Using the ISPS Architecture Simulation Facility", "URL": "https://dl.acm.org/doi/10.1109/TC.1981.1675830", "Full Abstract": "The microprogrammed filter engine (MICE) is a fast, microprogrammable processor built with ECL bit slices (Motorola ECL 10800 series) intended primarily to be used as an on-line data filtering engine for high energy physics experiments. In this note we describe the use of a hardware description language used to model and simulate the hardware during its development. We treat the problem of describing a pipelined, horizontal (112 bits wide) host machine, implemented using bit slices with considerable potential for parallelism. Several levels of modeling are conceptually applicable to a problem of this nature and the note describes the thorough process followed before we decided on a particular style of description and simulation."},
{"Title": "An integrated system for creating and presenting complex computer-based documents", "URL": "https://dl.acm.org/doi/10.1145/800224.806805", "Full Abstract": "An experimental system is described for the design, development, and presentation of computer-based documents that combine pictures and text on a high-resolution raster color display. Such documents can be used, for example, for maintenance and repair tasks or computer-aided instruction."},
{"Title": "An experimental system for creating and presenting interactive graphical documents", "URL": "https://dl.acm.org/doi/10.1145/357290.357296", "Full Abstract": "Copyright © 1982 ACM."},
{"Title": "Balanced allocations (extended abstract)", "URL": "https://dl.acm.org/doi/10.1145/195058.195412", "Full Abstract": "Copyright © 1994 ACM."},
{"Title": "Competitive randomized algorithms for nonuniform problems", "URL": "https://dl.acm.org/doi/10.1007/BF01189993", "Full Abstract": "Competitive analysis is concerned with comparing the performance of on-line algorithms with that of optimal off-line algorithms. In some cases randomization can lead to algorithms with improved performance ratios on worst-case sequences. In this paper we present new randomized on-line algorithms for snoopy caching and the spin-block problem. These algorithms achieve competitive ratios approachinge/(eź1) ź 1.58 against an oblivious adversary. These ratios are optimal and are a surprising improvement over the best possible ratio in the deterministic case, which is 2. We also consider the situation when the request sequences for these problems are generated according to an unknown probability distribution. In this case we show that deterministic algorithms that adapt to the observed request statistics also have competitive factors approachinge/(eź1). Finally, we obtain randomized algorithms for the 2-server problem on a class of isosceles triangles. These algorithms are optimal against an oblivious adversary and have competitive ratios that approache/(eź1). This compares with the ratio of 3/2 that can be achieved on an equilateral triangle."},
{"Title": "On-line load balancing", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2894%2990153-8", "Full Abstract": "No abstract available."},
{"Title": "Dynamic Perfect Hashing", "URL": "https://dl.acm.org/doi/10.1137/S0097539791194094", "Full Abstract": "The dynamic dictionary problem is considered: provide an algorithm for storing a dynamic set, allowing the operations insert, delete, and lookup. A dynamic perfect hashing strategy is given: a randomized algorithm for the dynamic dictionary problem that takes $O(1)$ worst-case time for lookups and $O(1)$ amortized expected time for insertions and deletions; it uses space proportional to the size of the set stored. Furthermore, lower bounds for the time complexity of a class of deterministic algorithms for the dictionary problem are proved. This class encompasses realistic hashing-based schemes that use linear space. Such algorithms have amortized worst-case time complexity $\\Omega(\\log n)$ for a sequence of $n$ insertions and lookups; if the worst-case lookup time is restricted to $k$, then the lower bound becomes $\\Omega(k\\cdot n^{1/k)$."},
{"Title": "Balanced Allocations (Extended abstract)", "URL": "https://dl.acm.org/doi/book/10.5555/903741", "Full Abstract": "Suppose that we sequentially place n balls into n boxes by putting each ball into a randomly chosen box. It is well known that when we are done, the fullest box has with high probability lnn/lnlnn(1 + o(1)) balls in it. Suppose instead, that for each ball we choose two boxes at random and place the ball into the one which is less full at the time of placement. We show that with high probability, the fullest box contains only lnlnn/ln2 + O(1) balls - exponentially less than before. Furthermore, we show that a similar gap exists in the infinite process, where at each step one ball, chosen uniformly at random, is deleted, and one ball is added in the manner above. We discuss consequences of this and related theorems for dynamic resource allocation, hashing, and on-line load balancing."},
{"Title": "A study of integrated prefetching and caching strategies", "URL": "https://dl.acm.org/doi/10.1145/223587.223608", "Full Abstract": "Prefetching and caching are effective techniques for improving the performance of file systems, but they have not been studied in an integrated fashion. This paper proposes four properties that optimal integrated strategies for prefetching and caching must satisfy, and then presents and studies two such integrated strategies, called"},
{"Title": "Reducing TLB and memory overhead using online superpage promotion", "URL": "https://dl.acm.org/doi/10.1145/223982.224419", "Full Abstract": "Modern microprocessors contain small TLBs that maintain a cache of recently used translations. A TLB's"},
{"Title": "Randomized and multipointer paging with locality of reference", "URL": "https://dl.acm.org/doi/10.1145/225058.225280", "Full Abstract": "Copyright © 1995 ACM."},
{"Title": "Implementing global memory management in a workstation cluster", "URL": "https://dl.acm.org/doi/10.1145/224056.224072", "Full Abstract": "Copyright © 1995 ACM."},
{"Title": "Two Adaptive Hybrid Cache Coherency Protocols", "URL": "https://dl.acm.org/doi/10.5555/525424.822636", "Full Abstract": "We present and evaluate adaptive, hybrid cache coherence protocols for bus-based, shared-memory multiprocessors. Such protocols are motivated by the observation that sharing patterns vary substantially between different programs and even cache blocks within the same program. Performance measurements across a range of parallel applications indicate that the adaptive protocols we present perform well compared to both Write-Invalidate and Write-Update protocols."},
{"Title": "Integrated parallel prefetching and caching", "URL": "https://dl.acm.org/doi/10.1145/233013.233052", "Full Abstract": "No abstract available."},
{"Title": "Strongly Competitive Algorithms for Paging with Locality of Reference", "URL": "https://dl.acm.org/doi/10.1137/S0097539792236353", "Full Abstract": "What is the best paging algorithm if one has partial information about the possible sequences of page requests__ __ We give a partial answer to this question by presenting the analysis of strongly competitive paging algorithms in the access graph model. This model restricts page requests so that they conform to a notion of locality of reference given by an arbitrary access graph."},
{"Title": "Online computation", "URL": "https://dl.acm.org/doi/10.5555/241938.241951", "Full Abstract": "No abstract available."},
{"Title": "Reducing network latency using subpages in a global memory environment", "URL": "https://dl.acm.org/doi/10.1145/237090.237198", "Full Abstract": "New high-speed networks greatly encourage the use of network memory as a cache for virtual memory and file pages, thereby reducing the need for disk access. Because pages are the fundamental transfer and access units in remote memory systems, page size is a key performance factor. Recently, page sizes of modern processors have been increasing in order to provide more TLB coverage and amortize disk access costs. Unfortunately, for high-speed networks,"},
{"Title": "Near-optimal parallel prefetching and caching", "URL": "https://dl.acm.org/doi/10.5555/874062.875485", "Full Abstract": "The authors consider algorithms for integrated prefetching and caching in a model with a fixed-size cache and any number of backing storage devices (disks). Previously, the single disk case was considered by Cao et al. (1995). They show that the natural extension of their aggressive algorithm to the parallel disk case is suboptimal by a factor near the number of disks in the worst case. The main result is a new algorithm, reverse aggressive, with near-optimal performance in the presence of multiple disks."},
{"Title": "A trace-driven comparison of algorithms for parallel prefetching and caching", "URL": "https://dl.acm.org/doi/10.1145/238721.238737", "Full Abstract": "No abstract available."},
{"Title": "Implementation and performance of integrated application-controlled file caching, prefetching, and disk scheduling", "URL": "https://dl.acm.org/doi/10.1145/235543.235544", "Full Abstract": "As the performance gap between disks and micropocessors continues to increase, effective utilization of the file cache becomes increasingly immportant. Application-controlled file caching and prefetching can apply application-specific knowledge to improve file cache management. However, supporting application-controlled file caching and prefetching is nontrivial because caching and prefetching need to be integrated carefully, and the kernel needs to allocate cache blocks among processes appropriately. This article presents the design, implementation, and performance of a file system that integrates application-controlled caching, prefetching, and disk scheduling. We use a two-level cache management strategy. The kernel uses the LRU-SP (Least-Recently-Used with Swapping and Placeholders) policy to allocate blocks to processes, and each process integrates application-specific caching and prefetching based on the"},
{"Title": "On the Performance of Competitive Algorithms in Practice", "URL": "https://dl.acm.org/doi/10.5555/647371.724037", "Full Abstract": "No abstract available."},
{"Title": "Implementing cooperative prefetching and caching in a globally-managed memory system", "URL": "https://dl.acm.org/doi/10.1145/277851.277869", "Full Abstract": "This paper presents"},
{"Title": "A Note on the Influence of an ε-Biased Random Source", "URL": "https://dl.acm.org/doi/10.1006/jcss.1997.1551", "Full Abstract": "An -biased random source is a sequenceX=(X1,X2,Xn) of 0, 1-valued random variables such that the conditional probability PrXi=1|X1,X2,Xi 1 is always between 12 and 12+ . Given a familyS {0,1nof binary strings of lengthn, its -enhanced probability Pr (S) is defined as the maximum of PrX(S) over all -biased random sourcesX. In this paper we establish a tight lower bound on Pr (S) as a function of |S|,nand ."},
{"Title": "Managing information extraction", "URL": "https://dl.acm.org/doi/10.1145/1142473.1142595", "Full Abstract": "This tutorial makes the case for developing a unified framework that manages information extraction from unstructured data (focusing in particular on text). We first survey research on information extraction in the database, AI, NLP, IR, and Web communities in recent years. Then we discuss why this is the right time for the database community to actively participate and address the problem of managing information extraction (including in particular the challenges of maintaining and querying the extracted information, and accounting for the imprecision and uncertainty inherent in the extraction process). Finally, we show how interested researchers can take the next step, by pointing to open problems, available datasets, applicable standards, and software tools. We do not assume prior knowledge of text management, NLP, extraction techniques, or machine learning."},
{"Title": "eTuner: tuning schema matching software using synthetic scenarios", "URL": "https://dl.acm.org/doi/10.1007/s00778-006-0024-z", "Full Abstract": "Most recent schema matching systems assemble"},
{"Title": "OLAP over imprecise data with domain constraints", "URL": "https://dl.acm.org/doi/10.5555/1325851.1325860", "Full Abstract": "Several recent papers have focused on OLAP over imprecise data, where each fact can be a region, instead of a point, in a multi-dimensional space. They have provided a multiple-world semantics for such data, and developed efficient ways to answer OLAP aggregation queries over the imprecise facts. These solutions, however, assume that the imprecise facts can be interpreted"},
{"Title": "Building structured web community portals", "URL": "https://dl.acm.org/doi/10.5555/1325851.1325899", "Full Abstract": "Structured community portals extract and integrate information from raw Web pages to present a unified view of entities and relationships in the community. In this paper we argue that to build such portals, a top-down, compositional, and incremental approach is a good way to proceed. Compared to current approaches that employ complex monolithic techniques, this approach is easier to develop, understand, debug, and optimize. In this approach, we first select a small set of important community sources. Next, we compose plans that extract and integrate data from these sources, using a set of extraction/integration operators. Executing these plans yields an initial structured portal. We then incrementally expand this portal by monitoring the evolution of current data sources, to detect and add new data sources. We describe our initial solutions to the above steps, and a case study of employing these solutions to build DBLife, a portal for the database community. We found that DBLife could be built quickly and achieve high accuracy using simple extraction/integration operators, and that it can be maintained and expanded with little human effort. The initial solutions together with the case study demonstrate the feasibility and potential of our approach."},
{"Title": "Declarative information extraction using datalog with embedded extraction predicates", "URL": "https://dl.acm.org/doi/10.5555/1325851.1325968", "Full Abstract": "In this paper we argue that developing information extraction (IE) programs using Datalog with embedded procedural extraction predicates is a good way to proceed. First, compared to current ad-hoc composition using, e.g., Perl or C++, Datalog provides a cleaner and more powerful way to compose small extraction modules into larger programs. Thus, writing IE programs this way retains and enhances the important advantages of current approaches: programs are easy to understand, debug, and modify. Second, once we write IE programs in this framework, we can apply query optimization techniques to them. This gives programs that, when run over a variety of data sets, are more efficient than any monolithic program because they are optimized based on the statistics of the data on which they are invoked. We show how optimizing such programs raises challenges specific to text data that cannot be accommodated in the current relational optimization framework, then provide initial solutions. Extensive experiments over real-world data demonstrate that optimization is indeed vital for IE programs and that we can effectively optimize IE programs written in this proposed framework."},
{"Title": "A relational approach to incrementally extracting and querying structure in unstructured data", "URL": "https://dl.acm.org/doi/10.5555/1325851.1325969", "Full Abstract": "There is a growing consensus that it is desirable to query over the structure implicit in unstructured documents, and that ideally this capability should be provided incrementally. However, there is no consensus about what kind of system should be used to support this kind of incremental capability. We explore using a relational system as the basis for a workbench for extracting and querying structure from unstructured data. As a proof of concept, we applied our relational approach to support structured queries over Wikipedia. We show that the data set is always available for some form of querying, and that as it is processed, users can pose a richer set of structured queries. We also provide examples of how we can incrementally evolve our understanding of the data in the context of the relational workbench."},
{"Title": "Databases and Web 2.0 panel at VLDB 2007", "URL": "https://dl.acm.org/doi/10.1145/1374780.1374794", "Full Abstract": "Web 2.0 refers to a set of technologies that enables indviduals to create and share content on the Web. The types of content that are shared on Web 2.0 are quite varied and include photos and videos (e.g., Flickr, YouTube), encyclopedic knowledge (e.g., Wikipedia), the blogosphere, social book-marking and even structured data (e.g., Swivel, Many-eyes). One of the important distinguishing features of Web 2.0 is the creation of"},
{"Title": "Matching Schemas in Online Communities", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2008.4497419", "Full Abstract": "When integrating data from multiple sources, a key task that online communities often face is to match the schemas of the data sources. Today, such matching often incurs a huge workload that overwhelms the relatively small set of volunteer integrators. In such cases, community members may not even volunteer to be integrators, due to the high workload, and consequently no integration systems can be built. To address this problem, we propose to enlist the multitude of users in the community to help match the schemas, in a Web 2.0 fashion. We discuss the challenges of this approach and provide initial solutions. Finally, we describe an extensive set of experiments on both real-world and synthetic data that demonstrate the utility of the approach."},
{"Title": "Optimizing SQL Queries over Text Databases", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2008.4497472", "Full Abstract": "Text documents often embed data that is structured in nature, and we can expose this structured data using information extraction technology. By processing a text database with information extraction systems, we can materialize a variety of structured \"relations,\" over which we can then issue regular SQL queries. A key challenge to process SQL queries in this text-based scenario is efficiency: information extraction is time-consuming, so query processing strategies should minimize the number of documents that they process. Another key challenge is result quality: in the traditional relational world, all correct execution strategies for a SQL query produce the same (correct) result; in contrast, a SQL query execution over a text database might produce answers that are not fully accurate or complete, for a number of reasons. To address these challenges, we study a family of select-project-join SQL queries over text databases, and characterize query processing strategies on their efficiency and - critically - on their result quality as well. We optimize the execution of SQL queries over text databases in a principled, cost-based manner, incorporating this tradeoff between efficiency and result quality in a user-specific fashion. Our large-scale experiments- over real data sets and multiple information extraction systems - show that our SQL query processing approach consistently picks appropriate execution strategies for the desired balance between efficiency and result quality."},
{"Title": "Building Community Wikipedias", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2008.4497473", "Full Abstract": "The rapid growth of Web communities has motivated many solutions for building community data portals. These solutions follow roughly two approaches. The first approach (e.g., Libra, Citeseer, Cimple) employs semi-automatic methods to extract and integrate data from a multitude of data sources. The second approach (e.g., Wikipedia, Intellipedia) deploys an initial portal in wiki format, then invites community members to revise and add material. In this paper we consider combining the above two approaches to building community portals. The new hybrid machine-human approach brings significant benefits. It can achieve broader and deeper coverage, provide more incentives for users to contribute, and keep the portal more up-to-date with less user effort. In a sense, it enables building \"community wikipedias\", backed by an underlying structured database that is continuously updated using automatic techniques. We outline our ideas for the new approach, describe its challenges and opportunities, and provide initial solutions. Finally, we describe a real-world implementation and preliminary experiments that demonstrate the utility of the new approach."},
{"Title": "Efficient Information Extraction over Evolving Text Data", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2008.4497503", "Full Abstract": "Most current information extraction (IE) approaches have considered only static text corpora, over which we typically have to apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and to keep extracted information up to date, we often must apply IE repeatedly, to consecutive corpus snapshots. We describe Cyclex, an approach that efficiently executes such repeated IE, by recycling previous IE efforts. Specifically, given a current corpus snapshot U, Cyclex identifies text portions of U that also appear in the previous corpus snapshot V. Since Cyclex has already executed IE over V, it can now recycle the IE results of these parts, by combining these results with the results of executing IE over the remaining parts of U, to produce the complete IE results for U. Realizing Cyclex raises many challenges, including modeling information extractors, exploring the trade-off between runtime and completeness in identifying overlapping text, and making informed, cost-based decisions between redoing IE from scratch and recycling previous IE results. We describe initial solutions to these challenges, and experiments over two real-world data sets that demonstrate the utility of our approach."},
{"Title": "Toward best-effort information extraction", "URL": "https://dl.acm.org/doi/10.1145/1376616.1376718", "Full Abstract": "Current approaches to develop information extraction (IE) programs have largely focused on producing"},
{"Title": "On the provenance of non-answers to queries over extracted data", "URL": "https://dl.acm.org/doi/10.14778/1453856.1453936", "Full Abstract": "In information extraction, uncertainty is ubiquitous. For this reason, it is useful to provide users querying extracted data with explanations for the answers they receive. Providing the provenance for tuples in a query result partially addresses this problem, in that provenance can explain why a tuple is in the result of a query. However, in some cases explaining why a tuple is not in the result may be just as helpful. In this work we focus on providing provenance-style explanations for non-answers and develop a mechanism for providing this new type of provenance. Our experience with an information extraction prototype suggests that our approach can provide effective provenance information that can help a user resolve their doubts over non-answers to a query."},
{"Title": "Analyzing and revising data integration schemas to improve their matchability", "URL": "https://dl.acm.org/doi/10.14778/1453856.1453940", "Full Abstract": "Data integration systems often provide a uniform query interface, called a"},
{"Title": "The Claremont report on database research", "URL": "https://dl.acm.org/doi/10.1145/1462571.1462573", "Full Abstract": "In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience."},
{"Title": "Building Structured Web Community Portals Via Extraction, Integration, and Mass Collaboration", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-89197-0_3", "Full Abstract": "The World-Wide Web hosts numerous communities, each focusing on a particular topic. As such communities proliferate, so do efforts to build community portals. Most current portals are organized according to topic taxonomies. Recently, however, there has been a growing effort to build structured data portals (e.g., IMDB, Citeseer) that present a unified view of entities and relationships in the community. Such portals can prove extremely valuable in a wide range of domains. But how can we build them efficiently?"},
{"Title": "Information extraction challenges in managing unstructured data", "URL": "https://dl.acm.org/doi/10.1145/1519103.1519106", "Full Abstract": "Over the past few years, we have been trying to build an end-to-end system at Wisconsin to manage unstructured data, using extraction, integration, and user interaction. This paper describes the key information extraction (IE) challenges that we have run into, and sketches our solutions. We discuss in particular developing a declarative IE language, optimizing for this language, generating IE provenance, incorporating user feedback into the IE process, developing a novel wiki-based user interface for feedback, best-effort IE, pushing IE into RDBMSs, and more. Our work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community."},
{"Title": "Join Optimization of Information Extraction Output", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2009.138", "Full Abstract": "Information extraction (IE) systems are trained to extract specific relations from text databases. Real-world applications often require that the output of multiple IE systems be joined to produce the data of interest. To optimize the execution of a join of multiple extracted relations, it is not sufficient to consider only execution time. In fact, the quality of the join output is of critical importance: unlike in the relational world, different join execution plans can produce join results of widely different quality whenever IE systems are involved. In this paper, we develop a principled approach to understand, estimate, and incorporate output quality into the join optimization process over extracted relations. We argue that the output quality is affected by (a) the configuration of the IE systems used to process documents, (b) the document retrieval strategies used to retrieve documents, and (c) the actual join algorithm used. Our analysis considers several alternatives for these factors, and predicts the output quality---and, of course, the execution time---of the alternate execution plans. We establish the accuracy of our analytical models, as well as study the effectiveness of a quality-aware join optimizer, with a large-scale experimental evaluation over real-world text collections and state-of-the-art IE systems."},
{"Title": "Weighted Proximity Best-Joins for Information Retrieval", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2009.61", "Full Abstract": "We consider the problem of efficiently computing weighted proximity best-joins over multiple lists, with applications in information retrieval and extraction. We are given a multi-term query, and for each query term, a list of all its matches with scores, sorted by locations. The problem is to find the overall best matchset, consisting of one match from each list, such that the combined score according to a scoring function is maximized. We study three types of functions that consider both individual match scores and proximity of match locations in scoring a matchset. We present algorithms that exploit the properties of the scoring functions in order to achieve time complexities linear in the size of the match lists. Experiments show that these algorithms greatly outperform the naive algorithm based on taking the cross product of all match lists. Finally, we extend our algorithms for an alternative problem definition applicable to information extraction, where we need to find all good matchsets in a document."},
{"Title": "Sampling search-engine results", "URL": "https://dl.acm.org/doi/10.1145/1060745.1060784", "Full Abstract": "We consider the problem of efficiently sampling Web search engine query results. In turn, using a small random sample instead of the full set of results leads to efficient approximate algorithms for several applications, such as:"},
{"Title": "Current trends in the integration of searching and browsing", "URL": "https://dl.acm.org/doi/10.1145/1062745.1062751", "Full Abstract": "Searching and browsing are the two basic information discovery paradigms, since the early days of the Web. After more than ten years down the road, three schools seem to have emerged: (1) The search-centric school argues that guided navigation is superfluous since free form search has become so good and the search UI so common, that users can satisfy all their needs via simple queries (2) The taxonomy navigation school claims that users have difficulties expressing informational needs and (3) The meta-data centric school advocates the use of meta-data for narrowing large sets of results, and is successful in e-commerce where it is known as \"multi faceted search\". This panel brings together experts and advocates for all three schools, who will discuss these approaches and share their experiences in the field. We will ask the audience to challenge our experts with real information architecture problems."},
{"Title": "Querying the past, present and future", "URL": "https://dl.acm.org/doi/10.1145/1062745.1062756", "Full Abstract": "This panel will focus on exploring future enhancements of Web technology for active Internet-scale information delivery and dissemination. It will ask the questions of whether the current Web technology is sufficient, what can be leveraged in this endeavor, and how a combination of ideas from a variety of existing disciplines can help in meeting the new challenges of large scale information dissemination. Relevant existing technologies and research areas include: active databases, agent systems, continual queries, event Web, publish/subscribe technology, sensor and stream data management. We expect that some suggestions may be in conflict with current, well-accepted approaches."},
{"Title": "How search engines shape the web", "URL": "https://dl.acm.org/doi/10.1145/1062745.1062776", "Full Abstract": "The state of the web today has been and continues to be greatly influenced by the existence of web-search engines. This panel will discuss the ways in which search engines have affected the web in the past and ways in which they may affect it in the future. Both positive and negative effects will be discussed as will potential measures to combat the latter. Besides the obvious ways in which search engines help people find content, other effects to be discussed include: the whole phenomenon of web-page spam, based on both text and link (e.g. link farms), the business of \"Search Engine Optimization\" (optimizing pages to rank highly in web-search results), the bided-terms business and the associated problem of click fraud, to name a few."},
{"Title": "Efficient PageRank approximation via graph aggregation", "URL": "https://dl.acm.org/doi/10.1007/s10791-006-7146-1", "Full Abstract": "We present a framework for approximating random-walk based probability distributions over Web pages using graph aggregation. The basic idea is to partition the graph into classes of quasi-equivalent vertices, to project the page-based random walk to be approximated onto those classes, and to compute the stationary probability distribution of the resulting class-based random walk. From this distribution we can quickly reconstruct a distribution on pages. In particular, our framework can approximate the well-known PageRank distribution by setting the classes according to the set of pages on each Web host."},
{"Title": "Indexing shared content in information retrieval systems", "URL": "https://dl.acm.org/doi/10.1007/11687238_21", "Full Abstract": "Modern document collections often contain groups of documents with overlapping or shared content. However, most information retrieval systems process each document separately, causing shared content to be indexed multiple times. In this paper, we describe a new document representation model where related documents are organized as a tree, allowing shared content to be indexed just once. We show how this representation model can be encoded in an inverted index and we describe algorithms for evaluating free-text queries based on this encoding. We also show how our representation model applies to web, email, and newsgroup search. Finally, we present experimental results showing that our methods can provide a significant reduction in the size of an inverted index as well as in the time to build and query it."},
{"Title": "The future of web search", "URL": "https://dl.acm.org/doi/10.1007/11780991_40", "Full Abstract": "In the past decade, Web search engines have evolved from a first generation based on classic Information Retrieval (IR) algorithms scaled to web size and thus supporting only informational queries, to a second generation supporting navigational queries using web specific information (primarily link analysis), to a third generation enabling transactional and other \"semantic\" queries based on a variety of technologies aimed to directly satisfy the unexpressed \"user intent.\""},
{"Title": "Effective and efficient classification on a search-engine model", "URL": "https://dl.acm.org/doi/10.1145/1183614.1183648", "Full Abstract": "Traditional document classification frameworks, which apply the learned classifier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the \"best\" short query that characterizes a document class using operators normally available within large engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. Moreover, we show that optimizing the efficiency of query execution by careful selection of these terms can further reduce the query costs. More precisely, we show that on our set-up the best 10 terms query canachieve 90% of the accuracy of the best SVM classifier (14000 terms), and if we are willing to tolerate a reduction to 86% of the best SVM, we can build a 10 terms query that can be executed more than twice as fast as the best 10 terms query."},
{"Title": "Estimating corpus size via queries", "URL": "https://dl.acm.org/doi/10.1145/1183614.1183699", "Full Abstract": "We consider the problem of estimating the size of a collection of documents using only a standard query interface. Our main idea is to construct an unbiased and low-variance estimator that can closely approximate the size of any set of documents defined by certain conditions, including that each document in the set must match at least one query from a uniformly sampleable query pool of known size, fixed in advance.Using this basic estimator, we propose two approaches to estimating corpus size. The first approach requires a uniform random sample of documents from the corpus. The second approach avoids this notoriously difficult sample generation problem, and instead uses two fairly uncorrelated sets of terms as query pools; the accuracy of the second approach depends on the degree of correlation among the two sets of terms.Experiments on a large TREC collection and on three major search engines demonstrates the effectiveness of our algorithms."},
{"Title": "Sampling Search-Engine Results", "URL": "https://dl.acm.org/doi/10.1007/s11280-006-0222-z", "Full Abstract": "We consider the problem of efficiently sampling Web search engine query results. In turn, using a small random sample instead of the full set of results leads to efficient approximate algorithms for several applications, such as: ."},
{"Title": "The next generation web search and the demise of the classic IR model", "URL": "https://dl.acm.org/doi/10.5555/1763653.1763655", "Full Abstract": "The classic IR model assumes a human engaged in activity that generates an \"information need\". This need is verbalized and then expressed as a query to search engine over a defined corpus. In the past decade, Web search engines have evolved from a first generation based on classic IR algorithms scaled to web size and thus supporting only informational queries, to a second generation supporting navigational queries using web specific information (primarily link analysis), to a third generation enabling transactional and other \"semantic\" queries based on a variety of technologies aimed to directly satisfy the unexpressed \"user intent\", thus moving further and further away from the classic model."},
{"Title": "Margin based active learning", "URL": "https://dl.acm.org/doi/10.5555/1768841.1768848", "Full Abstract": "We present a framework for margin based active learning of linear separators. We instantiate it for a few important cases, some of which have been previously considered in the literature.We analyze the effectiveness of our framework both in the realizable case and in a specific noisy setting related to the Tsybakov small noise condition."},
{"Title": "Robust classification of rare queries using web knowledge", "URL": "https://dl.acm.org/doi/10.1145/1277741.1277783", "Full Abstract": "We propose a methodology for building a practical robust query classification system that can identify thousands of query classes with reasonable accuracy, while dealing in real-time with the query volume of a commercial web search engine. We use a blind feedback technique: given a query, we determine its topic by classifying the web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregation account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience."},
{"Title": "A semantic approach to contextual advertising", "URL": "https://dl.acm.org/doi/10.1145/1277741.1277837", "Full Abstract": "Contextual advertising or Context Match (CM) refers to the placement of commercial textual advertisements within the content of a generic web page, while Sponsored Search (SS) advertising consists in placing ads on result pages from a web search engine, with ads driven by the originating query. In CM there is usually an intermediary commercial"},
{"Title": "Estimating rates of rare events at multiple resolutions", "URL": "https://dl.acm.org/doi/10.1145/1281192.1281198", "Full Abstract": "We consider the problem of estimating occurrence rates of rare eventsfor extremely sparse data, using pre-existing hierarchies to perform inference at multiple resolutions. In particular, we focus on the problem of estimating click rates for (webpage, advertisement) pairs (called"},
{"Title": "Modelling and Mining of Networked Information Spaces", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-78808-9_1", "Full Abstract": "In recent years, the emergence of the Web and the dramatic increase in computing, storage and networking capacity has given rise to the concept of networked information spaces. The prime example of a networked information space is the World Wide Web itself. The Web, in its pure form, is a set of hypertext documents, with links in one document pointing to another document."},
{"Title": "Workshop on Algorithms and Models for the Web Graph", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-78808-9_2", "Full Abstract": "For barely a decade now the Web graph (the network formed by Web pages and their hyperlinks) has been the focus of scientific study. In that short a time, this study has made a significant impact on research in physics, computer science and mathematics. It has focussed the attention of the scientific community on all the different kinds of networks that have arisen through technology and human activity; some speak of a \"new science of networks\". It has brought the computational and deductive power of computer science to the study of the complex social networks formed by inter-human relationships. And, it has given birth to new branches of research in different areas of mathematics, most notably graph theory and probability."},
{"Title": "Just-in-time contextual advertising", "URL": "https://dl.acm.org/doi/10.1145/1321440.1321488", "Full Abstract": "is a type of Web advertising, which, given the URL of a Web page, aims to embed into the page (typically via JavaScript) the most relevant textual ads available. For static pages that are displayed repeatedly, the matching of ads can be based on prior analysis of their entire content; however, ads need to be matched also to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire body of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low-relevance or high-latency or high-load, we propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time. Empirical evaluation proves that matching ads on the basis of such summaries does not sacrifice relevance, and is competitive with matching based on the entire page content. Specifically, we found that analyzing a carefully selected 5% fraction of the page text sacrifices only 1%-3% in ad relevance. Furthermore, our summaries are fully compatible with the standard JavaScript mechanisms used for ad placement: they can be produced at ad-display time by simple additions to the usual script, and they only add 500-600 bytes to the usual request."},
{"Title": "Computational advertising", "URL": "https://dl.acm.org/doi/10.5555/1347082.1347190", "Full Abstract": "Computational advertising is an emerging new scientific sub-discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, classification, optimization, and microeconomics. The central challenge of computational advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The context could be a user entering a query in a search engine (\"sponsored search\"), a user reading a web page (\"content match\" and \"display ads\"), a user watching a movie on a portable device, and so on. The information about the user can vary from scarily detailed to practically nil. The number of potential advertisements might be in the billions. Thus, depending on the definition of \"best match\" this challenge leads to a variety of massive optimization and search problems, with complicated constraints."},
{"Title": "The hiring problem and Lake Wobegon strategies", "URL": "https://dl.acm.org/doi/10.5555/1347082.1347211", "Full Abstract": "We introduce the"},
{"Title": "Formal verification of ACAS X, an industrial airborne collision avoidance system", "URL": "https://dl.acm.org/doi/10.5555/2830865.2830880", "Full Abstract": "Formal verification of industrial systems is very challenging, due to reasons ranging from scalability issues to communication difficulties with engineering-focused teams. More importantly, industrial systems are rarely designed for verification, but rather for operational needs. In this paper we present an overview of our experience using hybrid systems theorem proving to formally verify ACAS X, an airborne collision avoidance system for airliners scheduled to be operational around 2020. The methods and proof techniques presented here are an overview of the work already presented in [8], while the evaluation of ACAS X has been significantly expanded and updated to the most recent version of the system, run 13. The effort presented in this paper is an integral part of the ACAS X development and was performed in tight collaboration with the ACAS X development team."},
{"Title": "Forward invariant cuts to simplify proofs of safety", "URL": "https://dl.acm.org/doi/10.5555/2830865.2830890", "Full Abstract": "The use of deductive techniques, such as theorem provers, has several advantages in safety verification of hybrid systems; however, state-of-the-art theorem provers require manual intervention to handle complex systems. Furthermore, there is often a gap between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide directly. This paper presents an extension to KeYmaera, a deductive verification tool for differential dynamic logic; the new technique allows"},
{"Title": "Differential Game Logic", "URL": "https://dl.acm.org/doi/10.1145/2817824", "Full Abstract": "(dG"},
{"Title": "Numerically-aided Deductive Safety Proof for a Powertrain Control System", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2015.10.003", "Full Abstract": "The use of deductive techniques, such as theorem provers, has several advantages in safety verification of hybrid systems. There is often a gap, however, between the type of assistance that a theorem prover requires to make progress on a proof task and the assistance that a system designer is able to provide. To address this deficiency we present an extension to the deductive verification framework of differential dynamic logic that allows the theorem prover KeYmaera to locally reason about behaviors by leveraging forward invariant sets provided by external methods, such as numerical techniques and designer insights. Our key contribution is a new inference rule, the forward invariant cut rule, introduced into the proof calculus of KeYmaera. We demonstrate the cut rule in action on an example involving an automotive powertrain control systems, in which we make use of a simulation-driven numerical technique to compute a local barrier function."},
{"Title": "A Method for Invariant Generation for Polynomial Continuous Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-662-49122-5_13", "Full Abstract": "This paper presents a method for generating semi-algebraic invariants for systems governed by non-linear polynomial ordinary differential equations under semi-algebraic evolution constraints. Based on the notion of discrete abstraction, our method eliminates unsoundness and unnecessary coarseness found in existing approaches for computing abstractions for non-linear continuous systems and is able to construct invariants with intricate boolean structure, in contrast to invariants typically generated using template-based methods. In order to tackle the state explosion problem associated with discrete abstraction, we present invariant generation algorithms that exploit sound proof rules for safety verification, such as differential cut$${\\text {DC$$, and a new proof rule that we call differential divide-and-conquer$${\\text {DDC$$, which splits the verification problem into smaller sub-problems. The resulting invariant generation method is observed to be much more scalable and efficient than the naïve approach, exhibiting orders of magnitude performance improvement on many of the problems."},
{"Title": "A logic of proofs for differential dynamic logic: toward independently checkable proof certificates for dynamic logics", "URL": "https://dl.acm.org/doi/10.1145/2854065.2854078", "Full Abstract": "Differential dynamic logic is a logic for specifying and verifying safety, liveness, and other properties about models of cyber-physical systems. Theorem provers based on differential dynamic logic have been used to verify safety properties for models of self-driving cars and collision avoidance protocols for aircraft. Unfortunately, these theorem provers do not have explicit proof terms, which makes the implementation of a number of important features unnecessarily complicated without soundness-critical and extra-logical extensions to the theorem prover. Examples include: an unambiguous separation between proof checking and proof search, the ability to extract program traces corresponding to counter-examples, and synthesis of surely-live deterministic programs from liveness proofs for nondeterministic programs. This paper presents a differential dynamic logic with such an explicit representation of proofs. The resulting logic extends both the syntax and semantics of differential dynamic logic with proof terms -- syntactic representations of logical deductions. To support axiomatic theorem proving, the logic allows equivalence rewriting deep within formulas and supports both uniform renaming and uniform substitutions."},
{"Title": "How to model and prove hybrid systems with KeYmaera: a tutorial on safety", "URL": "https://dl.acm.org/doi/10.1007/s10009-015-0367-0", "Full Abstract": "This paper is a tutorial on how to model hybrid systems as hybrid programs in differential dynamic logic and how to prove complex properties about these complex hybrid systems in KeYmaera, an automatic and interactive formal verification tool for hybrid systems. Hybrid systems can model highly nontrivial controllers of physical plants, whose behaviors are often safety critical such as trains, cars, airplanes, or medical devices. Formal methods can help design systems that work correctly. This paper illustrates how KeYmaera can be used to systematically model, validate, and verify hybrid systems. We develop tutorial examples that illustrate challenges arising in many real-world systems. In the context of this tutorial, we identify the impact that modeling decisions have on the suitability of the model for verification purposes. We show how the interactive features of KeYmaera can help users understand their system designs better and prove complex properties for which the automatic prover of KeYmaera still takes an impractical amount of time. We hope this paper is a helpful resource for designers of embedded and cyber–physical systems and that it illustrates how to master common practical challenges in hybrid systems verification."},
{"Title": "A Component-Based Approach to Hybrid Systems Safety Verification", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-33693-0_28", "Full Abstract": "We study a component-based approach to simplify the challenges of verifying large-scale hybrid systems. Component-based modeling can be used to split large models into partial models to reduce modeling complexity. Yet, verification results also need to transfer from components to composites. In this paper, we propose a component-based hybrid system verification approach that combines the advantages of component-based modeling e.g., reduced model complexity with the advantages of formal verification e.g., guaranteed contract compliance. Our strategy is to decompose the system into components, verify their local safety individually and compose them to form an overall system that provably satisfies a global contract, without proving the whole system. We introduce the necessary formalism to define the structure and behavior of components and a technique how to compose components such that safety properties provably emerge from component safety."},
{"Title": "Logic & Proofs for Cyber-Physical Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-40229-1_3", "Full Abstract": "Cyber-physical systems CPS combine cyber aspects such as communication and computer control with physical aspects such as movement in space, which arise frequently in many safety-critical application domains, including aviation, automotive, railway, and robotics. But how can we ensure that these systems are guaranteed to meet their design goals, e.g., that an aircraft will not crash into another one__ __"},
{"Title": "Differential Refinement Logic", "URL": "https://dl.acm.org/doi/10.1145/2933575.2934555", "Full Abstract": "We introduce differential refinement logic (dRL), a logic with first-class support for refinement relations on hybrid systems, and a proof calculus for verifying such relations. dRL simultaneously solves several seemingly different challenges common in theorem proving for hybrid systems: 1. When hybrid systems are complicated, it is useful to prove properties about simpler and related subsystems before tackling the system as a whole. 2. Some models of hybrid systems can be implementation-specific. Verification can be aided by abstracting the system down to the core components necessary for safety, but only if the relations between the abstraction and the original system can be guaranteed. 3. One approach to taming the complexities of hybrid systems is to start with a simplified version of the system and iteratively expand it. However, this approach can be costly, since every iteration has to be proved safe from scratch, unless refinement relations can be leveraged in the proof. 4. When proofs become large, it is difficult to maintain a modular or comprehensible proof structure. By using a refinement relation to arrange proofs hierarchically according to the structure of natural subsystems, we can increase the readability and modularity of the resulting proof. dRL extends an existing specification and verification language for hybrid systems (differential dynamic logic, dL) by adding a refinement relation to directly compare hybrid systems. This paper gives a syntax, semantics, and proof calculus for dRL. We demonstrate its usefulness with examples where using refinement results in easier and better-structured proofs."},
{"Title": "How to prove hybrid systems", "URL": "https://dl.acm.org/doi/10.5555/3343414.3343415", "Full Abstract": "Hybrid systems combine discrete dynamics with continuous dynamics along differential equations. They arise frequently in many safety-critical application domains, including aviation, automotive, railway, and robotics. But how can we ensure that these systems are guaranteed to meet their design goals, e.g., that an aircraft will not crash into another one? This talk describes how hybrid systems can be proved using differential dynamic logic. Differential dynamic logic (dL) provides compositional logics, programming languages, and reasoning principles for hybrid systems. As implemented in the theorem prover KeYmaera X, dL has been instrumental in verifying many applications, including the Airborne Collision Avoidance System ACAS X, the European Train Control System ETCS, automotive systems, mobile robot navigation, and a surgical robot system for skull-base surgery."},
{"Title": "ModelPlex", "URL": "https://dl.acm.org/doi/10.1007/s10703-016-0241-z", "Full Abstract": "Formal verification and validation play a crucial role in making cyber-physical systems (CPS) safe. Formal methods make strong guarantees about the system behavior if accurate models of the system can be obtained, including models of the controller and of the physical dynamics. In CPS, models are essential; but any model we could possibly build necessarily deviates from the real world. If the real system fits to the model, its behavior is guaranteed to satisfy the correctness properties verified with respect to the model. Otherwise, all bets are off. This article introduces ModelPlex, a method ensuring that verification results about models apply to CPS implementations. ModelPlex provides correctness guarantees for CPS executions at runtime: it combines offline verification of CPS models with runtime validation of system executions for compliance with the model. ModelPlex ensures in a provably correct way that the verification results obtained for the model apply to the actual system runs by monitoring the behavior of the world for compliance with the model. If, at some point, the observed behavior no longer complies with the model so that offline verification results no longer apply, ModelPlex initiates provably safe fallback actions, assuming the system dynamics deviation is bounded. This article, furthermore, develops a systematic technique to synthesize provably correct monitors automatically from CPS proofs in differential dynamic logic by a correct-by-construction approach, leading to verifiably correct runtime model validation. Overall, ModelPlex generates provably correct monitor conditions that, if checked to hold at runtime, are provably guaranteed to imply that the offline safety verification results about the CPS model apply to the present run of the actual CPS implementation."},
{"Title": "Formally verified differential dynamic logic", "URL": "https://dl.acm.org/doi/10.1145/3018610.3018616", "Full Abstract": "We formalize the soundness theorem for differential dynamic logic, a logic for verifying hybrid systems. To increase confidence in the formalization, we present two versions: one in Isabelle/HOL and one in Coq. We extend the metatheory to include features used in practice, such as systems of differential equations and functions of multiple arguments. We demonstrate the viability of constructing a verified kernel for the hybrid systems theorem prover KeYmaera X by embedding proof checkers for differential dynamic logic in Coq and Isabelle. We discuss how different provers and libraries influence the design of the formalization."},
{"Title": "Change and Delay Contracts for Hybrid System Component Verification", "URL": "https://dl.acm.org/doi/10.1007/978-3-662-54494-5_8", "Full Abstract": "In this paper, we present reasoning techniques for a component-based modeling and verification approach for hybrid systems comprising discrete dynamics as well as continuous dynamics, in which the components have local responsibilities. Our approach supports component contracts i.e., input assumptions and output guarantees of interfaces that are more general than previous component-based hybrid systems verification techniques in the following ways: We introduce change contracts, which characterize how current values exchanged between components along ports relate to previous values. We also introduce delay contracts, which describe the change relative to the time that has passed since the last value was exchanged. Together, these contracts can take into account what has changed between two components in a given amount of time since the last exchange of information. Most crucially, we prove that the safety of compatible components implies safety of the composite. The proof steps of the theorem are also implemented as a tactic in KeYmaeraï źX, allowing automatic generation of a KeYmaeraï źX proof for the composite system from proofs of the concrete components."},
{"Title": "A Complete Uniform Substitution Calculus for Differential Dynamic Logic", "URL": "https://dl.acm.org/doi/10.1007/s10817-016-9385-1", "Full Abstract": "This article introduces a relatively complete proof calculus for differential dynamic logic (dL) that is entirely based on uniform substitution, a proof rule that substitutes a formula for a predicate symbol everywhere. Uniform substitutions make it possible to use axioms instead of axiom schemata, thereby substantially simplifying implementations. Instead of subtle schema variables and soundness-critical side conditions on the occurrence patterns of logical variables to restrict infinitely many axiom schema instances to sound ones, the resulting calculus adopts only a finite number of ordinary dLformulas as axioms, which uniform substitutions instantiate soundly. The static semantics of differential dynamic logic and the soundness-critical restrictions it imposes on proof steps is captured exclusively in uniform substitutions and variable renamings as opposed to being spread in delicate ways across the prover implementation. In addition to sound uniform substitutions, this article introduces differential forms for differential dynamic logic that make it possible to internalize differential invariants, differential substitutions, and derivatives as first-class axioms to reason about differential equations axiomatically. The resulting axiomatization of differential dynamic logic is proved to be sound and relatively complete."},
{"Title": "Differential Hybrid Games", "URL": "https://dl.acm.org/doi/10.1145/3091123", "Full Abstract": "This article introduces"},
{"Title": "Stochastic differential dynamic logic for stochastic hybrid programs", "URL": "https://dl.acm.org/doi/10.5555/2032266.2032300", "Full Abstract": "Logic is a powerful tool for analyzing and verifying systems, including programs, discrete systems, real-time systems, hybrid systems, and distributed systems. Some applications also have a stochastic behavior, however, either because of fundamental properties of nature, uncertain environments, or simplifications to overcome complexity. Discrete probabilistic systems have been studied using logic. But logic has been chronically underdeveloped in the context of stochastic hybrid systems, i.e., systems with interacting discrete, continuous, and stochastic dynamics. We aim at overcoming this deficiency and introduce a dynamic logic for stochastic hybrid systems. Our results indicate that logic is a promising tool for understanding stochastic hybrid systems and can help taming some of their complexity. We introduce a compositional model for stochastic hybrid systems. We prove adaptivity, càdlàg, and Markov time properties, and prove that the semantics of our logic is measurable. We present compositional proof rules, including rules for stochastic differential equations, and prove soundness."},
{"Title": "Statistical model checking for distributed probabilistic-control hybrid automata with smart grid applications", "URL": "https://dl.acm.org/doi/10.5555/2075089.2075103", "Full Abstract": "The power industry is currently moving towards a more dynamical, intelligent power grid. This Smart Grid is still in its infancy and a formal evaluation of the expensive technologies and ideas on the table is necessary before committing to a full investment. In this paper, we argue that a good model for the Smart Grid must match its basic properties: it must be hybrid (both evolve over time, and perform control/computation), distributed (multiple concurrently executing entities), and allow for asynchronous communication and stochastic behaviour (to accurately model real-world power consumption). We propose Distributed Probabilistic-Control Hybrid Automata (DPCHA) as a model for this purpose, and extend Bounded LTL to Quantified Bounded LTL in order to adapt and apply existing statistical model-checking techniques. We provide an implementation of a framework for developing and verifying DPCHAs. Finally, we conduct a case study for Smart Grid communications analysis."},
{"Title": "Distributed theorem proving for distributed hybrid systems", "URL": "https://dl.acm.org/doi/10.5555/2075089.2075121", "Full Abstract": "Distributed hybrid systems present extraordinarily challenging problems for verification. On top of the notorious difficulties associated with distributed systems, they also exhibit continuous dynamics described by quantified differential equations. All serious proofs rely on decision procedures for real arithmetic, which can be extremely expensive. Quantified Differential Dynamic Logic (QdL) has been identified as a promising approach for getting a handle in this domain. QdL has been proved to be complete relative to quantified differential equations. But important questions remain as to how best to translate this theoretical result into practice: how do we succinctly specify a proof search strategy, and how do we control the computational cost? We address the problem of automated theorem proving for distributed hybrid systems. We identify a simple mode of use of QdL that cuts down on the enormous number of choices that it otherwise allows during proof search. We have designed a powerful strategy and tactics language for directing proof search. With these techniques, we have implemented a new automated theorem prover called KeYmaeraD. To overcome the high computational complexity of distributed hybrid systems verification, KeYmaeraD uses a distributed proving backend. We have experimentally observed that calls to the real arithmetic decision procedure can effectively be made in parallel. In this paper, we demonstrate these findings through an extended case study where we prove absence of collisions in a distributed car control system with a varying number of arbitrarily many cars."},
{"Title": "Logical analysis of hybrid systems", "URL": "https://dl.acm.org/doi/10.5555/3173440.3173452", "Full Abstract": "Hybrid systems are systems with interacting discrete and continuous dynamics. They are models for understanding, e.g., computer systems interfacing with the physical environment. Hybrid systems have a complete axiomatization in differential dynamic logic relative to continuous systems. They also have a complete axiomatization relative to discrete systems. Moreover, there is a constructive reduction of properties of hybrid systems to corresponding properties of continuous systems or to corresponding properties of discrete systems. We briefly summarize and discuss some of the implications of these results."},
{"Title": "Towards Formal Verification of Freeway Traffic Control", "URL": "https://dl.acm.org/doi/10.1109/ICCPS.2012.25", "Full Abstract": "We study how CPS technology can help improve freeway traffic by combining local car GPS positioning, traffic center control decisions, and communication to achieve more tightly coupled feedback control in intelligent speed adaptation. We develop models for an intelligent speed adaptation that respects variable speed limit control and incident management. We identify safe ranges for crucial design parameters in these systems and, using the theorem prover KeYmaera, formally verify safety of the resulting CPS models. Finally, we show how those parameter ranges can be used to decide trade-offs for practical system implementations even for design parameters that are not modeled formally."},
{"Title": "Logics of Dynamical Systems", "URL": "https://dl.acm.org/doi/10.1109/LICS.2012.13", "Full Abstract": "We study the logic of dynamical systems, that is, logics and proof principles for properties of dynamical systems. Dynamical systems are mathematical models describing how the state of a system evolves over time. They are important in modeling and understanding many applications, including embedded systems and cyber-physical systems. In discrete dynamical systems, the state evolves in discrete steps, one step at a time, as described by a difference equation or discrete state transition relation. In continuous dynamical systems, the state evolves continuously along a function, typically described by a differential equation. Hybrid dynamical systems or hybrid systems combine both discrete and continuous dynamics. This is a brief survey of differential dynamic logic for specifying and verifying properties of hybrid systems. We explain hybrid system models, differential dynamic logic, its semantics, and its axiomatization for proving logical formulas about hybrid systems. We study differential invariants, i.e., induction principles for differential equations. We briefly survey theoretical results, including soundness and completeness and deductive power. Differential dynamic logic has been implemented in automatic and interactive theorem provers and has been used successfully to verify safety-critical applications in automotive, aviation, railway, robotics, and analogue electrical circuits."},
{"Title": "The Complete Proof Theory of Hybrid Systems", "URL": "https://dl.acm.org/doi/10.1109/LICS.2012.64", "Full Abstract": "Hybrid systems are a fusion of continuous dynamical systems and discrete dynamical systems. They freely combine dynamical features from both worlds. For that reason, it has often been claimed that hybrid systems are more challenging than continuous dynamical systems and than discrete systems. We now show that, proof-theoretically, this is not the case. We present a complete proof-theoretical alignment that interreduces the discrete dynamics and the continuous dynamics of hybrid systems. We give a sound and complete axiomatization of hybrid systems relative to continuous dynamical systems and a sound and complete axiomatization of hybrid systems relative to discrete dynamical systems. Thanks to our axiomatization, proving properties of hybrid systems is exactly the same as proving properties of continuous dynamical systems and again, exactly the same as proving properties of discrete dynamical systems. This fundamental cornerstone sheds light on the nature of hybridness and enables flexible and provably perfect combinations of discrete reasoning with continuous reasoning that lift to all aspects of hybrid systems and their fragments."},
{"Title": "Playing hybrid games with keymaera", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-31365-3_34", "Full Abstract": "We propose a new logic, called"},
{"Title": "Logical analysis of hybrid systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-31623-4_3", "Full Abstract": "Hybrid systems have a complete axiomatization in differential dynamic logic relative to continuous systems. They also have a complete axiomatization relative to discrete systems. Moreover, there is a constructive reduction of properties of hybrid systems to corresponding properties of continuous systems or to corresponding properties of discrete systems. We briefly summarize and discuss some of the implications of these results."},
{"Title": "Statistical Model Checking for Markov Decision Processes", "URL": "https://dl.acm.org/doi/10.1109/QEST.2012.19", "Full Abstract": "Statistical Model Checking (SMC) is a computationally very efficient verification technique based on selective system sampling. One well identified shortcoming of SMC is that, unlike probabilistic model checking, it cannot be applied to systems featuring nondeterminism, such as Markov Decision Processes (MDP). We address this limitation by developing an algorithm that resolves nondeterminism probabilistically, and then uses multiple rounds of sampling and Reinforcement Learning to provably improve resolutions of nondeterminism with respect to satisfying a Bounded Linear Temporal Logic (BLTL) property. Our algorithm thus reduces an MDP to a fully probabilistic Markov chain on which SMC may be applied to give an approximate solution to the problem of checking the probabilistic BLTL property. We integrate our algorithm in a parallelised modification of the PRISM simulation framework. Extensive validation with both new and PRISM benchmarks demonstrates that the approach scales very well in scenarios where symbolic algorithms fail to do so."},
{"Title": "Formal verification of distributed aircraft controllers", "URL": "https://dl.acm.org/doi/10.1145/2461328.2461350", "Full Abstract": "As airspace becomes ever more crowded, air traffic management must reduce both space and time between aircraft to increase throughput, making on-board collision avoidance systems ever more important. These safety-critical systems must be extremely reliable, and as such, many resources are invested into ensuring that the protocols they implement are accurate. Still, it is challenging to guarantee that such a controller works properly under every circumstance. In tough scenarios where a large number of aircraft must execute a collision avoidance maneuver, a human pilot under stress is not necessarily able to understand the complexity of the distributed system and may not take the right course, especially if actions must be taken quickly. We consider a class of distributed collision avoidance controllers designed to work even in environments with arbitrarily many aircraft or UAVs. We prove that the controllers never allow the aircraft to get too close to one another, even when new planes approach an in-progress avoidance maneuver that the new plane may not be aware of. Because these safety guarantees always hold, the aircraft are protected against unexpected emergent behavior which simulation and testing may miss. This is an important step in formally verified, flyable, and distributed air traffic control."},
{"Title": "Certifying the safe design of a virtual fixture control algorithm for a surgical robot", "URL": "https://dl.acm.org/doi/10.1145/2461328.2461369", "Full Abstract": "We applied quantified differential-dynamic logic (QdL) to analyze a control algorithm designed to provide directional force feedback for a surgical robot. We identified problems with the algorithm, proved that it was in general unsafe, and described exactly what could go wrong. We then applied QdL to guide the development of a new algorithm that provides safe operation along with directional force feedback. Using \\KeYmaeraD (a tool that mechanizes QdL), we created a machine-checked proof that guarantees the new algorithm is safe for all possible inputs."},
{"Title": "A generalization of SAT and #SAT for robust policy evaluation", "URL": "https://dl.acm.org/doi/10.5555/2540128.2540500", "Full Abstract": "Both SAT and #SAT can represent difficult problems in seemingly dissimilar areas such as planning, verification, and probabilistic inference. Here, we examine an expressive new language, #∃SAT, that generalizes both of these languages. #∃SAT problems require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified, propositional formulas. We characterize the expressiveness and worst-case difficulty of #∃SAT by proving it is complete for the complexity class #"},
{"Title": "Bayesian statistical model checking with application to Stateflow/Simulink verification", "URL": "https://dl.acm.org/doi/10.1007/s10703-013-0195-3", "Full Abstract": "We address the problem of model checking stochastic systems, i.e., checking whether a stochastic system satisfies a certain temporal property with a probability greater (or smaller) than a fixed threshold. In particular, we present a Statistical Model Checking (SMC) approach based on Bayesian statistics. We show that our approach is feasible for a certain class of hybrid systems with stochastic transitions, a generalization of Simulink/Stateflow models. Standard approaches to stochastic discrete systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces. Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging. The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case. It solves the verification problem by combining randomized sampling of system traces (which is very efficient for Simulink/Stateflow) with hypothesis testing (i.e., testing against a probability threshold) or estimation (i.e., computing with high probability a value close to the true probability). We believe SMC is essential for scaling up to large Stateflow/Simulink models. While the answer to the verification problem is not guaranteed to be correct, we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small. The advantage is that answers can usually be obtained much faster than with standard, exhaustive model checking techniques. We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink: a fuel control system featuring hybrid behavior and fault tolerance. We show that our technique enables faster verification than state-of-the-art statistical techniques. We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models. It is in principle applicable to a variety of stochastic models from other domains, e.g., systems biology."},
{"Title": "Refactoring, Refinement, and Reasoning", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-06410-9_33", "Full Abstract": "Refactoring of code is a common device in software engineering. As cyber-physical systems CPS become ever more complex, similar engineering practices become more common in CPS development. Proper safe developments of CPS designs are accompanied by a proof of correctness. Since the inherent complexities of CPS practically mandate iterative development, frequent changes of models are standard practice, but require reverification of the resulting models after every change."},
{"Title": "A Hierarchy of Proof Rules for Checking Differential Invariance of Algebraic Sets", "URL": "https://dl.acm.org/doi/10.1007/978-3-662-46081-8_24", "Full Abstract": "This paper presents a theoretical and experimental comparison of sound proof rules for proving invariance of algebraic sets, that is, sets satisfying polynomial equalities, under the flow of polynomial ordinary differential equations. Problems of this nature arise in formal verification of continuous and hybrid dynamical systems, where there is an increasing need for methods to expedite formal proofs. We study the trade-off between proof rule generality and practical performance and evaluate our theoretical observations on a set of heterogeneous benchmarks. The relationship between increased deductive power and running time performance of the proof rules is far from obvious; we discuss and illustrate certain classes of problems where this relationship is interesting."},
{"Title": "A Formally Verified Hybrid System for the Next-Generation Airborne Collision Avoidance System", "URL": "https://dl.acm.org/doi/10.1007/978-3-662-46681-0_2", "Full Abstract": "The"},
{"Title": "Differential Dynamic Logic for Verifying Parametric Hybrid Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-73099-6_17", "Full Abstract": "We introduce a first-order dynamic logic for reasoning about systems with discrete and continuous state transitions, and we present a sequent calculus for this logic. As a uniform model, our logic supports hybrid programs with discrete and differential actions. For handling real arithmetic during proofs, we lift quantifier elimination to dynamic logic. To obtain a modular combination, we use side deductions for verifying interacting dynamics. With this, our logic supports deductive verification of hybrid systems with symbolic parameters and first-order definable flows. Using our calculus, we prove a parametric inductive safety constraint for speed supervision in a train control system."},
{"Title": "Logical Verification and Systematic Parametric Analysis in Train Control", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-78929-1_55", "Full Abstract": "We formally verify hybrid safety properties of cooperation protocols in a fully parametric version of the"},
{"Title": "Computing Differential Invariants of Hybrid Systems as Fixedpoints", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-70545-1_17", "Full Abstract": "We introduce a fixedpoint algorithm for verifying safety properties of hybrid systems with differential equations whose right-hand sides are polynomials in the state variables. In order to verify nontrivial systems without solving their differential equations and without numerical errors, we use a continuous generalization of induction, for which our algorithm computes the required <em>differential invariants</em>. As a means for combining local differential invariants into global system invariants in a sound way, our fixedpoint algorithm works with a compositional verification logic for hybrid systems. To improve the verification power, we further introduce a <em>saturation procedure</em>that refines the system dynamics successively with differential invariants until safety becomes provable. By complementing our symbolic verification algorithm with a robust version of numerical falsification, we obtain a fast and sound verification procedure. We verify roundabout maneuvers in air traffic management and collision avoidance in train control."},
{"Title": "Differential Dynamic Logic for Hybrid Systems", "URL": "https://dl.acm.org/doi/10.1007/s10817-008-9103-8", "Full Abstract": "Hybrid systems are models for complex physical systems and are defined as dynamical systems with interacting discrete transitions and continuous evolutions along differential equations. With the goal of developing a theoretical and practical foundation for deductive verification of hybrid systems, we introduce a dynamic logic for hybrid programs, which is a program notation for hybrid systems. As a verification technique that is suitable for automation, we introduce a free variable proof calculus with a novel combination of real-valued free variables and Skolemisation for lifting quantifier elimination for real arithmetic to dynamic logic. The calculus is compositional, i.e., it reduces properties of hybrid programs to properties of their parts. Our main result proves that this calculus axiomatises the transition behaviour of hybrid systems completely relative to differential equations. In a case study with cooperating traffic agents of the European Train Control System, we further show that our calculus is well-suited for verifying realistic hybrid systems with parametric system dynamics."},
{"Title": "KeYmaera", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-71070-7_15", "Full Abstract": "KeYmaera is a hybrid verification tool for hybrid systems that combines deductive, real algebraic, and computer algebraic prover technologies. It is an automated and interactive theorem prover for a natural specification and verification logic for hybrid systems. KeYmaera supports <em>differential dynamic logic</em>, which is a real-valued first-order dynamic logic for hybrid programs, a program notation for hybrid automata. For automating the verification process, KeYmaera implements a generalized free-variable sequent calculus and automatic proof strategies that decompose the hybrid system specification symbolically. To overcome the complexity of real arithmetic, we integrate real quantifier elimination following an iterative background closure strategy. Our tool is particularly suitable for verifying parametric hybrid systems and has been used successfully for verifying collision avoidance in case studies from train control and air traffic management."},
{"Title": "Differential dynamic logics", "URL": "https://dl.acm.org/doi/book/10.5555/2073238", "Full Abstract": "Hybrid systems are models for complex physical systems and are defined as dynamical systems with interacting discrete transitions and continuous evolutions along differential equations. With the goal of developing a theoretical and practical foundation for deductive verification of hybrid systems, we introduce differential dynamic logic as a new logic with which correctness properties of hybrid systems with parameterized system dynamics can be specified and verified naturally. As a verification technique that is suitable for automation, we introduce a free variable proof calculus with a novel combination of real-valued free variables and Skolemisation for lifting quantifier elimination for real arithmetic to dynamic logic. The calculus is compositional, i.e., it reduces properties of hybrid systems successively to properties of their parts. Our main result proves that this calculus axiomatises the transition behaviour of hybrid systems completely relative to differential equations."},
{"Title": "Verification of Cyberphysical Transportation Systems", "URL": "https://dl.acm.org/doi/10.1109/MIS.2009.81", "Full Abstract": "Cyberphysical system technology has an important share in modern intelligent transportation systems, including next generation flight, rail, and car control. This control technology is intended to help improve performance objectives like throughput and improve overall system safety. To ensure that these transportation systems operate correctly, new analysis techniques are needed that consider physical movement combined with computational control to establish properties like collision freedom. Logic-based analysis can verify the correct functioning of these cyberphysical systems."},
{"Title": "Computing differential invariants of hybrid systems as fixedpoints", "URL": "https://dl.acm.org/doi/10.1007/s10703-009-0079-8", "Full Abstract": "We introduce a fixedpoint algorithm for verifying safety properties of hybrid systems with differential equations whose right-hand sides are polynomials in the state variables. In order to verify nontrivial systems without solving their differential equations and without numerical errors, we use a continuous generalization of induction, for which our algorithm computes the required"},
{"Title": "Differential-algebraic Dynamic Logic for Differential-algebraic Programs", "URL": "https://dl.acm.org/doi/10.1093/logcom/exn070", "Full Abstract": "We generalize dynamic logic to a logic for differential-algebraic (DA) programs, i.e. discrete programs augmented with first-order differential-algebraic formulas as continuous evolution constraints in addition to first-order discrete jump formulas. These programs characterize interacting discrete and continuous dynamics of hybrid systems elegantly and uniformly. For our logic, we introduce a calculus over real arithmetic with discrete induction and a new"},
{"Title": "Quantified differential dynamic logic for distributed hybrid systems", "URL": "https://dl.acm.org/doi/10.5555/1887459.1887497", "Full Abstract": "We address a fundamental mismatch between the combinations of dynamics that occur in complex physical systems and the limited kinds of dynamics supported in analysis. Modern applications combine communication, computation, and control. They may even form dynamic networks, where neither structure nor dimension stay the same while the system follows mixed discrete and continuous dynamics."},
{"Title": "Bayesian statistical model checking with application to Simulink/Stateflow verification", "URL": "https://dl.acm.org/doi/10.1145/1755952.1755987", "Full Abstract": "We address the problem of model checking stochastic systems, i.e.~checking whether a stochastic system satisfies a certain temporal property with a probability greater (or smaller) than a fixed threshold. In particular, we present a novel Statistical Model Checking (SMC) approach based on Bayesian statistics. We show that our approach is feasible for hybrid systems with stochastic transitions, a generalization of Simulink/Stateflow models. Standard approaches to stochastic (discrete) systems require numerical solutions for large optimization problems and quickly become infeasible with larger state spaces. Generalizations of these techniques to hybrid systems with stochastic effects are even more challenging. The SMC approach was pioneered by Younes and Simmons in the discrete and non-Bayesian case. It solves the verification problem by combining randomized sampling of system traces (which is very efficient for Simulink/Stateflow) with hypothesis testing or estimation. We believe SMC is essential for scaling up to large Stateflow/Simulink models. While the answer to the verification problem is not guaranteed to be correct, we prove that Bayesian SMC can make the probability of giving a wrong answer arbitrarily small. The advantage is that answers can usually be obtained much faster than with standard, exhaustive model checking techniques. We apply our Bayesian SMC approach to a representative example of stochastic discrete-time hybrid system models in Stateflow/Simulink: a fuel control system featuring hybrid behavior and fault tolerance. We show that our technique enables faster verification than state-of-the-art statistical techniques, while retaining the same error bounds. We emphasize that Bayesian SMC is by no means restricted to Stateflow/Simulink models: we have in fact successfully applied it to very large stochastic models from Systems Biology."},
{"Title": "European Train Control System", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-10373-5_13", "Full Abstract": "Complex physical systems have several degrees of freedom. They only work correctly when their control parameters obey corresponding constraints. Based on the informal specification of the <em>European Train Control System</em> (ETCS), we design a controller for its cooperation protocol. For its free parameters, we successively identify constraints that are required to ensure collision freedom. We formally prove the parameter constraints to be sharp by characterizing them equivalently in terms of reachability properties of the hybrid system dynamics. Using our deductive verification tool KeYmaera, we formally verify controllability, safety, liveness, and reactivity properties of the ETCS protocol that entail collision freedom. We prove that the ETCS protocol remains correct even in the presence of perturbation by disturbances in the dynamics. We verify that safety is preserved when a PI controlled speed supervision is used."},
{"Title": "Real World Verification", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02959-2_35", "Full Abstract": "Scalable handling of real arithmetic is a crucial part of the verification of hybrid systems, mathematical algorithms, and mixed analog/digital circuits. Despite substantial advances in verification technology, complexity issues with classical decision procedures are still a major obstacle for formal verification of real-world applications, e.g., in automotive and avionic industries. To identify strengths and weaknesses, we examine state of the art symbolic techniques and implementations for the universal fragment of real-closed fields: approaches based on quantifier elimination, Gröbner Bases, and semidefinite programming for the Positivstellensatz. Within a uniform context of the verification tool KeYmaera, we compare these approaches qualitatively and quantitatively on verification benchmarks from hybrid systems, textbook algorithms, and on geometric problems. Finally, we introduce a new decision procedure combining Gröbner Bases and semidefinite programming for the real Nullstellensatz that outperforms the individual approaches on an interesting set of problems."},
{"Title": "Logical Analysis of Hybrid Systems", "URL": "https://dl.acm.org/doi/book/10.5555/1869900", "Full Abstract": "Hybrid systems are models for complex physical systems and have become a widely used concept for understanding their behavior. Many applications are safety-critical, including car, railway, and air traffic control, robotics, physicalchemical process control, and biomedical devices. Hybrid systems analysis studies how we can build computerized controllers for physical systems which are guaranteed to meet their design goals. The author gives a unique, logic-based perspective on hybrid systems analysis. It is the first book that leverages the power of logic for hybrid systems. The author develops a coherent logical approach for systematic hybrid systems analysis, covering its theory, practice, and applications. It is further shown how the developed verification techniques can be used to study air traffic and railway control systems. This book is intended for researchers, postgraduates, and professionals who are interested in hybrid systems analysis, cyberphysical or embedded systems design, logic and theorem proving, or transportation and automation."},
{"Title": "Formal Verification of Curved Flight Collision Avoidance Maneuvers", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-05089-3_35", "Full Abstract": "Aircraft collision avoidance maneuvers are important and complex applications. Curved flight exhibits nontrivial continuous behavior. In combination with the control choices during air traffic maneuvers, this yields hybrid systems with challenging interactions of discrete and continuous dynamics. As a case study illustrating the use of a new proof assistant for a logic for nonlinear hybrid systems, we analyze collision freedom of roundabout maneuvers in air traffic control, where appropriate curved flight, good timing, and compatible maneuvering are crucial for guaranteeing safe spatial separation of aircraft throughout their flight. We show that formal verification of hybrid systems can scale to curved flight maneuvers required in aircraft control applications. We introduce a fully flyable variant of the roundabout collision avoidance maneuver and verify safety properties by compositional verification."},
{"Title": "A Bayesian Approach to Model Checking Biological Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-03845-7_15", "Full Abstract": "Recently, there has been considerable interest in the use of Model Checking for Systems Biology. Unfortunately, the state space of stochastic biological models is often too large for classical Model Checking techniques. For these models, a statistical approach to Model Checking has been shown to be an effective alternative. Extending our earlier work, we present the first algorithm for performing statistical Model Checking using Bayesian Sequential Hypothesis Testing. We show that our Bayesian approach outperforms current statistical Model Checking techniques, which rely on tests from Classical (aka Frequentist) statistics, by requiring fewer system simulations. Another advantage of our approach is the ability to incorporate prior Biological knowledge about the model being verified. We demonstrate our algorithm on a variety of models from the Systems Biology literature and show that it enables faster verification than state-of-the-art techniques, even when no prior knowledge is available."},
{"Title": "Dynamic logic with non-rigid functions", "URL": "https://dl.acm.org/doi/10.1007/11814771_23", "Full Abstract": "We introduce a dynamic logic that is enriched by non-rigid functions, i.e., functions that may change their value from state to state (during program execution), and we present a (relatively) complete sequent calculus for this logic. In conjunction with dynamically typed object enumerators, non-rigid functions allow to embed notions of object-orientation in dynamic logic, thereby forming a basis for verification of object-oriented programs. A semantical generalisation of substitutions, called state update, which we add to the logic, constitutes the central technical device for dealing with object aliasing during function modification. With these few extensions, our dynamic logic captures the essential aspects of the complex verification system KeY and, hence, constitutes a foundation for object-oriented verification with the principles of reasoning that underly the successful KeY case studies."},
{"Title": "Automating verification of cooperation, control, and design in traffic applications", "URL": "https://dl.acm.org/doi/10.5555/1793874.1793880", "Full Abstract": "We present a verification methodology for cooperating traffic agents covering analysis of cooperation strategies, realization of strategies through control, and implementation of control. For each layer, we provide dedicated approaches to formal verification of safety and stability properties of the design. The range of employed verification techniques invoked to span this verification space includes application of pre-verified design patterns, automatic synthesis of Lyapunov functions, constraint generation for parameterized designs, model-checking in rich theories, and abstraction refinement. We illustrate this approach with a variant of the European Train Control System (ETCS), employing layer specific verification techniques to layer specific views of an ETCS design."},
{"Title": "The image computation problem in hybrid systems model checking", "URL": "https://dl.acm.org/doi/10.5555/1760804.1760843", "Full Abstract": "In this paper, we analyze limits of approximation techniques for (non-linear) continuous image computation in model checking hybrid systems. In particular, we show that even a single step of continuous image computation is not semidecidable numerically even for a very restricted class of functions. Moreover, we show that symbolic insight about derivative bounds provides sufficient additional information for approximation refinement model checking. Finally, we prove that purely numerical algorithms can perform continuous image computation with arbitrarily high probability. Using these results, we analyze the prerequisites for a safe operation of the roundabout maneuver in air traffic collision avoidance."},
{"Title": "SAT-based Abstraction Refinement for Real-time Systems", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2006.09.034", "Full Abstract": "In this paper, we present an abstraction refinement approach for model checking safety properties of real-time systems using SAT-solving. We present a faithful embedding of bounded model checking for systems of timed automata into propositional logic with linear arithmetic and prove correctness. With this logical representation, we achieve a linear-size representation of parallel composition and introduce a quick abstraction technique that works uniformly for clocks, events, and states. When necessary, abstractions are refined by analysing spurious counterexamples using a promising extension of counterexample-guided abstraction refinement with syntactic information about Craig interpolants. To support generalisations, our overall approach identifies the algebraic and logical principles required for logic-based abstraction refinement."},
{"Title": "Differential logic for reasoning about hybrid systems", "URL": "https://dl.acm.org/doi/10.5555/1760804.1760882", "Full Abstract": "We propose a first-order dynamic logic for reasoning about hybrid systems. As a uniform model for discrete and continuous evolutions in hybrid systems, we introduce hybrid programs with differential actions. Our logic can be used to specify and verify correctness statements about hybrid programs, which are suitable for symbolic processing by calculus rules. Using first-order variables, our logic supports systems with symbolic parameters. With dynamic modalities, it is prepared to handle multiple system components."},
{"Title": "Towards a Hybrid Dynamic Logic for Hybrid Dynamic Systems", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2006.11.026", "Full Abstract": "We introduce a hybrid variant of a dynamic logic with continuous state transitions along differential equations, and we present a sequent calculus for this extended hybrid dynamic logic. With the addition of satisfaction operators, this hybrid logic provides improved system introspection by referring to properties of states during system evolution. In addition to this, our calculus introduces state-based reasoning as a paradigm for delaying expansion of transitions using nominals as symbolic state labels. With these extensions, our hybrid dynamic logic advances the capabilities for compositional reasoning about (semialgebraic) hybrid dynamic systems. Moreover, the constructive reasoning support for goal-oriented analytic verification of hybrid dynamic systems carries over from the base calculus to our extended calculus."},
{"Title": "A Temporal Dynamic Logic for Verifying Hybrid System Invariants", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-72734-7_32", "Full Abstract": "We combine first-order dynamic logic for reasoning about possible behaviour of hybrid systems with temporal logic for reasoning about the temporal behaviour during their operation. Our logic supports verification of hybrid programs with first-order definable flows and provides a uniform treatment of discrete and continuous evolution. For our combined logic, we generalise the semantics of dynamic modalities to refer to hybrid traces instead of final states. Further, we prove that this gives a conservative extension of dynamic logic. On this basis, we provide a modular verification calculus that reduces correctness of temporal behaviour of hybrid systems to non-temporal reasoning. Using this calculus, we analyse safety invariants in a train control system and symbolically synthesise parametric safety constraints."},
{"Title": "Quantified differential invariants", "URL": "https://dl.acm.org/doi/10.1145/1967701.1967713", "Full Abstract": "We address the verification problem for distributed hybrid systems with nontrivial dynamics. Consider air traffic collision avoidance maneuvers, for example. Verifying dynamic appearance of aircraft during an ongoing collision avoidance maneuver is a longstanding and essentially unsolved problem. The resulting systems are not hybrid systems and their state space is not of the form <bf>R</bf>"},
{"Title": "Adaptive cruise control", "URL": "https://dl.acm.org/doi/10.5555/2021296.2021304", "Full Abstract": "Car safety measures can be most effective when the cars on a street coordinate their control actions using distributed cooperative control. While each car optimizes its navigation planning locally to ensure the driver reaches his destination, all cars coordinate their actions in a distributed way in order to minimize the risk of safety hazards and collisions. These systems control the physical aspects of car movement using cyber technologies like local and remote sensor data and distributed V2V and V2I communication. They are thus cyber-physical systems. In this paper, we consider a distributed car control system that is inspired by the ambitions of the California PATH project, the CICAS system, SAFESPOT and PReVENT initiatives.We develop a formal model of a distributed car control system in which every car is controlled by adaptive cruise control. One of the major technical difficulties is that faithful models of distributed car control have both distributed systems and hybrid systems dynamics. They form distributed hybrid systems, which makes them very challenging for verification. In a formal proof system, we verify that the control model satisfies its main safety objective and guarantees collision freedom for arbitrarily many cars driving on a street, even if new cars enter the lane from on-ramps or multi-lane streets. The system we present is in many ways one of the most complicated cyber-physical systems that has ever been fully verified formally."},
{"Title": "Quantifier elimination over finite fields using Gröbner bases", "URL": "https://dl.acm.org/doi/10.5555/2022278.2022289", "Full Abstract": "We give an algebraic quantifier elimination algorithm for the first-order theory over any given finite field using Gröbner basis methods. The algorithm relies on the strong Nullstellensatz and properties of elimination ideals over finite fields. We analyze the theoretical complexity of the algorithm and show its application in the formal analysis of a biological controller model."},
{"Title": "Logic and compositional verification of hybrid systems", "URL": "https://dl.acm.org/doi/10.5555/2032305.2032309", "Full Abstract": "Hybrid systems are models for complex physical systems and have become a widely used concept for understanding their behavior. Many applications are safety-critical, including car, railway, and air traffic control, robotics, physical-chemical process control, and biomedical devices. Hybrid systems analysis studies how we can build computerised controllers for physical systems which are guaranteed to meet their design goals. The continuous dynamics of hybrid systems can be modeled by differential equations, the discrete dynamics by a combination of discrete state-transitions and conditional execution. The discrete and continuous dynamics interact to form hybrid systems, which makes them quite challenging for verification."},
{"Title": "Logic-Based Modeling Approaches for Qualitative and Hybrid Reasoning in Dynamic Spatial Systems", "URL": "https://dl.acm.org/doi/10.1145/2764901", "Full Abstract": "Autonomous agents that operate as components of dynamic spatial systems are becoming increasingly popular and mainstream. Applications can be found in consumer robotics, in road, rail, and air transportation, manufacturing, and military operations. Unfortunately, the approaches to modeling and analyzing the behavior of dynamic spatial systems are just as diverse as these application domains. In this article, we discuss reasoning approaches for the medium-term control of autonomous agents in dynamic spatial systems, which requires a sufficiently detailed description of the agent’s behavior and environment but may still be conducted in a qualitative manner. We survey logic-based qualitative and hybrid modeling and commonsense reasoning approaches with respect to their features for describing and analyzing dynamic spatial systems in general, and the actions of autonomous agents operating therein in particular. We introduce a conceptual reference model, which summarizes the current understanding of the characteristics of dynamic spatial systems based on a catalog of evaluation criteria derived from the model. We assess the modeling features provided by logic-based qualitative commonsense and hybrid approaches for projection, planning, simulation, and verification of dynamic spatial systems. We provide a comparative summary of the modeling features, discuss lessons learned, and introduce a research roadmap for integrating different approaches of dynamic spatial system analysis to achieve coverage of all required features."},
{"Title": "Verified Traffic Networks", "URL": "https://dl.acm.org/doi/10.1109/ITSC.2015.128", "Full Abstract": "We address the problem how high-fidelity verification results about the hybrid systems dynamics of cyber-physical flow systems can be provided at the scale of large (traffic) networks without prohibitive analytic cost. We propose the use of contracts for traffic flow components concisely capturing the conditions for a safe operation in the context of a traffic network. This reduces the analysis of flows in the full traffic network to simple arithmetic checks of the local compatibility of the traffic component contracts, while retaining higher-fidelity correctness guarantees of the global hybrid systems models that inherits from correct contracts of the hybrid system components. We evaluate our approach in a case study of a modular traffic network and a prototypical implementation in a model-based analysis and design tool for traffic flow networks."},
{"Title": "Proving hybrid systems", "URL": "https://dl.acm.org/doi/10.5555/2893529.2893530", "Full Abstract": "Cyber-physical systems (CPS) combine cyber aspects such as communication and computer control with physical aspects such as movement in space, which arise frequently in many safety-critical application domains, including aviation, automotive, railway, and robotics. But how can we ensure that these systems are guaranteed to meet their design goals, e.g., that an aircraft will not crash into another one?"},
{"Title": "Bellerophon: Tactical Theorem Proving for Hybrid Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-66107-0_14", "Full Abstract": "Hybrid systems combine discrete and continuous dynamics, which makes them attractive as models for systems that combine computer control with physical motion. Verification is undecidable for hybrid systems and challenging for many models and properties of practical interest. Thus, human interaction and insight are essential for verification. Interactive theorem provers seek to increase user productivity by allowing them to focus on those insights. We present a tactics language and library for hybrid systems verification, named Bellerophon, that provides a way to convey insights by programming hybrid systems proofs."},
{"Title": "Formal verification of obstacle avoidance and navigation of ground robots", "URL": "https://dl.acm.org/doi/10.1177/0278364917733549", "Full Abstract": "This article answers fundamental safety questions for ground robot navigation: under which circumstances does which control decision make a ground robot safely avoid obstacles__ __ Unsurprisingly, the answer depends on the exact formulation of the safety objective, as well as the physical capabilities and limitations of the robot and the obstacles. Because uncertainties about the exact future behavior of a robot's environment make this a challenging problem, we formally verify corresponding controllers and provide rigorous safety proofs justifying why the robots can never collide with the obstacle in the respective physical model. To account for ground robots in which different physical phenomena are important, we analyze a series of increasingly strong properties of controllers for increasingly rich dynamics and identify the impact that the additional model parameters have on the required safety margins. We analyze and formally verify: i static safety, which ensures that no collisions can happen with stationary obstacles; ii passive safety, which ensures that no collisions can happen with stationary or moving obstacles while the robot moves; iii the stronger passive-friendly safety, in which the robot further maintains sufficient maneuvering distance for obstacles to avoid collision as well; and iv passive orientation safety, which allows for imperfect sensor coverage of the robot, i.e., the robot is aware that not everything in its environment will be visible. We formally prove that safety can be guaranteed despite sensor uncertainty and actuator perturbation. We complement these provably correct safety properties with liveness properties: we prove that provably safe motion is flexible enough to let the robot navigate waypoints and pass intersections. To account for the mixed influence of discrete control decisions and the continuous physical motion of the ground robot, we develop corresponding hybrid system models and use differential dynamic logic theorem-proving techniques to formally verify their correctness. Since these models identify a broad range of conditions under which control decisions are provably safe, our results apply to any control algorithm for ground robots with the same dynamics. As a demonstration, we also synthesize provably correct runtime monitor conditions that check the compliance of any control algorithm with the verified control decisions."},
{"Title": "A formally verified hybrid system for safe advisories in the next-generation airborne collision avoidance system", "URL": "https://dl.acm.org/doi/10.1007/s10009-016-0434-1", "Full Abstract": "The Next-Generation Airborne Collision Avoidance System (ACAS X) is intended to be installed on all large aircraft to give advice to pilots and prevent mid-air collisions with other aircraft. It is currently being developed by the Federal Aviation Administration (FAA). In this paper, we determine the geometric configurations under which the advice given by ACAS X is safe under a precise set of assumptions and formally verify these configurations using hybrid systems theorem proving techniques. We consider subsequent advisories and show how to adapt our formal verification to take them into account. We examine the current version of the real ACAS X system and discuss some cases where our safety theorem conflicts with the actual advisory given by that version, demonstrating how formal hybrid systems proving approaches are helping to ensure the safety of ACAS X. Our approach is general and could also be used to identify unsafe advice issued by other collision avoidance systems or confirm their safety."},
{"Title": "Safe reinforcement learning via formal methods", "URL": "https://dl.acm.org/doi/10.5555/3504035.3504829", "Full Abstract": "Formal verification provides a high degree of confidence in safe system operation, but only if reality matches the verified model. Although a good model will be accurate most of the time, even the best models are incomplete. This is especially true in Cyber-Physical Systems because high-fidelity physical models of systems are expensive to develop and often intractable to verify. Conversely, reinforcement learning-based controllers are lauded for their flexibility in unmodeled environments, but do not provide guarantees of safe operation. This paper presents an approach for provably safe learning that provides the best of both worlds: the exploration and optimization capabilities of learning along with the safety guarantees of formal verification. Our main insight is that formal verification combined with verified runtime monitoring can ensure the safety of a learning agent. Verification results are preserved whenever learning agents limit exploration within the confounds of verified control choices as long as observed reality comports with the model used for off-line verification. When a model violation is detected, the agent abandons efficiency and instead attempts to learn a control strategy that guides the agent to a modeled portion of the state space."},
{"Title": "Security for the Mythical Air-Dropped Sensor Network", "URL": "https://dl.acm.org/doi/10.1109/ISCC.2006.143", "Full Abstract": "The research area of very large scale wireless sensor networks made of low-cost sensors is gaining a lot of interest as witnessed by the large number of published papers. The security aspects of such networks are addressed as well, and in particular many security papers investigating the security aspects of such networks make important assumptions about the capabilities of low-cost sensors. Consequently, the techniques proposed in the current literature to provide security properties for this low-cost wireless sensor networks are heavily shaped by such assumptions. In this position paper, we challenge such assumptions by presenting the results of an experiment we conducted using sensors representative of low cost units. And we show that the same security properties can be better provided using techniques based on application-specific knowledge, heuristics and statistical tests. Finally, we show that one of the most highly cited application scenarios to motivate such techniques, the air-dropped sensor network, is likely to be more a myth than a realistic scenario for low-cost sensors."},
{"Title": "RFID Malware", "URL": "https://dl.acm.org/doi/10.1109/MSP.2006.102", "Full Abstract": "On 15 March 2006, our research team at Vrije Universiteit published a paper about RFID malwareentitled \"Is Your Cat Infected with a Computer Virus?\" as well as a companion Web site(wwww.rfidvirus.org). Our paper introduced the concept of RFID malware and presented an accompanyingproof-of-concept RFID virus. It ultimately resulted in a huge amount of media attention; within 24 hours ofpresenting it at the Fourth Annual IEEE International Conference on Pervasive Computing and Communications(IEEE PerCom), we received more than 200 email messages. Amid this chaos, our research paper received theBest Paper Award for High Impact at the IEEE PerCom conference. In the months that followed, reports ofRFID malware prompted reactions from the RFID industry, the antivirus industry, and the US and Dutchgovernments. In this installment of Crypto Corner, we give our general impression of the paper's aftermath,addressing some important unanswered questions and distinguishing the truths from the myths about the workwe did."},
{"Title": "MINIX 3", "URL": "https://dl.acm.org/doi/10.1145/1151374.1151391", "Full Abstract": "Different kinds of people use computers now than several decades ago, but operating systems have not fully kept pace with this change. It is true that we have point-and-click GUIs now instead of command line interfaces, but the expectation of the average user is different from what it used to be, because the user is different. Thirty or 40 years ago, when operating systems began to solidify into their current form, almost all computer users were programmers, scientists, engineers, or similar professionals doing heavy-duty computation, and they cared a great deal about speed. Few teenagers and even fewer grandmothers spent hours a day behind their terminal. Early users expected the computer to crash often; reboots came as naturally as waiting for the neighborhood TV repairman to come replace the picture tube on their home TVs. All that has changed and operating systems need to change with the times."},
{"Title": "Taking Sensor Networks from the Lab to the Jungle", "URL": "https://dl.acm.org/doi/10.1109/MC.2006.280", "Full Abstract": "Researchers must address the systems aspects of wireless sensor networks."},
{"Title": "A wide-area Distribution Network for free software", "URL": "https://dl.acm.org/doi/10.1145/1151087.1151089", "Full Abstract": "The Globe Distribution Network (GDN) is an application for the efficient, worldwide distribution of freely redistributable software packages. Distribution is made efficient by encapsulating the software into special distributed objects which efficiently replicate themselves near to the downloading clients. The Globe Distribution Network takes a novel, optimistic approach to stop the illegal distribution of copyrighted and illicit material via the network. Instead of having moderators check the packages at upload time, illegal content is removed and its uploader's access to the network permanently revoked only when the violation is discovered. Other protective measures defend the GDN against internal and external attacks to its availability. By exploiting the replication of the software and using fault-tolerant server software, the Globe Distribution Network achieves high availability. A prototype implementation of the GDN is available from http://www.cs.vu.nl/globe/."},
{"Title": "Reorganizing UNIX for reliability", "URL": "https://dl.acm.org/doi/10.1007/11859802_8", "Full Abstract": "In this paper, we discuss the architecture of a modular UNIX-compatible operating system, MINIX3, that provides reliability beyond that of most other systems. With nearly the entire operating system running as a set of user-mode servers and drivers atop a minimal kernel, the system is fully compartmentalized."},
{"Title": "Distributed Systems", "URL": "https://dl.acm.org/doi/book/10.5555/1202502", "Full Abstract": "No abstract available."},
{"Title": "Construction of a Highly Dependable Operating System", "URL": "https://dl.acm.org/doi/10.1109/EDCC.2006.7", "Full Abstract": "It has been well established that most operating system crashes are due to bugs in device drivers. Because drivers are normally linked into the kernel address space, a buggy driver can wipe out kernel tables and bring the system crashing to a grinding halt. We have greatly mitigated this problem by reducing the kernel to an absolute minimum and running each driver as a separate, unprivileged user-mode process. In addition, we implemented a POSIX-conformant operating system, MINIX 3, as multiple user-mode servers. In this design, a server or driver failure no longer is fatal and does not require rebooting the computer. This paper discusses how we designed and implemented the system, which problems we encountered, and how we solved these problems. We also discuss the performance effects of our changes and evaluate how our multiserver design improves operating system dependability over monolithic designs."},
{"Title": "A platform for RFID security and privacy administration", "URL": "https://dl.acm.org/doi/10.5555/1267793.1267801", "Full Abstract": "This paper presents the design, implementation, and evaluation of the RFID Guardian, the first-ever unified platform for RFID security and privacy administration. The RFID Guardian resembles an \"RFID firewall,\" that monitors and controls access to RFID tags by combining a standard-issue RFID reader with unique RFID tag emulation capabilities. Our system provides a platform for both automated and coordinated usage of RFID security mechanisms, offering fine-grained control over RFID-based auditing, key management, access control, and authentication capabilities. We have prototyped the RFID Guardian using off-the-shelf components, and our experience has shown that active mobile devices are a valuable tool for managing the security of RFID tags in a variety of applications, including protecting low-cost tags that are unable to regulate their own usage."},
{"Title": "Guarding security sensitive content using confined mobile agents", "URL": "https://dl.acm.org/doi/10.1145/1244002.1244013", "Full Abstract": "Mobile code and mobile agents are generally associated with security vulnerabilities, rather than with increased security. This paper describes an approach in which mobile agents are"},
{"Title": "Failure Resilience for Device Drivers", "URL": "https://dl.acm.org/doi/10.1109/DSN.2007.46", "Full Abstract": "Studies have shown that device drivers and extensions contain 3-7 times more bugs than other operating system code and thus are more likely to fail. Therefore, we present a failure-resilient operating system design that can recover from dead drivers and other critical components--primarily through monitoring and replacing malfunctioning components on the fly--transparent to applications and without user intervention. This paper focuses on the post-mortem recovery procedure. We explain the working of our defect detection mechanism, the policy-driven recovery procedure, and post-restart reintegration of the components. Furthermore, we discuss the concrete steps taken to recover from network, block device, and character device driver failures. Finally, we evaluate our design using performance measurements, software fault-injection experiments, and an analysis of the reengineering effort."},
{"Title": "An IPC model for extended asymmetric trust", "URL": "https://dl.acm.org/doi/10.5555/1323140.1323155", "Full Abstract": "No abstract available."},
{"Title": "Design and implementation of a secure wide-area object middleware", "URL": "https://dl.acm.org/doi/10.1016/j.comnet.2006.11.008", "Full Abstract": "Wide-area service replication is becoming increasingly common, with the emergence of new operational models such as content delivery networks and computational grids. This paper describes the security architecture for Globe, an object-based middleware specifically designed to support dynamic replication of services over wide-area networks. Replication introduces a series of new security issues, including the need to restrict replica privileges with respect to method execution, and protection of distributed objects against malicious hosts running instances of their code. Our modular security design addresses these new threats, as well as a broad range of traditional ones, and is validated through a series of performance measurements. Additional contributions include a novel authentication mechanism specifically designed for wide-area deployment, which combines some of the best features of public key authentication protocols (reliance on an offline trusted third party in particular) with the computational efficiency characteristic to symmetric key schemes."},
{"Title": "Modern Operating Systems", "URL": "https://dl.acm.org/doi/book/10.5555/1410217", "Full Abstract": "For software development professionals and computer science students, Modern Operating Systems gives a solid conceptual overview of operating system design, including detailed case studies of Unix/Linux and Windows 2000. What makes an operating system modern? According to author Andrew Tanenbaum, it is the awareness of high-demand computer applications--primarily in the areas of multimedia, parallel and distributed computing, and security. The development of faster and more advanced hardware has driven progress in software, including enhancements to the operating system. It is one thing to run an old operating system on current hardware, and another to effectively leverage current hardware to best serve modern software applications. If you don't believe it, install Windows 3.0 on a modern PC and try surfing the Internet or burning a CD. Readers familiar with Tanenbaum's previous text, Operating Systems, know the author is a great proponent of simple design and hands-on experimentation. His earlier book came bundled with the source code for an operating system called Minux, a simple variant of Unix and the platform used by Linus Torvalds to develop Linux. Although this book does not come with any source code, he illustrates many of his points with code fragments (C, usually with Unix system calls). The first half of Modern Operating Systems focuses on traditional operating systems concepts: processes, deadlocks, memory management, I/O, and file systems. There is nothing groundbreaking in these early chapters, but all topics are well covered, each including sections on current research and a set of student problems. It is enlightening to read Tanenbaum's explanations of the design decisions made by past operating systems gurus, including his view that additional research on the problem of deadlocks is impractical except for \"keeping otherwise unemployed graph theorists off the streets.\" It is the second half of the book that differentiates itself from older operating systems texts. Here, each chapter describes an element of what constitutes a modern operating system--awareness of multimedia applications, multiple processors, computer networks, and a high level of security. The chapter on multimedia functionality focuses on such features as handling massive files and providing video-on-demand. Included in the discussion on multiprocessor platforms are clustered computers and distributed computing. Finally, the importance of security is discussed--a lively enumeration of the scores of ways operating systems can be vulnerable to attack, from password security to computer viruses and Internet worms. Included at the end of the book are case studies of two popular operating systems: Unix/Linux and Windows 2000. There is a bias toward the Unix/Linux approach, not surprising given the author's experience and academic bent, but this bias does not detract from Tanenbaum's analysis. Both operating systems are dissected, describing how each implements processes, file systems, memory management, and other operating system fundamentals. Tanenbaum's mantra is simple, accessible operating system design. Given that modern operating systems have extensive features, he is forced to reconcile physical size with simplicity. Toward this end, he makes frequent references to the Frederick Brooks classic The Mythical Man-Month for wisdom on managing large, complex software development projects. He finds both Windows 2000 and Unix/Linux guilty of being too complicated--with a particular skewering of Windows 2000 and its \"mammoth Win32 API.\" A primary culprit is the attempt to make operating systems more \"user-friendly,\" which Tanenbaum views as an excuse for bloated code. The solution is to have smart people, the smallest possible team, and well-defined interactions between various operating systems components. Future operating system design will benefit if the advice in this book is taken to heart. --Pete Ostenson"},
{"Title": "A Virtual Machine Based Information Flow Control System for Policy Enforcement", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2007.10.010", "Full Abstract": "The ability to enforce usage policies attached to data in a fine grained manner requires that the system be able to trace and control the flow of information within it. This paper presents the design and implementation of such an information flow control system, named Trishul, as a Java Virtual Machine. In particular we address the problem of tracing implicit information flow, which had not been resolved by previous run-time systems and the additional intricacies added on by the Java architecture. We argue that the security benefits offered by Trishul are substantial enough to counter-weigh the performance overhead of the system as shown by our experiments."},
{"Title": "Turning Teenagers into Stores", "URL": "https://dl.acm.org/doi/10.1109/MC.2008.66", "Full Abstract": "Paradiso is a prototype of a system that lets consumers contact content providers to buy songs and videos—and to buy optional content-resale rights. In essence, the scheme would turn customers into content distributors, provide wider reach, and free up content providers' bandwidth. However, such an architecture requires strict security precautions and interoperable digital rights management standards among player manufacturers and content providers."},
{"Title": "Enforcing DRM policies across applications", "URL": "https://dl.acm.org/doi/10.1145/1456520.1456535", "Full Abstract": "In this paper we present Trishul-UCON (T-UCON), a DRM system based on the UCON_ABC model. T-UCON is designed to be capable of enforcing not only application-specific policies, as any existing software-based DRM solution does, but also DRM policies across applications. This is achieved by binding the DRM policy only to the content it protects with no relations to the application(s) which will use this content. Furthermore, to guarantee that the policy is continuously enforced, we designed T-UCON as a JVM-based middleware that mediates the usage requests of any Java application to the protected content. Each request is granted or denied according to the content policy. We illustrate the unique features of T-UCON by using typical examples of DRM policies such as the"},
{"Title": "Countering IPC Threats in Multiserver Operating Systems (A Fundamental Requirement for Dependability)", "URL": "https://dl.acm.org/doi/10.1109/PRDC.2008.25", "Full Abstract": "Multiserver operating systems have great potential to improve dependability, but, paradoxically, are paired with inherently more complex interprocess communication (IPC). Several projects have attempted to run drivers and extensions in isolated protection domains, but a systematic way to deal with IPC threats posed by untrusted parties is not yet available in the literature. IPC is fundamental to the dependability of multiserver systems.In this paper, we present a classification of IPC threats in multiserver systems with unreliable and hostile senders and receivers, such as resource exhaustion, spoofing, and unauthorized access. We also introduce an extended asymmetric trust model, describing two new IPC vulnerabilities relating to caller blockage. Based on our classification of IPC threats we present the IPC defense mechanisms and architecture of MINIX 3."},
{"Title": "Commentary on Standard ML", "URL": "https://dl.acm.org/doi/book/10.5555/574829", "Full Abstract": "No abstract available."},
{"Title": "Operational and algebraic semantics of concurrent processes", "URL": "https://dl.acm.org/doi/10.5555/114891.114910", "Full Abstract": "No abstract available."},
{"Title": "Commentary on standard ML", "URL": "https://dl.acm.org/doi/book/10.5555/103021", "Full Abstract": "No abstract available."},
{"Title": "What Is An Object?", "URL": "https://dl.acm.org/doi/10.5555/646774.705719", "Full Abstract": "No abstract available."},
{"Title": "Concurrent Processes as Objects (Abstract)", "URL": "https://dl.acm.org/doi/10.5555/647542.730310", "Full Abstract": "No abstract available."},
{"Title": "Co-induction in relational semantics", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2891%2990033-X", "Full Abstract": "No abstract available."},
{"Title": "Modal Logics for Mobile Processes", "URL": "https://dl.acm.org/doi/10.5555/646726.703054", "Full Abstract": "No abstract available."},
{"Title": "A semantics for ML concurrency primitives", "URL": "https://dl.acm.org/doi/10.1145/143165.143191", "Full Abstract": "We present a set of concurrency primitives for Standard ML. We define these by giving the transitional semantics of a simple language. We prove that our semantics preserves the expected behaviour of sequential programs. We also show that we can define stores as processes, such that the representation has the same behaviour as a direct definition. These proofs are the first steps towards integrating our semantics with the full definition of Standard ML."},
{"Title": "A compositional protocol verification using relativized bisimulation", "URL": "https://dl.acm.org/doi/10.1016/0890-5401%2892%2990025-B", "Full Abstract": "No abstract available."},
{"Title": "Barbed Bisimulation", "URL": "https://dl.acm.org/doi/10.5555/646246.684864", "Full Abstract": "No abstract available."},
{"Title": "The Polyadic Pi-calculus (Abstract)", "URL": "https://dl.acm.org/doi/10.5555/646727.703200", "Full Abstract": "No abstract available."},
{"Title": "The Problem of ``Weak Bisimulation up to''", "URL": "https://dl.acm.org/doi/10.5555/646727.703207", "Full Abstract": "No abstract available."},
{"Title": "A calculus of mobile processes, I", "URL": "https://dl.acm.org/doi/10.1016/0890-5401%2892%2990008-4", "Full Abstract": "No abstract available."},
{"Title": "A calculus of mobile processes, II", "URL": "https://dl.acm.org/doi/10.1016/0890-5401%2892%2990009-5", "Full Abstract": "No abstract available."},
{"Title": "Elements of interaction", "URL": "https://dl.acm.org/doi/10.1145/151233.151240", "Full Abstract": "Copyright © 1993 ACM."},
{"Title": "Unique decomposition of processes", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2893%2990176-T", "Full Abstract": "No abstract available."},
{"Title": "Modal logics for mobile processes", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2893%2990156-N", "Full Abstract": "No abstract available."},
{"Title": "Modal logics for mobile processes", "URL": "https://dl.acm.org/doi/10.5555/153841.153848", "Full Abstract": "No abstract available."},
{"Title": "A theory of strict P-completeness", "URL": "https://dl.acm.org/doi/10.1007/BF01206637", "Full Abstract": "No abstract available."},
{"Title": "Interactive Proof Systems with Polynomially Bounded Strategies", "URL": "https://dl.acm.org/doi/10.1006/jcss.1995.1040", "Full Abstract": "Interactive proof systems in which the Prover is restricted to have a polynomial size strategy are investigated. The restriction of polynomial size computation tree, visible to the Prover, or logarithmically bounded number of coin flips by the Verifier guarantee a polynomial size strategy. The additional restriction of logarithmic space is also investigated. A main result of the paper is that interactive proof systems in which the Prover is restricted to a polynomial size strategy are equivalent to MA, Merlin-Arthur games, defined by Babai and Moran. Polynomial tree size is also equivalent to MA, but when logarithmic space is added as a restriction, the power of polynomial tree size reduces to NP. Logarithmically bounded number of coin flips are equivalent to MP, and when logarithmic space is added as a restriction, the power is not diminished. The proof that NP of or equal to IP (log-space, log-random-bits) illustrates an interesting application of the new \"fingerprinting\" method of Lipton. Public interactive proof systems which have polynomial size strategies are also investigated."},
{"Title": "Approximate solutions to problems in PSPACE", "URL": "https://dl.acm.org/doi/10.1145/202840.202841", "Full Abstract": "Copyright © 1995 Author."},
{"Title": "On approximation algorithms for hierarchical MAX-SAT", "URL": "https://dl.acm.org/doi/10.5555/829497.829789", "Full Abstract": "We prove upper and lower bounds on performance guarantees of approximation algorithms for the hierarchical MAX-SAT (H-MAX-SAT) problem. This problem is representative of an important class of PSPACE-hard problems involving graphs, Boolean formulas and other structures that are defined \"succinctly\". Our first result is that for some constant /spl epsiv/>1, it is PSPACE-hard to approximate the function H-MAX-SAT to within ratio /spl epsiv/. We obtain our result using a known characterization of PSPACE in terms of probabilistically checkable debate systems. As an immediate application, we obtain non-approximability results for functions on hierarchical graphs by combining our result with previously known approximation-preserving reductions to other problems. For example, it is PSPACE-hard to approximate H-MAX-CUT and H-MAX-INDEPENDENT-SET to within some constant factor. Our second result is that there is an efficient approximation algorithm for H-MAX-SAT with performance guarantee 2/3. The previous best bound claimed for this problem was 1/2. One new technique of our algorithm can be used to obtain approximation algorithms for other problems, such as hierarchical MAX-CUT, which are simpler than previously known algorithms and which have performance guarantees that match the previous best bounds."},
{"Title": "Probabilistically Checkable Debate Systems and Nonapproximabilityof PSPACE-Hard Functions", "URL": "https://dl.acm.org/doi/book/10.5555/866046", "Full Abstract": "We initiate an investigation of probabilistically checkable debate systems (PCDS), a natural generalization of probabilistically checkable proof systems (PCPS). A PCDS for a language L consists of a probabilistic polynomial-time verifier V and a debate between player 1, who claims that the input x is in L, and player 0, who claims that the input x is not in L. We show that there is a PCDS for L in which V flips O(log n) random coins and reads O(1) bits of the debate if and only if L is in PSPACE. This characterization of PSPACE is used to show that certain PSPACE-hard functions are as hard to approximate closely as they are to compute exactly."},
{"Title": "Asynchronous Analysis of Parallel Dynamic Programming Algorithms", "URL": "https://dl.acm.org/doi/10.1109/71.494636", "Full Abstract": "We examine a very simple asynchronous model of parallel computation that assumes the time to compute a task is random, following some probability distribution. The goal of this model is to capture the effects of unpredictable delays on processors, due to communication delays or cache misses, for example. Using techniques from queueing theory and occupancy problems, we use this model to analyze two parallel dynamic programming algorithms. We show that this model is simple to analyze and correctly predicts which algorithm will perform better in practice. The algorithms we consider are a pipeline algorithm, where each processor i computes in order the entries of rows i, i + p, and so on, where p is the number of processors; and a diagonal algorithm, where entries along each diagonal extending from the left to the top of the table are computed in turn. It is likely that the techniques used here can be useful in the analysis of other algorithms that use barriers or pipelining techniques."},
{"Title": "Parallel Implementation of Borvka's Minimum Spanning Tree Algorithm", "URL": "https://dl.acm.org/doi/10.5555/645606.661036", "Full Abstract": "We study parallel algorithms for the minimum spanning tree problem, based on the sequential algorithm of Boruvka. The target architectures for our algorithm are asynchronous, distributed-memory machines. Analysis of our parallel algorithm, on a simple model that is reminiscent of the LogP model, shows that in principle a speedup proportional to the number of processors can be achieved, but that communication costs can be significant. To reduce these costs, we develop a new randomized linear work pointer jumping scheme that performs better than previous linear work algorithms. We also consider empirically the effects of data imbalance on the running time. For the graphs used in our experiments, load balancing schemes result in little improvement in running times. Our implementations on sparse graphs with 64,000 vertices on Thinking Machine's CM-5 achieve a speedup factor of about 4 on 16 processors. On this environment, packaging of messages turns out to be the most effective way to reduce communication costs."},
{"Title": "DNA models and algorithms for NP-complete problems", "URL": "https://dl.acm.org/doi/10.5555/791229.792249", "Full Abstract": "A goal of research on DNA computing is to solve problems that are beyond the capabilities of the fastest silicon-based supercomputers. Adleman and Lipton present exhaustive search algorithms for 3Sat and 3-Coloring, which can only be run on small instances and hence are not practical. In this paper, we show how improved algorithms can be developed for the 3-Coloring and Independent Set problems. Our algorithms use only the DNA operations proposed by Adleman and Lipton, but combine them in more powerful ways, and use polynomial preprocessing on a standard computer to tailor them to the specific instance to be solved. The main contribution of this paper is a more general model of DNA algorithms than that proposed by Lipton. We show that DNA computation for NP-complete problems can do more than just exhaustive search. Further research in this direction will help determine whether or not DNA computing is viable for NP-hard problems. A second contribution is the first analysis of errors that arise in generating the solution space for DNA computation."},
{"Title": "Complexity of Sub-Bus Mesh Computations", "URL": "https://dl.acm.org/doi/10.1137/S0097539793256879", "Full Abstract": "The time complexity of several fundamental problems on the sub-bus mesh parallel computer with $p$ processors is investigated. The problems include computing the PARITY and MAJORITY of $p$ bits, the SUM of $p$ numbers of length $O(\\log p)$, and the MINIMUM of $p$ numbers. It is shown that in one dimension the time to compute any of these problems is $\\Theta(\\log p)$. In two dimensions the time to compute any of PARITY, MAJORITY, and SUM is $\\Theta(\\frac{\\log p{\\log\\log p)$. It was previously shown that the time to compute MINIMUM in two dimensions is $\\Theta(\\log\\log p)$ [R. Miller et al.,"},
{"Title": "The power of surface-based DNA computation (extended abstract)", "URL": "https://dl.acm.org/doi/10.1145/267521.267530", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "Random Debaters and the Hardness of Approximating Stochastic Functions", "URL": "https://dl.acm.org/doi/10.1137/S0097539793260738", "Full Abstract": "A"},
{"Title": "Strategic directions in research in theory of computing", "URL": "https://dl.acm.org/doi/10.1145/262301.262310", "Full Abstract": "This report focuses oll two core areas of theory of computing: discrete algorithms and computational complexity theory. The report reviews the purposes and goals of theoretical research, summarizes selected past and recent achievements, explains the importance of sustaining core research, and identifies promising opportunities for filture research. Some research opportunities build bridges between theory of computing and other areas of computer science, and other science and engineering disciplines."},
{"Title": "On Approximation Algorithms for Hierarchical MAX-SAT", "URL": "https://dl.acm.org/doi/10.1006/jagm.1997.0902", "Full Abstract": "We prove upper and lower bounds on performance guarantees of approximation algorithms for the hierarchical MAX-SAT (H-MAX-SAT) problem. This problem is representative of a broad class of PSPACE-hard problems involving graphs, Boolean formulas, and other structures that are defined succinctly.Our first result is that, for some constant <1, it is PSPACE-hard to approximate the function H-MAX-SAT to within ratio . We obtain our result using a reduction from the language recognition problem for a model of PSPACE called the probabilistically checkable debate system. As an immediate application, we obtain nonapproximability results for functions on hierarchical graphs by combining our result with previously known approximation-preserving reductions to other problems. For example, it is PSPACE-hard to approximate H-MAX-CUT and H-MAX-INDEPENDENT-SET to within some constant factor.Our second result is that there is an efficient approximation algorithm for H-MAX-SAT with performance guarantee 2/3. The previous best bound claimed for this problem was 1/2. One new technique of our algorithm can be used to obtain approximation algorithms for other problems, such as hierarchical MAX-CUT, which are simpler than previously known algorithms and which have performance guarantees that match the previous best bounds."},
{"Title": "On the Power of Finite Automata with both Nondeterministic and Probabilistic States", "URL": "https://dl.acm.org/doi/10.1137/S0097539794265578", "Full Abstract": "We study finite automata with both nondeterministic and random states (npfa's). We restrict our attention to those npfa's that accept their languages with a small probability of error and run in polynomial expected time. Equivalently, we study Arthur--Merlin games where Arthur is limited to polynomial time and constant space."},
{"Title": "Lamport clocks", "URL": "https://dl.acm.org/doi/10.1145/277651.277672", "Full Abstract": "Copyright © 1998 ACM."},
{"Title": "DNA Models and Algorithms for NP-Complete Problems", "URL": "https://dl.acm.org/doi/10.1006/jcss.1998.1586", "Full Abstract": "A goal of research on DNA computing is to solve problems that are beyond the capabilities of the fastest silicon-based supercomputers. Adleman and Lipton present exhaustive search algorithms for 3Sat and 3-coloring, which can only be run on small instances and, hence, are not practical. In this paper, we show how improved algorithms can be developed for the 3-coloring and independent set problems. Our algorithms use only the DNA operations proposed by Adleman and Lipton, but combine them in more powerful ways and use polynomial preprocessing on a standard computer to tailor them to the specific instance to be solved. The main contribution of this paper is a more general model of DNA algorithms than that proposed by Lipton. We show that DNA computation for NP-complete problems can do more than just exhaustive search. Further research in this direction will help determine whether or not DNA computing is viable for NP-hard problems. A second contribution is the first analysis of errors that arise in generating the solution space for DNA computation."},
{"Title": "Using Lamport Clocks to Reason About Relaxed Memory Models", "URL": "https://dl.acm.org/doi/10.5555/520549.822782", "Full Abstract": "No abstract available."},
{"Title": "A system-level specification framework for I/O architectures", "URL": "https://dl.acm.org/doi/10.1145/305619.305634", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Algorithms for Graph Partitioning on the Planted Partition Model", "URL": "https://dl.acm.org/doi/10.5555/646976.711555", "Full Abstract": "No abstract available."},
{"Title": "On the undecidability of probabilistic planning and infinite-horizon partially observable Markov decision problems", "URL": "https://dl.acm.org/doi/10.5555/315149.315395", "Full Abstract": "We investigate the computability of problems in probabilistic planning and partially observable infinite-horizon Markov decision processes. The undecidability of the string-existence problem for probabilistic finite automata is adapted to show that the following problem of plan existence in probabilistic planning is undecidable: given a probabilistic planning problem, determine whether there exists a plan with success probability exceeding a desirable threshold. Analogous policy-existence problems for partially observable infinite-horizon Markov decision processes under discounted and undiscounted total reward models, average-reward models, and state-avoidance models are all shown to be undecidable. The results apply to corresponding approximation problems as well."},
{"Title": "Verification of parameterized programs", "URL": "https://dl.acm.org/doi/10.5555/233976.233988", "Full Abstract": "No abstract available."},
{"Title": "Automatic Generation of Invariants and Assertions", "URL": "https://dl.acm.org/doi/10.5555/647484.760398", "Full Abstract": "No abstract available."},
{"Title": "Temporal verification of reactive systems", "URL": "https://dl.acm.org/doi/book/10.5555/211468", "Full Abstract": "No abstract available."},
{"Title": "STeP: The Stanford Temporal Prover (Educational Release) User''s Manual", "URL": "https://dl.acm.org/doi/book/10.5555/892587", "Full Abstract": "The STeP (Stanford Temporal Prover) system supports the computer-aided verification of reactive and real-time systems. It combines deductive methods with algorithmic techniques to allow the verification of a broad class of systems, including infinite-state systems and parameterized N-process programs. STeP provides the visual language of verification diagrams that allow the user to construct proofs hierarchically, starting from a high-level proof sketch. The availability of automatically generated bottom-up and top-down invariants and an integrated suite of decision procedures allow most verification conditions to be checked without user intervention."},
{"Title": "Generalized Temporal Verification Diagrams", "URL": "https://dl.acm.org/doi/10.5555/646833.708045", "Full Abstract": "No abstract available."},
{"Title": "Clocked Transition Systems", "URL": "https://dl.acm.org/doi/book/10.5555/892591", "Full Abstract": "This paper presents a new computational model for real-time systems, called the clocked transition system model. The model is a development of our previous timed transition model, where some of the changes are inspired by the model of timed automata. The new model leads to a simpler style of temporal specification and verification, requiring no extension of the temporal language. For verifying safety properties, we present a run-preserving reduction from the new real-time model to the untimed model of fair transition systems. This reduction allows the (re)use of safety verification methods and tools, developed for untimed reactive systems, for proving safety properties of real-time systems."},
{"Title": "Verifying clocked transition systems", "URL": "https://dl.acm.org/doi/10.5555/239587.239589", "Full Abstract": "No abstract available."},
{"Title": "STeP", "URL": "https://dl.acm.org/doi/10.5555/647765.735848", "Full Abstract": "No abstract available."},
{"Title": "Temporal Verification by Diagram Transformations", "URL": "https://dl.acm.org/doi/10.5555/647765.735983", "Full Abstract": "No abstract available."},
{"Title": "Deductive Model Checking", "URL": "https://dl.acm.org/doi/10.5555/647765.735990", "Full Abstract": "No abstract available."},
{"Title": "Verification of Clocked and Hybrid Systems", "URL": "https://dl.acm.org/doi/10.5555/647870.737275", "Full Abstract": "No abstract available."},
{"Title": "Hierarchical Verification Using Verification Diagrams", "URL": "https://dl.acm.org/doi/10.5555/646063.676473", "Full Abstract": "No abstract available."},
{"Title": "Automatic generation of invariants and intermediate assertions", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2896%2900191-0", "Full Abstract": "No abstract available."},
{"Title": "Hybrid Diagrams", "URL": "https://dl.acm.org/doi/10.5555/646512.695481", "Full Abstract": "No abstract available."},
{"Title": "Visual Verification of Reactive Systems", "URL": "https://dl.acm.org/doi/10.5555/646481.691436", "Full Abstract": "No abstract available."},
{"Title": "Deductive Verification of Real-Time Systems Using STeP", "URL": "https://dl.acm.org/doi/10.5555/648201.749120", "Full Abstract": "No abstract available."},
{"Title": "Deductive Verification of Modular Systems", "URL": "https://dl.acm.org/doi/10.5555/646738.701965", "Full Abstract": "Effective verification methods, both deductive and algorithmic, exist for the verification of global system properties. In this paper, we introduce a formal framework for the modular description and verification of parameterized fair transition systems. The framework allows us to apply existing global verification methods, such as verification rules and diagrams, in a modular setting. Transition systems and transition modules can be described by recursive module expressions, allowing the description of hierarchical systems of unbounded depth. Apart from the usual parallel composition, hiding and renaming operations, our module description language provides constructs to augment and restrict the module interface, capablilities that are essential for recursive descriptions. We present proof rules for property inheritance between modules. Finally, module abstraction and induction allow the verification of recursively defined systems. Our approach is illustrated with a recursively defined arbiter for which we verify mutual exclusion and eventual access."},
{"Title": "Abstraction and Modular Verification of Infinite-State Reactive Systems", "URL": "https://dl.acm.org/doi/10.5555/645866.670769", "Full Abstract": "No abstract available."},
{"Title": "Deductive Verification of Hybrid Systems Using STeP", "URL": "https://dl.acm.org/doi/10.5555/646878.710285", "Full Abstract": "No abstract available."},
{"Title": "Decomposing, Transforming and Composing Diagrams: The Joys of Modular Verification", "URL": "https://dl.acm.org/doi/book/10.5555/892633", "Full Abstract": "The paper proposes a modular framework for the verification of temporal logic properties of systems based on the deductive transformation and composition of diagrams. The diagrams represent abstractions of the modules composing the system, together with information about the environment of the modules. The proof of a temporal specification is constructed with the help of diagram transformation and composition rules, which enable the gradual decomposition of the system into manageable modules, the study of the modules, and the final combination of the diagrams into a proof of the specification. We illustrate our methodology with the modular verification of a database demarcation protocol."},
{"Title": "THE REDUCTION METHOD FOR ESTABLISHING LOWER BOUNDS ON THE NUMBER OF ADDITIONS", "URL": "https://dl.acm.org/doi/book/10.5555/889600", "Full Abstract": "No abstract available."},
{"Title": "Addition Requirements for Rational Functions", "URL": "https://dl.acm.org/doi/book/10.5555/867356", "Full Abstract": "A notion of rank or independence for arbitrary sets of rational functions is developed, which bounds from below the number of additions and subtractions required of all straight-line algorithms which compute those functions. This permits a uniform derivation of the best lower bounds known for a number of familiar sets of rational functions. The result is proved without the use of substitution arguments. This not only provides an interesting contrast to standard approaches for arithmetic lower bounds, but also allows the algebraic setting to be somewhat generalized. Keywords: additions, algorithms, analysis of algorithms, arithmetic complexity, computational complexity, dimensionality, lower bounds, matrix multiplication, optimality, polynomials, rational functions."},
{"Title": "Addition Requirements for Rational Functions", "URL": "https://dl.acm.org/doi/10.1137/0206015", "Full Abstract": "A notion of rank or independence for arbitrary sets of rational functions is developed, which bounds from below the number of additions and subtractions required of all straight-line algorithms which compute those functions. This permits a uniform derivation of the best lower bounds known for a number of familiar sets of rational functions."},
{"Title": "Optimal surface reconstruction from planar contours", "URL": "https://dl.acm.org/doi/10.1145/563858.563899", "Full Abstract": "No abstract available."},
{"Title": "Optimal surface reconstruction from planar contours", "URL": "https://dl.acm.org/doi/10.1145/359842.359846", "Full Abstract": "In many scientific and technical endeavors, a three-dimensional solid must be reconstructed from serial sections, either to aid in the comprehension of the object's structure or to facilitate its automatic manipulation and analysis. This paper presents a general solution to the problem of constructing a surface over a set of cross-sectional contours. This surface, to be composed of triangular tiles, is constructed by separately determining an optimal surface between each pair of consecutive contours. Determining such a surface is reduced to the problem of finding certain minimum cost cycles in a directed toroidal graph. A new fast algorithm for finding such cycles is utilized. Also developed is a closed-form expression, in terms of the number of contour points, for an upper bound on the number of operations required to execute the algorithm. An illustrated example which involves the construction of a minimum area surface describing a human head is included."},
{"Title": "The “highly intelligent” tablet as an efficient pointing device for interactive graphics (Preliminary Report)", "URL": "https://dl.acm.org/doi/10.1145/800178.810125", "Full Abstract": "Described is a simple, efficient algorithm for determining the nearest displayed point on a screen to an arbitrary cursor position. The algorithm seems particularly appropriate for interactive systems using a data tablet with a “smart” controller. The algorithm is based on partitioning the screen among the currently displayed points and minimally modifing this structure as points are added and deleted. Finding the nearest point for cursor position consists then of moving through this partitioning structure until the region is determined. A divide-and-conquer method is used for both inclusion testing in a particular region and also for speeding the search for the proper nearest point."},
{"Title": "Combining Dimensionality and Rate of Growth Arguments for Establishing Lower Bounds on the Number of Multiplications and Divisions", "URL": "https://dl.acm.org/doi/10.1145/322139.322153", "Full Abstract": "Copyright © 1979 ACM."},
{"Title": "Predetermining visibility priority in 3-D scenes (Preliminary Report)", "URL": "https://dl.acm.org/doi/10.1145/800249.807441", "Full Abstract": "The principal calculation performed by all visible surface algorithms is the determination of the visible polygon at each pixel in the image. Of the many possible speedups and efficiencies found for this problem, only one published algorithm (developed almost a decade ago by a group at General Electric) took advantage of an observation that many visibility calculations could be performed without knowledge of the eventual viewing position and orientation—"},
{"Title": "Controlling concurrency using locking protocols", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1979.12", "Full Abstract": "This paper is concerned with the problem of developing locking protocols for ensuring the consistency of database systems that are accessed concurrently by a number of independent transactions. It is assumed that the database is modelled by a directed acyclic graph whose vertices correspond to the database entities, and whose arcs correspond to certain locking restrictions. Several locking protocols are presented. The weak protocol is shown to ensure consistency and deadlock-freedom only for databases that are organized as trees. For the databases that are organized as directed acyclic graphs, the strong protocol is presented. Discussion of SHARED and EXCLUSIVE locks is also included."},
{"Title": "Consistency in Hierarchical Database Systems", "URL": "https://dl.acm.org/doi/10.1145/322169.322176", "Full Abstract": "Copyright © 1980 ACM."},
{"Title": "On visible surface generation by a priori tree structures", "URL": "https://dl.acm.org/doi/10.1145/800250.807481", "Full Abstract": "This paper describes a new algorithm for solving the hidden surface (or line) problem, to more rapidly generate realistic images of 3-D scenes composed of polygons, and presents the development of theoretical foundations in the area as well as additional related algorithms. As in many applications the environment to be displayed consists of polygons many of whose relative geometric relations are static, we attempt to capitalize on this by preprocessing the environment's database so as to decrease the run-time computations required to generate a scene. This preprocessing is based on generating a “binary space partitioning” tree whose in order traversal of visibility priority at run-time will produce a linear order, dependent upon the viewing position, on (parts of) the polygons, which can then be used to easily solve the hidden surface problem. In the application where the entire environment is static with only the viewing-position changing, as is common in simulation, the results presented will be sufficient to solve completely the hidden surface problem."},
{"Title": "Non-two-phase locking protocols with shared and exclusive locks", "URL": "https://dl.acm.org/doi/10.5555/1286887.1286920", "Full Abstract": "This paper is concerned with the problem of developing a family of locking protocols which employ both SHARED and EXCLUSIVE locks and which ensure the consistency of database systems that are accessed concurrently by a number of asynchronously running transactions. The protocols in the family are not two-phase. They are applicable to database systems which are hierarchically organized as well as database systems which are modeled by directed acyclic graphs. A comparison with other previously published protocols is also presented."},
{"Title": "Deadlock removal using partial rollback in database systems", "URL": "https://dl.acm.org/doi/10.1145/582318.582329", "Full Abstract": "The problem of removing deadlocks from concurrent database systems using the two-phase locking protocol is considered. In particular, for systems which use no a priori information about transaction behavior in order to avoid deadlocks, it has generally been assumed necessary to totally remove and restart some transaction involved in a deadlock in order to relieve the situation. In this paper, a new approach to deadlock removal in such systems based on partial rollbacks is introduced. This approach does not in general require the total removal of a transaction to eliminate a deadlock. The task of optimizing deadlock removal using this method is discussed for systems allowing both exclusive and shared locking. A method is given for implementing this approach with no more storage overhead than that required for total removal and restart."},
{"Title": "A theory of correct locking protocols for database systems", "URL": "https://dl.acm.org/doi/10.5555/1286831.1286843", "Full Abstract": "In database systems which allow concurrent processing, it is necessary to control the interaction among the concurrent transactions in order to prevent them from destroying the consistency of the database. The most common mechanism proposed to achieve this involves the use of locking and unlocking instrucions to provide controlled access to units of shared data. These instructions are embedded in the transactions according to rules which are called locking protocols. A correct locking protocol assures that the consistency of the database is preserved and that no deadlocks occur to prevent termination of the transactions. In this paper, a theory is developed of how a priori syntactic information about the behavior of the transactions in a system can be used to construct correct protocols. The relationship between the problems of assuring correctness and deadlock-freedom is explored in a unified model which applies to systems which allow only exclusive locks as well as to systems which allow both exclusive and shared access nodes."},
{"Title": "Lower bounds for algebraic computation trees with integer inputs", "URL": "https://dl.acm.org/doi/10.1137/0220041", "Full Abstract": "No abstract available."},
{"Title": "Recent Progress in Circuit and Communication Complexity (Abstract)", "URL": "https://dl.acm.org/doi/10.5555/647895.740433", "Full Abstract": "No abstract available."},
{"Title": "Weighted Random Assignments with Application to Hashing", "URL": "https://dl.acm.org/doi/10.5555/648003.743107", "Full Abstract": "No abstract available."},
{"Title": "Linear decision trees", "URL": "https://dl.acm.org/doi/10.1145/129712.129730", "Full Abstract": "Copyright © 1992 ACM."},
{"Title": "A circuit-based proof of Toda's theorem", "URL": "https://dl.acm.org/doi/10.1006/inco.1993.1033", "Full Abstract": "No abstract available."},
{"Title": "Groups and Algebraic Complexity (Abstract)", "URL": "https://dl.acm.org/doi/10.5555/645929.672557", "Full Abstract": "No abstract available."},
{"Title": "Decision tree complexity and Betti numbers", "URL": "https://dl.acm.org/doi/10.1145/195058.195414", "Full Abstract": "Copyright © 1994 ACM."},
{"Title": "A randomized algorithm for finding maximum with ", "URL": "https://dl.acm.org/doi/10.1016/0020-0190%2894%2990052-3", "Full Abstract": "No abstract available."},
{"Title": "Near-Optimal Time-Space Tradeoff for Element Distinctness", "URL": "https://dl.acm.org/doi/10.1137/S0097539788148959", "Full Abstract": "It was conjectured in Borodin et al. ["},
{"Title": "On Computing Algebraic Functions using Logarithms andExponentials", "URL": "https://dl.acm.org/doi/10.1137/S0097539793245015", "Full Abstract": "Let $\\rho$ be a set of algebraic expressions constructed with radicals and arithmetic operations, and which generate the splitting field $F$ of some polynomial. Let $N_{\\beta(\\rho)$ be the minimum total number of root-takings and exponentiations used in any straightline program for computing the functions in $\\rho$ by taking roots, exponentials, logarithms, and performing arithmetic operations. In this paper it is proved that $N_{\\beta(\\rho) = \\nu(G)$, where $\\nu(G)$ is the minimum length of any cyclic Jordan--Holder tower for the Galois group $G$ of $F$. This generalizes a result of Ja'Ja' ["},
{"Title": "On the shrinkage exponent for read-once formulae", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2894%2900081-S", "Full Abstract": "No abstract available."},
{"Title": "Algebraic decision trees and Euler characteristics", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2894%2900082-T", "Full Abstract": "No abstract available."},
{"Title": "Security of quantum protocols against coherent measurements", "URL": "https://dl.acm.org/doi/10.1145/225058.225085", "Full Abstract": "Copyright © 1995 ACM."},
{"Title": "A Lower Bound on the Size of Algebraic Decision Trees for the MAX Problem", "URL": "https://dl.acm.org/doi/book/10.5555/895380", "Full Abstract": "We prove an exponential lower bound on the size of (ternary) algebraic decision trees for the MAX Problem of finding maximum of n real numbers. This complements $n-1$ lower bound (cf\\@. M. O. Rabin \\cite{R72) on the depth of algebraic decision trees for this problem. The method yields also for the first time a lower size bound for a polyhedral decision problem MAX= of testing whether the $ith$ number is the maximum among a list of n real numbers, and gives the first nonlinear size lower bound on algebraic decision trees for the selection problems."},
{"Title": "Hypergraphs and Decision Trees (Abstract)", "URL": "https://dl.acm.org/doi/10.5555/647677.731843", "Full Abstract": "No abstract available."},
{"Title": "Read-once branching programs, rectangular proofs of the pigeonhole principle and the transversal calculus", "URL": "https://dl.acm.org/doi/10.1145/258533.258673", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "Decision Tree Complexity and Betti Numbers", "URL": "https://dl.acm.org/doi/10.1006/jcss.1997.1495", "Full Abstract": "We show that any algebraic computation tree or any fixed-degree algebraic tree for solving the membership question of a compact setS Rnmust have height greater than (log( i(S))) cnfor eachi, where i(S) is theith Betti number. This generalizes a well-known result by Ben-Or who proved this lower bound for the casei=0, and a recent result by Bj rner and Lov sz who proved this lower bound for allifor linear decision trees."},
{"Title": "RAPID", "URL": "https://dl.acm.org/doi/10.1145/262839.262993", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "Dictionary Look-Up with One Error", "URL": "https://dl.acm.org/doi/10.1006/jagm.1997.0875", "Full Abstract": "LetWbe a set ofnbinary strings of lengthmeach. We are interested in designing data structures forWthat can answerd-queriesquickly; that is, given in a binary string, decide whether there is any member ofWwithin Hamming distancedof . The problem, originally raised by Minsky and Papert, remains a challenge in data structure design. In this paper, we make an initial effort toward a theoretical study of the smalldcase. Our main result is a data structure that achievesO(mloglogn) query time withO(nmlogm) space for thed=1 case."},
{"Title": "Interactive Editing Systems: Part I", "URL": "https://dl.acm.org/doi/10.1145/356887.356889", "Full Abstract": "Copyright © 1982 ACM."},
{"Title": "Interactive Editing Systems: Part II", "URL": "https://dl.acm.org/doi/10.1145/356887.356890", "Full Abstract": "Copyright © 1982 ACM."},
{"Title": "Fundamentals of interactive computer graphics", "URL": "https://dl.acm.org/doi/book/10.5555/6684", "Full Abstract": "No abstract available."},
{"Title": "Computer graphics in higher education (Panel Session)", "URL": "https://dl.acm.org/doi/10.1145/800059.801129", "Full Abstract": "Copyright © 1983 ACM."},
{"Title": "Meeting the Crisis in Computer Science", "URL": "https://dl.acm.org/doi/10.1109/MC.1983.1654271", "Full Abstract": "First Page of the Article"},
{"Title": "Meeting the crisis in computer science", "URL": "https://dl.acm.org/doi/10.1145/358476.358488", "Full Abstract": "Copyright © 1983 ACM."},
{"Title": "The electronic classroom", "URL": "https://dl.acm.org/doi/10.1145/800014.808141", "Full Abstract": "Continuing advances in hardware have made it possible to replace mainframe time-sharing systems (with their inherent performance limitations and poor user interfaces based on low-speed alphanumeric terminals) by powerful personal computers. When these personal computers have high-resolution bit-mapped graphics displays, fast processors, and virtual memory, and are linked in networks, they combine the best of dedicated computing (e.g., immediate response) with the best of time-sharing (e.g., resource-sharing of programs, data and peripherals). Personal computers so configured are typically called workstations. Such workstations have been introduced in the last few years primarily for professionals in productivity-sensitive areas like engineering design and office automation, but recent price/performance improvement has made it possible to consider them for use in education as well."},
{"Title": "Computer graphics comes of age", "URL": "https://dl.acm.org/doi/10.1145/358105.358190", "Full Abstract": "Copyright © 1984 ACM."},
{"Title": "The electronic classroom:  workstations for teaching", "URL": "https://dl.acm.org/doi/10.1016/S0020-7373%2884%2980053-X", "Full Abstract": "No abstract available."},
{"Title": "1984 Snowbird Report", "URL": "https://dl.acm.org/doi/10.1109/MC.1985.1662897", "Full Abstract": "First Page of the Article"},
{"Title": "Reading and Writing the Electronic Book", "URL": "https://dl.acm.org/doi/10.1109/MC.1985.1662710", "Full Abstract": "First Page of the Article"},
{"Title": "High performance graphics systems (panel session)", "URL": "https://dl.acm.org/doi/10.1145/320435.320518", "Full Abstract": "No abstract available."},
{"Title": "Pascal on the Macintosh: a graphical approach", "URL": "https://dl.acm.org/doi/book/10.5555/21671", "Full Abstract": "No abstract available."},
{"Title": "Hypertext '87: keynote address", "URL": "https://dl.acm.org/doi/10.1145/48511.48519", "Full Abstract": "Copyright © 1988 ACM."},
{"Title": "PHIGS+ functional description revision", "URL": "https://dl.acm.org/doi/10.1145/51683.51684", "Full Abstract": "This is a set of proposed extensions to the proposed PHIGS graphics standard (dpANS X3.144-198x. DIS 9592) to cover the areas of lighting, shading and advanced primitives which have thus far not been addressed by that standard. This document is organized to promote its eventual integration with the existing PHIGS documentation and is therefore not tutorial in nature. It assumes that the reader is familiar with PHIGS. with rendering and with curves and surfaces. This specification has been made available to standards bodies for their consideration."},
{"Title": "The Application Visualization System", "URL": "https://dl.acm.org/doi/10.1109/38.31462", "Full Abstract": "A software system for developing interactive scientific visualization applications quickly, with a minimum of programming effort, is described. This application visualization system (AVS) is an application framework targeted at scientists and engineers. The goal of the system is to make applications that combine interactive graphics and high computational requirements easier to develop for both programmers and nonprogrammers. AVS is designed around the concept of software building blocks, or modules, which can be interconnected to form visualization applications. AVS allows flow networks of existing modules to be constructed using a direct-manipulation user interface, and it automatically generates a simple user interface to each module."},
{"Title": "Simulation of a horizontal bit-sliced processor using the ISPS architecture simulation facility", "URL": "https://dl.acm.org/doi/10.5555/72832.72865", "Full Abstract": "No abstract available."},
{"Title": "Trends in Computer Graphics", "URL": "https://dl.acm.org/doi/10.5555/645819.669395", "Full Abstract": "No abstract available."},
{"Title": "Computer graphics: principles and practice (2nd ed.)", "URL": "https://dl.acm.org/doi/book/10.5555/83821", "Full Abstract": "No abstract available."},
{"Title": "An Object-Oriented Framework for the Integration of Interactive Animation Techniques", "URL": "https://dl.acm.org/doi/book/10.5555/864881", "Full Abstract": "We present an interactive modeling and animation system that facilitates the integration of a variety of simulation and animation paradigms. This system permits the modeling of diverse objects that change in shape, appearance, and behavior. Because all properties are functions of time, and because we integrate modeling and animation, we refer to our system as a ``4D modeler.'' Changes in properties can be effected by various methods of control, including scripted, gestural, and behavioral specification. The system is an extensible testbed that supports research in the interaction of disparate control methods. The paper discusses some of the issues in modeling such interactions and mechanisms implemented to provide solutions. An object-oriented architecture uses delegation hierarchies to exploit multiple inheritance and to permit objects to change all of their attributes dynamically, including object type. Objects include displayable objects, controllers, cameras, lights, renderers, and user interfaces. Further, the delegation system permits network distribution of objects and their associated computation. Additional ways to obtain real-time update speeds include the use of data dependency networks, extensive caching to exploit inter- and intra-frame coherency, and lazy evaluation."},
{"Title": "Potentials and limitations of fault-based Markov prefetching for virtual memory pages", "URL": "https://dl.acm.org/doi/10.1145/301453.301572", "Full Abstract": "No abstract available."},
{"Title": "On List Update and Work Function Algorithms", "URL": "https://dl.acm.org/doi/10.5555/647909.740306", "Full Abstract": "The list update problem, a well-studied problem in dynamic data structures, can be described abstractly as a metrical task system. In this paper, we prove that a generic metrical task system algorithm, called the work function algorithm, has constant competitive ratio for list update. In the process, we present a new formulation of the well-known \"list factoring\" technique in terms of a partial order on the elements of the list. This approach leads to a new simple proof that a large class of online algorithms, including Move-To-Front, is (2 - 1/"},
{"Title": "Balanced Allocations", "URL": "https://dl.acm.org/doi/10.1137/S0097539795288490", "Full Abstract": "Suppose that we sequentially place $n$ balls into"},
{"Title": "Organization-based analysis of web-object sharing and caching", "URL": "https://dl.acm.org/doi/10.5555/1251480.1251483", "Full Abstract": "Performance-enhancing mechanisms in the World Wide Web primarily exploit repeated requests to Web documents by multiple clients. However, little is known about patterns of shared document access, particularly from diverse client populations. The principal goal of this paper is to examine the sharing of Web documents from an organizational point of view. An organizational analysis of sharing is important, because caching is often performed on an organizational basis; i.e., proxies are typically placed in front of large and small companies, universities, departments, and so on. Unfortunately, simultaneous multi-organizational traces do not currently exist and are difficult to obtain in practice."},
{"Title": "On the scale and performance of cooperative Web proxy caching", "URL": "https://dl.acm.org/doi/10.1145/319151.319153", "Full Abstract": "While algorithms for cooperative proxy caching have been widely studied, little is understood about cooperative-caching performance in the large-scale World Wide Web environment. This paper uses both trace-based analysis and analytic modelling to show the potential advantages and drawbacks of inter-proxy cooperation. With our traces, we evaluate quantitatively the performance-improvement potential of cooperation between 200 small-organization proxies within a university environment, and between two large-organization proxies handling 23,000 and 60,000 clients, respectively. With our model, we extend beyond these populations to project cooperative caching behavior in regions with millions of clients. Overall, we demonstrate that cooperative caching has performance benefits only within limited population bounds. We also use our model to examine the implications of future trends in Web-access behavior and traffic."},
{"Title": "Near-Optimal Parallel Prefetching and Caching", "URL": "https://dl.acm.org/doi/10.1137/S0097539797326976", "Full Abstract": "Recently there has been a great deal of interest in the operating systems research community in prefetching and caching data from parallel disks, as a technique for enabling serial applications to improve input--output (I/O) performance. In this paper, algorithms are considered for integrated prefetching and caching in a model with a fixed-size cache and any number of backing storage devices (disks). The integration of caching and prefetching with a single disk was previously considered by Cao, Felten, Karlin, and Li. Here, it is shown that the natural extension of their"},
{"Title": "On the scale and performance of cooperative Web proxy caching", "URL": "https://dl.acm.org/doi/10.1145/346152.346166", "Full Abstract": "No abstract available."},
{"Title": "Markov Paging", "URL": "https://dl.acm.org/doi/10.1137/S0097539794268042", "Full Abstract": "This paper considers the problem of paging under the assumption that the sequence of pages accessed is generated by a Markov chain. We use this model to study the fault-rate of paging algorithms. We first draw on the theory of Markov decision processes to characterize the paging algorithm that achieves optimal fault-rate on any Markov chain. Next, we address the problem of devising a paging strategy with low fault-rate for a given Markov chain. We show that a number of intuitive approaches fail. Our main result is a polynomial-time procedure that, on any Markov chain, will give a paging algorithm with fault-rate at most a constant times optimal. Our techniques show also that some algorithms that do poorly in practice fail in the Markov setting, despite known (good) performance guarantees when the requests are generated independently from a probability distribution."},
{"Title": "Random walks with “back buttons” (extended abstract)", "URL": "https://dl.acm.org/doi/10.1145/335305.335362", "Full Abstract": "Copyright © 2000 ACM."},
{"Title": "Practical network support for IP traceback", "URL": "https://dl.acm.org/doi/10.1145/347059.347560", "Full Abstract": "This paper describes a technique for tracing anonymous packet flooding attacks in the Internet back towards their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or ``spoofed', source addresses. In this paper we describe a general purpose traceback mechanism based on probabilistic packet marking in the network. Our approach allows a victim to identify the network path(s) traversed by attack traffic without requiring interactive operational support from Internet Service Providers (ISPs). Moreover, this traceback can be performed ``post-mortem' -- after an attack has completed. We present an implementation of this technology that is incrementally deployable, (mostly) backwards compatible and can be efficiently implemented using conventional technology."},
{"Title": "Spectral Analysis for Data Mining", "URL": "https://dl.acm.org/doi/10.5555/646679.702315", "Full Abstract": "Experimental evidence suggests that spectral techniques are valuable for a wide range of applications. A partial list of such applications include (i) semantic analysis of documents used to cluster documents into areas of interest, (ii) collaborative filtering -- the reconstruction of missing data items, and (iii) determining the relative importance of documents based on citation/link structure. Intuitive arguments can explain some of the phenomena that has been observed but little theoretical study has been done. In this talk, we present a model for framing data mining tasks and a unified approach to solving the resulting data mining problems using spectral analysis. In particular we describe the solution to an open problem of Papadimitriou, Ragha van, Tamaki and Vempala in the context of modeling latent semantic indexing. We also give theoretical justification for the use of spectral algorithms for collaborative filtering, and show how a reasonable model of web links justifies the robustness of Kleinberg's web authority/hub algorithm. A major focus of the talk will be a description of the tight feedback loop between theory and empirical work and how it has led on this project to both new theory and new empirical questions of interest."},
{"Title": "On algorithms for efficient data migration", "URL": "https://dl.acm.org/doi/10.5555/365411.365549", "Full Abstract": "The"},
{"Title": "Network support for IP traceback", "URL": "https://dl.acm.org/doi/10.1109/90.929847", "Full Abstract": "This paper describes a technique for tracing anonymous packet flooding attacks in the Internet back toward their source. This work is motivated by the increased frequency and sophistication of denial-of-service attacks and by the difficulty in tracing packets with incorrect, or “spoofed,” source addresses. In this paper, we describe a general purpose traceback mechanism based on probabilistic packet marking in the network. Our approach allows a victim to identify the network path(s) traversed by attack traffic without requiring interactive operational support from Internet Service Providers (ISPs). Moreover, this traceback can be performed “post mortem”—after an attack has completed. We present an implementation of this technology that is incrementally deployable, (mostly) backward compatible, and can be efficiently implemented using conventional technology."},
{"Title": "Dynamic TCP acknowledgement and other stories about e/(e-1)", "URL": "https://dl.acm.org/doi/10.1145/380752.380845", "Full Abstract": "We present the first optimal randomized online algorithms for the TCP acknowledgment problem [5] and the Bahncard problem [7]. These problems are well-known to be generalizations of the classical online ski rental problem, however, they appeared to be harder. In this paper, we demonstrate that a number of online algorithms which have optimal competitive ratios of"},
{"Title": "Spectral analysis of data", "URL": "https://dl.acm.org/doi/10.1145/380752.380859", "Full Abstract": "Experimental evidence suggests that spectral techniques are valuable for a wide range of applications. A partial list of such applications include (i) semantic analysis of documents used to cluster documents into areas of interest, (ii) collaborative filtering --- the reconstruction of missing data items, and (iii) determining the relative importance of documents based on citation/link structure. Intuitive arguments can explain some of the phenomena that has been observed but little theoretical study has been done. In this paper we present a model for framing data mining tasks and a unified approach to solving the resulting data mining problems using spectral analysis. These results give strong justification to the use of spectral techniques for latent semantic indexing, collaborative filtering, and web site ranking."},
{"Title": "Web Search via Hub Synthesis", "URL": "https://dl.acm.org/doi/10.5555/646977.711699", "Full Abstract": "We present a probabilistic generative model for web search which captures in a unified manner three critical components of web search: how the link structure of the web is generated, how the content of a web document is generated, and how a human searcher generates a query. The key to this unification lies in capturing the correlations between each of these components in terms of proximity in latent semantic space. Given such a combined model, the correct answer to a search query is well defined, and thus it becomes possible to evaluate web search algorithms rigorously. We present a new web search algorithm, based on spectral techniques, and prove that it is guaranteed to produce an approximately correct answer in our model. The algorithm assumes no knowledge of the model, and is well-defined regardless of the accuracy of the model."},
{"Title": "An Experimental Study of Data Migration Algorithms", "URL": "https://dl.acm.org/doi/10.5555/647258.720796", "Full Abstract": "The data migration problem is the problem ofc omputing a plan for moving data objects stored on devices in a network from one configuration to another. Load balancing or changing usage patterns might necessitate such a rearrangement ofda ta. In this paper, we consider the case where the objects are fixed-size and the network is complete. We introduce two new data migration algorithms, one ofwh ich has provably good bounds. We empirically compare the performance of these new algorithms against similar algorithms from Hall et al. [7] which have better theoretical guarantees and find that in almost all cases, the new algorithms perform better. We also find that both the new algorithms and the ones from Hall et al. perform much better in practice than the theoretical bounds suggest."},
{"Title": "Web Search via Hub Synthesis", "URL": "https://dl.acm.org/doi/10.5555/874063.875586", "Full Abstract": "We present a model for web search that captures in a unified manner three critical components of the problem: how the link structure of the web is generated, how the contentof a web document is generated, and how a human searcher generates a query. The key to this unification lies in capturing the correlations between these components in terms of proximity in a shared latent semantic space. Given such a combined model, the correct answer to a search query is well defined, and thus it becomes possible to evaluate web search algorithms rigorously. We present a new web search algorithm, based on spectral techniques, and prove that it is guaranteed to produce an approximately correct answer in our model. The algorithm assumes no knowledge of the model, and is well-defined regardless of the model's accuracy."},
{"Title": "A quantitative evaluation of traffic-aware routing strategies", "URL": "https://dl.acm.org/doi/10.1145/510726.510741", "Full Abstract": "No abstract available."},
{"Title": "Dynamically Fault-Tolerant Content Addressable Networks", "URL": "https://dl.acm.org/doi/10.5555/646334.687807", "Full Abstract": "We describe a content addressable network which is robust in the face of massive adversarial attacks and in a highly dynamic environment. Our network is robust in the sense that at any time, an arbitrarily large fraction of the peers can reach an arbitrarily large fraction of the data items. The network can be created and maintained in a completely distributed fashion."},
{"Title": "Efficiently incorporating user feedback into information extraction and integration programs", "URL": "https://dl.acm.org/doi/10.1145/1559845.1559857", "Full Abstract": "Many applications increasingly employ information extraction and integration (IE/II) programs to infer structures from unstructured data. Automatic IE/II are inherently imprecise. Hence such programs often make many IE/II mistakes, and thus can significantly benefit from user feedback. Today, however, there is no good way to automatically provide and process such feedback. When finding an IE/II mistake, users often must alert the developer team (e.g., via email or Web form) about the mistake, and then wait for the team to manually examine the program internals to locate and fix the mistake, a slow, error-prone, and frustrating process."},
{"Title": "Optimizing complex extraction programs over evolving text data", "URL": "https://dl.acm.org/doi/10.1145/1559845.1559881", "Full Abstract": "Most information extraction (IE) approaches have considered only static text corpora, over which we apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and so to keep extracted information up to date we often must apply IE repeatedly, to consecutive corpus snapshots. Applying IE"},
{"Title": "Combining keyword search and forms for ad hoc querying of databases", "URL": "https://dl.acm.org/doi/10.1145/1559845.1559883", "Full Abstract": "A common criticism of database systems is that they are hard to query for users uncomfortable with a formal query language. To address this problem, form-based interfaces and keyword search have been proposed; while both have benefits, both also have limitations. In this paper, we investigate combining the two with the hopes of creating an approach that provides the best of both. Specifically, we propose to take as input a target database and then generate and index a set of query forms offline. At query time, a user with a question to be answered issues standard keyword search queries; but instead of returning tuples, the system returns forms relevant to the question. The user may then build a structured query with one of these forms and submit it back to the system for evaluation. In this paper, we address challenges that arise in form generation, keyword search over forms, and ranking and displaying these forms. We explore techniques to tackle these challenges, and present experimental results suggesting that the approach of combining keyword search and form-based interfaces is promising."},
{"Title": "Crowds, clouds, and algorithms", "URL": "https://dl.acm.org/doi/10.1145/1807167.1807341", "Full Abstract": "No abstract available."},
{"Title": "Toward scalable keyword search over relational data", "URL": "https://dl.acm.org/doi/10.14778/1920841.1920863", "Full Abstract": "Keyword search (KWS) over relational databases has recently received significant attention. Many solutions and many prototypes have been developed. This task requires addressing many issues, including robustness, accuracy, reliability, and privacy. An emerging issue, however, appears to be performance related: current KWS systems have unpredictable running times. In particular, for certain queries it takes too long to produce answers, and for others the system may even fail to return (e.g., after exhausting memory). In this paper we argue that as today's users have been \"spoiled\" by the performance of Internet search engines, KWS systems should return whatever answers they can produce quickly and then provide users with options for exploring any portion of the answer space not covered by these answers. Our basic idea is to produce answers that can be generated quickly as in today's KWS systems, then to show users query forms that characterize the unexplored portion of the answer space. Combining KWS systems with forms allows us to bypass the performance problems inherent to KWS without compromising query coverage. We provide a proof of concept for this proposed approach, and discuss the challenges encountered in building this hybrid system. Finally, we present experiments over real-world datasets to demonstrate the feasibility of the proposed solution."},
{"Title": "Tuffy", "URL": "https://dl.acm.org/doi/10.14778/1978665.1978669", "Full Abstract": "Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their widespread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets."},
{"Title": "Crowdsourcing systems on the World-Wide Web", "URL": "https://dl.acm.org/doi/10.1145/1924421.1924442", "Full Abstract": "The practice of crowdsourcing is transforming the Web and giving rise to a new field."},
{"Title": "Crowdsourcing applications and platforms", "URL": "https://dl.acm.org/doi/10.14778/3402755.3402809", "Full Abstract": "Over the past decade, crowdsourcing has emerged as a major problem-solving and data-gathering paradigm on the World-Wide Web. Well-known examples of crowdsourcing include Wikipedia, Linux, Yahoo! Answers, YouTube, Mechanical Turk-based applications, and much effort is being directed toward developing many more."},
{"Title": "Social media, data integration, and human computation", "URL": "https://dl.acm.org/doi/10.1145/2331801.2331810", "Full Abstract": "Social media has emerged as a major frontier on the World-Wide Web, with applications ranging from helping teenagers track Justin Bieber to e-commerce to fostering revolutions. In this talk I will discuss our work in this area, as carried out at Wisconsin, Kosmix, and @WalmartLabs. I describe how we integrate data from \"traditional\" Web sources to build a global taxonomy, greatly expand it with social-media data, then leverage it to build consumer-facing applications. Example applications include building topic pages, detecting Twitter events, and monitoring these events. I discuss the critical role of data integration and human computation in processing social media. Finally, I discuss how all of these can help the emerging area of social commerce, and why Walmart recently acquired Kosmix to make inroads into this new and exciting area."},
{"Title": "Principles of Data Integration", "URL": "https://dl.acm.org/doi/book/10.5555/2401764", "Full Abstract": "How do you approach answering queries when your data is stored in multiple databases that were designed independently by different people? This is first comprehensive book on data integration and is written by three of the most respected experts in the field. This book provides an extensive introduction to the theory and concepts underlying today's data integration techniques, with detailed, instruction for their application using concrete examples throughout to explain the concepts. Data integration is the problem of answering queries that span multiple data sources (e.g., databases, web pages). Data integration problems surface in multiple contexts, including enterprise information integration, query processing on the Web, coordination between government agencies and collaboration between scientists. In some cases, data integration is the key bottleneck to making progress in a field. The authors provide a working knowledge of data integration concepts and techniques, giving you the tools you need to develop a complete and concise package of algorithms and applications. *Offers a range of data integration solutions enabling you to focus on what is most relevant to the problem at hand. *Enables you to build your own algorithms and implement your own data integration applications *Companion website with numerous project-based exercises and solutions and slides. Links to commercially available software allowing readers to build their own algorithms and implement their own data integration applications. Facebook page for reader input during and after publication."},
{"Title": "Muppet", "URL": "https://dl.acm.org/doi/10.14778/2367502.2367520", "Full Abstract": "MapReduce has emerged as a popular method to process big data. In the past few years, however, not just big data, but fast data has also exploded in volume and availability. Examples of such data include sensor data streams, the Twitter Firehose, and Facebook updates. Numerous applications must process fast data. Can we provide a MapReduce-style framework so that developers can quickly write such applications and execute them over a cluster of machines, to achieve low latency and high scalability?"},
{"Title": "Building, maintaining, and using knowledge bases", "URL": "https://dl.acm.org/doi/10.1145/2463676.2465297", "Full Abstract": "A knowledge base (KB) contains a set of concepts, instances, and relationships. Over the past decade, numerous KBs have been built, and used to power a growing array of applications. Despite this flurry of activities, however, surprisingly little has been published about the end-to-end process of building, maintaining, and using such KBs in industry. In this paper we describe such a process. In particular, we describe how we build, update, and curate a large KB at Kosmix, a Bay Area startup, and later at WalmartLabs, a development and research lab of Walmart. We discuss how we use this KB to power a range of applications, including query understanding, Deep Web search, in-context advertising, event monitoring in social media, product search, social gifting, and social mining. Finally, we discuss how the KB team is organized, and the lessons learned. Our goal with this paper is to provide a real-world case study, and to contribute to the emerging direction of building, maintaining, and using knowledge bases for data management applications."},
{"Title": "Entity extraction, linking, classification, and tagging for social media", "URL": "https://dl.acm.org/doi/10.14778/2536222.2536237", "Full Abstract": "Many applications that process social data, such as tweets, must extract entities from tweets (e.g., \"Obama\" and \"Hawaii\" in \"Obama went to Hawaii\"), link them to entities in a knowledge base (e.g., Wikipedia), classify tweets into a set of predefined topics, and assign descriptive tags to tweets. Few solutions exist today to solve these problems for social data, and they are limited in important ways. Further, even though several industrial systems such as OpenCalais have been deployed to solve these problems for text data, little if any has been published about them, and it is unclear if any of the systems has been tailored for social media."},
{"Title": "Tracking entities in the dynamic world", "URL": "https://dl.acm.org/doi/10.14778/2732279.2732284", "Full Abstract": "Identifying records referring to the same real world entity over time enables longitudinal data analysis. However, difficulties arise from the dynamic nature of the world: the entities described by a temporal data set often evolve their states over time. While the state of the art approach to temporal entity matching achieves high accuracy, this approach is computationally expensive and cannot handle large data sets. In this paper, we present an approach that achieves equivalent matching accuracy but takes far less time. Our key insight is \"static first, dynamic second.\" Our approach first runs an evidence-collection pass, grouping records without considering the possibility of entity evolution, as if the world were \"static.\" Then, it merges clusters from the initial grouping by determining whether an entity might evolve from the state described in one cluster to the state described in another cluster. This intuitively reduces a difficult problem, record matching with evolution, to two simpler problems: record matching without evolution, then \"evolution detection\" among the resulting clusters. Experimental results on several temporal data sets show that our approach provides an order of magnitude improvement in run time over the state-of-the-art approach while producing equivalent matching accuracy."},
{"Title": "Modeling entity evolution for temporal record matching", "URL": "https://dl.acm.org/doi/10.1145/2588555.2588560", "Full Abstract": "Temporal record matching recognizes that if the entities represented by the records change over time, approaches that use temporal information may do better than approaches that do not. Any such temporal matching method relies at its heart on a temporal model that captures information about how entities evolve. In their pioneering work, Li {\\it et al. used an efficiently computable model that simply tries to predict if an attribute is expected to change over a given time interval. In our work, we propose and evaluate a more detailed model that focuses on the probability that a given attribute value reappears over time. The intuition here is that an entity might change its attribute value in the way that is dependent on its past values. In addition, our model considers sets of records (rather than simply pairs of records) to improve robustness and accuracy. Experimental results show that the resulting approach improves both accuracy and resistance to noise while incurring a minimal overhead."},
{"Title": "Corleone", "URL": "https://dl.acm.org/doi/10.1145/2588555.2588576", "Full Abstract": "Recent approaches to crowdsourcing entity matching (EM) are limited in that they crowdsource only parts of the EM workflow, requiring a developer to execute the remaining parts. Consequently, these approaches do not scale to the growing EM need at enterprises and crowdsourcing startups, and cannot handle scenarios where ordinary users (i.e., the masses) want to leverage crowdsourcing to match entities. In response, we propose the notion of hands-off crowdsourcing (HOC), which crowdsources the entire workflow of a task, thus requiring no developers. We show how HOC can represent a next logical direction for crowdsourcing research, scale up EM at enterprises and crowdsourcing startups, and open up crowdsourcing for the masses. We describe Corleone, a HOC solution for EM, which uses the crowd in all major steps of the EM process. Finally, we discuss the implications of our work to executing crowdsourced RDBMS joins, cleaning learning models, and soliciting complex information types from crowd workers."},
{"Title": "Chimera", "URL": "https://dl.acm.org/doi/10.14778/2733004.2733024", "Full Abstract": "Large-scale classification is an increasingly critical Big Data problem. So far, however, very little has been published on how this is done in practice. In this paper we describe Chimera, our solution to classify tens of millions of products into 5000+ product types at WalmartLabs. We show that at this scale, many conventional assumptions regarding learning and crowdsourcing break down, and that existing solutions cease to work. We describe how Chimera employs a combination of learning, rules (created by in-house analysts), and crowdsourcing to achieve accurate, continuously improving, and cost-effective classification. We discuss a set of lessons learned for other similar Big Data systems. In particular, we argue that at large scales crowdsourcing is critical, but must be used in combination with learning, rules, and in-house analysts. We also argue that using rules (in conjunction with learning) is a must, and that more research attention should be paid to helping analysts create and manage (tens of thousands of) rules more effectively."},
{"Title": "The Beckman Report on Database Research", "URL": "https://dl.acm.org/doi/10.1145/2694428.2694441", "Full Abstract": "Every few years a group of database researchers meets to discuss the state of database research, its impact on practice, and important new directions. This report summarizes the discussion and conclusions of the eighth such meeting, held October 14- 15, 2013 in Irvine, California. It observes that Big Data has now become a defining challenge of our time, and that the database research community is uniquely positioned to address it, with enormous opportunities to make transformative impact. To do so, the report recommends significantly more attention to five research areas: scalable big/fast data infrastructures; coping with diversity in the data management landscape; end-to-end processing and understanding of data; cloud services; and managing the diverse roles of people in the data life cycle."},
{"Title": "Why Big Data Industrial Systems Need Rules and What We Can Do About It", "URL": "https://dl.acm.org/doi/10.1145/2723372.2742784", "Full Abstract": "Big Data industrial systems that address problems such as classification, information extraction, and entity matching very commonly use hand-crafted rules. Today, however, little is understood about the usage of such rules. In this paper we explore this issue. We discuss how these systems differ from those considered in academia. We describe default solutions, their limitations, and reasons for using rules. We show examples of extensive rule usage in industry. Contrary to popular perceptions, we show that there is a rich set of research challenges in rule generation, evaluation, execution, optimization, and maintenance. We discuss ongoing work at WalmartLabs and UW-Madison that illustrate these challenges. Our main conclusions are (1) using rules (together with techniques such as learning and crowdsourcing) is fundamental to building semantics-intensive Big Data systems, and (2) it is increasingly critical to address rule management, given the tens of thousands of rules industrial systems often manage today in an ad-hoc fashion."},
{"Title": "Differential Equation Axiomatization", "URL": "https://dl.acm.org/doi/10.1145/3209108.3209147", "Full Abstract": "We prove the completeness of an axiomatization for differential equation invariants. First, we show that the differential equation axioms in differential dynamic logic are complete for all algebraic invariants. Our proof exploits differential ghosts, which introduce additional variables that can be chosen to evolve freely along new differential equations. Cleverly chosen differential ghosts are the proof-theoretical counterpart of dark matter. They create new hypothetical state, whose relationship to the original state variables satisfies invariants that did not exist before. The reflection of these new invariants in the original system then enables its analysis."},
{"Title": "A Hybrid, Dynamic Logic for Hybrid-Dynamic Information Flow", "URL": "https://dl.acm.org/doi/10.1145/3209108.3209151", "Full Abstract": "Information-flow security is important to the safety and privacy of cyber-physical systems (CPSs) across many domains: information leakage can both violate user privacy and reveal vulnerabilities to physical attacks. CPSs face the challenge that information can flow both in discrete cyber channels and in continuous real-valued physical channels ranging from time to motion to electrical currents. We call these hybrid-dynamic information flows (HDIFs) and introduce dHL, the first logic for verifying HDIFs in hybrid-dynamical models of CPSs. Our logic extends differential dynamic logic (dL) for hybrid-dynamical systems with hybrid-logical features for explicit program state representation, supporting relational reasoning used for information flow arguments. By verifying HDIFs, we ensure security even under a strong attacker model wherein an attacker can observe time and physical values continuously. We present a Hilbert-style proof calculus for dHL, prove it sound, and compare the expressive power of dHL with dL. We develop a hybrid system model based on the smart electrical grid FREEDM, with which we showcase dHL. We prove that the naive model has a previously unknown information flow vulnerability, which we verify is resolved in a revised model. This is the first information flow proof both for HDIFs and for a hybrid-dynamical model in general."},
{"Title": "Logical Foundations of Cyber-Physical Systems", "URL": "https://dl.acm.org/doi/book/10.5555/3281334", "Full Abstract": "Cyber-physical systems (CPSs) combine cyber capabilities, such as computation orcommunication, with physical capabilities, such as motion or other physical processes. Cars,aircraft, and robots are prime examples, because they move physically in space in a way that isdetermined by discrete computerized control algorithms. Designing these algorithms ischallenging due to their tight coupling with physical behavior, while it is vital that thesealgorithms be correct because we rely on them for safety-critical tasks. This textbook teaches undergraduate students the core principles behind CPSs. Itshows them how to develop models and controls; identify safety specifications andcritical properties; reason rigorously about CPS models; leverage multi-dynamicalsystems compositionality to tame CPS complexity;identify required control constraints;verify CPS models of appropriate scale in logic; and develop an intuitionfor operational effects. The book is supported with homework exercises, lecture videos, and slides."},
{"Title": "Tactical contract composition for hybrid system component verification", "URL": "https://dl.acm.org/doi/10.5555/3288063.3288082", "Full Abstract": "We present an approach for hybrid systems that combines the advantages of component-based modeling (e.g., reduced model complexity) with the advantages of formal verification (e.g., guaranteed contract compliance). Component-based modeling can be used to split large models into multiple component models with local responsibilities to reduce modeling complexity. Yet, this only helps the analysis if verification proceeds one component at a time. In order to benefit from the decomposition of a system into components for both modeling and verification purposes, we prove that the safety of compatible components implies safety of the composed system. We implement our composition theorem as a tactic in the KeYmaera X theorem prover, allowing automatic generation of a KeYmaera X proof for the composite system from proofs for the components without soundness-critical changes to KeYmaera X. Our approach supports component contracts (i.e., input assumptions and output guarantees for each component) that characterize the magnitude and rate of change of values exchanged between components. These contracts can take into account what has changed between two components in a given amount of time since the last exchange of information."},
{"Title": "Verifiably Safe Off-Model Reinforcement Learning", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-17462-0_28", "Full Abstract": "The desire to use reinforcement learning in safety-critical settings has inspired a recent interest in formal methods for learning algorithms. Existing formal methods for learning and optimization primarily consider the problem of constrained learning or constrained optimization. Given a single correct model and associated safety constraint, these approaches guarantee efficient learning while provably avoiding behaviors outside the safety constraint. Acting well given an accurate environmental model is an important pre-requisite for safe learning, but is ultimately insufficient for systems that operate in complex heterogeneous environments. This paper introduces verification-preserving model updates, the first approach toward obtaining formal safety guarantees for reinforcement learning in settings where multiple possible environmental models must be taken into account. Through a combination of inductive data and deductive proving with design-time model updates and runtime model falsification, we provide a first approach toward obtaining formal safety proofs for autonomous systems acting in heterogeneous environments."},
{"Title": "HyPLC", "URL": "https://dl.acm.org/doi/10.1145/3302509.3311036", "Full Abstract": "Programmable Logic Controllers"},
{"Title": "Toward multi-task support and security analyses in PLC program translation for verification", "URL": "https://dl.acm.org/doi/10.1145/3302509.3313335", "Full Abstract": "In this poster, we will present new tool, H"},
{"Title": "Towards Physical Hybrid Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-29436-6_13", "Full Abstract": "Some hybrid systems models are unsafe for mathematically correct but physically unrealistic reasons. For example, mathematical models can classify a system as being unsafe on a set that is too small to have physical importance. In particular, differences in measure zero sets in models of cyber-physical systems (CPS) have significant mathematical impact on the mathematical safety of these models even though differences on measure zero sets have no tangible physical effect in a real system. We develop the concept of “physical hybrid systems” (PHS) to help reunite mathematical models with physical reality. We modify a hybrid systems logic (differential temporal dynamic logic) by adding a first-class operator to elide distinctions on measure zero sets of time within CPS models. This approach facilitates modeling since it admits the verification of a wider class of models, including some physically realistic models that would otherwise be classified as mathematically unsafe. We also develop a proof calculus to help with the verification of PHS."},
{"Title": "Uniform Substitution at One Fell Swoop", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-29436-6_25", "Full Abstract": "Uniform substitution of function, predicate, program or game symbols is the core operation in parsimonious provers for hybrid systems and hybrid games. By postponing soundness-critical admissibility checks, this paper introduces a uniform substitution mechanism that proceeds in a linear pass homomorphically along the formula. Soundness is recovered using a simple variable condition at the replacements performed by the substitution. The setting in this paper is that of differential hybrid games, in which discrete, continuous, and adversarial dynamics interact in differential game logic [inline-graphic not available: see fulltext] . This paper proves soundness and completeness of one-pass uniform substitutions for [inline-graphic not available: see fulltext] ."},
{"Title": "dL", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-29436-6_6", "Full Abstract": "We introduce"},
{"Title": "Dynamic Doxastic Differential Dynamic Logic for Belief-Aware Cyber-Physical Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-29026-9_24", "Full Abstract": "Cyber-physical systems (CPS), such as airplanes, operate based on sensor and communication data, i.e. on potentially noisy or erroneous beliefs about the world. Realistic CPS models must therefore incorporate the notion of beliefs if they are to provide safety guarantees in practice as well as in theory. To fundamentally address this challenge, this paper introduces a first-principles framework for reasoning about CPS models where control decisions are explicitly driven by controller beliefs arrived at through observation and reasoning. We extend the differential dynamic logic [inline-graphic not available: see fulltext] for CPS dynamics with belief modalities, and a learning operator for belief change. This new dynamic doxastic differential dynamic logic [inline-graphic not available: see fulltext] does due justice to the challenges of CPS verification by having (1) real arithmetic for describing the world and beliefs about the world; (2) continuous and discrete world change; (3) discrete belief change by means of the learning operator. We develop a sound sequent calculus for [inline-graphic not available: see fulltext], which enables us to illustrate the applicability of [inline-graphic not available: see fulltext] by proving the safety of a simplified belief-triggered controller for an airplane."},
{"Title": "The Logical Path to Autonomous Cyber-Physical Systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-30281-8_2", "Full Abstract": "Autonomous cyber-physical systems are systems that combine the physics of motion with advanced cyber algorithms to act on their own without close human supervision. The present consensus is that reasonable levels of autonomy, such as for self-driving cars or autonomous drones, can only be reached with the help of artificial intelligence and machine learning algorithms that cope with the uncertainties of the real world. That makes safety assurance even more challenging than it already is in cyber-physical systems (CPSs) with classically programmed control, precisely because AI techniques are lauded for their flexibility in handling unpredictable situations, but are themselves harder to predict. This paper identifies the logical path toward autonomous cyber-physical systems in multiple steps. First, differential dynamic logic ( [inline-graphic not available: see fulltext] ) provides a logical foundation for developing cyber-physical system models with the mathematical rigor that their safety-critical nature demands. Then, its ModelPlex technique provides a logically correct way to tame the subtle relationship of CPS models to CPS implementations. Finally, the resulting logical monitor conditions can then be exploited to safeguard the decisions of learning agents, guide the optimization of learning processes, and resolve the nondeterminism frequently found in verification models. Overall, logic leads the way in combining the best of both worlds: the strong predictions that formal verification techniques provide alongside the strong flexibility that the use of AI provides."},
{"Title": "Pegasus: A Framework for Sound Continuous Invariant Generation", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-30942-8_10", "Full Abstract": "are an important component in deductive verification of hybrid and continuous systems. Just like discrete invariants are used to reason about correctness in discrete systems without unrolling their loops forever, continuous invariants are used to reason about differential equations without having to solve them."},
{"Title": "An Axiomatic Approach to Liveness for Differential Equations", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-30942-8_23", "Full Abstract": "This paper presents an approach for deductive liveness verification for ordinary differential equations (ODEs) with differential dynamic logic. Numerous subtleties complicate the generalization of well-known discrete liveness verification techniques, such as loop variants, to the continuous setting. For example, ODE solutions may blow up in finite time or their progress towards the goal may converge to zero. Our approach handles these subtleties by successively refining ODE liveness properties using ODE invariance properties which have a well-understood deductive proof theory. This approach is widely applicable: we survey several liveness arguments in the literature and derive them all as special instances of our axiomatic refinement approach. We also correct several soundness errors in the surveyed arguments, which further highlights the subtlety of ODE liveness reasoning and the utility of our deductive approach. The library of common refinement steps identified through our approach enables both the sound development and justification of new ODE liveness proof rules from our axioms."},
{"Title": "Verifiably safe SCUBA diving using commodity sensors", "URL": "https://dl.acm.org/doi/10.1145/3349568.3351554", "Full Abstract": "SCUBA diving is an activity in which divers remain underwater for prolonged periods by using a self-contained breathing apparatus. Diving is safety critical because changing depth too rapidly or running out of oxygen before surfacing can result in life-threatening consequences. These risks are currently minimized by using a wrist-mounted, 'air-integrated' dive computer that monitors time, depth and air tank pressure (received through expensive wireless transceivers or a hose). These computers are costly for the average recreational dive."},
{"Title": "Constructive Game Logic", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-44914-8_4", "Full Abstract": "Game Logic is an excellent setting to study proofs-about-programs via the interpretation of those proofs as programs, because constructive proofs for games correspond to effective winning strategies to follow in response to the opponent’s actions. We thus develop"},
{"Title": "Differential Equation Invariance Axiomatization", "URL": "https://dl.acm.org/doi/10.1145/3380825", "Full Abstract": "This article proves the completeness of an axiomatization for differential equation invariants described by Noetherian functions. First, the differential equation axioms of differential dynamic logic are shown to be complete for reasoning about analytic invariants. Completeness crucially exploits differential ghosts, which introduce additional variables that can be chosen to evolve freely along new differential equations. Cleverly chosen differential ghosts are the proof-theoretical counterpart of dark matter. They create a new hypothetical state, whose relationship to the original state variables satisfies invariants that did not exist before. The reflection of these new invariants in the original system then enables its analysis."},
{"Title": "Constructive Hybrid Games", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-51074-9_26", "Full Abstract": "Hybrid games combine discrete, continuous, and adversarial dynamics. Differential game logic ("},
{"Title": "Deductive Stability Proofs for Ordinary Differential Equations", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-72013-1_10", "Full Abstract": "Stability is required for real world controlled systems as it ensures that those systems can tolerate small, real world perturbations around their desired operating states. This paper shows how stability for continuous systems modeled by ordinary differential equations (ODEs) can be formally verified in differential dynamic logic (dL). The key insight is to specify ODE stability by suitably nesting the dynamic modalities of dL with first-order logic quantifiers. Elucidating the logical structure of stability properties in this way has three key benefits:"},
{"Title": "Optimizing relevance and revenue in ad search", "URL": "https://dl.acm.org/doi/10.1145/1390334.1390404", "Full Abstract": "The primary business model behind Web search is based on textual advertising, where contextually relevant ads are displayed alongside search results. We address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine, showing that optimizing ad relevance and revenue is not equivalent. Selecting the best ads that satisfy these constraints also naturally incurs high computational costs, and time constraints can lead to reduced relevance and profitability. We propose a novel two-stage approach, which conducts most of the analysis ahead of time. An offine preprocessing phase leverages additional knowledge that is impractical to use in real time, and rewrites frequent queries in a way that subsequently facilitates fast and accurate online matching. Empirical evaluation shows that our method optimized for relevance matches a state-of-the-art method while improving expected revenue. When optimizing for revenue, we see even more substantial improvements in expected revenue."},
{"Title": "Effective and efficient classification on a search-engine model", "URL": "https://dl.acm.org/doi/10.5555/3227237.3227510", "Full Abstract": "Traditional document classification frameworks, which apply the learned classifier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the \"best\" short query that characterizes a document class using operators normally available within search engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. As part of our study, we enhance some of the feature-selection techniques that are found in the literature by forcing the inclusion of terms that are negatively correlated with the target class and by making use of term correlations; we show that both of those techniques can offer significant advantages. Moreover, we show that optimizing the efficiency of query execution by careful selection of terms can further reduce the query costs. More precisely, we show that on our set-up the best 10-term query can achieve 93% of the accuracy of the best SVM classifier (14,000 terms), and if we are willing to tolerate a reduction to 89% of the best SVM, we can build a 10-term query that can be executed more than twice as fast as the best 10-term query."},
{"Title": "Computational advertising and recommender systems", "URL": "https://dl.acm.org/doi/10.1145/1454008.1454009", "Full Abstract": "Computational advertising is an emerging scientific discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, optimization, and microeconomics. The central challenge of computational advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The context could be a user entering a query in a search engine (\"sponsored search\"), a user reading a web page (\"content match\" and \"display ads\"), a user conversing on a cell phone (\"mobile advertising\"), and so on. The information about the user can vary from scarily detailed to practically nil. The number of potential advertisements might be in the billions. Thus, depending on the definition of \"best match\" this challenge leads to a variety of massive optimization and search problems, with complicated constraints."},
{"Title": "To swing or not to swing", "URL": "https://dl.acm.org/doi/10.1145/1458082.1458216", "Full Abstract": "Web textual advertising can be interpreted as a search problem over the corpus of ads available for display in a particular context. In contrast to conventional information retrieval systems, which always return results if the corpus contains any documents lexically related to the query, in Web advertising it is acceptable, and occasionally even desirable, not to show any results. When no ads are relevant to the user's interests, then showing irrelevant ads should be avoided since they annoy the user and produce no economic benefit. In this paper we pose a decision problem \"whether to swing\", that is, whether or not to show any of the ads for the incoming request. We propose two methods for addressing this problem, a simple thresholding approach and a machine learning approach, which collectively analyzes the set of candidate ads augmented with external knowledge. Our experimental evaluation, based on over 28,000 editorial judgments, shows that we are able to predict, with high accuracy, when to \"swing\" for both content match and sponsored search advertising."},
{"Title": "Search advertising using web relevance feedback", "URL": "https://dl.acm.org/doi/10.1145/1458082.1458217", "Full Abstract": "The business of Web search, a $10 billion industry, relies heavily on"},
{"Title": "A note on search based forecasting of ad volume in contextual advertising", "URL": "https://dl.acm.org/doi/10.1145/1458082.1458270", "Full Abstract": "In contextual advertising, estimating the number of impressions of an ad is critical in planning and budgeting advertising campaigns. However, producing this forecast, even within large margins of error, is quite challenging. We attack this problem by simulating the presence of a given ad with its associated bid over historical data, involving billions of impressions. This apparently enormous computational task is reduced to a search task involving only the set of distinct pages in the data. Furthermore the search is made more efficient using a two-level search process. Experimental results show that our approach can accurately forecast the expected number of impressions of contextual ads in real time."},
{"Title": "Introduction to special issue on query log analysis", "URL": "https://dl.acm.org/doi/10.1145/1409220.1409221", "Full Abstract": "No abstract available."},
{"Title": "Cross-lingual query classification", "URL": "https://dl.acm.org/doi/10.1145/1460027.1460046", "Full Abstract": "The non-English Web is growing at breakneck speed, but available language processing tools are mostly English based. Taxonomies are a case in point: while there are plenty of commercial and non-commercial taxonomies for the English Web, taxonomies for other languages are either not available or of very limited quality. Given that building taxonomies in all non-English languages is prohibitively expensive, it is natural to ask whether existing English taxonomies can be leveraged, possibly via machine translation, to enable information processing tasks in other languages. Preliminary results presented in this paper indicate that the answer is affirmative with respect to query classification, a task which is essential both for understanding the user intent and thus provide better search results, and for better targeting of search-based advertising, the economic underpinning of commercial Web search engines. We propose a robust method for classifying non-English queries against an English taxonomy and classifier using widely available, off-the-shelf machine translation systems. In particular, we show that by viewing the search results in the query's original language as independent sources of information, we can alleviate the impact of poor quality or erroneous machine translations. Empirical results for Chinese queries show that we achieve remarkably encouraging results."},
{"Title": "Cross-language query classification using web search for exogenous knowledge", "URL": "https://dl.acm.org/doi/10.1145/1498759.1498811", "Full Abstract": "The non-English Web is growing at phenomenal speed, but available language processing tools and resources are predominantly English-based. Taxonomies are a case in point: while there are plenty of commercial and non-commercial taxonomies for the English Web, taxonomies for other languages are either not available or of arguable quality. Given that building comprehensive taxonomies for each language is prohibitively expensive, it is natural to ask whether existing English taxonomies can be leveraged, possibly via machine translation, to enable text processing tasks in other languages. Our experimental results confirm that the answer is affirmative with respect to at least one task. In this study we focus on query classification, which is essential for understanding the user intent both in Web search and in online advertising. We propose a robust method for classifying non-English queries into an English taxonomy, using an existing English text classifier and off-the-shelf machine translation systems. In particular, we show that by considering the Web search results in the query's original language as additional sources of information, we can alleviate the effect of erroneous machine translation. Empirical evaluation on query sets in languages as diverse as Chinese and Russian yields very encouraging results; consequently, we believe that our approach is also applicable to many additional languages."},
{"Title": "Nearest-neighbor caching for content-match applications", "URL": "https://dl.acm.org/doi/10.1145/1526709.1526769", "Full Abstract": "Motivated by contextual advertising systems and other web applications involving efficiency-accuracy tradeoffs, we study similarity caching. Here, a cache hit is said to occur if the requested item is similar but not necessarily equal to some cached item. We study two objectives that dictate the efficiency-accuracy tradeoff and provide our caching policies for these objectives. By conducting extensive experiments on real data we show similarity caching can significantly improve the efficiency of contextual advertising systems, with minimal impact on accuracy. Inspired by the above, we propose a simple generative model that embodies two fundamental characteristics of page requests arriving to advertising systems, namely, long-range dependences and similarities. We provide theoretical bounds on the gains of similarity caching in this model and demonstrate these gains empirically by fitting the actual data to the model."},
{"Title": "A search-based method for forecasting ad impression in contextual advertising", "URL": "https://dl.acm.org/doi/10.1145/1526709.1526776", "Full Abstract": "Contextual advertising (also called content match) refers to the placement of small textual ads within the content of a generic web page. It has become a significant source of revenue for publishers ranging from individual bloggers to major newspapers. At the same time it is an important way for advertisers to reach their intended audience. This reach depends on the total number of exposures of the ad (impressions) and its click-through-rate (CTR) that can be viewed as the probability of an end-user clicking on the ad when shown. These two orthogonal, critical factors are both difficult to estimate and even individually can still be very informative and useful in planning and budgeting advertising campaigns."},
{"Title": "Online expansion of rare queries for sponsored search", "URL": "https://dl.acm.org/doi/10.1145/1526709.1526778", "Full Abstract": "Sponsored search systems are tasked with matching queries"},
{"Title": "Classifying search queries using the Web as a source of knowledge", "URL": "https://dl.acm.org/doi/10.1145/1513876.1513877", "Full Abstract": "We propose a methodology for building a robust query classification system that can identify thousands of query classes, while dealing in real time with the query volume of a commercial Web search engine. We use a pseudo relevance feedback technique: given a query, we determine its topic by classifying the Web search results retrieved by the query. Motivated by the needs of search advertising, we primarily focus on rare queries, which are the hardest from the point of view of machine learning, yet in aggregate account for a considerable fraction of search engine traffic. Empirical evaluation confirms that our methodology yields a considerably higher classification accuracy than previously reported. We believe that the proposed methodology will lead to better matching of online ads to rare queries and overall to a better user experience."},
{"Title": "Algorithmic Challenge in Online Advertising", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02158-9_1", "Full Abstract": "Computational advertising is an emerging new scientific sub-discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, classification, optimization, and microeconomics. The central challenge of computational advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The context could be a user entering a query in a search engine (\"sponsored search\"), a user reading a web page (\"content match\" and \"display ads\"), a user watching a movie on a portable device, and so on. The information about the user can vary from scarily detailed to practically nil. The number of potential advertisements might be in the billions. Thus, depending on the definition of \"best match\" this challenge leads to a variety of massive optimization and search problems, with complicated constraints."},
{"Title": "Context transfer in search advertising", "URL": "https://dl.acm.org/doi/10.1145/1571941.1572063", "Full Abstract": "We define and study the process of context transfer in search advertising, which is the transition of a user from the context of Web search to the context of the landing page that follows an ad-click. We conclude that in the vast majority of cases, the user is shown one of three types of pages, which can be accurately distinguished using automatic text classification."},
{"Title": "The Hiring Problem and Lake Wobegon Strategies", "URL": "https://dl.acm.org/doi/10.5555/1957995.1957997", "Full Abstract": "We introduce the"},
{"Title": "What happens after an ad click?", "URL": "https://dl.acm.org/doi/10.1145/1645953.1645964", "Full Abstract": "Unbeknownst to most users, when a query is submitted to a search engine two distinct searches are performed: the organic or algorithmic search that returns relevant Web pages and related data (maps, images, etc.), and the sponsored search that returns paid advertisements. While an enormous amount of work has been invested in understanding the user interaction with organic search, surprisingly little research has been dedicated to what happens after an ad is clicked, a situation we aim to correct."},
{"Title": "Information extraction meets relation databases", "URL": "https://dl.acm.org/doi/10.1145/1645953.1646067", "Full Abstract": "Information extraction from unstructured text has much in common with querying in databases systems. Despite some differences on how data is modeled or represented, the general goal remains the same, i.e. to retrieve data or tag elements that satisfy some user-specified constraints. In recent years, the two paradigms have become much closer thanks to the large volume of data on the World Wide Web and the need for more automated search tools for information extraction and often the need for relating the extracted pieces. Several developments have contributed to the growth of the area including the work on named entity recognition (marked by MUC-6 and subsequent conferences) and natural language processing, Web information retrieval and mining, and Web query languages inspired by the query languages in the relational world. This panel explores the areas where the two paradigms overlap, the impacts and contributions they have had on each other and the areas that may be open for further research. The panel will bring together researchers who have worked in some established areas that closely relate to extracting structured information from unstructured text. In the first (role-playing) round, each panelist will strongly take a side on where the intersection is heading, arguing that one area will subsume the other area in near future. In the second round, the panelists will counter one or two others, pointing out the challenges that one area would be facing in subsuming the other and implications for future research directions."},
{"Title": "Anatomy of the long tail", "URL": "https://dl.acm.org/doi/10.1145/1718487.1718513", "Full Abstract": "The success of \"infinite-inventory\" retailers such as Amazon.com and Netflix has been ascribed to a \"long tail\" phenomenon. To wit, while the majority of their inventory is not in high demand, in aggregate these \"worst sellers,\" unavailable at limited-inventory competitors, generate a significant fraction of total revenue. The long tail phenomenon, however, is in principle consistent with two fundamentally different theories. The first, and more popular hypothesis, is that a majority of consumers consistently follow the crowds and only a minority have any interest in niche content; the second hypothesis is that everyone is a bit eccentric, consuming both popular and specialty products. Based on examining extensive data on user preferences for movies, music, Web search, and Web browsing, we find overwhelming support for the latter theory. However, the observed eccentricity is much less than what is predicted by a fully random model whereby every consumer makes his product choices independently and proportional to product popularity; so consumers do indeed exhibit at least some a priori propensity toward either the popular or the exotic."},
{"Title": "Automatic generation of bid phrases for online advertising", "URL": "https://dl.acm.org/doi/10.1145/1718487.1718530", "Full Abstract": "One of the most prevalent online advertising methods is textual advertising. To produce a textual ad, an advertiser must craft a short creative (the text of the ad) linking to a landing page, which describes the product or service being promoted. Furthermore, the advertiser must associate the creative to a set of manually chosen bid phrases representing those Web search queries that should trigger the ad. For efficiency, given a landing page, the bid phrases are often chosen first, and then for each bid phrase the creative is produced using a template. Nevertheless, an ad campaign (e.g., for a large retailer) might involve thousands of landing pages and tens or hundreds of thousands of bid phrases, hence the entire process is very laborious."},
{"Title": "An initiative to attract students to computing", "URL": "https://dl.acm.org/doi/10.1145/1227310.1227360", "Full Abstract": "No abstract available."},
{"Title": "The current crisis in computing", "URL": "https://dl.acm.org/doi/10.1145/1227310.1227426", "Full Abstract": "No abstract available."},
{"Title": "The computing ontology project", "URL": "https://dl.acm.org/doi/10.1145/1227310.1227486", "Full Abstract": "No abstract available."},
{"Title": "IEEE Software Engineering Standards", "URL": "https://dl.acm.org/doi/10.1109/CSEET.2007.28", "Full Abstract": "This paper describes the uses of software engineering standards in a university curriculum. The paper points out the importance of software engineering standards, the issues relating to their use in software engineering and computer science courses, and which of these courses would benefit from using these standards. This information is documented in a new textbook, the IEEE Software Engineering Standards: A Students' Version published by the IEEE Computer Society Press in 2007. This textbook lists the standards necessary to support the software engineering laboratory and other software courses. The book also provides a detailed description of each of these listed standards that have been tailored to match the requirements of a university course in software development. These tailored standards are based on the \"look and feel\" of as well as the structure of the IEEE's commercial software engineering standards. In addition, the book is \"salted\" with stories from both software development work places and the university classroom, both of which provide illustrations relating to some of the problems in developing software, and in some cases solutions to these problems."},
{"Title": "The computing ontology", "URL": "https://dl.acm.org/doi/10.1145/1345443.1345439", "Full Abstract": "Working Group 3 at ITiCSE 2007 continued the ongoing work of the Ontology of Computing project. The working group brought several new people into the project and addressed areas of the ontology of particular interest to these participants. In particular, the group worked on the Ontology sections related to History of Computing, Computing Security and Social and Ethical issues. With the intention of applying the ontology to the support of curriculum development in mind, the group also reviewed and discussed proposed means of presenting a visual representation of the ontology. There was also some work on the present structure of the ontology and future possibilities."},
{"Title": "Rediscovering the passion, beauty, joy and awe", "URL": "https://dl.acm.org/doi/10.1145/1352135.1352213", "Full Abstract": "No abstract available."},
{"Title": "Curriculum update from the ACM education board", "URL": "https://dl.acm.org/doi/10.1145/1352135.1352313", "Full Abstract": "No abstract available."},
{"Title": "Computer Science Curriculum 2008", "URL": "https://dl.acm.org/doi/book/10.1145/2593246", "Full Abstract": "No abstract available."},
{"Title": "A historical look at curricula and materials", "URL": "https://dl.acm.org/doi/10.1145/1508865.1508935", "Full Abstract": "No abstract available."},
{"Title": "Report on the ACM/IEEE-CS undergraduate curricula recommendations", "URL": "https://dl.acm.org/doi/10.1145/1508865.1508963", "Full Abstract": "No abstract available."},
{"Title": "Computing education matters", "URL": "https://dl.acm.org/doi/10.1145/1498765.1498767", "Full Abstract": "No abstract available."},
{"Title": "Concurrency and parallelism in the computing ontology", "URL": "https://dl.acm.org/doi/10.1145/1562877.1563044", "Full Abstract": "This poster will describe ongoing work to modify the Computing Ontology to incorporate issues of parallelism and concurrency, motivated by recent developments in computer hardware design."},
{"Title": "Setting the stage for computing curricula 2013", "URL": "https://dl.acm.org/doi/10.1145/1953163.1953213", "Full Abstract": "Following a roughly 10 year cycle, the Computing Curricula volumes have helped to set international curricular guidelines for undergraduate programs in computing. In the summer of 2010, planning for the next volume in the series, Computer Science 2013, began. This panel seeks to update and engage the SIGCSE community on the Computer Science 2013 effort. The development of curricular guidelines in Computer Science is particularly challenging given the rapid evolution and expansion of the field. Moreover, the growing diversity of topics in Computer Science and the integration of computing with other disciplines create additional challenges and opportunities in defining computing curricula. As a result, it is particularly important to engage the broader computer science education community in a dialog to better understand new opportunities, local needs, and novel successful models of computing curriculum. The last complete Computer Science curricular volume was released in 2001 [3] and followed by a review effort that concluded in 2008 [2]. While the review helped to update some of the knowledge units in the 2001 volume, it was not aimed at producing an entirely new curricular volume and deferred some of the more significant questions that arose at the time. The Computer Science 2013 effort seeks to provide a new volume reflecting the current state of the field and highlighting promising future directions through revisiting and redefining the knowledge units in CS, rethinking the essentials necessary for a CS curriculum, and identifying working exemplars of courses and curricula along these lines."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2003616.2003639", "Full Abstract": "No abstract available."},
{"Title": "Computer engineering review task force report", "URL": "https://dl.acm.org/doi/10.1145/2157136.2157256", "Full Abstract": "In early 2011, the ACM and the IEEE Computer Society (IEEE/CS) created the CE2004 Review Task Force (RTF) and charged it with the task of reviewing and determining the extent to which the document \"Curriculum Guidelines for Undergraduate Degree Programs in Computer Engineering,\" produced 2004 December 12 and known as CE2004 [1] required revisions. The RTF submitted a report to both societies in July of 2011. The report summarized a survey of academic and industry constituents conducted by the RTF. It recommended keeping the structure and the vast majority of the content of the original CE2004 document. However, it also recommended that contemporary topics should be strengthened or added while de-emphasizing other topics that appeared to be waning from the curricular mainstream of computer engineering. Additionally, the RTF recommended that the two societies form a joint special-purpose committee to update and edit the earlier document and to seek input and review from the computer engineering industrial and academic communities through presentations and workshops co-located at major conferences. The presenters of this special session were members of the 2011 RTF and two presenters were members of the original curricular task force from 2004. The presentation will provide insights in the RTF findings and thoughts on how a future computer engineering report might evolve."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2077808.2077820", "Full Abstract": "Copyright © 2012 Copryright held by authors."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2189835.2189846", "Full Abstract": "Welcome to the third installment of EduBits, your quarterly pipeline to new and exciting happenings in the world of ACM education. Starting with this March issue of ACM Inroads, we are introducing a new thread that will highlight principal educational activities within ACM and affiliated organizations. Starting with this issue, you'll find a special focus on an ACM Special Interest Group (SIG) engaged in educational activities. News summaries will also include updates from task groups and committees of the ACM Education Council and within the institution itself."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2339055.2339058", "Full Abstract": "Copyright © 2012 Copyright held by authors."},
{"Title": "Software Specification Techniques (International Computer Science Series)", "URL": "https://dl.acm.org/doi/book/10.5555/576977", "Full Abstract": "No abstract available."},
{"Title": "The relation between programming and specification languages with particular reference to Anna", "URL": "https://dl.acm.org/doi/10.5555/60769.60776", "Full Abstract": "No abstract available."},
{"Title": "A view of formal semantics", "URL": "https://dl.acm.org/doi/10.1016/0920-5489%2889%2990039-1", "Full Abstract": "No abstract available."},
{"Title": "Computing curricula 1991—a review", "URL": "https://dl.acm.org/doi/10.5555/116199.116207", "Full Abstract": "No abstract available."},
{"Title": "Software Engineering", "URL": "https://dl.acm.org/doi/book/10.5555/573002", "Full Abstract": "No abstract available."},
{"Title": "Software Engineering; A European Perspective", "URL": "https://dl.acm.org/doi/book/10.5555/573006", "Full Abstract": "No abstract available."},
{"Title": "Software Engineering", "URL": "https://dl.acm.org/doi/book/10.5555/573007", "Full Abstract": "No abstract available."},
{"Title": "Verification", "URL": "https://dl.acm.org/doi/10.5555/646020.678574", "Full Abstract": "No abstract available."},
{"Title": "Computing curricula 2001 how will it work for you?", "URL": "https://dl.acm.org/doi/10.1145/364447.364825", "Full Abstract": "In the fall of 1998, the ACM Education Board and the Educational Activities Board of the IEEE Computer Society appointed representatives to a joint task force to prepare Computing Curricula 2001 (CC2001), the next installment in a series of reports on the undergraduate computer science curriculum that began in 1968 and was then updated in 1978 and 1991. Interim reports on the initial planning of the curriculum were presented at the SIGCSE symposium and the IEEE Frontiers in Education Conference in both 1999 and 2000. The CC2001 Task Force released its first draft report at the 2000 SIGCSE conference and plans to release its penultimate draft at SIGCSE 2001. The purpose of this session is to describe how we expect the recommendations of the report to apply in practice. The panelists represent a range of institutions and can therefore speak to the questions that audience members from similar institutions might have."},
{"Title": "Computer engineering computing curricula", "URL": "https://dl.acm.org/doi/10.1145/611892.611915", "Full Abstract": "No abstract available."},
{"Title": "Trusting collaboration in global computing systems", "URL": "https://dl.acm.org/doi/10.5555/1759008.1759018", "Full Abstract": "A significant characteristic of global computing is the need for secure interactions between highly mobile entities and the services in their environment. Moreover, these decentralised systems are also characterised by partial views over the state of the global environment, implying that we cannot guarantee verification of the properties of the mobile entity entering an unfamiliar domain. Secure in this context encompasses both the need for cryptographic security and the need for trust, on the part of both parties, that the interaction will function as expected. In this paper, we explore an architecture for interaction/ collaboration in global computing systems. This architecture reflects the aspects of the trust lifecycle in three stages: trust formation, trust evolution and trust exploitation, forming a basis for risk assessment and interaction decisions."},
{"Title": "Trust Dynamics for Collaborative Global Computing", "URL": "https://dl.acm.org/doi/10.5555/938984.939763", "Full Abstract": "Recent advances in networking technology have increasedthe potential for dynamic enterprise collaborationsbetween an open set of entities on a global scale. The securityof these collaborations is a major concern, and requiresnovel approaches suited to this new environment tobe developed. Trust management appears to be a promisingapproach. Due to the dynamic nature of these collaborations,dynamism in the formation, evolution and exploitationof trust is essential. In this paper we explore the propertiesof trust dynamics in this context. Trust is formedand evolves according to personal experience and recommendations.The properties of trust dynamics are expressedthrough a formal model of trust. Specific examples, basedon an e-purse application scenario are used to demonstratethese properties."},
{"Title": "IEEE-CS/ACM computing curricula", "URL": "https://dl.acm.org/doi/10.1145/971300.971453", "Full Abstract": "Copyright © 2004 ACM."},
{"Title": "Computing curricula 2004", "URL": "https://dl.acm.org/doi/10.1145/971300.971470", "Full Abstract": "In 2001, the ACM and the IEEE-CS published"},
{"Title": "Grand Challenges in Computing", "URL": "https://dl.acm.org/doi/10.1093/comjnl/bxh064", "Full Abstract": "The conference on grand challenges, held in Newcastle on 30 and 31 March 2004, occurred at a particularly opportune time. The strand on the educational aspects was particularly relevant and the idea innovative in the sense that this was the first occasion on which a grand challenge event with a focus on educational issues in computing had taken place. This paper provides some of the background and includes a distillation of the educational challenges that emerged from that event."},
{"Title": "Recentering computer science", "URL": "https://dl.acm.org/doi/10.1145/1096000.1096018", "Full Abstract": "The recent decreases of enrollment in computer science programs signal a chasm between our historical emphasis on programming and the contemporary concerns of those choosing careers."},
{"Title": "LL versus LR parsing with illustrations from ALGOL 68", "URL": "https://dl.acm.org/doi/10.1145/800238.807142", "Full Abstract": "The relative merits of LL and LR parsing methods are compared, particular reference being made to ALGOL 68. The fact that LR methods can be applied to a wider class of languages does not seem to give them a significant advantage in practice."},
{"Title": "Some ALGOL 68 compilers", "URL": "https://dl.acm.org/doi/10.5555/1061688.1061697", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Aspects of the ALGOL 68 mode structure", "URL": "https://dl.acm.org/doi/10.1145/954245.954251", "Full Abstract": "Copyright © 1979 Author."},
{"Title": "The  Definition of Programming Languages", "URL": "https://dl.acm.org/doi/book/10.5555/539787", "Full Abstract": "No abstract available."},
{"Title": "The  Definition of Programming Languages", "URL": "https://dl.acm.org/doi/book/10.5555/539788", "Full Abstract": "No abstract available."},
{"Title": "Program verification using Ada", "URL": "https://dl.acm.org/doi/book/10.5555/69553", "Full Abstract": "No abstract available."},
{"Title": "Program Verification Using ADA", "URL": "https://dl.acm.org/doi/book/10.5555/539111", "Full Abstract": "No abstract available."},
{"Title": "A synthesis of computing concepts", "URL": "https://dl.acm.org/doi/10.1145/1113847.1113894", "Full Abstract": "This is the report of Working Group 4 of the ITiCSE Conference of 2005. The working group met to introduce some new participants into an ongoing project designed to explore the representation of all the computing and information related disciplines in a single, comprehensive, graphical and interactive structure. The goal of the work is to support the classification of research work, the development of curriculum recommendations and accreditation criteria, and the analysis of proposed programs of study."},
{"Title": "Re-centering computer science", "URL": "https://dl.acm.org/doi/10.1145/1121341.1121364", "Full Abstract": "No abstract available."},
{"Title": "A comprehensive representation of the computing and information disciplines", "URL": "https://dl.acm.org/doi/10.1145/1121341.1121404", "Full Abstract": "No abstract available."},
{"Title": "Computing Curricula 2005: The Overview Report", "URL": "https://dl.acm.org/doi/10.1145/1121341.1121482", "Full Abstract": "In 2001, the ACM and the IEEE-CS published"},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2381083.2381085", "Full Abstract": "EduBits, your quarterly roundup of ACM educational activities, focuses on education policy and the way it affects the K--12 educational space. Cameron Wilson, Director of Public Policy for ACM, gives us a glimpse into the challenges of improving CS education throughout the U.S. He issues an important challenge to our community to get involved in K--12 scaling."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2432596.2432598", "Full Abstract": "Welcome to the latest installment of \"EduBits,\" your quarterly pipeline to new and exciting happenings in the world of ACM education. In this edition, the ACM Education Board publishes an important white paper on online learning. In addition, Cameron Wilson, Director of ACM's Public Policy Office, offers his perspectives."},
{"Title": "Character streams", "URL": "https://dl.acm.org/doi/10.1145/775396.775399", "Full Abstract": "Copyright © 1978 Authors."},
{"Title": "A Universal File Server", "URL": "https://dl.acm.org/doi/10.1109/TSE.1980.230493", "Full Abstract": "A file server is a utility provided in a computer connected via a local communications network to a number of other computer. File servers exist to preserve material for the benefit of client machines or systems. It is desirable for a file server to be able to support multiple file directory and access management systems, so that the designer of a client system retains the freedom to design the system that best suits him. For example, he may wish to use the rile server to support a predefimed directory structure or as a swapping disk. The paper explores the dedgn issues associated with such a file server and proposes some solutions."},
{"Title": "Grapevine", "URL": "https://dl.acm.org/doi/10.1145/800216.806606", "Full Abstract": "Grapevine is a distributed, replicated system running on a large internet within the Xerox research and development community. The internet extends from coast to coast in the USA, to Canada and to Europe, and contains more than 50 Ethernet local networks linked by leased telephone lines. Over 1500 computers are attached to the internet. Most computers are used an personal workstations, but some are used as"},
{"Title": "Grapevine", "URL": "https://dl.acm.org/doi/10.1145/358468.358487", "Full Abstract": "Grapevine is a multicomputer system on the Xerox research internet. It provides facilities for the delivery of digital messages such as computer mail; for naming people, machines, and services; for authenticating people and machines; and for locating services on the internet. This paper has two goals: to describe the system itself and to serve as a case study of a real application of distributed computing. Part I describes the set of services provided by Grapevine and how its data and function are divided among computers on the internet. Part II presents in more detail selected aspects of Grapevine that illustrate novel facilities or implementation techniques, or that provide insight into the structure of a distributed system. Part III summarizes the current state of the system and the lesson learned from it so far."},
{"Title": "Implementing Remote procedure calls", "URL": "https://dl.acm.org/doi/10.1145/800217.806609", "Full Abstract": "Remote procedure calls ("},
{"Title": "Experience with Grapevine (Summary)", "URL": "https://dl.acm.org/doi/10.1145/800217.806622", "Full Abstract": "Grapevine is a distributed, replicated system that provides message delivery, naming, authentication, resource location, and access control services in an internet of computers. The system, described in a previous paper [1], was designed and implemented several years ago. We now have had operational experience with the system under substantial load. This experience has proved the original design sound in most aspects, but there also have been some surprises. In this paper we report what we have learned from using Grapevine. Our experience may offer some help to designers of new systems."},
{"Title": "Experience with Grapevine:  the growth of a distributed system", "URL": "https://dl.acm.org/doi/10.1145/2080.2081", "Full Abstract": "Copyright © 1984 ACM."},
{"Title": "Implementing remote procedure calls", "URL": "https://dl.acm.org/doi/10.1145/2080.357392", "Full Abstract": "Copyright © 1984 ACM."},
{"Title": "Current work on authentication", "URL": "https://dl.acm.org/doi/10.1145/503956.503966", "Full Abstract": "No abstract available."},
{"Title": "Secure communication using remote procedure calls", "URL": "https://dl.acm.org/doi/10.1145/214451.214452", "Full Abstract": "Research on encryption-based secure communication protocols has reached a stage where it is feasible to construct end-to-end secure protocols. The design of such a protocol, built as part of a remote procedure call package, is described. The security abstraction presented to users of the package, the authentication mechanisms, and the protocol for encrypting and verifying remote calls are also described."},
{"Title": "Synchronization primitives for a multiprocessor: a formal specification", "URL": "https://dl.acm.org/doi/10.1145/41457.37509", "Full Abstract": "Formal specifications of operating system interfaces can be a useful part of their documentation. We illustrate this by documenting the Threads synchronization primitives of the Taos operating system. We start with an informal description, present a way to formally specify interfaces in concurrent systems, give a formal specification of the synchronization primitives, briefly discuss the implementation, and conclude with a discussion of what we have learned from using the specification for more than a year."},
{"Title": "A simple and efficient implementation of a small database", "URL": "https://dl.acm.org/doi/10.1145/41457.37517", "Full Abstract": "Copyright © 1987 ACM."},
{"Title": "Position paper for the ACM SIGOPS workshop 1988", "URL": "https://dl.acm.org/doi/10.1145/504092.504101", "Full Abstract": "Copyright © 1988 ACM."},
{"Title": "Implementing remote procedure calls", "URL": "https://dl.acm.org/doi/10.5555/59309.59336", "Full Abstract": "No abstract available."},
{"Title": "Enhancing a dependable multiserver operating system with temporal protection via resource reservations", "URL": "https://dl.acm.org/doi/10.1007/s11241-009-9086-5", "Full Abstract": "Nowadays, microkernel-based systems are getting studied and adopted with a renewed interest in a wide number of IT scenarios. Their advantages over classical monolithic solutions mainly concern the dependability domain. By being capable of dynamically detect and solve non-expected behaviours within its core components, a microkernel-based OS would eventually run forever with no need to be restarted. Dependability in this context mainly aims at isolating components from a spatial point of view: a microkernel-based system may definitely not be adopted in the context of real-time environments, simply basing on this kind of protection only."},
{"Title": "Towards a Secure Application-Semantic Aware Policy Enforcement Architecture", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-04904-0_5", "Full Abstract": "Even though policy enforcement has been studied from different angles including notation, negotiation and enforcement, the development of an application-semantic aware enforcement architecture remains an open problem. In this paper we present and discuss the design of such an architecture."},
{"Title": "Cooperative update", "URL": "https://dl.acm.org/doi/10.1145/1656437.1656439", "Full Abstract": "Many real-world systems require continuous operation. Downtime is ill-affordable and scheduling maintenance for regular software updates is a tremendous challenge for system administrators. For this reason, live update is a potential solution as it allows running software to be replaced by a newer version without stopping the system. The vast majority of live update approaches proposed as a solution to this problem aims to support existing software systems, while striving to maintain a good level of safety and flexibility."},
{"Title": "A scalable middleware solution for advanced wide-area web services", "URL": "https://dl.acm.org/doi/10.5555/1659232.1659237", "Full Abstract": "To alleviate scalability problems in the Web, many researchers concentrate on how to incorporate advanced caching and replication techniques. Many solutions incorporate object-based techniques. In particular, Web resources are considered as distributed objects offering a well-defined interface."},
{"Title": "Constructing secure mobile agent systems using the agent operating system", "URL": "https://dl.acm.org/doi/10.1504/IJIIDS.2009.030435", "Full Abstract": "Designing a secure and reliable mobile agent system is a difficult task. The agent operating system (AOS) is a building block that simplifies this task. AOS provides common primitives required by most mobile agent middleware systems, such as primitives for secure communication, secure and tamper-evident agent packaging and agent migration. Different middleware processes can use AOS at the same time; effective security mechanisms protect AOS resources owned by different middleware processes. Designed as a portable and language-neutral middleware layer residing between the mobile agent system and the operating system, AOS facilitates interoperability between agent platforms and between different implementations of AOS itself. AOS has been implemented in both C&#43;&#43; and Java. This paper motivates the design of AOS, describes how AOS is used in a mobile agent system, and presents performance measures for an agent transfer protocol layered upon AOS."},
{"Title": "Block-level RAID is dead", "URL": "https://dl.acm.org/doi/10.5555/1863122.1863126", "Full Abstract": "The common storage stack as found in most operating systems has remained unchanged for several decades. In this stack, the RAID layer operates under the file system layer, at the block abstraction level. We argue that this arrangement of layers has fatal flaws. In this paper, we highlight its main problems, and present a new storage stack arrangement that solves these problems."},
{"Title": "We crashed, now what?", "URL": "https://dl.acm.org/doi/10.5555/1924908.1924912", "Full Abstract": "We present an in-depth analysis of the crash-recovery problem and propose a novel approach to recover from otherwise fatal operating system (OS) crashes. We show how an unconventional, but careful, OS design, aided by automatic compiler-based code instrumentation, offers a practical solution towards the survivability of the entire system. Current results are encouraging and show that our approach is able to recover even the most critical OS subsystems without exposing the failure to user applications or hampering the scalability of the system."},
{"Title": "Classifying RFID attacks and defenses", "URL": "https://dl.acm.org/doi/10.1007/s10796-009-9210-z", "Full Abstract": "RFID (Radio Frequency Identification) systems are one of the most pervasive computing technologies with technical potential and profitable opportunities in a diverse area of applications. Among their advantages is included their low cost and their broad applicability. However, they also present a number of inherent vulnerabilities. This paper develops a structural methodology for risks that RFID networks face by developing a classification of RFID attacks, presenting their important features, and discussing possible countermeasures. The goal of the paper is to categorize the existing weaknesses of RFID communication so that a better understanding of RFID attacks can be achieved and subsequently more efficient and effective algorithms, techniques and procedures to combat these attacks may be developed."},
{"Title": "Loris - A Dependable, Modular File-Based Storage Stack", "URL": "https://dl.acm.org/doi/10.1109/PRDC.2010.41", "Full Abstract": "The arrangement of file systems and volume management/RAID systems, together commonly referred to as the storage stack, has remained the same for several decades, despite significant changes in hardware, software and usage scenarios. In this paper, we evaluate the traditional storage stack along three dimensions: reliability, heterogeneity and flexibility. We highlight several major problems with the traditional stack. We then present Loris, our redesign of the storage stack, and we evaluate several aspects of Loris."},
{"Title": "Flexible, modular file volume virtualization in Loris", "URL": "https://dl.acm.org/doi/10.1109/MSST.2011.5937218", "Full Abstract": "Traditional file systems made it possible for administrators to create file volumes, on a one-file-volume-per-disk basis. With the advent of RAID algorithms and their integration at the block level, this \"one file volume per disk\" bond forced administrators to create a single, shared file volume across all users to maximize storage efficiency, thereby complicating administration. To simplify administration, and to introduce new functionalities, file volume virtualization support was added at the block level. This new virtualization engine is commonly referred to as the volume manager, and the resulting arrangement, with volume managers operating below file systems, has been referred to as the traditional storage stack. In this paper, we present several problems associated with the compatibility-driven integration of file volume virtualization at the block level. In earlier work, we presented Loris, a reliable, modular storage stack, that solved several problems with the traditional storage stack by design. In this paper, we extend Loris to support file volume virtualization. In doing so, we first present \"File pools\", our novel storage model to simplify storage administration, and support efficient file volume virtualization. Following this, we will describe how our single unified virtualization infrastructure, with a modular division of labor, is used to support several new functionalities like 1) instantaneous snapshoting of both files and file volumes, 2) efficient snapshot deletion through information sharing, and 3) open-close versioning of files. We then present \"Version directories,\" our unified interface for browsing file history information. Finally, we will evaluate the infrastructure, and provide an in-depth comparison of our approach with other competing approaches"},
{"Title": "Integrated end-to-end dependability in the Loris storage stack", "URL": "https://dl.acm.org/doi/10.1109/DSNW.2011.5958807", "Full Abstract": "The storage stack in an operating system faces a number of dependability threats. The importance of keeping users' data safe makes this area particularly worth investigating. We briefly describe the main threats (disk device failures, whole-system failures, software bugs, and memory corruption), and outline the Loris storage stack that we developed previously. We then present an integrated approach that combines several techniques to protect the Loris storage stack against these dependability threats, all the way from the disk driver layer to the virtual file system (VFS) layer."},
{"Title": "Systems Security at VU University Amsterdam", "URL": "https://dl.acm.org/doi/10.1109/SysSec.2011.26", "Full Abstract": "The systems and network security, and the secure and reliable systems groups carry out research in computer and network dependability at the Vrije Universiteit Amsterdam. The former group, led by Prof. Herbert Bos, has a strong and historical background in attack detection, dynamic analysis, and reverse engineering of software. The secure and reliable systems group, led by Prof. Andrews. Tanenbaum, is instead traditionally rooted on studying and guaranteeing dependability properties of systems. The two groups interact in a natural way and blend their knowledge and expertise to contribute on the security of networks and systems."},
{"Title": "Safe and automated state transfer for secure and reliable live update", "URL": "https://dl.acm.org/doi/10.5555/2664350.2664354", "Full Abstract": "Traditional live update systems offer little or no automated support for state transfer between two different program versions with changes in the program state. In this paper, we report our efforts to build a safe and automated state transfer framework for C programs that requires a minimal number of program state annotations and handles common structural state changes with no programmer assistance. To handle more complex state transformations, the framework includes a number of extension mechanisms designed to minimize the overall programming effort. Our experience with real-world programs suggests that our framework can handle all the standard C idioms and support safe and automated state transfer for complex state changes. We believe our approach is effective in several update scenarios and significantly raises the bar on the security and reliability of live update."},
{"Title": "Keep net working - on a dependable and fast networking stack", "URL": "https://dl.acm.org/doi/10.5555/2354410.2355161", "Full Abstract": "For many years, multiserver1 operating systems have been demonstrating, by their design, high dependability and reliability. However, the design has inherent performance implications which were not easy to overcome. Until now the context switching and kernel involvement in the message passing was the performance bottleneck for such systems to get broader acceptance beyond niche domains. In contrast to other areas of software development where fitting the software to the parallelism is difficult, the new multicore hardware is a great match for the multiserver systems. We can run individual servers on different cores. This opens more room for further decomposition of the existing servers and thus improving dependability and live-updatability. We discuss in general the implications for the multiserver systems design and cover in detail the implementation and evaluation of a more dependable networking stack. We split the single stack into multiple servers which run on dedicated cores and communicate without kernel involvement. We think that the performance problems that have dogged multiserver operating systems since their inception should be reconsidered: it is possible to make multiserver systems fast on multicores."},
{"Title": "Structured Computer Organization", "URL": "https://dl.acm.org/doi/book/10.5555/2424048", "Full Abstract": "Structured Computer Organization, specifically written for undergraduate students, is a best-selling guide that provides an accessible introduction to computer hardware and architecture. This text will also serve as a useful resource for all computer professionals and engineers who need an overview or introduction to computer architecture. This book takes a modern structured, layered approach to understanding computer systems. It's highly accessible - and it's been thoroughly updated to reflect today's most critical new technologies and the latest developments in computer organization and architecture. Tanenbaums renowned writing style and painstaking research make this one of the most accessible and accurate books available, maintaining the authors popular method of presenting a computer as a series of layers, each one built upon the ones below it, and understandable as a separate entity."},
{"Title": "Enhanced operating system security through efficient and fine-grained address space randomization", "URL": "https://dl.acm.org/doi/10.5555/2362793.2362833", "Full Abstract": "In recent years, the deployment of many application-level countermeasures against memory errors and the increasing number of vulnerabilities discovered in the kernel has fostered a renewed interest in kernel-level exploitation. Unfortunately, no comprehensive and well-established mechanism exists to protect the operating system from arbitrary attacks, due to the relatively new development of the area and the challenges involved."},
{"Title": "Safe and automatic live update for operating systems", "URL": "https://dl.acm.org/doi/10.1145/2451116.2451147", "Full Abstract": "Increasingly many systems have to run all the time with no downtime allowed. Consider, for example, systems controlling electric power plants and e-banking servers. Nevertheless, security patches and a constant stream of new operating system versions need to be deployed without stopping running programs. These factors naturally lead to a pressing demand for live update---upgrading all or parts of the operating system without rebooting. Unfortunately, existing solutions require significant manual intervention and thus work reliably only for small operating system patches."},
{"Title": "Practical automated vulnerability monitoring using program state invariants", "URL": "https://dl.acm.org/doi/10.1109/DSN.2013.6575318", "Full Abstract": "Despite the growing attention to security concerns and advances in code verification tools, many memory errors still escape testing and plague production applications with security vulnerabilities. We present RCORE, an efficient dynamic program monitoring infrastructure to perform automated security vulnerability monitoring. Our approach is to perform extensive static analysis at compile time to automatically index program state invariants (PSIs). At runtime, our novel dynamic analysis continuously inspects the program state and produces a report when PSI violations are found. Our technique retrofits existing applications and is designed for both offline and production runs. To avoid slowing down production applications, we can perform our dynamic analysis on idle cores to detect suspicious behavior in the background. The alerts raised by our analysis are symptoms of memory corruption or other—potentially exploitable—dangerous behavior. Our experimental evaluation confirms that RCORE can report on several classes of vulnerabilities with very low overhead."},
{"Title": "When slower is faster", "URL": "https://dl.acm.org/doi/10.5555/2535461.2535493", "Full Abstract": "Breaking up the OS in many small components is attractive from a dependability point of view. If one of the components crashes or needs an update, we can replace it on the fly without taking down the system. The question is how to achieve this without sacrificing performance and without wasting resources unnecessarily. In this paper, we show that heterogeneous multicore architectures allow us to run OS code efficiently by executing each of the OS components on the most suitable core. Thus, components that require high single-thread performance run on (expensive) high-performance cores, while components that are less performance critical run on wimpy cores. Moreover, as current trends suggest that there will be no shortage of cores, we can give each component its own dedicated core when performance is of the essence, and consolidate multiple functions on a single core (saving power and resources) when performance is less critical for these components. Using frequency scaling to emulate different ×86 cores, we evaluate our design on the most demanding subsystem of our operating system--the network stack. We show that less is sometimes more and that we can deliver better throughput with slower and, likely, less power hungry cores. For instance, we support network processing at close to 10 Gbps (the maximum speed of our NIC), while using an average of just 60% of the core speeds. Moreover, even if we scale all the cores of the network stack down to as little as 200 MHz, we still achieve 1.8 Gbps, which may be enough for many applications."},
{"Title": "Distributed Systems", "URL": "https://dl.acm.org/doi/book/10.5555/2559328", "Full Abstract": "No abstract available."},
{"Title": "An Action Structure for Synchronous pi-Calculus", "URL": "https://dl.acm.org/doi/10.5555/647896.757244", "Full Abstract": "No abstract available."},
{"Title": "Action Calculi, or Syntactic Action Structures", "URL": "https://dl.acm.org/doi/10.5555/645722.666408", "Full Abstract": "No abstract available."},
{"Title": "Higher-Order Action Calculi", "URL": "https://dl.acm.org/doi/10.5555/647843.736432", "Full Abstract": "No abstract available."},
{"Title": "Pi-Nets", "URL": "https://dl.acm.org/doi/10.5555/645390.651630", "Full Abstract": "No abstract available."},
{"Title": "Control Structures", "URL": "https://dl.acm.org/doi/10.5555/788017.788730", "Full Abstract": "Action calculi are a class of action structures with added structure. Each action calculus AC(K) is determined by a set K of controls, equipped with reaction rules; calculi such as Petri nets, the typed lambda calculus and the pi calculus are obtained by varying K. This paper defines for each K a category CS(K), characterized by equational axioms, of action structures with added structure; they are called control structures and provide models of the calculus AC(K), which is initial in the category. The surface of an action is defined; it is an abstract correlate of the syntactic notion of free name. Three equational characterizations of surface are found equivalent. It permits a non-syntactic treatment of the linkage among the components of an interactive system. Finally, control structures and their morphisms offer a means of classifying the variety of dynamic disciplines in models of concurrency, such as the mobility present in the pi calculus but absent in other calculi."},
{"Title": "Control Structures", "URL": "https://dl.acm.org/doi/10.5555/648334.755715", "Full Abstract": "No abstract available."},
{"Title": "Communication and concurrency", "URL": "https://dl.acm.org/doi/book/10.5555/225494", "Full Abstract": "No abstract available."},
{"Title": "The Pi calculus and its applications", "URL": "https://dl.acm.org/doi/10.5555/278701.278707", "Full Abstract": "No abstract available."},
{"Title": "Semantic ideas in computing", "URL": "https://dl.acm.org/doi/10.5555/242807.242820", "Full Abstract": "No abstract available."},
{"Title": "Semantic ideas in computing", "URL": "https://dl.acm.org/doi/10.5555/266240.266253", "Full Abstract": "No abstract available."},
{"Title": "Calculi for interaction", "URL": "https://dl.acm.org/doi/10.1007/BF03036472", "Full Abstract": "Action structures have previously been proposed as an algebra for both the syntax and the semantics of interactive computation. Here, a class of concrete action structures called"},
{"Title": "The  Definition of Standard ML", "URL": "https://dl.acm.org/doi/book/10.5555/549659", "Full Abstract": "From the Publisher:"},
{"Title": "Graphical Calculi for Interaction (Abstract)", "URL": "https://dl.acm.org/doi/10.5555/646251.685828", "Full Abstract": "No abstract available."},
{"Title": "Strong Normalisation in Higher-Order Action Calculi", "URL": "https://dl.acm.org/doi/10.5555/645869.668526", "Full Abstract": "No abstract available."},
{"Title": "The pi calculus and its applications", "URL": "https://dl.acm.org/doi/10.5555/299315.300844", "Full Abstract": "No abstract available."},
{"Title": "Communicating and mobile systems", "URL": "https://dl.acm.org/doi/book/10.5555/329902", "Full Abstract": "No abstract available."},
{"Title": "Graphical Theories of Interactive Systems", "URL": "https://dl.acm.org/doi/10.5555/646527.695023", "Full Abstract": "No abstract available."},
{"Title": "Deriving Bisimulation Congruences for Reactive Systems", "URL": "https://dl.acm.org/doi/10.5555/646735.701620", "Full Abstract": "The dynamics of reactive systems, e.g. CCS, has often been defined using a labelled transition system (LTS). More recently it has become natural in defining dynamics to use reaction rules - i.e. unlabelled transition rules - together with a structural congruence. But LTSs lead more naturally to behavioural equivalences. So one would like to derive from reaction rules a suitable LTS."},
{"Title": "Computational flux", "URL": "https://dl.acm.org/doi/10.1145/360204.360222", "Full Abstract": "No abstract available."},
{"Title": "The Flux of Interaction", "URL": "https://dl.acm.org/doi/10.5555/647747.734212", "Full Abstract": "A graphical model of interactive systems called bigraphs is introduced, resting on the orthogonal treatment of connectivity and locality. The model will be shown to underlie several calculi for mobile systems, in particular the π-calculus and the ambient calculus. Its core behavioural theory will be outlined."},
{"Title": "Algorithms for graph partitioning on the planted partition model", "URL": "https://dl.acm.org/doi/10.5555/373515.373517", "Full Abstract": "No abstract available."},
{"Title": "Proving Sequential Consistency by Model Checking", "URL": "https://dl.acm.org/doi/book/10.5555/901966", "Full Abstract": "Sequential consistency is a multiprocessor memory model of both practical and theoretical importance. The general problem of deciding whether a finite-state protocol implements sequential consistency is undecidable. In this paper, however, we show that for the protocols that arise in practice, proving sequential consistency can be done automatically in theory and can be reduced to regular language inclusion via a small amount of manual effort. In particular, we introduce an approach to construct finite-state ``observers'' that guarantee that a protocol is sequentially consistent. We have developed possible observers for several cache coherence protocols and present our experimental model checking results on a substantial directory-based cache coherence protocol. From a theoretical perspective, our work characterizes a class of protocols, which we believe encompasses all real protocols, for which sequential consistency can be decided. From a practical perspective, we are presenting a methodology for designing memory protocols such that sequential consistency may be proven automatically via model~checking."},
{"Title": "Automatable verification of sequential consistency", "URL": "https://dl.acm.org/doi/10.1145/378580.378604", "Full Abstract": "is a multiprocessor memory model of both practical and theoretical importance. Designing and implementing a memory system that efficiently provides a given memory model is a challenging and error-prone task, so automated verification support would be invaluable. Unfortunately, the general problem of deciding whether a finite-state protocol implements sequential consistency is undecidable. In this paper, we identify a restricted class of protocols for which verifying sequential consistency is decidable. The class includes all published sequentially consistent protocols that are known to us, and we argue why the class is likely to include all real sequentially consistent protocols. In principle, our method can be applied in a completely automated fashion for verification of all implemented protocols."},
{"Title": "Proving Sequential Consistency by Model Checking", "URL": "https://dl.acm.org/doi/10.5555/822072.822327", "Full Abstract": "Sequential consistency is a multiprocessor memory model of both practical and theoretical importance. Unfortunately, the general problem of verifying that a finite-state protocol implements sequential consistency is undecidable, and in practice, validating that a real-world, finite-state protocol implements sequential consistency is very time-consuming and costly. In this work, we show that for memory protocols that occur in practice, a small amount of manual effort can reduce the problem of verifying sequential consistency into a verification task that can be discharged automatically via model checking. Furthermore, we present experimental results on a substantial, directory-based cache coherence protocol, which demonstrate the practicality of our approach."},
{"Title": "Specifying and Verifying a Broadcast and a Multicast Snooping Cache Coherence Protocol", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2002.1011412", "Full Abstract": "In this paper, we develop a specification methodology that documents and specifies a cache coherence protocol in eight tables: the states, events, actions, and transitions of the cache and memory controllers. We then use this methodology to specify a detailed, modern three-state broadcast snooping protocol with an unordered data network and an ordered address network that allows arbitrary skew. We also present a detailed specification of a new protocol called Multicast Snooping and, in doing so, we better illustrate the utility of the table-based specification methodology. Finally, we demonstrate a technique for verification of the Multicast Snooping protocol, through the sketch of a manual proof that the specification satisfies a sequentially consistent memory model."},
{"Title": "Stochastic Local Search Algorithms for DNA Word Design", "URL": "https://dl.acm.org/doi/10.5555/647863.736615", "Full Abstract": "We present results on the performance of a stochastic local search algorithm for the design of DNA codes, namely sets of equallength words over the nucleotides alphabet {"},
{"Title": "From RNA Secondary Structure to Coding Theory", "URL": "https://dl.acm.org/doi/10.5555/647863.736622", "Full Abstract": "We use combinatorial analysis to transform a special case of the computational problem of designing RNA base sequences with a given minimal free energy secondary structure into a coding theory question. The function of RNA molecules is largely determined by their molecular form, which in turn is significantly related to the base pairings of the secondary structure. Hence, this is crucial initial work in the design of RNA molecules with desired three-dimensional structures and specific functional properties. The biological importance of RNA only continues to grow with the discoveries of many different RNA molecules having vital functions other than mediating the production of proteins from DNA. Furthermore, RNA has the same potential as DNA in terms of nanotechnology and biomolecular computing."},
{"Title": "Algorithms for Testing That Sets of DNA Words Concatenate without Secondary Structure", "URL": "https://dl.acm.org/doi/10.5555/647863.736635", "Full Abstract": "We present an efficient algorithm for determining whether all molecules in a combinatorial set of DNA or RNA strands are structure free, and thus available for bonding to their Watson-Crick complements. This work is motivated by the goal of testing whether strands used in DNA computations or as molecular bar-codes are structure free, where the strands are concatenations of short words. We also present an algorithm for determining whether all words in"},
{"Title": "Strand design for biomolecular computation", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2802%2900135-4", "Full Abstract": "The design of DNA or RNA strands for DNA computations poses many new questions in algorithms and coding theory. DNA strand design also arises in use of molecular bar codes to manipulate and identify individual molecules in complex chemical libraries, and to attach molecules to DNA chips. We survey several formulations of the DNA strand design problem, along with results and open questions in this area."},
{"Title": "Toward a decidable notion of sequential consistency", "URL": "https://dl.acm.org/doi/10.1145/777412.777467", "Full Abstract": "A memory model specifies a correctness requirement for a distributed shared memory protocol. Sequential consistency (SC) is the most widely researched model; previous work citealur1996 has shown that, in general, the SC verification problem is undecidable. We identify two aspects of the formulation found in citealur1996 that we consider to be highly unnatural; we call these non-prefix-closedness and prophetic inheritance. We conjecture that preclusion of such behavior yields a decidable version of SC, which we call decisive sequential consistency (DSC). We also introduce a structure called a phview window (VW), which retains information about a protocol's history, and we define the notion of a phVW-bound, which essentially bounds the size of the VWs needed to maintain DSC. We prove that the class of DSC protocols with VW-bound"},
{"Title": "Problems on RNA secondary structure prediction and design", "URL": "https://dl.acm.org/doi/10.5555/1759210.1759213", "Full Abstract": "We describe several computational problems on prediction and design of RNA molecules."},
{"Title": "On the undecidability of probabilistic planning and related stochastic optimization problems", "URL": "https://dl.acm.org/doi/10.1016/S0004-3702%2802%2900378-8", "Full Abstract": ", the problem of how an agent achieves a"},
{"Title": "Algorithms for testing that sets of DNA words concatenate without secondary structure", "URL": "https://dl.acm.org/doi/10.1023/B%3ANACO.0000006770.91995.ec", "Full Abstract": "We present an efficient algorithm for determining whether"},
{"Title": "Classifying RNA pseudoknotted structures", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2004.03.042", "Full Abstract": "Computational prediction of the minimum free energy (mfe) secondary structure of an RNA molecule from its base sequence is valuable in understanding the structure and function of the molecule. Since the general problem of predicting pseudoknotted secondary structures is NP-hard, several algorithms have been proposed that find the mfe secondary structure from a restricted class of secondary structures. In this work, we order the algorithms by generality of the structure classes that they handle. We provide simple characterizations of the classes of structures handled by four algorithms, as well as linear time methods to test whether a given secondary structure is in three of these classes. We report on the percentage of biological structures from the PseudoBase and Gutell databases that are handled by these three algorithms."},
{"Title": "Editorial", "URL": "https://dl.acm.org/doi/10.1016/j.jcss.2004.01.005", "Full Abstract": "No abstract available."},
{"Title": "Linear time algorithm for parsing RNA secondary structure", "URL": "https://dl.acm.org/doi/10.1007/11557067_28", "Full Abstract": "Accurate prediction of pseudoknotted RNA secondary structure is an important computational challenge. Typical prediction algorithms aim to find a structure with minimum free energy according to some thermodynamic (“sum of loop energies”) model that is implicit in the recurrences of the algorithm. However, a clear definition of what exactly are the loops and stems in pseudoknotted structures, and their associated energies, has been lacking."},
{"Title": "Flow algorithms for two pipelined filter ordering problems", "URL": "https://dl.acm.org/doi/10.1145/1142351.1142379", "Full Abstract": "Pipelined filter ordering is a central problem in database query optimization, and has received renewed attention recently in the context of environments such as the web, continuous high-speed data streams and sensor networks. We present algorithms for two natural extensions of the classical pipelined filter ordering problem: (1) a"},
{"Title": "Efficient parameter estimation for RNA secondary structure prediction", "URL": "https://dl.acm.org/doi/10.1093/bioinformatics/btm223", "Full Abstract": "Accurate prediction of RNA secondary structure from the base sequence is an unsolved computational challenge. The accuracy of predictions made by free energy minimization is limited by the quality of the energy parameters in the underlying free energy model. The most widely used model, the Turner99 model, has hundreds of parameters, and so a robust parameter estimation scheme should efficiently handle large data sets with thousands of structures. Moreover, the estimation scheme should also be trained using available experimental free energy data in addition to structural data."},
{"Title": "Revenue monotonicity in combinatorial auctions", "URL": "https://dl.acm.org/doi/10.5555/1619645.1619665", "Full Abstract": "Intuitively, one might expect that a seller's revenue from an auction weakly increases as the number of bidders grows, as this increases competition. However, it is known that for combinatorial auctions that use the VCG mechanism, a seller can sometimes increase revenue by dropping bidders. In this paper we investigate the extent to which this problem can occur under other dominant-strategy combinatorial auction mechanisms. Our main result is that such failures of \"revenue monotonicity\" are not limited to mechanisms that achieve efficient allocations. Instead, they can occur under any dominant-strategy direct mechanism that sets prices using critical values, and that always chooses an allocation that cannot be augmented to make some bidder better off, while making none worse off."},
{"Title": "HFold", "URL": "https://dl.acm.org/doi/10.5555/2391870.2391900", "Full Abstract": "Improving the accuracy and efficiency of computational RNA secondary structure prediction is an important challenge, particularly for pseudoknotted secondary structures. We propose a new approach for prediction of pseudoknotted structures, motivated by the hypothesis that RNA structures fold hierarchically, with pseudoknot free pairs forming initially, and pseudoknots forming later so as to minimize energy relative to the initial pseudoknot free structure. Our HFold (Hierarchical Fold) algorithm has"},
{"Title": "Implementation of an Interpreter for a Parallel Language in Centaur", "URL": "https://dl.acm.org/doi/10.5555/645388.651572", "Full Abstract": "No abstract available."},
{"Title": "Occurrences in debugger specifications", "URL": "https://dl.acm.org/doi/10.1145/113446.113473", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "Origin Functions in Lambda-Calculus and Term Rewriting Systems", "URL": "https://dl.acm.org/doi/10.5555/648221.751377", "Full Abstract": "No abstract available."},
{"Title": "Real theorem provers deserve real user-interfaces", "URL": "https://dl.acm.org/doi/10.1145/142868.143760", "Full Abstract": "This paper explains how to add a modern user interface to existing theorem provers, using principles and tools designed for programming environments."},
{"Title": "A canonical calculus of residuals", "URL": "https://dl.acm.org/doi/10.5555/185881.185911", "Full Abstract": "No abstract available."},
{"Title": "Proof by Pointing", "URL": "https://dl.acm.org/doi/10.5555/645868.668515", "Full Abstract": "No abstract available."},
{"Title": "Reasoning with Executable Specifications", "URL": "https://dl.acm.org/doi/10.5555/646619.697409", "Full Abstract": "No abstract available."},
{"Title": "CtCoq", "URL": "https://dl.acm.org/doi/10.5555/646057.678192", "Full Abstract": "No abstract available."},
{"Title": "CtCoq", "URL": "https://dl.acm.org/doi/10.5555/648232.752990", "Full Abstract": "No abstract available."},
{"Title": "Head-Tactics Simplification", "URL": "https://dl.acm.org/doi/10.5555/646058.678365", "Full Abstract": "No abstract available."},
{"Title": "A Generic Approach to Building User Interfaces for Theorem Provers", "URL": "https://dl.acm.org/doi/10.1006/jsco.1997.0171", "Full Abstract": "In this paper, we present the results of an ongoing effort in building user interfaces for proof systems. Our approach is generic: we are not constructing a user interface for a particular proof system, rather we have developed techniques and tools that have been applied to several proof systems. We first propose and motivate a distributed architecture, where the proof system and the interface are two separate processes communicating through a protocol. Then we describe three high-level features:proof-by-pointing,script management, andtextual explanation. Altogether, they take advantage of the underlying architecture and yield a more user-friendly proof environment."},
{"Title": "Fix-Point Equations for Well-Founded Recursion in Type Theory", "URL": "https://dl.acm.org/doi/10.5555/646527.695036", "Full Abstract": "No abstract available."},
{"Title": "Changing Data Structures in Type Theory", "URL": "https://dl.acm.org/doi/10.5555/646540.696039", "Full Abstract": "In type-theory based proof systems that provide inductive structures, computation tools are automatically associated to inductive definitions. Choosing a particular representation for a given concept has a strong influence on proof structure. We propose a method to make the change from one representation to another easier, by systematically translating proofs from one context to another. Weshow how this method works by using it on natural numbers, for which a unary representation (based on Peano axioms) and a binary representation are available. This method leads to an automatic translation tool that we have implemented in Coq and successfully applied to several arithmetical theorems."},
{"Title": "Formalizing a JVML Verifier for Initialization in a Theorem Prover", "URL": "https://dl.acm.org/doi/10.5555/647770.734123", "Full Abstract": "No abstract available."},
{"Title": "Formalizing Convex Hull Algorithms", "URL": "https://dl.acm.org/doi/10.5555/646528.695057", "Full Abstract": "We study the development of formally proved algorithms for computational geometry. The result of this work is a formal description of the basic principles that make convex hull algorithms work and two programs that implement convex hull computation and have been automatically obtained from formally verified mathematical proofs. A special attention has been given to handling degenerate cases that are often overlooked by conventional algorithm presentations."},
{"Title": "Type-Theoretic Functional Semantics", "URL": "https://dl.acm.org/doi/10.5555/646529.695218", "Full Abstract": "We describe the operational and denotational semantics of a small imperative language in type theory with inductive and recursive definitions. The operational semantics is given by natural inference rules, implemented as an inductive relation. The realization of the denotational semantics is more delicate: The nature of the language imposes a few difficulties on us. First, the language is Turing-complete, and therefore the interpretation function we consider is necessarily partial. Second, the language contains strict sequential operators, and therefore the function necessarily exhibits nested recursion. Our solution combines and extends recent work by the authors and others on the treatment of general recursive functions and partial and nested recursive functions. The first new result is a technique to encode the approach of Bove and Capretta for partial and nested recursive functions in type theories that do not provide simultaneous induction-recursion. A second result is a clear understanding of the characterization of the definition domain for general recursive functions, a key aspect in the approach by iteration of Balaa and Bertot. In this respect, the work on operational semantics is a meaningful example, but the applicability of the technique should extend to other circumstances where complex recursive functions need to be described formally."},
{"Title": "Visual Abstractions for Temporal Verification", "URL": "https://dl.acm.org/doi/10.5555/646059.678540", "Full Abstract": "Generalized Verification Diagrams combine deductive and algorithmic verification to establish general temporal properties of finite-and infinite-state reactive systems. The diagram serves as an abstraction of the system. This abstraction is deductively justified and algorithmically model checked. We present a new simple class of verification diagrams, using Müller acceptance conditions, and show how they can be used to verify general temporal properties of reactive systems."},
{"Title": "A Proof of GMP Square Root", "URL": "https://dl.acm.org/doi/10.1023/A%3A1021987403425", "Full Abstract": "We present a formal proof (at the implementation level) of an efficient algorithm proposed by P. Zimmermann in 1999 to compute square roots of arbitrarily large integers. This program, which is part of the GNU Multiple Precision Arithmetic Library, is completely proven within the COQ system. Proofs are developed using the CORRECTNESS tool to deal with imperative features of the program. The formalization is rather large (more than 13,000 lines) and requires some advanced techniques for proof management and reuse."},
{"Title": "Visual Verification of Temporal Properties", "URL": "https://dl.acm.org/doi/10.5555/832247.832527", "Full Abstract": "The deductive approach to verifying temporal properties of reactive systems is based on verification rules, which reduce the system validity of a temporal property to the general validity of first-order verification conditions. This methodology is complete relative to the underlying first-order reasoning. However, the proofs can be difficult to construct and understand, particularly as the complexity of the system increases.We have developed diagram-based formalisms for the verification of general temporal properties of reactive, real-time and hybrid systems. A diagram is a visual abstraction of the system to be verified; it represents the aspects of the system relevant to the property to be proved. The diagram represents a schematic overview of a deductive proof, and therefore it is more intuitive and easier to construct and understand.These methods combine deductive and algorithmic verification, and are being extended to include modularity, abstraction and refinement to facilitate the verification of larger, more complex systems."},
{"Title": "Deductive Model Checking", "URL": "https://dl.acm.org/doi/10.1023/A%3A1008791913551", "Full Abstract": "We present an extension of classical tableau-based model checking procedures to the case of infinite-state systems, using deductive methods in an incremental construction of the behavior graph. Logical formulas are used to represent infinite sets of states in an abstraction of this graph, which is repeatedly refined in the search for a counterexample computation, ruling out large portions of the graph before they are expanded to the state-level. This can lead to large savings, even in the case of finite-state systems. Only local conditions need to be checked at each step, and previously proven properties can be used to further constrain the search. Although the resulting method is not always automatic, it provides a flexible, general and complete framework that can integrate a diverse number of other verification tools."},
{"Title": "Verification of Parameterized Systems by Dynamic Induction on Diagrams", "URL": "https://dl.acm.org/doi/10.5555/647768.733794", "Full Abstract": "In this paper we present a visual approach to proving progress properties of parameterized systems using induction on verification diagrams. The inductive hypothesis is represented by an automaton and is based on a state-dependent order on process indices, for increased flexibility. This approach yields more intuitive proofs for progress properties and simpler verification conditions that are more likely to be proved automatically."},
{"Title": "Verifying Temporal Properties of Reactive Systems", "URL": "https://dl.acm.org/doi/10.1023/A%3A1008700623084", "Full Abstract": "We review a number of formal verification techniques supported by STeP, the Stanford Temporal Prover, describing how the tool can be used to verify properties of several versions of the Bakery Mutual exclusion algorithm for mutual exclusion. We verify the classic two-process algorithm and simple variants, as well as an atomic parameterized version. The methods used include deductive verification rules, verification diagrams, automatic invariant generation, and finite-state model checking and abstraction."},
{"Title": "Alternating the Temporal Picture for Safety", "URL": "https://dl.acm.org/doi/10.5555/646253.686326", "Full Abstract": "We use alternating automata on infinite words to reduce the verification of linear temporal logic (LTL) safety properties over infinite-state systems to the proof of first-order verification conditions. Thus method generalizes the traditional deductive verification approach of providing verification rules for particular classes of formulas, such as invariances, nested precedence formulas, etc. It facilitates the deductive verification of arbitrary safety properties without the need for explicit temporal reasoning."},
{"Title": "The ‘Cash-Point’ Service: A Verification Case Study Using STeP", "URL": "https://dl.acm.org/doi/10.1007/s001650070014", "Full Abstract": "STeP, the Stanford Temporal Prover, supports the computer-aided formal verification of concurrent and reactive systems based on temporal specifications [MBB99]. Automated"},
{"Title": "Deductive verification of real-time systems using STeP", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2800%2900088-8", "Full Abstract": "No abstract available."},
{"Title": "Non-linear loop invariant generation using Gröbner bases", "URL": "https://dl.acm.org/doi/10.1145/964001.964028", "Full Abstract": "We present a new technique for the generation of non-linear (algebraic) invariants of a program. Our technique uses the theory of ideals over polynomial rings to reduce the non-linear invariant generation problem to a numerical constraint solving problem. So far, the literature on invariant generation has been focussed on the construction of linear invariants for linear programs. Consequently, there has been little progress toward non-linear invariant generation. In this paper, we demonstrate a technique that encodes the conditions for a given template assertion being an invariant into a set of constraints, such that all the solutions to these constraints correspond to non-linear (algebraic) loop invariants of the program. We discuss some trade-offs between the completeness of the technique and the tractability of the constraint-solving problem generated. The application of the technique is demonstrated on a few examples."},
{"Title": "Scalable analysis of linear systems using mathematical programming", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-30579-8_2", "Full Abstract": "We present a method for generating linear invariants for large systems. The method performs forward propagation in an abstract domain consisting of arbitrary polyhedra of a predefined fixed shape. The basic operations on the domain like abstraction, intersection, join and inclusion tests are all posed as linear optimization queries, which can be solved efficiently by existing LP solvers. The number and dimensionality of the LP queries are polynomial in the program dimensionality, size and the number of target invariants. The method generalizes similar analyses in the interval, octagon, and octahedra domains, without resorting to polyhedral manipulations. We demonstrate the performance of our method on some benchmark programs."},
{"Title": "Termination of polynomial programs", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-30579-8_8", "Full Abstract": "We present a technique to prove termination of multipath polynomial programs, an expressive class of loops that enables practical code abstraction and analysis. The technique is based on finite differences of expressions over transition systems. Although no complete method exists for determining termination for this class of loops, we show that our technique is useful in practice. We demonstrate that our prototype implementation for C source code readily scales to large software projects, proving termination for a high percentage of targeted loops."},
{"Title": "LOLA", "URL": "https://dl.acm.org/doi/10.1109/TIME.2005.26", "Full Abstract": "We present a specification language and algorithms for the online and offline monitoring of synchronous systems including circuits and embedded systems. Such monitoring is useful not only for testing, but also under actual deployment. The specification language is simple and expressive; it can describe both correctness/failure assertions along with interesting statistical measures that are useful for system profiling and coverage analysis. The algorithm for online monitoring of queries in this language follows a partial evaluation strategy: it incrementally constructs output streams from input streams, while maintaining a store of partially evaluated expressions for forward references. We identify a class of specifications, characterized syntactically, for which the algorithmýs memory requirement is independent of the length of the input streams. Being able to bound memory requirements is especially important in online monitoring of large input streams. We extend the concepts used in the online algorithm to construct an efficient offline monitoring algorithm for large traces. We have implemented our algorithm and applied it to two industrial systems, the PCI bus protocol and a memory controller. The results demonstrate that our algorithms are practical and that our specification language is sufficiently expressive to handle specifications of interest to industry."},
{"Title": "Linear ranking with reachability", "URL": "https://dl.acm.org/doi/10.1007/11513988_48", "Full Abstract": "We present a complete method for synthesizing"},
{"Title": "The polyranking principle", "URL": "https://dl.acm.org/doi/10.1007/11523468_109", "Full Abstract": "Although every terminating loop has a ranking function, not every loop has a ranking function of a restricted form, such as a lexicographic tuple of polynomials over program variables. The"},
{"Title": "The decidability of the first-order theory of knuth-bendix order", "URL": "https://dl.acm.org/doi/10.1007/11532231_10", "Full Abstract": "Two kinds of orderings are widely used in term rewriting and theorem proving, namely"},
{"Title": "Termination analysis of integer linear loops", "URL": "https://dl.acm.org/doi/10.1007/11539452_37", "Full Abstract": "Usually, ranking function synthesis and invariant generation over a loop with integer variables involves abstracting the loop to have real variables. Integer division and modulo arithmetic must be soundly abstracted away so that the analysis over the abstracted loop is sound for the original loop. Consequently, the analysis loses precision. In contrast, we introduce a technique for handling loops over integer variables directly. The resulting analysis is more precise than previous analyses."},
{"Title": "Final semantics for event-pattern reactive programs", "URL": "https://dl.acm.org/doi/10.1007/11548133_23", "Full Abstract": "Event-pattern reactive programs are front-end programs for distributed reactive components that preprocess an incoming stream of event stimuli. Their purpose is to recognize temporal patterns of events that are relevant to the serviced program and ignore all other events, outsourcing some of the component's complexity and shielding it from event overload. Correctness of event-pattern reactive programs is essential, because bugs may result in loss of relevant events and hence failure to react appropriately."},
{"Title": "Thread allocation protocols for distributed real-time and embedded systems", "URL": "https://dl.acm.org/doi/10.1007/11562436_13", "Full Abstract": "We study the problem of thread allocation in asynchronous distributed real-time and embedded systems. Each distributed node handles a limited set of resources, in particular a limited thread pool. Different methods can be invoked concurrently in each node, either by external agents or as a remote call during the execution of a method. In this pa- per we study thread allocation under a"},
{"Title": "Expressive completeness of an event-pattern reactive programming language", "URL": "https://dl.acm.org/doi/10.1007/11562436_39", "Full Abstract": "Event-pattern reactive programs serve reactive components by pre-processing the input event stream and generating notifications according to temporal patterns. The declarative language PAR allows the expression of complex event-pattern reactions. Despite its simplicity and deterministic nature, PAR is expressively complete in the following sense:"},
{"Title": "Termination and invariance analysis of loops", "URL": "https://dl.acm.org/doi/10.1007/11562948_2", "Full Abstract": "Deductive verification aims to prove deep properties about programs. The classic Floyd-Hoare-style approach to verifying sequential programs reduces program validity queries to first-order validity queries via verification conditions. Proving that a program is totally correct requires proving the safety aspect with invariants and the progress aspect with invariants and ranking functions. Where do the invariants and ranking functions come from?"},
{"Title": "On relations between input and communication/computation in VLSI", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1981.26", "Full Abstract": "No abstract available."},
{"Title": "An Efficient Deadlock Removal Scheme for Non-Two-Phase Locking Protocols", "URL": "https://dl.acm.org/doi/10.5555/645910.673597", "Full Abstract": "No abstract available."},
{"Title": "Optimal allocation of computational resources in VLSI", "URL": "https://dl.acm.org/doi/10.5555/1382436.1382776", "Full Abstract": "No abstract available."},
{"Title": "Locking Protocols", "URL": "https://dl.acm.org/doi/10.1145/2157.322406", "Full Abstract": "Copyright © 1983 ACM."},
{"Title": "A Non-Two-Phase Locking Protocol for Concurrency Control in General Databases", "URL": "https://dl.acm.org/doi/10.5555/645911.673599", "Full Abstract": "No abstract available."},
{"Title": "Optimal Allocation of Area for Single-Chip Computations", "URL": "https://dl.acm.org/doi/10.1137/0214053", "Full Abstract": "This paper presents initial results on the problem of allocation of the available VLSI chip’s area among various functional components such as I/Q pads, memory cells, and internal wiring. First, a general lower bound for any chip computing a transitive function is derived; this bound is tight for certain functions. The arguments used in the various derivations are later used to specify which of the components are critical depending on the relative sizes of the chip and the number of variables of the function to be computed. The general lower bound is powerful enough that many of the previously proved lower bounds (which could account only for some of the functional requirements) are obtained as explicit special cases of the new result."},
{"Title": "On high-speed computing with a programmable linear array", "URL": "https://dl.acm.org/doi/10.5555/62972.63026", "Full Abstract": "It has been observed by many researchers that systolic arrays are very suitable for certain high-speed computations. In this paper, using a formal methodology, a single simple programmable linear systolic array capable of solving large number of problems drawn from a variety of applications is designed. The methodology is applicable to problems solvable by sequential algorithms that can be specified as nested for loops of arbitrary depth. The algorithms of this form that can be computed on the array presented in this paper include 25 algorithms dealing with signal and image processing, algebraic computations, matrix arithmetic, pattern matching, database operations, sorting, and transitive closure. Assuming bounded I/O, for 18 of those algorithms the time and storage complexities are optimal, and therefore no improvement can be expected by utilizing dedicated special purpose linear systolic arrays designed for individual algorithms."},
{"Title": "Synthesizing Linear Array Algorithms from Nested FOR Loop Algorithms", "URL": "https://dl.acm.org/doi/10.1109/12.9735", "Full Abstract": "The mapping of algorithms structured as depth-p nested FOR loops into special-purpose systolic VLSI linear arrays is addressed. The mappings are done by using linear functions to transform the original sequential algorithms into a form suitable for parallel execution on linear arrays. A feasible mapping is derived by identifying formal criteria to be satisfied by both the original sequential algorithm and the proposed transformation function. The methodology is illustrated by synthesizing algorithms for matrix multiplication and a version of the Warshall-Floyd transitive closure algorithm."},
{"Title": "On visible surface generation by a priori tree structures", "URL": "https://dl.acm.org/doi/10.5555/95075.95084", "Full Abstract": "No abstract available."},
{"Title": "Querying and Controlling the Future Behaviour of Complex Objects", "URL": "https://dl.acm.org/doi/10.5555/645474.653854", "Full Abstract": "No abstract available."},
{"Title": "Optimal parallel suffix-prefix matching algorithm and applications", "URL": "https://dl.acm.org/doi/10.1145/72935.72977", "Full Abstract": "Copyright © 1989 ACM."},
{"Title": "Relational database behavior: utilizing relational discrete event systems and models", "URL": "https://dl.acm.org/doi/10.1145/73721.73754", "Full Abstract": "Behavior of relational databases is studied within the framework of"},
{"Title": "Mapping Nested Loop Algorithms into Multidimensional Systolic Arrays", "URL": "https://dl.acm.org/doi/10.1109/71.80125", "Full Abstract": "Consideration is given to transforming depth p-nested for loop algorithms into q-dimensional systolic VLSI arrays where 1>or=q>or=p-1. Previously, there existed complete characterizations of correct transformation only for the cases where q=p-1 orq=1. This gap is filled by giving formal necessary and sufficient conditions for correct transformation of a p-nested loop algorithm into a q-dimensional systolic array for any q,1>or=q>or=p-1. Practical methods are presented. The techniques developed are applied to the automatic design of special purpose and programmable systolic arrays. The results also contribute toward automatic compilation onto more general purpose programmable arrays. Synthesis of linear and planar systolic array implementations for a three-dimensional cube-graph algorithm and a reindexed Warshall-Floyd path-finding algorithm are used to illustrate the method."},
{"Title": "Efficient robust parallel computations", "URL": "https://dl.acm.org/doi/10.1145/100216.100231", "Full Abstract": "Copyright © 1990 ACM."},
{"Title": "The ", "URL": "https://dl.acm.org/doi/10.1145/78922.78927", "Full Abstract": "Concurrency control protocols based on two-phase locking are a popular family of locking protocols that preserve serializability in general (unstructured) database systems. A concurrency control algorithm (for databases with no inherent structure) is presented that is practical, non two-phase, and allows varieties of serializable logs not possible with any commonly known locking schemes. All transactions are required to predeclare the data they intend to read or write. Using this information, the protocol anticipates the existence (or absence) of possible conflicts and hence can allow non-two-phase locking."},
{"Title": "On high-speed computing with a programmable linear array", "URL": "https://dl.acm.org/doi/10.1007/BF00127833", "Full Abstract": "No abstract available."},
{"Title": "Combining tentative and definite executions for dependable parallel computing", "URL": "https://dl.acm.org/doi/book/10.5555/99342", "Full Abstract": "No abstract available."},
{"Title": "Combining tentative and definite executions for very fast dependable parallel computing", "URL": "https://dl.acm.org/doi/10.1145/103418.103459", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "Fast Parallel Algorithms for Coloring Random Graphs", "URL": "https://dl.acm.org/doi/10.5555/647672.757138", "Full Abstract": "No abstract available."},
{"Title": "Optimal parallel algorithms for forest and term matching", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2892%2990332-A", "Full Abstract": "No abstract available."},
{"Title": "Quantum Cryptography with Imperfect Apparatus", "URL": "https://dl.acm.org/doi/10.5555/795664.796390", "Full Abstract": "Quantum key distribution, first proposed by Bennett and Brassard, provides a possible key distribution scheme whose security depends only on the quantum laws of physics. So far the protocol has been proved secure even under channel noise and detector faults of the receiver, but is vulnerable if the photon source used is imperfect. In this paper we propose and give a concrete design for a new concept, \"self-checking source\", which requires the manufacturer of the photon source to provide certain tests; these tests are designed such that, if passed, the source is guaranteed to be adequate for the security of the quantum key distribution protocol, even though the testing devices may not be built to the original specification. The main mathematical result is a structural theorem which states that, for any state in a Hilbert space, if certain EPR-type equations are satisfied, the state must be essentially the orthogonal sum of EPR pairs."},
{"Title": "An exponential lower bound on the size of algebraic decision trees for max", "URL": "https://dl.acm.org/doi/10.1007/s000370050010", "Full Abstract": "No abstract available."},
{"Title": "NQP", "URL": "https://dl.acm.org/doi/10.1016/S0020-0190%2899%2900084-8", "Full Abstract": "No abstract available."},
{"Title": "Quantum bit escrow", "URL": "https://dl.acm.org/doi/10.1145/335305.335404", "Full Abstract": "Copyright © 2000 ACM."},
{"Title": "Some perspectives on computational complexity", "URL": "https://dl.acm.org/doi/10.1145/380752.380856", "Full Abstract": "In past decades, the theory of computational complexity has flourished in terms of both the revelation of its internal structures and the unfolding of its numerous applications. In this paper we discuss several persistent and interwoven themes underlying many of these accomplishments. Chief among them are the interplay between communication and computation, the power of problem reduction, and the increasingly prominent role played by classical mathematics. We will also speculate on a few promising directions for future development of computational complexity."},
{"Title": "Read-Once Branching Programs, Rectangular Proofs of the Pigeonhole Principle and the Transversal Calculus", "URL": "https://dl.acm.org/doi/10.1007/s00493-002-0007-7", "Full Abstract": "We investigate read-once branching programs for the following search problem: given a Boolean"},
{"Title": "Classical physics and the Church--Turing Thesis", "URL": "https://dl.acm.org/doi/10.1145/602382.602411", "Full Abstract": "Would physical laws permit the construction of computing machines that are capable of solving some problems much faster than the standard computational model? Recent evidence suggests that this might be the case in the quantum world. But the question is of great interest even in the realm of classical physics. In this article, we observe that there is fundamental tension between the Extended Church--Turing Thesis and the existence of numerous seemingly intractable computational problems arising from classical physics. Efforts to resolve this incompatibility could both advance our knowledge of the theory of computation, as well as serve the needs of scientific computing."},
{"Title": "On the power of quantum fingerprinting", "URL": "https://dl.acm.org/doi/10.1145/780542.780554", "Full Abstract": "In the simultaneous message model, two parties holding"},
{"Title": "Graph entropy and quantum sorting problems", "URL": "https://dl.acm.org/doi/10.1145/1007352.1007377", "Full Abstract": "Let P = (X, <"},
{"Title": "Graph Properties and Circular Functions", "URL": "https://dl.acm.org/doi/10.5555/1009378.1009564", "Full Abstract": "In decision tree models, considerable attention has beenpaid on the effect of symmetry on computational complexity.That is, for a permutation group, how low can thecomplexity be for any boolean function invariant under __ __In this paper we investigate this question for quantum decisiontrees for graph properties, directed graph properties,and circular functions. In particular, we prove that the n-vertexScorpion graph property has quantum query complexity\\widetilde\\Theta (n^{1/2), which implies that the minimum quantumcomplexity for graph properties is strictly less than thatfor monotone graph properties (known to be \\widetilde\\Theta (n^{2/3)). Adirected graph property, SINK, is also shown to have the\\widetilde\\Theta (n^{1/2) quantum query complexity. Furthermore, we givean N-ary circular function which has the quantum querycomplexity \\widetilde\\Theta (n^{1/4). Finally, we show that for any permutationgroup, as long as is transitive, the quantumquery complexity of any function invariant to is at least\\widetilde\\Theta (n^{1/4), which implies that our examples are (almost) thebest ones in the sense of pinning down the complexity forthe corresponding permutation group."},
{"Title": "Self testing quantum apparatus", "URL": "https://dl.acm.org/doi/10.5555/2011827.2011830", "Full Abstract": "We study, in the context of quantum information and quantum communication, a configuration of devices that includes (1) a source of some unknown bipartite quantum state that is claimed to be the Bell state Φ"},
{"Title": "Oblivious and Adaptive Strategies for the Majority and Plurality Problems", "URL": "https://dl.acm.org/doi/10.5555/2958119.2958222", "Full Abstract": "In the well-studied Majority problem, we are given a set of n balls colored with two or more colors, and the goal is to use the minimum number of color comparisons to find a ball of the majority color i.e., a color that occurs for more than ï ź n/2 ï ź times. The Plurality problem has exactly the same setting while the goal is to find a ball of the dominant color i.e., a color that occurs most often. Previous literature regarding this topic dealt mainly with adaptive strategies, whereas in this paper we focus more on the oblivious i.e., non-adaptive strategies. Given that our strategies are oblivious, we establish a linear upper bound for the Majority problem with arbitrarily many different colors. We then show that the Plurality problem is significantly more difficult by establishing quadratic lower and upper bounds. In the end, we also discuss some generalized upper bounds for adaptive strategies in the k-color Plurality problem."},
{"Title": "On the communication complexity of co-linearity problems", "URL": "https://dl.acm.org/doi/10.1007/11549345_6", "Full Abstract": "In the"},
{"Title": "On the security of public key protocols", "URL": "https://dl.acm.org/doi/10.1109/TIT.1983.1056650", "Full Abstract": "Recently the use of public key encryption to provide secure network communication has received considerable attention. Such public key systems are usually effective against passive eavesdroppers, who merely tap the lines and try to decipher the message. It has been pointed out, however, that an improperly designed protocol could be vulnerable to an active saboteur, one who may impersonate another user or alter the message being transmitted. Several models are formulated in which the security of protocols can be discussed precisely. Algorithms and characterizations that can be used to determine protocol security in these models are given."},
{"Title": "On the Quantum Query Complexity of Local Search in Two and Three Dimensions", "URL": "https://dl.acm.org/doi/10.1109/FOCS.2006.57", "Full Abstract": "The quantum query complexity of searching for local optima has been a subject of much interest in the recent literature. For the d-dimensional grid graphs, the complexity has been determined asymptotically for all fixed d \\geqslant 5, but the lower dimensional cases present special difficulties, and considerable gaps exist in our knowledge. In the present paper we present near-optimal lower bounds, showing that the quantum query complexity for the 2-dimensional grid [n]^2 is \\Omega \\left( {n^{1/2 - \\delta   \\right), and that for the 3-dimensional grid [n]^3 is \\Omega (n^{1- \\delta  ), for any fixed \\delta \\ge 0. A general lower bound approach for this problem, initiated by Aaronson [1](based on Ambainis' adversary method [3] for quantum lower bounds), uses random walks with low collision probabilities. This approach encounters obstacles in deriving tight lower bounds in low dimensions due to the lack of degrees of freedom in such spaces. We solve this problem by the novel construction and analysis of random walks with non-uniform step lengths. The proof employs in a nontrivial way sophisticated results of Sarkozy and Szemeredi [14], Bose and Chowla [5], and Halasz [9] from combinatorial number theory, as well as less familiar probability tools like Esseen's Inequality."},
{"Title": "Oblivious and Adaptive Strategies for the Majority and Plurality Problems", "URL": "https://dl.acm.org/doi/10.1007/s00453-007-0060-0", "Full Abstract": "In the well-studied Majority problem, we are given a set of n balls colored with two or more colors, and the goal is to use the minimum number of color comparisons to find a ball of the majority color (i.e., a color that occurs for more than n/2 times). The Plurality problem has exactly the same setting while the goal is to find a ball of the dominant color (i.e., a color that occurs most often). Previous literature regarding this topic dealt mainly with adaptive strategies, whereas in this paper we focus more on the oblivious (i.e., non-adaptive) strategies. Given that our strategies are oblivious, we establish a linear upper bound for the Majority problem with arbitrarily many different colors assuming a majority label exists. We then show that the Plurality problem is significantly more difficult by establishing quadratic lower and upper bounds. In the end we also discuss some generalized upper bounds for adaptive strategies in the k-color Plurality problem."},
{"Title": "A note on universal composable zero knowledge in common reference string model", "URL": "https://dl.acm.org/doi/10.5555/1767854.1767898", "Full Abstract": "Pass observed that universal composable zero-knowledge (UCZK) protocols in the common reference string (CRS) model, where a common reference string is selected trustily by a trusted third party and is known to all players, lose deniability that is a natural property of any ZK protocol in the plain model [33]. An open problem (or, natural query) raised in the literature is: are there any other essential security properties, other than the well-known deniability property, that could be lost by universal composable zero-knowledge in the common reference string model, in comparison with UC security in the plain model? In this work, we answer this open question (or, natural query), by showing that UCZK protocols in the CRS model could lose concurrent general composability (CGC) and proof of knowledge (POK) properties that are very important and essential security implications of UCZK in the plain model. This is demonstrated by concrete attacks."},
{"Title": "A note on the feasibility of generalized universal composability", "URL": "https://dl.acm.org/doi/10.5555/1767854.1767899", "Full Abstract": "We clarify the potential limitation of the general feasibility for generalized universal composability (GUC) proposed in the recent work [8], and discuss a general principle for fully realizing universal composability. This in particular demonstrates the hardness of achieving generalized universal composability, and prevents potential misinterpretation in applications. We also propose some fixing approaches, which involve a source/session-authentic ID-based trapdoor commitment scheme via the hash-then-commit paradigm that could possibly be of independent interest."},
{"Title": "Graph Design for Secure Multiparty Computation over Non-Abelian Groups", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-89255-7_3", "Full Abstract": "Recently, Desmedt et al. studied the problem of achieving secure<em>n</em> -party computation over non-Abelian groups. Theyconsidered the passive adversary model and they assumed that theparties were only allowed to perform black-box operations over thefinite group <em>G</em> . They showed three results for the<em>n</em> -product function <em>f</em>"},
{"Title": "An object-oriented framework for the integration of interactive animation techniques", "URL": "https://dl.acm.org/doi/10.1145/122718.122730", "Full Abstract": "We present an interactive modeling and animation system that facilitates the integration of a variety of simulation and animation paradigms. This system permits the modeling of diverse objects that change in shape, appearance, and behaviour over time. Our system thus extends modeling tools to include animation controls. Changes can be effected by various methods of control, including scripted, gestural, and behavioral specification. The system is an extensible testbed that supports research in the interaction of disparate control methods embodied in controller objects. This paper discusses some of the issues involved in modeling such interactions and the mechanisms implemented to provide solutions to some of these issues.The system's object-oriented architecture uses delegation hierarchies to let objects change all of their attributes dynamically. Objects include displayable objects, controllers, cameras, lights, renderers, and user interfaces. Techniques used to obtain interactive performance include the use of data-dependency networks, lazy evaluation, and extensive caching to exploit inter- and intra-frame coherency."},
{"Title": "User-Interface Developments for the Nineties", "URL": "https://dl.acm.org/doi/10.1109/2.84899", "Full Abstract": "The purpose and history of user interfaces are briefly recounted. The language model and the implementation model of the user-computer dialogue are examined. User-centered design is discussed, and approaches to design tools are described. Key developments for the 1990s are considered. These include base technologies, 3-D user interfaces, virtual realities, multimedia and hypermedia, groupware, and intelligent agents, i.e., computer-based assistants or guides"},
{"Title": "Three-dimensional widgets", "URL": "https://dl.acm.org/doi/10.1145/147156.147199", "Full Abstract": "Copyright © 1992 ACM."},
{"Title": "Using deformations to explore 3D widget design", "URL": "https://dl.acm.org/doi/10.1145/133994.134091", "Full Abstract": "No abstract available."},
{"Title": "From Discipline in Crisis to Mature Science", "URL": "https://dl.acm.org/doi/10.5555/618976.619846", "Full Abstract": "No abstract available."},
{"Title": "Interactive shadows", "URL": "https://dl.acm.org/doi/10.1145/142621.142622", "Full Abstract": "It is often difficult in computer graphics applications to understand spatial relationships between objects in a 3D scene or effect changes to those objects without specialized visualization and manipulation techniques. We present a set of three-dimensional tools (widgets) called “shadows” that not only provide valuable perceptual cues about the spatial relationships between objects, but also provide a direct manipulation interface to constrained transformation techniques. These shadow widgets provide two advances over previous techniques. First, they provide high correlation between their own geometric feedback and their effects on the objects they control. Second, unlike some other 3D widgets, they do not obscure the objects they control."},
{"Title": "The challenges of 3D interaction", "URL": "https://dl.acm.org/doi/10.1145/259963.260500", "Full Abstract": "No abstract available."},
{"Title": "Implementing virtual reality", "URL": "https://dl.acm.org/doi/10.1145/259963.260525", "Full Abstract": "No abstract available."},
{"Title": "Introduction to Computer Graphics Macintosh Version Software of SRGP and SPHIGS Software", "URL": "https://dl.acm.org/doi/book/10.5555/1208920", "Full Abstract": "No abstract available."},
{"Title": "Introduction to Computer Graphics", "URL": "https://dl.acm.org/doi/book/10.5555/561541", "Full Abstract": "From the Publisher: This new introductory text to computer graphics is an adaptation of Computer Graphics: Principles and Practice, Second Edition, which remains the most comprehensive and authoritative work in the field. While retaining the currency and accuracy of the larger book, this abbreviated version focuses on topics essential for all beginners in computer graphics and provides expanded explanations for readers with little or no technical background. Worked examples have been added to illustrate important concepts and techniques, and program code has been written in the C language to enhance the book's usefulness. In addition, the book contains an extensive illustration program, with more than 50 full-color images. Topic coverage includes basic graphics programming, hardware, and applications. Important algorithms are included to facilitate implementation of both 2D and 3D graphics. A separate chapter covers SPHIGSa simplified dialect of the PHIGS 3D standardand coincides with the availability of an updated version of the software. Chapter 9 and presents a concise overview of interaction issues and techniques. Advanced material from the larger book has been condensed, and the mathematics needed for it has been explained carefully . The result is an accessible introduction to computer graphics, crafted to provide a solid foundation for further work in this exciting field. Features Adaptation of the definitive computer graphics book in the fieldhalf the length. Presents key concepts geared toward students with minimal technical background. Provides worked examples in C. Retains the highlevel of teaching standards of the parent graphics text."},
{"Title": "Research frontiers in virtual reality", "URL": "https://dl.acm.org/doi/10.1145/192161.192287", "Full Abstract": "No abstract available."},
{"Title": "Why is 3-D interaction so hard and what can we really do about it?", "URL": "https://dl.acm.org/doi/10.1145/192161.192299", "Full Abstract": "No abstract available."},
{"Title": "The challenges of 3D interaction", "URL": "https://dl.acm.org/doi/10.1145/191642.191652", "Full Abstract": "3D computer graphics is becoming more and more popular due to the increased availability of 3D hardware and software on all classes of computers. However, despite this growing popularity and the existence of a number of successful 3D graphics applications, particularly in CAD, CAE, and medical and scientific visualization, the field is still very immature, There are no widely accepted standards for hardware or software platforms; learning to implement or use 3D graphics software is still extremely laborious; and the most effective ways for humans to interact with synthetic 3D environments are still not clear."},
{"Title": "Interactive visualization via 3D user interfaces", "URL": "https://dl.acm.org/doi/10.5555/951087.951089", "Full Abstract": "No abstract available."},
{"Title": "Computer Graphics SRGP/SPHIGS for Macintosh", "URL": "https://dl.acm.org/doi/book/10.5555/1211196", "Full Abstract": "No abstract available."},
{"Title": "Computer graphics (2nd ed. in C)", "URL": "https://dl.acm.org/doi/book/10.5555/208249", "Full Abstract": "No abstract available."},
{"Title": "Standardisation—opportunity or constraint? (panel session)", "URL": "https://dl.acm.org/doi/10.1145/218380.218534", "Full Abstract": "Copyright © 1995 Copyright is held by the owner/author(s)."},
{"Title": "50 years after “As we may think”", "URL": "https://dl.acm.org/doi/10.1145/227181.227187", "Full Abstract": "Copyright © 1996 ACM."},
{"Title": "A case study in high-end communications technology in distance learning", "URL": "https://dl.acm.org/doi/10.5555/1253524.1253778", "Full Abstract": "The five principal investigators (PIs) of the graphics and visualization center, a distributed NSF science and technology center, report on the lessons learned from reaching an interactive graduate-level seminar involving their five sites over a high-end communications infrastructure. The experiences related will be of value to institutions presently conducting or considering distance learning. Although the experiences reported here are based an a high-end scenario available to this research center, the present trends in technology and distance education should make these observations applicable in the imminent future to a large number of educational institutions."},
{"Title": "Reflections and observations", "URL": "https://dl.acm.org/doi/10.5555/1253524.1253826", "Full Abstract": "van Dam began teaching computer graphics just as the field was beginning, in the early 1960s. His first class was not of graduate students or post-docs, but was a group of forward thinking high school teachers and their best students. It was this experience that inspired van Dam to pursue a career in academia. For the past 30 years he has taught at Brown University where he co-founded the department of computer science and for 10 years served as its first chairman. The past five years have seen exceptionally strong demand for integration of computers into K-12 curricula driven both by the media hype surrounding computer technology and networks and the increasing pressure on educational institutions to improve scope and quality of education in a publicly demonstrable fashion. These forces have combined to produce a time of great potential for education but also one of great danger. Educators and administrators must consider how can this technology be used to promote rigorous and useful learning rather merely adding glitz to traditional types of assignments. Educators and administrators must also be aware that although computers can bring education to a more diverse and expanding audience, the same technology can serve to divide societies into groups of haves and have nots. van Dam analyzes several means of approaching these and other issues, drawing on current research findings and using real life examples from his experience as a researcher educator, and Director of the Graphics and Visualization Center."},
{"Title": "Truthful and Competitive Double Auctions", "URL": "https://dl.acm.org/doi/10.5555/647912.740970", "Full Abstract": "In this paper we consider the problem of designing a mechanism for double auctions where bidders each bid to buy or sell one unit of a single commodity. We assume that each bidder's utility value for the item is private to them and we focus on truthful mechanisms, ones were the bidders' optimal strategy is to bid their true utility. The profit of the auctioneer is the difference between the total payments from buyers and total to the sellers. We aim to maximize this profit. We extend the competitive analysis framework of basic auctions [9] and give an upper bound on the profit of any truthful double auction. We then reduce the competitive double auction problem to basic auctions by showing that any competitive basic auction can be converted into a competitive double auction with a competitive ratio of twice that of the basic auction. In addition, we show that better competitive ratios can be obtained by directly adapting basic auction techniques to the double auction problem. This result provides insight into the design of profit maximizing mechanisms in general."},
{"Title": "Mechanism Design for Fun and Profit", "URL": "https://dl.acm.org/doi/10.5555/647912.740971", "Full Abstract": "The emergence of the Internet as one of the most important arenas for resource sharing between parties with diverse and selfish interests has led to a number of fascinating and new algorithmic problems. In these problems, one must solicit the inputs to each computation from participants (or agents) whose goal is to manipulate the computation to their own advantage. Until fairly recently, failure models in computer science have not dealt the notion of selfish participants who \"play by the rules\" only when it fits them. To deal with this, algorithms must be designed so as to provide motivation to the participants to \"play along\". Recent work in this area, has drawn on ideas from game theory and microeconomics, and specifically from the field of mechanism design. The goal is to design protocols so that rational agents will be motivated to adhere to the protocol. A specific focus has been on truthful mechanisms in which selfish agents are motivated to reveal their true inputs.In the first part of the talk, we survey recent work in the area of algorithm mechanism design. In the second part of the talk, we focus on mechanism design specifically geared at maximizing the profit of the mechanism designer. In particular, we consider a class of dynamic pricing problems motivated by the same computational and economic trends. We describe a class of generalized auction problems as well as a competitive framework that can be used to evaluate solutions to these problems. We present a number of results on the design of profit-maximizing truthful generalized auctions. This is joint work with Amos Fiat, Andrew Goldberg and Jason Hartline."},
{"Title": "On profit-maximizing envy-free pricing", "URL": "https://dl.acm.org/doi/10.5555/1070432.1070598", "Full Abstract": "We study the problem of pricing items for sale to consumers so as to maximize the seller's revenue. We assume that for each consumer, we know the maximum amount he would be willing to pay for each bundle of items, and want to find pricings of the items with corresponding allocations that maximize seller profit and at the same time are"},
{"Title": "Beyond VCG", "URL": "https://dl.acm.org/doi/10.1109/SFCS.2005.25", "Full Abstract": "We study truthful mechanisms for auctions in which the auctioneer is trying to hire a team of agents to perform a complex task, and paying them for their work. As common in the field of mechanism design, we assume that the agents are selfish and will act in such a way as to maximize their profit, which in particular may include misrepresenting their true incurred cost. Our first contribution is a new and natural definition of the frugality ratio of a mechanism, measuring the amount by which a mechanism \"overpays\", and extending previous definitions to all monopoly-free set systems. After reexamining several known results in light of this new definition, we proceed to study in detail shortest path auctions and \"r-out-of-k sets\" auctions. We show that when individual set systems (e.g., graphs) are considered instead of worst cases over all instances, these problems exhibit a rich structure, and the performance of mechanisms may be vastly different. In particular, we show that the wellknown VCG mechanism may be far from optimal in these settings, and we propose and analyze a mechanism that is always within a constant factor of optimal."},
{"Title": "On list update and work function algorithms", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2801%2900253-5", "Full Abstract": "The"},
{"Title": "Is the Open Way a Better Way? Digital Forensics Using Open Source Tools", "URL": "https://dl.acm.org/doi/10.1109/HICSS.2007.301", "Full Abstract": "The subject of digital forensics can be quite challenging. Digital forensics is in its infancy and teaching digital forensics includes the techniques as well as the tools that assist in the process. This article discusses the tools used in computer forensics, compares an open source tool to two commercial tools, and the advantages and disadvantages of all three tools in an academic environment. A team of four senior students sponsored by two faculty members established the project scope and requirements, presented three prototypes, and detailed the considerations of using open source tools. The same image was used to measure the performance of each software tool. The team found that the three tools provided the same results with different degrees of difficulty. The end results indicate that Open Source tools are a very good verification of evidence found using other products and should be included in the academic environment."},
{"Title": "Cheap labor can be expensive", "URL": "https://dl.acm.org/doi/10.5555/1283383.1283459", "Full Abstract": "We study markets in which consumers are trying to hire a team of agents to perform a complex task. Each agent in the market prices their labor, and based on these prices, consumers hire the cheapest available team capable of doing the job they need done. We define the"},
{"Title": "Ad Auctions --- Current and Future Research", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-72870-2_41", "Full Abstract": "An exploding market has emerged during the last few years on the internet, the market of sponsored search slots. Advertisers are able to buy space on the webpages produced by popular search engines and place advertisements to promote their products alongside the regular algorithmic search results. The allocation of these advertising slots and their pricing is done via auctions. Since the introduction of this concept in 1998, sponsored search has evolved into a major source of revenue for internet giants such as Google, Yahoo!, MSN and others. Its success can be attributed partly to its effectiveness as a form of highly targeted advertising, and partly to the appealing framework that allows even small-scale advertisers to use it easily and effectively while only paying when their ad is clicked upon."},
{"Title": "Greedy bidding strategies for keyword auctions", "URL": "https://dl.acm.org/doi/10.1145/1250910.1250949", "Full Abstract": "How should players bid in keyword auctions such as those used by Google, Yahoo! and MSN?allWe consider greedy bidding strategies for a repeated auction on a single keyword, where in each round, each player chooses some optimal bid for the next round, assuming that the other players merely repeat their previous bid. We study the revenue, convergence and robustness properties of such strategies. Most interesting among these is a strategy we call the"},
{"Title": "Balloon Popping With Applications to Ascending Auctions", "URL": "https://dl.acm.org/doi/10.1109/FOCS.2007.15", "Full Abstract": "We study the power of ascending auctions in a scenario in which a seller is selling a collection of identical items to anonymous unit-demand bidders. We show that even with full knowledge of the set of bidders' private valuations for the items, if the bidders are ex-ante identical, no ascending auction can extract more than a constant times the revenue of the best fixed price scheme. This problem is equivalent to the problem of coming up with an optimal strategy for blowing up indistinguishable balloons with known capacities in order to maximize the amount of contained air. We show that the algorithm which simply inflates all balloons to a fixed volume is close to optimal in this setting."},
{"Title": "Auctions for structured procurement", "URL": "https://dl.acm.org/doi/10.5555/1347082.1347116", "Full Abstract": "This paper considers a general setting for structured procurement and the problem a buyer faces in designing a procurement mechanism to maximize profit. This brings together two agendas in algorithmic mechanism design, frugality in procurement mechanisms (e.g., for paths and spanning trees) and profit maximization in auctions (e.g., for digital goods). In the standard approach to frugality in procurement, a buyer attempts to purchase a set of elements that satisfy a feasibility requirement as cheaply as possible. For profit maximization in auctions, a seller wishes to sell some number of goods for as much as possible. We unify these objectives by endowing the buyer with a decreasing marginal benefit per feasible set purchased and then considering the problem of designing a mechanism to buy a number of sets which maximize the buyer's profit, i.e., the difference between their benefit for the sets and the cost of procurement. For the case where the feasible sets are bases of a matroid, we follow the approach of reducing the mechanism design optimization problem to a mechanism design decision problem. We give a"},
{"Title": "Improved Approximation Algorithms for Budgeted Allocations", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-70575-8_16", "Full Abstract": "We provide a 3/2-approximation algorithm for an offline budgetedallocations problem with applications to sponsored search auctions.This an improvement over the"},
{"Title": "On the Equilibria and Efficiency of the GSP Mechanism in Keyword Auctions with Externalities", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-92185-1_69", "Full Abstract": "In the increasingly important market of online search advertising, a multitude of parameters affect the performance of advertising campaigns and their ability to attract users' attention enough to produce clicks. Thus far, the majority of the relevant literature assumed an advertisement's probability of receiving a click to be dependent on the advertisement's quality and its position in the sponsored search list, but independent of the other advertisements shown on the same webpage."},
{"Title": "Approximating Matches Made in Heaven", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02927-1_23", "Full Abstract": "Motivated by applications in online dating and kidney exchange, we study a stochastic matching problem in which we have a random graph"},
{"Title": "Algorithms for Data Migration", "URL": "https://dl.acm.org/doi/10.5555/3118232.3118517", "Full Abstract": "The data migration problem is the problem of computing a plan for moving data objects stored on devices in a network from one configuration to another. Load balancing or changing usage patterns might necessitate such a rearrangement of data. In this paper, we consider the case where the objects are fixed-size and the network is complete. Our results are both theoretical and empirical. Our main theoretical results are (1) a polynomial time algorithm for finding a near-optimal migration plan in the presence of space constraints when a certain number of additional nodes is available as temporary storage, and (2) a 3/2-approximation algorithm for the case where data must be migrated directly to its destination. We also run extensive experiments on several algorithms for various data migration problems and show that empirically, many algorithms perform better in practice than their theoretical bounds suggest. We conclude that many of the algorithms we present are both practical and effective for data migration."},
{"Title": "Integrality gaps of linear and semi-definite programming relaxations for Knapsack", "URL": "https://dl.acm.org/doi/10.5555/2018158.2018182", "Full Abstract": "In this paper, we study the integrality gap of the Knapsack linear program in the Sherali-Adams and Lasserre hierarchies. First, we show that an integrality gap of 2 - ε persists up to a linear number of rounds of Sherali-Adams, despite the fact that Knapsack admits a fully polynomial time approximation scheme [24, 30]. Second, we show that the Lasserre hierarchy closes the gap quickly. Specifically, after"},
{"Title": "Prior-independent multi-parameter mechanism design", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-25510-6_11", "Full Abstract": "In a unit-demand multi-unit multi-item auction, an auctioneer is selling a collection of different items to a set of agents each interested in buying at most unit. Each agent has a different private value for each of the items. We consider the problem of designing a truthful auction that maximizes the auctioneer's profit in this setting. Previously, there has been progress on this problem in the setting in which each value is drawn from a known prior distribution. Specifically, it has been shown how to design auctions tailored to these priors that achieve a constant factor approximation ratio [2, 5]. In this paper, we present a prior-independent auction for this setting. This auction is guaranteed to achieve a constant fraction of the optimal expected profit for a large class of, so called, \"regular\" distributions, without specific knowledge of the distributions."},
{"Title": "Approximately revenue-maximizing auctions for deliberative agents", "URL": "https://dl.acm.org/doi/10.5555/2900728.2900914", "Full Abstract": "In many real-world auctions, a bidder does not know her exact value for an item, but can perform a costly deliberation to reduce her uncertainty. Relatively little is known about such deliberative environments, which are fundamentally different from classical auction environments. In this paper, we propose a new approach that allows us to leverage classical revenue-maximization results in deliberative environments. In particular, we use Myerson (1981) to construct the first nontrivial (i.e., dependent on deliberation costs) upper bound on revenue in deliberative auctions. This bound allows us to apply existing results in the classical environment to a deliberative environment. In addition, we show that in many deliberative environments the only optimal dominant-strategy mechanisms take the form of sequential posted-price auctions."},
{"Title": "Evaluating competitive game balance with restricted play", "URL": "https://dl.acm.org/doi/10.5555/3014629.3014635", "Full Abstract": "Game balancing is the fine-tuning phase in which a functioning game is adjusted to be deep, fair, and interesting. Balancing is difficult and time-consuming, as designers must repeatedly tweak parameters, and run lengthy playtests to evaluate the effects of these changes. If designers could receive immediate feedback on their designs, they could explore a vast space of variations, and select only the most promising games for playtesting. Such automated design feedback has been difficult to achieve, as there is no mathematical formulation of game balance that unifies many of its forms. We argue for a formulation in which carefully restricted agents are played against standard agents. We develop this"},
{"Title": "Magellan", "URL": "https://dl.acm.org/doi/10.14778/2994509.2994535", "Full Abstract": "Entity matching (EM) has been a long-standing challenge in data management. Most current EM works focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present as a solution Magellan, a new kind of EM systems. Magellan is novel in four important aspects. (1) It provides how-to guides that tell users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data analysis and Big Data stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provides a powerful scripting environment to facilitate interactive experimentation and quick \"patching\" of the system. We describe research challenges raised by Magellan, then present extensive experiments with 44 students and users at several organizations that show the promise of the Magellan approach."},
{"Title": "Magellan", "URL": "https://dl.acm.org/doi/10.14778/3007263.3007314", "Full Abstract": "Entity matching (EM) has been a long-standing challenge in data management. Most current EM works, however, focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present Magellan, a new kind of EM systems that addresses these limitations. Magellan is novel in four important aspects. (1) It provides a how-to guide that tells users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data science stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provide a powerful scripting environment to facilitate interactive experimentation and allow users to quickly write code to \"patch\" the system. We have extensively evaluated Magellan with 44 students and users at various organizations. In this paper we propose demonstration scenarios that show the promise of the Magellan approach."},
{"Title": "Falcon", "URL": "https://dl.acm.org/doi/10.1145/3035918.3035960", "Full Abstract": "Many works have applied crowdsourcing to entity matching (EM). While promising, these approaches are limited in that they often require a developer to be in the loop. As such, it is difficult for an organization to deploy multiple crowdsourced EM solutions, because there are simply not enough developers. To address this problem, a recent work has proposed"},
{"Title": "Human-in-the-Loop Challenges for Entity Matching", "URL": "https://dl.acm.org/doi/10.1145/3077257.3077268", "Full Abstract": "Entity matching (EM) has been a long-standing challenge in data management. In the past few years we have started two major projects on EM (Magellan and Corleone/Falcon). These projects have raised many human-in-the-loop (HIL) challenges. In this paper we discuss these challenges. In particular, we show how these challenges forced us to revise our solution architecture, from a typical RDBMS-style architecture to a very human-centric one, in which human users are first-class objects driving the EM process, using tools at pain-point places. We discuss how such solution architectures can be viewed as combining \"tools in the loop\" with \"human in the loop\". Finally, we discuss lessons learned which can potentially be applied to other problem settings. We also hope that more researchers will investigate EM, as it can be a rich \"playground\" for HIL research."},
{"Title": "Deep Learning for Entity Matching", "URL": "https://dl.acm.org/doi/10.1145/3183713.3196926", "Full Abstract": "Entity matching (EM) finds data instances that refer to the same real-world entity. In this paper we examine applying deep learning (DL) to EM, to understand DL's benefits and limitations. We review many DL solutions that have been developed for related matching tasks in text processing (e.g., entity linking, textual entailment, etc.). We categorize these solutions and define a space of DL solutions for EM, as embodied by four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Next, we investigate the types of EM problems for which DL can be helpful. We consider three such problem types, which match structured data instances, textual instances, and dirty instances, respectively. We empirically compare the above four DL solutions with Magellan, a state-of-the-art learning-based EM solution. The results show that DL does not outperform current solutions on structured EM, but it can significantly outperform them on textual and dirty EM. For practitioners, this suggests that they should seriously consider using DL for textual and dirty EM problems. Finally, we analyze DL's performance and discuss future research directions."},
{"Title": "Human-in-the-Loop Data Analysis", "URL": "https://dl.acm.org/doi/10.1145/3209900.3209913", "Full Abstract": "In the past few years human-in-the-loop data analysis (HILDA) has received significant growing attention. Most HILDA works have focused on concrete problems. In this paper I take a step back and discuss several \"big picture\" questions regarding HILDA. First, I discuss problems that I believe should fall under the scope of the field, including some that have received little attention, such as fostering user communities that develop data repositories and tools. Next, I discuss important aspects in developing HILDA solutions that I believe should receive more attention. These include solving problems that real users care about, developing how-to guides to users, building end-to-end systems (such as extending the \"Pandas system\"), developing challenges and benchmarks, and developing a theory of human data interaction. Finally, I speculate about the future of the field, and discuss the dangers it can face, given that many other communities are also working on related problems. I argue that a focus on end-to-end problems and system building is important for us to thrive and make significant impacts."},
{"Title": "Cloudmatcher", "URL": "https://dl.acm.org/doi/10.14778/3229863.3236255", "Full Abstract": "As data science applications proliferate, more and more lay users must perform data integration (DI) tasks, which used to be done by sophisticated CS developers. Thus, it is increasingly critical that we develop hands-off DI services, which lay users can use to perform such tasks without asking for help from developers. We propose to demonstrate such a service. Specifically, we will demonstrate CloudMatcher, a hands-off cloud/crowd service for entity matching (EM). To use CloudMatcher to match two tables, a lay user only needs to upload them to the CloudMatcher's Web page then iteratively label a set of tuple pairs as match/no-match. Alternatively, the user can enlist a crowd of workers to label the pairs. In either case, the lay user can easily perform EM end-to-end without having to involve any developers. Cloud-Matcher has been used in several domain science projects at UW-Madison and at several organizations, and is scheduled to be deployed in a large company in Summer 2018. In the demonstration we will show how easy it is for lay users to perform EM (either via interactive labeling or crowdsourcing), how users can easily create and experiment with a range of EM workflows, and how CloudMatcher can scale to many concurrent users and large datasets."},
{"Title": "Smurf", "URL": "https://dl.acm.org/doi/10.14778/3291264.3291272", "Full Abstract": "We argue that more attention should be devoted to developing self-service string matching (SM) solutions, which lay users can easily use. We show that Falcon, a self-service entity matching (EM) solution, can be applied to SM and is more accurate than current self-service SM solutions. However, Falcon often asks lay users to label many string pairs (e.g., 770-1050 in our experiments). This is expensive, can significantly compound labeling mistakes, and takes a long time. We developed Smurf, a self-service SM solution that reduces the labeling effort by 43-76%, yet achieves comparable F"},
{"Title": "Entity Matching Meets Data Science", "URL": "https://dl.acm.org/doi/10.1145/3299869.3314042", "Full Abstract": "Entity matching (EM) finds data instances that refer to the same real-world entity. In 2015, we started the Magellan project at UW-Madison, joint with industrial partners, to build EM systems. Most current EM systems are stand-alone monoliths. In contrast, Magellan borrows ideas from the field of data science (DS), to build a new kind of EM systems, which is an ecosystem of interoperable tools. \\em This paper provides a progress report on the past 3.5 years of Magellan, focusing on the system aspects and on how ideas from the field of data science have been adapted to the EM context. We argue why EM can be viewed as a special class of DS problems, and thus can benefit from system building ideas in DS. We discuss how these ideas have been adapted to build \\pymatcher\\ and \\cloudmatcher, EM tools for power users and lay users. These tools have been successfully used in 21 EM tasks at 12 companies and domain science groups, and have been pushed into production for many customers. We report on the lessons learned, and outline a new envisioned Magellan ecosystem, which consists of not just on-premise Python tools, but also interoperable microservices deployed, executed, and scaled out on the cloud, using tools such as Dockers and Kubernetes."},
{"Title": "The Seattle Report on Database Research", "URL": "https://dl.acm.org/doi/10.1145/3385658.3385668", "Full Abstract": "Approximately every five years, a group of database researchers meet to do a self-assessment of our community, including reflections on our impact on the industry as well as challenges facing our research community. This report summarizes the discussion and conclusions of the 9th such meeting, held during October 9-10, 2018 in Seattle."},
{"Title": "CoClean: Collaborative Data Cleaning", "URL": "https://dl.acm.org/doi/10.1145/3318464.3384698", "Full Abstract": "High quality data is crucial for many applications but real-life data is often dirty. Unfortunately, automated solutions are often not trustable and are thus seldom employed in practice. In real-world scenarios, it is often necessary to resort to manual cleaning for obtaining pristine data. Existing human-in-the-loop solutions, such as Trifacta and OpenRefine, typically involve a single user. This is often error-prone, limited to a single-person expertise, and cannot scale with the ever growing volume, variety and veracity of data."},
{"Title": "Magellan", "URL": "https://dl.acm.org/doi/10.1145/3405476", "Full Abstract": "Entity matching (EM) finds data instances that refer to the same real-world entity. In 2015, we started the Magellan project at UW-Madison, jointly with industrial partners, to build EM systems. Most current EM systems are stand-alone monoliths. In contrast, Magellan borrows ideas from the field of data science (DS), to build a new kind of EM systems, which is ecosystems of interoperable tools for multiple execution environments, such as on-premise, cloud, and mobile. This paper describes Magellan, focusing on the system aspects. We argue why EM can be viewed as a special class of DS problems and thus can benefit from system building ideas in DS. We discuss how these ideas have been adapted to build <code>PyMatcher</code> and <code>CloudMatcher</code>, sophisticated on-premise tools for power users and self-service cloud tools for lay users. These tools exploit techniques from the fields of machine learning, big data scaling, efficient user interaction, databases, and cloud systems. They have been successfully used in 13 companies and domain science groups, have been pushed into production for many customers, and are being commercialized. We discuss the lessons learned and explore applying the Magellan template to other tasks in data exploration, cleaning, and integration."},
{"Title": "Deep entity matching with pre-trained language models", "URL": "https://dl.acm.org/doi/10.14778/3421424.3421431", "Full Abstract": "We present Ditto, a novel entity matching system based on pre-trained Transformer-based language models. We fine-tune and cast EM as a sequence-pair classification problem to leverage such models with a simple architecture. Our experiments show that a straight-forward application of language models such as BERT, DistilBERT, or RoBERTa pre-trained on large text corpora already significantly improves the matching quality and outperforms previous state-of-the-art (SOTA), by up to 29% of F1 score on benchmark datasets. We also developed three optimization techniques to further improve Ditto's matching capability. Ditto allows domain knowledge to be injected by highlighting important pieces of input information that may be of interest when making matching decisions. Ditto also summarizes strings that are too long so that only the essential information is retained and used for EM. Finally, Ditto adapts a SOTA technique on data augmentation for text to EM to augment the training data with (difficult) examples. This way, Ditto is forced to learn \"harder\" to improve the model's matching capability. The optimizations we developed further boost the performance of Ditto by up to 9.8%. Perhaps more surprisingly, we establish that Ditto can achieve the previous SOTA results with at most half the number of labeled data. Finally, we demonstrate Ditto's effectiveness on a real-world large-scale EM task. On matching two company datasets consisting of 789K and 412K records, Ditto achieves a high F1 score of 96.5%."},
{"Title": "Deep learning for blocking in entity matching", "URL": "https://dl.acm.org/doi/10.14778/3476249.3476294", "Full Abstract": "Entity matching (EM) finds data instances that refer to the same real-world entity. Most EM solutions perform blocking then matching. Many works have applied deep learning (DL) to matching, but far fewer works have applied DL to blocking. These blocking works are also limited in that they consider only a simple form of DL and some of them require labeled training data. In this paper, we develop the DeepBlocker framework that significantly advances the state of the art in applying DL to blocking for EM. We first define a large space of DL solutions for blocking, which contains solutions of varying complexity and subsumes most previous works. Next, we develop eight representative solutions in this space. These solutions do not require labeled training data and exploit recent advances in DL (e.g., sequence modeling, transformer, self supervision). We empirically determine which solutions perform best on what kind of datasets (structured, textual, or dirty). We show that the best solutions (among the above eight) outperform the best existing DL solution and the best existing non-DL solutions (including a state-of-the-art industrial non-DL solution), on dirty and textual data, and are comparable on structured data. Finally, we show that the combination of the best DL and non-DL solutions can perform even better, suggesting a new venue for research."},
{"Title": "The Seattle report on database research", "URL": "https://dl.acm.org/doi/10.1145/3524284", "Full Abstract": "Every five years, a group of the leading database researchers meet to reflect on their community's impact on the computing industry as well as examine current research challenges."},
{"Title": "Cloud data systems", "URL": "https://dl.acm.org/doi/10.14778/3554821.3554905", "Full Abstract": "The panel will discuss the research opportunities for the database research community in the context of cloud native data services."},
{"Title": "Effective entity matching with transformers", "URL": "https://dl.acm.org/doi/10.1007/s00778-023-00779-z", "Full Abstract": "We present"},
{"Title": "Sparkly: A Simple yet Surprisingly Strong TF/IDF Blocker for Entity Matching", "URL": "https://dl.acm.org/doi/10.14778/3583140.3583163", "Full Abstract": "Blocking is a major task in entity matching. Numerous blocking solutions have been developed, but as far as we can tell, blocking using the well-known tf/idf measure has received virtually no attention. Yet, when we experimented with tf/idf blocking using Lucene, we found it did quite well. So in this paper we examine tf/idf blocking in depth. We develop Sparkly, which uses Lucene to perform top-k tf/idf blocking in a distributed share-nothing fashion on a Spark cluster. We develop techniques to identify good attributes and tokenizers that can be used to block on, making Sparkly completely automatic. We perform extensive experiments showing that Sparkly outperforms 8 state-of-the-art blockers. Finally, we provide an in-depth analysis of Sparkly's performance, regarding both recall/output size and runtime. Our findings suggest that (a) tf/idf blocking needs more attention, (b) Sparkly forms a strong baseline that future blocking work should compare against, and (c) future blocking work should seriously consider top-k blocking, which helps improve recall, and a distributed share-nothing architecture, which helps improve scalability, predictability, and extensibility."},
{"Title": "Technical Perspective: Unicorn: A Unified Multi-Tasking Matching Model", "URL": "https://dl.acm.org/doi/10.1145/3665252.3665262", "Full Abstract": "Data integration has been a long-standing challenge for data management. It has recently received significant attention due to at least three main reasons. First, many data science projects require integrating data from disparate sources before analysis can be carried out to extract insights. Second, many organizations want to build knowledge graphs, such as Customer 360s, Product 360s, and Supplier 360s, which capture all available information about the customers, products, and suppliers of an organization. Building such knowledge graphs often requires integrating data from multiple sources. Finally, there is also an increasing need to integrate a massive amount of data to create training data for AI models, such as large language models."},
{"Title": "An axiomatic approach to existence and liveness for differential equations", "URL": "https://dl.acm.org/doi/10.1007/s00165-020-00525-0", "Full Abstract": "This article presents an axiomatic approach for deductive verification of existence and liveness for ordinary differential equations (ODEs) with differential dynamic logic (dL). The approach yields proofs that the solution of a given ODE exists long enough to reach a given target region without leaving a given evolution domain. Numerous subtleties complicate the generalization of discrete liveness verification techniques, such as loop variants, to the continuous setting. For example, ODE solutions may blow up in finite time or their progress towards the goal may converge to zero. These subtleties are handled in dL by successively refining ODE liveness properties using ODE invariance properties which have a complete axiomatization. This approach is widely applicable: several liveness arguments from the literature are surveyed and derived as special instances of axiomatic refinement in dL. These derivations also correct several soundness errors in the surveyed literature, which further highlights the subtlety of ODE liveness reasoning and the utility of an axiomatic approach. An important special case of this approach deduces (global) existence properties of ODEs, which are a fundamental part of every ODE liveness argument. Thus, all generalizations of existence properties and their proofs immediately lead to corresponding generalizations of ODE liveness arguments. Overall, the resulting library of common refinement steps enables both the sound development and justification of new ODE existence and of liveness proof rules from dL axioms. These insights are put into practice through an implementation of ODE liveness proofs in the KeYmaera X theorem prover for hybrid systems."},
{"Title": "Structured Proofs for Adversarial Cyber-Physical Systems", "URL": "https://dl.acm.org/doi/10.1145/3477024", "Full Abstract": "Many cyber-physical systems (CPS) are safety-critical, so it is important to formally verify them, e.g. in formal logics that show a model’s correctness specification"},
{"Title": "Correction to: How to model and prove hybrid systems with KeYmaera: a tutorial on safety", "URL": "https://dl.acm.org/doi/10.1007/s10009-021-00643-x", "Full Abstract": "No abstract available."},
{"Title": "Pegasus: sound continuous invariant generation", "URL": "https://dl.acm.org/doi/10.1007/s10703-020-00355-z", "Full Abstract": "are an important component in deductive verification of hybrid and continuous systems. Just like discrete invariants are used to reason about correctness in discrete systems without having to unroll their loops, continuous invariants are used to reason about differential equations without having to solve them."},
{"Title": "Verified Quadratic Virtual Substitution for Real Arithmetic", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-90870-6_11", "Full Abstract": "This paper presents a formally verified quantifier elimination (QE) algorithm for first-order real arithmetic by linear and quadratic virtual substitution (VS) in Isabelle/HOL. The Tarski-Seidenberg theorem established that the first-order logic of real arithmetic is decidable by QE. However, in practice, QE algorithms are highly complicated and often combine multiple methods for performance. VS is a practically successful method for QE that targets formulas with low-degree polynomials. To our knowledge, this is the first work to formalize VS for quadratic real arithmetic including inequalities. The proofs necessitate various contributions to the existing multivariate polynomial libraries in Isabelle/HOL. Our framework is modularized and easily expandable (to facilitate integrating future optimizations), and could serve as a basis for developing practical general-purpose QE algorithms. Further, as our formalization is designed with practicality in mind, we export our development to SML and test the resulting code on 378 benchmarks from the literature, comparing to Redlog, Z3, Wolfram Engine, and SMT-RAT. This identified inconsistencies in some tools, underscoring the significance of a verified approach for the intricacies of real arithmetic."},
{"Title": "Correction to: Differential Dynamic Logic for Hybrid Systems", "URL": "https://dl.acm.org/doi/10.1007/s10817-021-09608-w", "Full Abstract": "No abstract available."},
{"Title": "Verifying Switched System Stability With Logic", "URL": "https://dl.acm.org/doi/10.1145/3501710.3519541", "Full Abstract": "Switched systems are known to exhibit subtle (in)stability behaviors requiring system designers to carefully analyze the stability of closed-loop systems that arise from their proposed switching control laws. This paper presents a formal approach for verifying switched system stability that blends classical ideas from the controls and verification literature using differential dynamic logic (dL), a logic for deductive verification of hybrid systems. From controls, we use standard stability notions for various classes of switching mechanisms and their corresponding Lyapunov function-based analysis techniques. From verification, we use dL’s ability to verify quantified properties of hybrid systems and dL models of switched systems as looping hybrid programs whose stability can be formally specified and proven by finding appropriate loop invariants, i.e., properties that are preserved across each loop iteration. This blend of ideas enables a trustworthy implementation of switched system stability verification in the KeYmaera X prover based on dL. For standard classes of switching mechanisms, the implementation provides fully automated stability proofs, including searching for suitable Lyapunov functions. Moreover, the generality of the deductive approach also enables verification of switching control laws that require non-standard stability arguments through the design of loop invariants that suitably express specific intuitions behind those control laws. This flexibility is demonstrated on three case studies: a model for longitudinal flight control by Branicky, an automatic cruise controller, and Brockett’s nonholonomic integrator."},
{"Title": "Implicit Definitions with Differential Equations for KeYmaera X", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-10769-6_42", "Full Abstract": "Definition packages in theorem provers provide users with means of defining and organizing concepts of interest. This system description presents a new definition package for the hybrid systems theorem prover KeYmaera X based on differential dynamic logic (dL). The package adds KeYmaera X support for user-defined smooth functions whose graphs can be implicitly characterized by dL  formulas. Notably, this makes it possible to implicitly characterize functions, such as the exponential and trigonometric functions, as solutions of differential equations and then prove properties of those functions using dL’s differential equation reasoning principles. Trustworthiness of the package is achieved by minimally extending KeYmaera X ’s soundness-critical kernel with a single axiom scheme that expands function occurrences with their implicit characterization. Users are provided with a high-level interface for defining functions and non-soundness-critical tactics that automate low-level reasoning over implicit characterizations in hybrid system proofs."},
{"Title": "Formally Verified Next-generation Airborne Collision Avoidance Games in ACAS X", "URL": "https://dl.acm.org/doi/10.1145/3544970", "Full Abstract": "The design of aircraft collision avoidance algorithms is a subtle but important challenge that merits the need for provable safety guarantees. Obtaining such guarantees is nontrivial given the unpredictability of the interplay of the intruder aircraft decisions, the ownship pilot reactions, and the subtlety of the continuous motion dynamics of aircraft. Existing collision avoidance systems, such as TCAS and the Next-Generation Airborne Collision Avoidance System ACAS X, have been analyzed assuming severe restrictions on the intruder’s flight maneuvers, limiting their safety guarantees in real-world scenarios where the intruder may change its course."},
{"Title": "Verified Train Controllers for the Federal Railroad Administration Train Kinematics Model: Balancing Competing Brake and Track Forces", "URL": "https://dl.acm.org/doi/10.1109/TCAD.2022.3197690", "Full Abstract": "Automated train control improves railroad operation by safeguarding the motion of trains while increasing efficiency by enabling motion within a safe envelope. Train controllers decide when to slow trains down to avoid collisions with other trains on the track, stay inside movement authorities, and navigate slopes, curves, and tunnels safely. These systems must base their decisions on detailed motion models to guarantee the absence of overshoot of the movement authority (safety) and limit undershoot (efficiency). This article is the first to formally verify the safety of the Federal Railroad Administration freight train kinematics model with all its relevant forces and parameters, including track slope and curvature, air brake propagation, and resistive forces as computed by the Davis equation. Due to the significant competing influence of these parameters on train stopping distances, even designing train controllers is a nontrivial control challenge, which we solve using formal verification. For increased generality at reduced verification effort, we verify symbolic mathematical generalizations of the train control models and subsequently apply efficient uniform substitutions to obtain verification results for physical train control models."},
{"Title": "A First Complete Algorithm for Real Quantifier Elimination in Isabelle/HOL", "URL": "https://dl.acm.org/doi/10.1145/3573105.3575672", "Full Abstract": "We formalize a multivariate quantifier elimination (QE) algorithm in the theorem prover Isabelle/HOL. Our algorithm is complete, in that it is able to reduce"},
{"Title": "Refinements of Hybrid Dynamical Systems Logic", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-33163-3_1", "Full Abstract": "Hybrid dynamical systems describe the mixed discrete dynamics and continuous dynamics of cyber-physical systems such as aircraft, cars, trains, and robots. To justify correctness of their safety-critical controls for their physical models, differential dynamic logic ("},
{"Title": "Uniform Substitution for Dynamic Logic with Communicating Hybrid Programs", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-38499-8_6", "Full Abstract": "This paper introduces a uniform substitution calculus for"},
{"Title": "Learning to find proofs and theorems by learning to refine search strategies", "URL": "https://dl.acm.org/doi/10.5555/3600270.3600620", "Full Abstract": "We propose a new approach to automated theorem proving where an AlphaZero-style agent is self-training to refine a generic high-level expert strategy expressed as a nondeterministic program. An analogous teacher agent is self-training to generate tasks of suitable relevance and difficulty for the learner. This allows leveraging minimal amounts of domain knowledge to tackle problems for which training data is unavailable or hard to synthesize. As a specific illustration, we consider loop invariant synthesis for imperative programs and use neural networks to refine both the teacher and solver strategies."},
{"Title": "CESAR: Control Envelope Synthesis via Angelic Refinements", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-57246-3_9", "Full Abstract": "This paper presents an approach for synthesizing provably correct control envelopes for hybrid systems. Control envelopes characterize families of safe controllers and are used to monitor untrusted controllers at runtime. Our algorithm fills in the blanks of a hybrid system’s sketch specifying the desired shape of the control envelope, the possible control actions, and the system’s differential equations. In order to maximize the flexibility of the control envelope, the synthesized conditions saying which control action can be chosen when should be as permissive as possible while establishing a desired safety condition from the available assumptions, which are augmented if needed. An implicit, optimal solution to this synthesis problem is characterized using hybrid systems game theory, from which explicit solutions can be derived via symbolic execution and sound, systematic game refinements. Optimality can be recovered in the face of approximation via a dual game characterization. The resulting algorithm,"},
{"Title": "Does Every Computer Scientist Need to Know Formal Methods?", "URL": "https://dl.acm.org/doi/10.1145/3670795", "Full Abstract": "We focus on the integration of Formal Methods as mandatory theme in any Computer Science University curriculum. In particular, when considering the ACM Curriculum for Computer Science, the inclusion of Formal Methods as a mandatory Knowledge Area needs arguing for why and how does every computer science graduate benefit from such knowledge. We do not agree with the sentence “While there is a belief that formal methods are important and they are growing in importance, we cannot state that every computer science graduate will need to use formal methods in their career.” We argue that formal methods are and have to be an integral part of every computer science curriculum. Just as not all graduates will need to know how to work with databases either, it is still important for students to have a basic understanding of how data is stored and managed efficiently. The same way, students have to understand why and how methods work, what their formal background is, and how they are justified. No engineer should be ignorant of the foundations of their subject and the formal methods based on these."},
{"Title": "Complete Game Logic with Sabotage", "URL": "https://dl.acm.org/doi/10.1145/3661814.3662121", "Full Abstract": "The authors have requested that a Corrected Version of Record of this published Work be published by ACM, due to the inadvertent removal of all citations and references during the manuscript production phase. This issue affects all sections of the Work. ACM has agreed to the author's request and, in accordance with ACM policies, a Corrected Version of Record was published on August 1, 2024. For reference purposes, the originally published Version of Record is being hidden to avoid confusion within the community. The original version may be requested in connection with official legal proceedings to validate the exact version of the Work published on the original publication date."},
{"Title": "The Significance of Symbolic Logic for Scientific Education", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-71379-8_1", "Full Abstract": "This invited paper is a passionate pitch for the significance of logic in scientific education. Logic helps focus on the essential core to identify the foundations of ideas and provides corresponding longevity with the resulting approach to new and old problems. Logic operates symbolically, where each part has a precise meaning and the meaning of the whole is compositional, so a simple function of the meaning of the pieces. This compositionality in the meaning of logical operators is the basis for compositionality in reasoning about logical operators. Both semantic and deductive compositionalities help explain what happens in reasoning. The correctness-critical core of an idea or an algorithm is often expressible eloquently and particularly concisely in logic. The opinions voiced in this paper are influenced by the author’s teaching of courses on cyber-physical systems, constructive logic, compiler design, programming language semantics, and imperative programming principles. In each of those courses, different aspects of logic come up for different purposes to elucidate significant ideas particularly clearly. While there is a bias of the thoughts in this paper toward computer science, some courses have been heavily frequented by students from other majors so that some transfer of the thoughts to other science and engineering disciplines is plausible."},
{"Title": "Intersymbolic AI", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-75387-9_11", "Full Abstract": "This perspective piece calls for the study of the new field of"},
{"Title": "Competing for users' attention", "URL": "https://dl.acm.org/doi/10.1145/1772690.1772721", "Full Abstract": "Queries on major Web search engines produce complex result pages, primarily composed of two types of information: organic results, that is, short descriptions and links to relevant Web pages, and sponsored search results, the small textual advertisements often displayed above or to the right of the organic results. Strategies for optimizing each type of result in isolation and the consequent user reaction have been extensively studied; however, the interplay between these two complementary sources of information has been ignored, a situation we aim to change. Our findings indicate that their perceived relative usefulness (as evidenced by user clicks) depends on the nature of the query. Specifically, we found that, when both sources focus on the same intent, for navigational queries there is a clear competition between ads and organic results, while for non-navigational queries this competition turns into synergy."},
{"Title": "Search is dead!", "URL": "https://dl.acm.org/doi/10.1145/1772690.1772917", "Full Abstract": "Back in the heady days of 1999 and WWW8 (Toronto) we held a panel titled \"Finding Anything in the Billion Page Web: Are Algorithms the Key?\" In retrospect the answer to this question seems laughably obvious - the search industry has burgeoned on a foundation of algorithms, cloud computing and machine learning. As we move into the second decade of this millennium, we are confronted with a dizzying array of new paradigms for finding content, including social networks and location-based search and advertising. This panel pulls together senior experts from academia and the major search principals to debate whether search will continue to look anything like the 2-keywords-give-10-blue-links paradigm that Google has popularized. What do emerging approaches and paradigms - natural language search, social search, location-based search - mean for the future of search in general?"},
{"Title": "Hybrid dynamical systems logic and its refinements", "URL": "https://dl.acm.org/doi/10.1016/j.scico.2024.103179", "Full Abstract": "Hybrid dynamical systems describe the mixed discrete dynamics and continuous dynamics of cyber-physical systems such as aircraft, cars, trains, and robots. To justify correctness properties of the safety-critical control algorithms for their physical models,"},
{"Title": "Information retrieval challenges in computational advertising", "URL": "https://dl.acm.org/doi/10.1145/1835449.1835680", "Full Abstract": "Computational advertising is an emerging scientific sub-discipline, at the intersection of large scale search and text analysis, information retrieval, statistical modeling, machine learning, classification, optimization, and microeconomics. The central challenge of computational advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The aim of this tutorial is to present the state of the art in Computational Advertising, in particular in its IR-related aspects, and to expose the participants to the current research challenges in this field. The tutorial does not assume any prior knowledge of Web advertising, and will begin with a comprehensive background survey. Going deeper, our focus will be on using a textual representation of the user context to retrieve relevant ads. At first approximation, this process can be reduced to a conventional setup by constructing a query that describes the user context and executing the query against a large inverted index of ads. We show how to augment this approach using query expansion and text classification techniques tuned for the ad-retrieval problem. In particular, we show how to use the Web as a repository of query-specific knowledge and use the Web search results retrieved by the query as a form of a relevance feedback and query expansion. We also present solutions that go beyond the conventional bag of words indexing by constructing additional features using a large external taxonomy and a lexicon of named entities obtained by analyzing the entire Web as a corpus. The last part of the tutorial will be devoted to a potpourri of recent research results and open problems inspired by Computational Advertising challenges in text summarization, natural language generation, named entity recognition, computer-human interaction, and other SIGIR-relevant areas."},
{"Title": "Exploiting site-level information to improve web search", "URL": "https://dl.acm.org/doi/10.1145/1871437.1871630", "Full Abstract": "Ranking Web search results has long evolved beyond simple bag-of-words retrieval models. Modern search engines routinely employ machine learning ranking that relies on exogenous relevance signals. Yet the majority of current methods still evaluate each Web page out of context. In this work, we introduce a novel source of relevance information for Web search by evaluating each page"},
{"Title": "The new frontier of web search technology", "URL": "https://dl.acm.org/doi/10.5555/1983774.1983776", "Full Abstract": "The classic Web search experience, consisting of returning \"ten blue links\" in response to a short user query, is powered today by a mature technology where progress has become incremental and expensive. Furthermore, the \"ten blue links\" represent only a fractional part of the total Web search experience: today, what users expect and receive in response to a \"web query\" is a plethora of multi-media information extracted and synthesized from numerous sources on and off the Web. In consequence, we argue that the major technical challenges in Web search are now driven by the quest to satisfy the implicit and explicit needs of users, continuing a long evolutionary trend in commercial Web search engines going back more than fifteen years, moving from relevant document selection towards satisfactory task completion. We identify seven of these challenges and discuss them in some detail."},
{"Title": "Introduction to display advertising", "URL": "https://dl.acm.org/doi/10.1145/1935826.1935832", "Full Abstract": "Display advertising is one of the two major advertising channels on the web (in addition to search advertising). Display advertising on the Web is usually done by"},
{"Title": "Bid generation for advanced match in sponsored search", "URL": "https://dl.acm.org/doi/10.1145/1935826.1935901", "Full Abstract": "Sponsored search is a three-way interaction between advertisers, users, and the search engine. The basic ad selection in sponsored search, lets the advertiser choose the exact queries where the ad is to be shown. To increase advertising volume, many advertisers opt into"},
{"Title": "An introduction to online targeted advertising", "URL": "https://dl.acm.org/doi/10.1145/1943403.1943420", "Full Abstract": "Online user interaction is becoming increasingly personalized both via explicit means: customizations, options, add-ons, skins, apps, etc. and via implicit means, that is, deep data mining of user activities that allows automated selection of content and experiences, e.g. individualized top news stories, personalized ranking of search results, personal \"radio stations\" that capture idiosyncratic tastes from past choices, individually recommended purchases, and so on. On the other hand, the vast majority of providers of content and services (e.g. portals, search engines, social sites) are supported by advertising, which at core, is just a different type of information. Thus, not surprisingly, on-line advertising is becoming increasingly personalized as well, supported by an emerging new scientific sub-discipline,"},
{"Title": "Efficiently evaluating graph constraints in content-based publish/subscribe", "URL": "https://dl.acm.org/doi/10.1145/1963405.1963476", "Full Abstract": "We introduce the problem of evaluating graph constraints in content-based publish/subscribe (pub/sub) systems. This problem formulation extends traditional content-based pub/sub systems in the following manner: publishers and subscribers are connected via a (logical) directed graph"},
{"Title": "Highly dimensional problems in computational advertising", "URL": "https://dl.acm.org/doi/10.5555/2034063.2034068", "Full Abstract": "The central problem of Computational Advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The context could be a user entering a query in a search engine (\"sponsored search\"), a user reading a web page (\"content match\" and \"display ads\"), a user interacting with a portable device, and so on. The information about the user can vary from scarily detailed to practically nil. The number of potential advertisements might be in the billions. The number of contexts is unbound. Thus, depending on the definition of \"best match\" this problem leads to a variety of massive optimization and search problems, with complicated constraints. The solution to these problems provides the scientific and technical underpinnings of the online advertising industry, an industry estimated to surpass 28 billion dollars in US alone in 2011."},
{"Title": "Highly dimensional problems in computational advertising", "URL": "https://dl.acm.org/doi/10.5555/3121838.3121843", "Full Abstract": "The central problem of Computational Advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The context could be a user entering a query in a search engine (\"sponsored search\"), a user reading a web page (\"content match\" and \"display ads\"), a user interacting with a portable device, and so on. The information about the user can vary from scarily detailed to practically nil. The number of potential advertisements might be in the billions. The number of contexts is unbound. Thus, depending on the definition of \"best match\" this problem leads to a variety of massive optimization and search problems, with complicated constraints. The solution to these problems provides the scientific and technical underpinnings of the online advertising industry, an industry estimated to surpass 28 billion dollars in US alone in 2011."},
{"Title": "Web Page Summarization for Just-in-Time Contextual Advertising", "URL": "https://dl.acm.org/doi/10.1145/2036264.2036278", "Full Abstract": "is a type of Web advertising, which, given the URL of a Web page, aims to embed into the page the most relevant textual ads available. For static pages that are displayed repeatedly, the matching of ads can be based on prior analysis of their entire content; however, often ads need to be matched to new or dynamically created pages that cannot be processed ahead of time. Analyzing the entire content of such pages on-the-fly entails prohibitive communication and latency costs. To solve the three-horned dilemma of either low relevance or high latency or high load, we propose to use text summarization techniques paired with external knowledge (exogenous to the page) to craft short page summaries in real time."},
{"Title": "Information retrieval challenges in computational advertising", "URL": "https://dl.acm.org/doi/10.1145/2063576.2064037", "Full Abstract": "No abstract available."},
{"Title": "IR paradigms in computational advertising", "URL": "https://dl.acm.org/doi/10.1145/2348283.2348445", "Full Abstract": "The central problem in the emerging discipline of computational advertising is to find the \"best match\" between a given user in a given context and a suitable advertisement. The context could be a user entering a query in a search engine (\"sponsored search\"), a user reading a web page (\"content match\" and \"display ads\"), a user streaming a movie, and so on. In some situations, it is desirable to solve the \"dual\" optimization problem: rather then find the best ad given a user in a context, the goal is to identify the \"best audience\", i.e. the most receptive set of users and/or the most suitable contexts for a given advertising campaign. The information about the user can vary from scarily detailed to practically nil. The number of potential advertisements might be in the billions. Thus, depending on the definition of \"best match\" and \"best audience\" these problems lead to a variety of massive optimization problems, with complicated constraints, and challenging data representation and access issues."},
{"Title": "Scalable K-Means by ranked retrieval", "URL": "https://dl.acm.org/doi/10.1145/2556195.2556260", "Full Abstract": "The k-means clustering algorithm has a long history and a proven practical performance, however it does not scale to clustering millions of data points into thousands of clusters in high dimensional spaces. The main computational bottleneck is the need to recompute the nearest centroid for every data point at every iteration, aprohibitive cost when the number of clusters is large. In this paper we show how to reduce the cost of the k-means algorithm by large factors by adapting ranked retrieval techniques. Using a combination of heuristics, on two real life data sets the wall clock time per iteration is reduced from 445 minutes to less than 4, and from 705 minutes to 1.4, while the clustering quality remains within 0.5% of the k-means quality."},
{"Title": "Big Data", "URL": "https://dl.acm.org/doi/10.1145/2684822.2697027", "Full Abstract": "The Gartner's 2014 Hype Cycle released last August moves Big Data technology from the Peak of Inflated Expectations to the beginning of the Trough of Disillusionment when interest starts to wane as reality does not live up to previous promises. As the hype is starting to dissipate it is worth asking what Big Data (however defined) means from a scientific perspective: Did the emergence of gigantic corpora exposed the limits of classical information retrieval and data mining and led to new concepts and challenges, the way say, the study of electromagnetism showed the limits of Newtonian mechanics and led to Relativity Theory, or is it all just \"sound and fury, signifying nothing\", simply a matter of scaling up well understood technologies? To answer this question, we have assembled a distinguished panel of eminent scientists, from both Industry and Academia: Lada Adamic (Facebook), Michael Franklin (University of California at Berkeley), Maarten de Rijke (University of Amsterdam), Eric Xing (Carnegie Mellon University), and Kai Yu (Baidu) will share their point of view and take questions from the moderator and the audience."},
{"Title": "A Personal Perspective and Retrospective on Web Search Technology", "URL": "https://dl.acm.org/doi/10.1145/2983323.2983368", "Full Abstract": "This talk is a review of some Web research and predictions that I co-authored over the last two decades: both what turned out gratifyingly right and what turned out embarrassingly wrong. Topics will include near-duplicates, the Web graph, query intent, inverted indices efficiency, and others. While this seems a completely idiosyncratic collection there are in fact concealed connections that offer good clues to the big question: what will happen next?"},
{"Title": "Email Category Prediction", "URL": "https://dl.acm.org/doi/10.1145/3041021.3055166", "Full Abstract": "According to recent estimates, about 90% of consumer received emails are machine-generated. Such messages include shopping receipts, promotional campaigns, newsletters, booking confirmations, etc. Most such messages are created by populating a fixed template with a small amount of personalized information, such as name, salutation, reservation numbers, dates, etc. Web mail providers (Gmail, Hotmail, Yahoo) are leveraging the structured nature of such emails to extract salient information and use it to improve the user experience: e.g. by automatically entering reservation data into a user calendar, or by sending alerts about upcoming shipments. To facilitate these extraction tasks it is helpful to classify templates according to their category, e.g. restaurant reservations or bill reminders, since each category triggers a particular user experience."},
{"Title": "A Call to Arms", "URL": "https://dl.acm.org/doi/10.1145/3159652.3160603", "Full Abstract": "A quarter-century ago Web search stormed the world: within a few years the Web search box became a standard tool of daily life ready to satisfy informational, transactional, and navigational queries needed for some task completion. However, two recent trends are dramatically changing the box»s role: first, the explosive spread of smartphones brings significant computational resources literally into the pockets of billions of users; second, recent technological advances in machine learning and artificial intelligence, and in particular in speech processing led to the wide deployment of assistive AI systems, culminating in personal digital assistants. Along the way, the \"Web search box\" has become an \"assistance request box\" (implicit, in the case of voice-activated assistants) and likewise, many other information processing systems (e.g. e-mail, navigation, personal search, etc) have adopted assistive aspects."},
{"Title": "Delphic Costs and Benefits in Web Search: A Utilitarian and Historical Analysis", "URL": "https://dl.acm.org/doi/10.1145/3616855.3638208", "Full Abstract": "We present a new framework to conceptualize and operationalize the total user experience of search, by studying the entirety of a search journey from an utilitarian point of view."},
{"Title": "Experience with grapevine: the growth of a distributed system", "URL": "https://dl.acm.org/doi/10.5555/59309.59340", "Full Abstract": "No abstract available."},
{"Title": "Fault tolerance support in distributed systems", "URL": "https://dl.acm.org/doi/10.1145/504136.504145", "Full Abstract": "No abstract available."},
{"Title": "Position paper for SIGOPS workshop on fault tolerance support in distributed systems", "URL": "https://dl.acm.org/doi/10.1145/122140.122152", "Full Abstract": "No abstract available."},
{"Title": "An assessment of the remote procedure call mechanism", "URL": "https://dl.acm.org/doi/10.1145/506378.506388", "Full Abstract": "For more than ten years, many people (including myself) have been advocating the use of \"Remote Procedure Calls\" (RPC) as the primary technique for communication among the components of a distributed system. Many researchers have explored the design space for RPC, and they have written many worthy papers on what they discovered. Many commercial vendors have constructed and purveyed RPC systems. Many standards committees have added their own distinctive values to the enterprise. Where has all this effort got us ?It is time to consider the successes and failures, benefits and costs, of our advocacy of RCP."},
{"Title": "Network objects", "URL": "https://dl.acm.org/doi/10.1145/168619.168637", "Full Abstract": "A network object is an object whose methods can be invoked over a network. This paper describes the design, implementation, and early experience with a network objects system for Modula-3. The system is novel for its overall simplicity. The paper includes a thorough description of realistic marshaling algorithms for network objects."},
{"Title": "A coherent distributed file cache with directory write-behind", "URL": "https://dl.acm.org/doi/10.1145/176575.176577", "Full Abstract": "Extensive caching is a key feature of the Echo distributed file system. Echo client machines maintain coherent caches of file and directory data and properties, with write-behind (delayed write-back) of"},
{"Title": "Network objects", "URL": "https://dl.acm.org/doi/10.1002/spe.4380251305", "Full Abstract": "No abstract available."},
{"Title": "Secure Web tunneling", "URL": "https://dl.acm.org/doi/10.1016/S0169-7552%2898%2900048-8", "Full Abstract": "No abstract available."},
{"Title": "Secure Web tunneling", "URL": "https://dl.acm.org/doi/10.5555/297805.297953", "Full Abstract": "No abstract available."},
{"Title": "A cooperative internet backup scheme", "URL": "https://dl.acm.org/doi/10.5555/1247340.1247343", "Full Abstract": "We present a novel peer-to-peer backup technique that allows computers connected to the Internet to back up their data cooperatively: Each computer has a set of partner computers, which collectively hold its backup data. In return, it holds a part of each partner's backup data. By adding redundancy and distributing the backup data across many partners, a highly-reliable backup can be obtained in spite of the low reliability of the average Internet machine."},
{"Title": "Access control in a world of software diversity", "URL": "https://dl.acm.org/doi/10.5555/1251123.1251145", "Full Abstract": "We describe a new design for authentication and access control. In this design, principals embody a flexible notion of authentication. They are compound principals that reflect the identities of the programs that have executed, even those of login programs. These identities are based on a naming tree. Our access control lists are patterns that recognize principals. We show how this design supports a variety of access control scenarios."},
{"Title": "Autonet", "URL": "https://dl.acm.org/doi/10.1109/49.105178", "Full Abstract": "Autonet is a self-configuring local area network composed of switches interconnected by 100 Mb/s, full-duplex, point-to-point links. The switches contain 12 ports that are internally connected by a full crossbar. Switches use cut-through to achieve a packet forwarding latency as low as 2 ms/switch. Any switch port can be cabled to any other switch port or to a host network controller. A processor in each switch monitors the network's physical configuration. A distributed algorithm running on the switch processor computes the routes packets are to follow and fills in the packet forwarding table in each switch. With Autonet, distinct paths through the set of network links can carry packets in parallel, allowing many pairs of hosts to communicate simultaneously at full link bandwidth. A 30-switch network with more than 100 hosts has been the service network for Digital's Systems Research Center since February 1990"},
{"Title": "Dryad", "URL": "https://dl.acm.org/doi/10.1145/1272996.1273005", "Full Abstract": "Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad application combines computational \"vertices\" with communication \"channels\" to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through flies, TCP pipes, and shared-memory FIFOs."},
{"Title": "Authorizing applications in singularity", "URL": "https://dl.acm.org/doi/10.1145/1272996.1273033", "Full Abstract": "We describe a new design for authorization in operating systems in which applications are first-class entities. In this design, principals reflect application identities. Access control lists are patterns that recognize principals. We present a security model that embodies this design in an experimental operating system, and we describe the implementation of our design and its performance in the context of this operating system."},
{"Title": "A design for high-performance flash disks", "URL": "https://dl.acm.org/doi/10.1145/1243418.1243429", "Full Abstract": "Most commodity flash disks exhibit very poor performance when presented with writes that are not sequentially ordered. We argue that performance can be significantly improved through the addition of sufficient RAM to hold data structures describing a fine-grain mapping between disk logical blocks and physical flash addresses. We present a design that accomplishes this."},
{"Title": "Automatic mutual exclusion", "URL": "https://dl.acm.org/doi/10.5555/1361397.1361400", "Full Abstract": "We propose a new concurrent programming model, Automatic Mutual Exclusion (AME). In contrast to lock-based programming, and to other programming models built over software transactional memory (STM), we arrange that all shared state is implicitly protected unless the programmer explicitly specifies otherwise. An AME program is composed from serializable atomic fragments. We include features allowing the programmer to delimit and manage the fragments to achieve appropriate program structure and performance. We explain how I/O activity and legacy code can be incorporated within an AME program. Finally, we outline ways in which future work might expand on these ideas. The resulting programming model makes it easier to write correct code than incorrect code. It favors correctness over performance for simple programs, while allowing advanced programmers the expressivity they need."},
{"Title": "Semantics of transactional memory and automatic mutual exclusion", "URL": "https://dl.acm.org/doi/10.1145/1328438.1328449", "Full Abstract": "Software Transactional Memory (STM) is an attractive basis for the development of language features for concurrent programming. However, the semantics of these features can be delicate and problematic. In this paper we explore the tradeoffs between semantic simplicity, the viability of efficient implementation strategies, and the flexibilityof language constructs. Specifically, we develop semantics and type systems for the constructs of the Automatic Mutual Exclusion (AME) programming model; our results apply also to other constructs, such as atomic blocks. With this semantics as a point of reference, we study several implementation strategies. We model STM systems that use in-place update, optimistic concurrency, lazy conflict detection, and roll-back. These strategies are correct only under non-trivial assumptions that we identify and analyze. One important source of errors is that some efficient implementations create dangerous 'zombie' computations where a transaction keeps running after experiencing a conflict; the assumptions confine the effects of these computations."},
{"Title": "Implementation and Use of Transactional Memory with Dynamic Separation", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-00722-4_6", "Full Abstract": "We introduce the design and implementation of dynamic separation (DS) as a programming discipline for using transactional memory. Our approach is based on the programmer indicating which objects can be updated in transactions, which can be updated outside transactions, and which are read-only. We introduce explicit operations that identify transitions between these modes of access. We show how to guarantee strong semantics for programs that use these DS operations correctly, even over an STM implementation that provides only weak atomicity. We describe a run-time checking tool (analogous to a data-race detector) that can test whether or not a program is using DS operations correctly. We also examine the use of DS in an asynchronous IO library."},
{"Title": "Semantics of transactional memory and automatic mutual exclusion", "URL": "https://dl.acm.org/doi/10.1145/1889997.1889999", "Full Abstract": "Software Transactional Memory (STM) is an attractive basis for the development of language features for concurrent programming. However, the semantics of these features can be delicate and problematic. In this article we explore the trade-offs semantic simplicity, the viability of efficient implementation strategies, and the flexibility of language constructs. Specifically, we develop semantics and type systems for the constructs of the Automatic Mutual Exclusion (AME) programming model; our results apply also to other constructs, such as atomic blocks. With this semantics as a point of reference, we study several implementation strategies. We model STM systems that use in-place update, optimistic concurrency, lazy conflict detection, and rollback. These strategies are correct only under nontrivial assumptions that we identify and analyze. One important source of errors is that some efficient implementations create dangerous “zombie” computations where a transaction keeps running after experiencing a conflict; the assumptions confine the effects of these computations."},
{"Title": "Global authentication in an untrustworthy world", "URL": "https://dl.acm.org/doi/10.5555/2490483.2490502", "Full Abstract": "With the advent in the 1980's of truly global hierarchical naming (via the Domain Name Service), security researchers realized that the trust relationships needed to authenticate principals would often not follow the naming hierarchy [1,13]. The most successful non-hierarchical authentication schemes are based on X.509 and RFC 5280, as used for example in TLS and Authenticode. These are extremely widely deployed, and are trusted for most people's everyday use of the Internet. Unfortunately several incidents in the last few years have proved that this trust is misplaced [9,14]. We explore the weaknesses of this machinery, helped by a large database of X.509 certificates, and we offer an analysis technique and a suggestion for how the trust could be enhanced."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2465085.2465090", "Full Abstract": "Welcome to the latest installment of \"EduBits,\" your quarterly pipeline to new and exciting happenings in the world of ACM education. In this edition, the ACM Education Board kicks off a National Science Foundation (NSF)-funded effort to advance cybersecurity education. Also, Cameron Wilson, Director of ACM's Public Policy Office, offers his perspectives from Capitol Hill, and Cherri M. Pancake, Chair of the ACM Special Interest Group on High Performance Computing (SIGHPC), shares key programs for HPC students, academics, and practitioners. When it comes to computing education, there are as many interests as there are challenges---and the Education Council strives to represent them all. Although the Council typically meets only once a year, members of the Board are involved in ongoing initiatives throughout the year, undertaking many projects between Council meetings."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2505990.2505992", "Full Abstract": "Copyright © 2013 Copryright held by authors."},
{"Title": "Toward Effective Cybersecurity Education", "URL": "https://dl.acm.org/doi/10.1109/MSP.2013.155", "Full Abstract": "A February 2013 workshop addressed the challenges of higher education in cybersecurity. The participants discussed what advice to offer regarding the most effective way to produce graduates of the highest caliber who will become the leading cybersecurity professionals."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2537753.2537755", "Full Abstract": "Copyright © 2013 Copyright held by authors."},
{"Title": "Education, always", "URL": "https://dl.acm.org/doi/10.1145/2534707", "Full Abstract": "In a recent issue of"},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2568195.2568197", "Full Abstract": "Copyright © 2014 Copyright is held by the owner/author(s)."},
{"Title": "Toward curricular guidelines for cybersecurity", "URL": "https://dl.acm.org/doi/10.1145/2538862.2538990", "Full Abstract": "This session reports on a workshop convened by the ACM Education Board with funding by the US National Science Foundation and invites discussion from the community on the workshop findings. The topic, curricular directions for cybersecurity, is one that resonates in many departments considering how best to prepare graduates to face the challenges of security issues in employment and future research. The session will include presentation of the workshop context and conclusions, but will be open to participant discussion. This will be the first public presentation of the results of the workshop and the first opportunity for significant response."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2614512.2614514", "Full Abstract": "Welcome to the latest installment of \"EduBits,\" your quarterly pipeline to new and exciting happenings in the world of ACM education. In this edition, we visit the ACM Education Board-sponsored Learning @ Scale conference, ACM's first annual symposium on MOOCs, and learn about a new report from the ACM Education Policy Committee urging states to expand computer science education. Also, updates from the ACM Public Policy Office and Code.org take stock of the extraordinary number of first-time-coders participating in the Hour of Code."},
{"Title": "Digest of ACM educational activities", "URL": "https://dl.acm.org/doi/10.1145/2666099", "Full Abstract": "Copyright © 2014 Copyright is held by the owner/author(s)."},
{"Title": "Towards Grand Challenges in Computing Education Across Disciplines", "URL": "https://dl.acm.org/doi/10.1145/2676723.2677326", "Full Abstract": "No abstract available."},
{"Title": "CE2016", "URL": "https://dl.acm.org/doi/10.1109/FIE.2015.7344157", "Full Abstract": "No abstract available."},
{"Title": "When Computers Decide", "URL": "https://dl.acm.org/doi/book/10.1145/3185595", "Full Abstract": "Over the past two decades, the ability of machines to challenge and beat humans at complex games has made “quantum” leaps, rhetorically if not in technical computing terms."},
{"Title": "Informatics for All The strategy", "URL": "https://dl.acm.org/doi/book/10.1145/3185594", "Full Abstract": "Informatics for All is an initiative devised jointly by ACM Europe and Informatics Europe. Its purpose is to give due recognition to Informatics as an essential foundational discipline for education in the twenty-first century. Informatics is the science underpinning the development of the digital world, and it is having a profound effect on all aspects of modern society. The discipline has fundamental conceptual and practical facets. The considerable economic impact of its technological developments, as well as its role in empowering research and development across all sectors, has created an imperative to address educational issues."},
{"Title": "Informatics as a fundamental discipline for the 21", "URL": "https://dl.acm.org/doi/10.1145/3310330", "Full Abstract": "Copyright © 2019 ACM."},
{"Title": "Data Science Education", "URL": "https://dl.acm.org/doi/10.1145/3304221.3325533", "Full Abstract": "Over the past two decades, data science or data analytics degree programs have begun to emerge, reflecting the world's demand for data specialists to make sense of the vast amounts of collected data in the sciences, engineering, business, and other domains. As degree creation has occurred mainly due to demand, ACM and other professional bodies have recently stepped in to provide curricular guidance. However, no \\em shared global framework for data science as an academic discipline exists, making growth unfocused and driven by employer demands. More recently, the growth of artificial intelligence has also impacted data science programs. This working group builds on prior efforts and participant experiences to develop a global taxonomy of approaches to data science education and expectations for graduates of data science programs to \\em think like data scientists."},
{"Title": "An Empirical Approach to Understanding Data Science and Engineering Education", "URL": "https://dl.acm.org/doi/10.1145/3344429.3372503", "Full Abstract": "As data science is an evolving field, existing definitions reflect this uncertainty with overloaded terms and inconsistency. As a result of the field's fluidity, there is often a mismatch between what data-related programs teach, what employers expect, and the actual tasks data scientists are performing. In addition, the tools available to data scientists are not necessarily the tools being taught; textbooks do not seem to meet curricular needs; and empirical evidence does not seem to support existing program design. Currently, the field appears to be bifurcating into data science (DS) and data engineering (DE), with specific but overlapping roles in the combined data science and engineering (DSE) lifecycle. However, curriculum design has not yet caught up to this evolution. This working group report shows an empirical and data-driven view of the data-related education landscape, and includes several recommendations for both academia and industry that are based on this analysis."},
{"Title": "Computing Competencies for Undergraduate Data Science Programs", "URL": "https://dl.acm.org/doi/10.1145/3408877.3432586", "Full Abstract": "In this session, members of the ACM Data Science (DS) Task Force will present the final draft of Computing Competencies for Undergraduate Data Science Programs. Drafting this document has been a three-year process, in which the task force has released preliminary drafts, sought input from the community, and responded to the community's helpful feedback. Our intent is that the session be an exchange that will clarify the contents of the report and provide participants with ways to put the report into practice at their own institutions. This session should be of interest to all SIGCSE attendees, but especially to faculty developing college-level curricula in Data Science."},
{"Title": "Informatics Education for School", "URL": "https://dl.acm.org/doi/10.1145/3583088", "Full Abstract": "Copyright © 2023 ACM."},
{"Title": "European Digital Transformation Needs Indicators of Informatics Competence", "URL": "https://dl.acm.org/doi/10.1145/3696791", "Full Abstract": "Copyright © 2024 ACM."},
{"Title": "Informatics Reference Framework for School", "URL": "https://dl.acm.org/doi/book/10.1145/3592625", "Full Abstract": "The contribution that informatics has made since the last century has fuelled innovative and signifi-cant technological advances and vice versa. It makes fundamental contributions to current economic, educational, industrial and social development."},
{"Title": "Techniques for efficient in-memory checkpointing", "URL": "https://dl.acm.org/doi/10.1145/2524224.2524236", "Full Abstract": "Checkpointing is a pivotal technique in system research, with applications ranging from crash recovery to replay debugging. In this paper, we evaluate a number of in-memory checkpointing techniques and compare their properties. We also present a new compiler-based checkpointing scheme which improves state-of-the-art performance and memory guarantees in the general case. Our solution relies on a"},
{"Title": "Back to the future", "URL": "https://dl.acm.org/doi/10.5555/2555492.2555500", "Full Abstract": "Live update is a promising solution to bridge the need to frequently update a software system with the pressing demand for high availability in mission-critical environments. While many research solutions have been proposed over the years, systems that allow software to be updated on the fly are still far from reaching widespread adoption in the system administration community. We believe this trend is largely motivated by the lack of tools to automate and validate the live update process. A major obstacle, in particular, is represented by state transfer, which existing live update tools largely delegate to the programmer despite the great effort involved."},
{"Title": "Back to the future", "URL": "https://dl.acm.org/doi/10.5555/2717477.2717485", "Full Abstract": "Live update is a promising solution to bridge the need to frequently update a software system with the pressing demand for high availability in mission-critical environments. While many research solutions have been proposed over the years, systems that allow software to be updated on the fly are still far from reaching widespread adoption in the system administration community. We believe this trend is largely motivated by the lack of tools to automate and validate the live update process. A major obstacle, in particular, is represented by state transfer, which existing live update tools largely delegate to the programmer despite the great effort involved."},
{"Title": "EDFI", "URL": "https://dl.acm.org/doi/10.1109/PRDC.2013.12", "Full Abstract": "Fault injection is a pivotal technique in dependability benchmarking. Unfortunately, existing general-purpose fault injection tools either inject faults in predetermined memory locations or resort to random injection, approaches that generally result in poor fault coverage and controllability guarantees. This makes it difficult to reproduce or compare experiments across different systems or workloads. This paper presents EDFI, a new tool for dependable general-purpose fault injection experiments. EDFI combines static and dynamic program instrumentation to perform execution-driven fault injection, a technique which allows realistic software faults to be injected in a controlled way as the target system executes. Our instrumentation strategy guarantees a predetermined fault load distribution during the entirety of the experiment, independently of the particular system or workload considered. Our evaluation confirms that EDFI significantly improves the precision and controllability of prior tools, at the cost of only modest memory and performance overhead during fault-free execution."},
{"Title": "Evaluating Distortion in Fault Injection Experiments", "URL": "https://dl.acm.org/doi/10.1109/HASE.2014.13", "Full Abstract": "It has become well-established that software will never become bug-free, which has spurred research in mechanisms to contain faults and recover from them. Since such mechanisms deal with faults, fault injection is necessary to evaluate their effectiveness. However, little thought has been put into the question whether fault injection experiments faithfully represent the fault model designed by the user. Correspondence with the fault model is crucial to be able to draw strong and general conclusions from experimental results. The aim of this paper is twofold: to make a case for carefully evaluating whether activated faults match the fault model and to gain a better understanding of which parameters affect the deviation of the activated faults from the fault model. To achieve the latter, we instrumented a number of programs with our LLVM-based fault injection framework. We investigated the biases introduced by limited coverage, parts of the program executed more often than others and the nature of the workload. We evaluated the key factors that cause activated faults to deviate from the model and from these results provide recommendations on how to reduce such deviations."},
{"Title": "Modern Operating Systems", "URL": "https://dl.acm.org/doi/book/10.5555/2655363", "Full Abstract": "Modern Operating Systems, Fourth Edition, is intended for introductory courses in Operating Systems in Computer Science, Computer Engineering, and Electrical Engineering programs. It also serves as a useful reference for OS professionals The widely anticipated revision of this worldwide best-seller incorporates the latest developments in operating systems (OS) technologies. The Fourth Edition includes up-to-date materials on relevantOS. Tanenbaum also provides information on current research based on his experience as an operating systems researcher. Modern Operating Systems, Third Editionwas the recipient of the 2010 McGuffey Longevity Award. The McGuffey Longevity Award recognizes textbooks whose excellence has been demonstrated over time.http://taaonline.net/index.html Teaching and Learning Experience This program will provide a better teaching and learning experiencefor you and your students. It will help: Provide Practical Detail on the Big Picture Concepts: A clear and entertaining writing style outlines the concepts every OS designer needs to master. Keep Your Course Current: This edition includes information on the latest OS technologies and developments Enhance Learning with Student and Instructor Resources: Students will gain hands-on experience using the simulation exercises and lab experiments."},
{"Title": "On the Soundness of Silence", "URL": "https://dl.acm.org/doi/10.1109/EDCC.2014.16", "Full Abstract": "Fault injection campaigns have been used extensively to characterize the behavior of systems under errors. Traditional characterization studies, however, focus only on analyzing fail-stop behavior, incorrect test results, and other obvious failures observed during the experiment. More research is needed to evaluate the impact of silent failures-a relevant and insidious class of real-world failures-and doing so in a fully automated way in a fault injection setting. This paper presents a new methodology to identify fault injection-induced silent failures and assess their impact in a fully automated way. Drawing inspiration from system call-based anomaly detection, we compare faulty and fault-free execution runs and pinpoint behavioral differences that result in externally visible changes-not reported to the user-to detect silent failures. Our investigation across several different programs demonstrates that the impact of silent failures is relevant, consistent with field data, and should be carefully considered to avoid compromising the soundness of fault injection results."},
{"Title": "Techniques for efficient in-memory checkpointing", "URL": "https://dl.acm.org/doi/10.1145/2626401.2626406", "Full Abstract": "Checkpointing is a pivotal technique in system research, with applications ranging from crash recovery to replay debugging. In this paper, we evaluate a number of in-memory checkpointing techniques and compare their properties. We also present a new compiler-based checkpointing scheme which improves state-of-the-art performance and memory guarantees in the general case. Our solution relies on a shadow state to efficiently store incremental in-memory checkpoints, at the cost of a smaller user-addressable virtual address space. Contrary to common belief, our results show that in-memory checkpointing can be implemented efficiently with moderate impact on production systems."},
{"Title": "Towards a Flexible, Lightweight Virtualization Alternative", "URL": "https://dl.acm.org/doi/10.1145/2611354.2611369", "Full Abstract": "In recent times, two virtualization approaches have become dominant: hardware-level and operating system-level virtualization. They differ by where they draw the virtualization boundary between the virtualizing and the virtualized part of the system, resulting in vastly different properties. We argue that these two approaches are extremes in a continuum, and that boundaries in between the extremes may combine several good properties of both. We propose abstractions to make up one such new virtualization boundary, which combines hardware-level flexibility with OS-level resource sharing. We implement and evaluate a first prototype."},
{"Title": "On sockets and system calls minimizing context switches for the socket API", "URL": "https://dl.acm.org/doi/10.5555/2750315.2750323", "Full Abstract": "Traditionally, applications use sockets to access the network. The socket API is well understood and simple to use. However, its simplicity has also limited its efficiency in existing implementations. Specifically, the socket API requires the application to execute many system calls like select, accept, read, and write. Each of these calls crosses the protection boundary between user space and the operating system, which is expensive. Moreover, the system calls themselves were not designed for high concurrency and have become bottlenecks in modern systems where processing simultaneous tasks is key to performance. We show that we can retain the original socket API without the current limitations. Specifically, our sockets almost completely avoid system calls on the \"fast path\". We show that our design eliminates up to 99% of the system calls under high load. Perhaps more tellingly, we used our sockets to boost NewtOS, a microkernel-based multiserver system, so that the performance of its network I/O approaches, and sometimes surpasses, the performance of the highly-optimized Linux network stack."},
{"Title": "Mutable checkpoint-restart", "URL": "https://dl.acm.org/doi/10.1145/2663165.2663328", "Full Abstract": "The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage live update adoption."},
{"Title": "A Methodology to Efficiently Compare Operating System Stability", "URL": "https://dl.acm.org/doi/10.1109/HASE.2015.22", "Full Abstract": "Despite decades of advances in software engineering, operating systems (OSes) are still plagued by crashes due to software faults, calling for techniques to improve OS stability when faults occur. Evaluating such techniques requires a way to compare the stability of different OSes that is both representative of real faults and scales to the large code bases of modern OSes and a large (and statistically sound) number of experiments. In this paper, we propose a widely applicable methodology meeting all such requirements. Our methodology relies on a novel fault injection strategy based on a combination of static and run-time instrumentation, which yields representative software faults while drastically reducing the instrumentation time and thus greatly enhancing scalability. To guarantee unbiased and comparable results, finally, our methodology relies on the use of pre- and post tests to isolate the direct impact of faults from the stability of the OS itself. We demonstrate our methodology by comparing the stability of Linux and MINIX 3, saving a total of 115 computer-days for the 12,000 Linux fault injection runs compared to the traditional approach of re-instrumenting for every run."},
{"Title": "Lightweight Memory Checkpointing", "URL": "https://dl.acm.org/doi/10.1109/DSN.2015.45", "Full Abstract": "Memory check pointing is a pivotal technique in systems reliability, with applications ranging from crash recovery to replay debugging. Unfortunately, many traditional memory check pointing use-cases require high-frequency checkpoints, something for which existing application-level solutions are not well-suited. The problem is that they incur either substantial run-time performance overhead, or poor memory usage guarantees. As a result, their application in practice is hampered. This paper presents Lightweight Memory Check pointing (LMC), a new user-level memory check pointing technique that combines low performance overhead with strong memory usage guarantees for high check pointing frequencies. To this end, LMC relies on compiler-based instrumentation to shadow the entire memory address space of the running program and incrementally checkpoint modified memory bytes in a LMC-maintained shadow state. Our evaluation on popular server applications demonstrates the viability of our approach in practice, confirming that LMC imposes low performance overhead with strictly bounded memory usage at runtime."},
{"Title": "Speculative Memory Checkpointing", "URL": "https://dl.acm.org/doi/10.1145/2814576.2814802", "Full Abstract": "High-frequency memory checkpointing is an important technique in several application domains, such as automatic error recovery (where frequent checkpoints allow the system to transparently mask failures) and application debugging (where frequent checkpoints enable fast and accurate time-traveling support). Unfortunately, existing (typically incremental) checkpointing frameworks incur substantial performance overhead in high-frequency memory checkpointing applications, thus discouraging their adoption in practice."},
{"Title": "Lessons learned from 30 years of MINIX", "URL": "https://dl.acm.org/doi/10.1145/2795228", "Full Abstract": "MINIX shows even an operating system can be made to be self-healing."},
{"Title": "Distributed Systems", "URL": "https://dl.acm.org/doi/book/10.5555/3019390", "Full Abstract": "This second edition of Distributed Systems, Principles & Paradigms, covers the principles, advanced concepts, and technologies of distributed systems in detail, including: communication, replication, fault tolerance, and security. Intended for use in a senior/graduate level distributed systems course or by professionals, this text systematically shows how distributed systems are designed and implemented in real systems."},
{"Title": "Finding fault with fault injection", "URL": "https://dl.acm.org/doi/10.1007/s11219-014-9261-3", "Full Abstract": "It has become well established that software will never become bug free, which has spurred research in mechanisms to contain faults and recover from them. Since such mechanisms deal with faults, fault injection is necessary to evaluate their effectiveness. However, little thought has been put into the question whether fault injection experiments faithfully represent the fault model designed by the user. Correspondence with the fault model is crucial to be able to draw strong and general conclusions from experimental results. The aim of this paper is twofold: to make a case for carefully evaluating whether activated faults match the fault model and to gain a better understanding of which parameters affect the deviation of the activated faults from the fault model. To achieve the latter, we instrumented a number of programs with our LLVM-based fault injection framework. We investigated the biases introduced by limited coverage, parts of the program executed more often than others and the nature of the workload. We evaluated the key factors that cause activated faults to deviate from the model and from these results provide recommendations on how to reduce such deviations."},
{"Title": "A brief introduction to distributed systems", "URL": "https://dl.acm.org/doi/10.1007/s00607-016-0508-7", "Full Abstract": "Distributed systems are by now commonplace, yet remain an often difficult area of research. This is partly explained by the many facets of such systems and the inherent difficulty to isolate these facets from each other. In this paper we provide a brief overview of distributed systems: what they are, their general design goals, and some of the most common types."},
{"Title": "A NEaT Design for Reliable and Scalable Network Stacks", "URL": "https://dl.acm.org/doi/10.1145/2999572.2999579", "Full Abstract": "Operating systems provide a wide range of services, which are crucial for the increasingly high reliability and scalability demands of modern applications. Providing both reliability and scalability at the same time is hard. Commodity OS architectures simply lack the design abstractions to do so for demanding core OS services such as the network stack. For reliability and scalability guarantees, they rely almost exclusively on ensuring a high-quality implementation, rather than a reliable and scalable design. This results in complex error recovery paths and hard-to-maintain synchronization code."},
{"Title": "Automating Live Update for Generic Server Programs", "URL": "https://dl.acm.org/doi/10.1109/TSE.2016.2584066", "Full Abstract": "The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents"},
{"Title": "Bigraphical Reactive Systems", "URL": "https://dl.acm.org/doi/10.5555/646736.701773", "Full Abstract": "A notion of bigraph is introduced as a model of mobile interaction. A bigraph consists of two independent structures: a topograph representing locality and an edge net representing connectivity. Bigraphs are equipped with reaction rules to form bigraphical reactive systems (BRSs), which include versions of the π-calculus and the ambient calculus. A behavioural theory is established, using the categorical notion of relative pushout; it allows labelled transition systems to be derived uniformly for a wide variety of BRSs, in such a way that familiar behavioural preorders and equivalences, in particular bisimilarity, are congruential. An example of the derivation is discussed."},
{"Title": "Shallow Linear Action Graphs and their Embeddings", "URL": "https://dl.acm.org/doi/10.1007/s001650200015", "Full Abstract": "Action calculi, which generalise process calculi such as Petri nets, π-calculusand ambient calculus, have been presented in terms of"},
{"Title": "Bigraphs as a Model for Mobile Interaction", "URL": "https://dl.acm.org/doi/10.5555/647562.760499", "Full Abstract": "A bigraphical reactive system (BRS) involves bigraphs, in which the nesting of nodes represents locality, independently of the edges connecting them. BRSs represent a wide variety of calculi for mobility, including the -calculus. This short essay explains how bigraphs compose, and uses the -calculus to illustrate how they already provide elements of a unifying theory for calculi of mobile interactive processes."},
{"Title": "Bigraphs and transitions", "URL": "https://dl.acm.org/doi/10.1145/604131.604135", "Full Abstract": "A bigraphical reactive system (BRS) involves"},
{"Title": "Grand Challenges for Computing Research", "URL": "https://dl.acm.org/doi/10.1093/comjnl/bxh065", "Full Abstract": "What are the major research challenges that face the world of computing today? Are there any of them that match the grandeur of well-known challenges in other branches of science? This article is a report on an exercise by the Computing Research Community in the UK to answer these questions, and includes a summary of the outcomes of a BCS-sponsored conference held in Newcastle-upon-Tyne from 29 to 31 March this year."},
{"Title": "Embeddings and contexts for link graphs", "URL": "https://dl.acm.org/doi/10.5555/2137662.2137685", "Full Abstract": "Graph-rewriting has been a growing discipline for over three decades. It grew out of the study of graph grammars, in which – analogously to string and tree grammars – a principal interest was to describe the families of graphs that could be generated from a given set of productions. A fundamental contribution was, of course, the"},
{"Title": "Axioms for bigraphical structure", "URL": "https://dl.acm.org/doi/10.1017/S0960129505004809", "Full Abstract": "This paper axiomatises the structure of bigraphs, and proves that the resulting theory is complete. Bigraphs are graphs with double structure, representing locality and connectivity. They have been shown to represent dynamic theories for the $\\pi$-calculus, mobile ambients and Petri nets in a way that is faithful to each of those models of discrete behaviour. While the main purpose of bigraphs is to understand mobile systems, a prerequisite for this understanding is a well-behaved theory of the structure of states in such systems. The algebra of bigraph structure is surprisingly simple, as this paper demonstrates; this is because bigraphs treat locality and connectivity orthogonally."},
{"Title": "Scientific foundation for global computing", "URL": "https://dl.acm.org/doi/10.5555/2168320.2168321", "Full Abstract": "It is a big honour to be able to speak at one of the most exciting conferences I have been to for 20 years."},
{"Title": "Pure bigraphs", "URL": "https://dl.acm.org/doi/10.5555/1140956.1709621", "Full Abstract": "Bigraphs are graphs whose nodes may be nested, representing locality, independently of the edges connecting them. They may be equipped with reaction rules, forming a bigraphical reactive system (Brs) in which bigraphs can reconfigure themselves. Following an earlier paper describing link graphs, a constituent of bigraphs, this paper is a devoted to pure bigraphs, which in turn underlie various more refined forms. Elsewhere it is shown that behavioural analysis for Petri nets, @p-calculus and mobile ambients can all be recovered in the uniform framework of bigraphs. The paper first develops the dynamic theory of an abstract structure, a wide reactive system (Wrs), of which a Brs is an instance. In this context, labelled transitions are defined in such a way that the induced bisimilarity is a congruence. This work is then specialised to Brss, whose graphical structure allows many refinements of the theory. The latter part of the paper emphasizes bigraphical theory that is relevant to the treatment of dynamics via labelled transitions. As a running example, the theory is applied to finite pure CCS, whose resulting transition system and bisimilarity are analysed in detail. The paper also mentions briefly the use of bigraphs to model pervasive computing and biological systems. computing and biological systems."},
{"Title": "Ubiquitous Computing", "URL": "https://dl.acm.org/doi/10.1093/comjnl/bxl015", "Full Abstract": "No abstract available."},
{"Title": "Transition systems, link graphs and Petri nets", "URL": "https://dl.acm.org/doi/10.1017/S0960129506005664", "Full Abstract": "A framework is defined within which reactive systems can be studied formally. The framework is based on"},
{"Title": "Elements of interaction", "URL": "https://dl.acm.org/doi/10.1145/1283920.1283948", "Full Abstract": "Copyright © 2007."},
{"Title": "Local Bigraphs and Confluence", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2006.07.035", "Full Abstract": "The notion of confluence is studied on the context of bigraphs. Confluence will be important in modelling real-world systems, both natural (as in biology) and artificial (as in pervasive computing). The paper uses bigraphs in which names have multiple locality; this enables a formulation of the lambda calculus with explicit substitutions. The paper reports work in progress, seeking conditions on a bigraphical reactive system that are sufficient to ensure confluence; the conditions must deal with the way that bigraphical redexes can be intricately intertwined. The conditions should also be satisfied by the lambda calculus. After discussion of these issues, two conjectures are put forward."},
{"Title": "Matching of Bigraphs", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2007.04.013", "Full Abstract": "We analyze the matching problem for bigraphs. In particular, we present a sound and complete inductive characterization of matching of binding bigraphs. Our results pave the way for a provably correct matching algorithm, as needed for an implementation of bigraphical reactive systems."},
{"Title": "Categories, Software and Meaning", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-68679-8_50", "Full Abstract": "I am delighted to be able to share in celebrating Ugo's birthday, if not with a formal paper then at least with some loosely-knit philosophical ideas."},
{"Title": "Bigraphs and Their Algebra", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2008.04.002", "Full Abstract": "Bigraphs are a framework in which both existing process calculi and new models of behaviour can be formulated, yielding theory that is shared among these models. A short survey of the main features of bigraphs is presented, showing how they can be developed from standard graph theory using elementary category theory. The algebraic manipulation of bigraphs is outlined with the help of illustrations. The treatment of dynamics is then summarised. Finally, origins and some related work are discussed. The paper provides a motivating introduction to bigraphs."},
{"Title": "Stochastic Bigraphs", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2008.10.006", "Full Abstract": "In this paper we present a stochastic semantics for Bigraphical Reactive Systems. A reduction and a labelled stochastic semantics for bigraphs are defined. As a sanity check, we prove that the two semantics are consistent with each other. We illustrate the expressiveness of the framework with an example of membrane budding in a biological system."},
{"Title": "Computing Tomorrow", "URL": "https://dl.acm.org/doi/book/10.5555/1592907", "Full Abstract": "Computer science is no longer just a technology--for nearly all of us, it has become a way of life. Whether we spend our days surfing the Internet, or merely use an automatic teller machine on occasion, computers have affected our lives. This collection of sixteen original essays by distinguished computer scientists celebrates the achievements of computer science research, and speculates about the unsolved problems in the field. Various essays address artificial intelligence, parallel programming, global information systems, and a host of other relevant topics. The book shows that long-term research in computer science is crucial and must not be driven solely by commercial considerations. The authors expose the difficult aspects of their topics in clear terms, and illustrate that computer science is now a full-fledged and growing intellectual discipline."},
{"Title": "Bigraphical Categories", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-04081-8_3", "Full Abstract": "Bigraphs are a candidate model that aims to provide a theoretical platform for ubiquitous computing systems. This short paper summarises the categories, and the functors between them, that represent the structure of that theory."},
{"Title": "An inductive characterization of matching in binding bigraphs", "URL": "https://dl.acm.org/doi/10.1007/s00165-011-0184-5", "Full Abstract": "We analyze the matching problem for bigraphs. In particular, we present a sound and complete inductive characterization of matching in bigraphs with binding. Our results yield a specification for a provably correct matching algorithm, as needed by our prototype tool implementing bigraphical reactive systems."},
{"Title": "Revenue monotonicity in combinatorial auctions", "URL": "https://dl.acm.org/doi/10.1145/1345037.1345048", "Full Abstract": "Copyright © 2007 Authors."},
{"Title": "Complexity of a Collision-Aware String Partition Problem and Its Relation to Oligo Design for Gene Synthesis", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-69733-6_27", "Full Abstract": "Artificial synthesis of long genes and entire genomes is achieved by self-assembly of DNA oligo fragments - fragments which are short enough to be generated using a DNA synthesizer. Given a description of the duplex to be synthesized, a computational challenge is to select the short oligos so that, once synthesized, they will self-assemble without error. In this paper, we show that a natural abstraction of this problem, the <em>collision-aware string partition problem</em>, is NP-complete."},
{"Title": "Computational Challenges and Opportunities in the Design of Unconventional Machines from Nucleic Acids", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-85194-3_2", "Full Abstract": "DNA and RNA molecules have proven to be very versatile materials for programmable construction of nano-scale structures and for controlling motion in molecular machines. RNA molecules are also increasingly in the spotlight in recognition of their important regulatory and catalytic roles in the cell and their promise in therapeutics. Function follows form in the molecular world and so our ability to understand nucleic acid function in the cell, as well as to design novel structures, is enhanced by reliable means for structure prediction."},
{"Title": "Stepwise randomized combinatorial auctions achieve revenue monotonicity", "URL": "https://dl.acm.org/doi/10.5555/1496770.1496851", "Full Abstract": "In combinatorial auctions that use VCG, a seller can sometimes increase revenue by dropping bidders (see e.g. [5]). In our previous work [26], we showed that such failures of \"revenue monotonicity\" occur under an extremely broad range of deterministic strategyproof combinatorial auction mechanisms, even when bidders have \"known single-minded\" valuations. In this work we consider the question of whether revenue monotonic, strategyproof mechanisms for such bidders can be found in the broader class of randomized mechanisms. We demonstrate that---surprisingly- such mechanisms do exist, show how they can be constructed, and consider algorithmic techniques for implementing them in polynomial time."},
{"Title": "Computational prediction of nucleic acid secondary structure", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2008.09.042", "Full Abstract": "RNA molecules are crucial in different levels of cellular function, ranging from translation and regulating genes to coding for proteins. Additionally, nucleic acids (RNA and DNA molecules) are designed for novel applications in biotechnology. Understanding the structure of a molecule is important in inferring its function, and computational methods for structure prediction have captured the interest of many researchers. Some functions of RNA molecules in cells, such as gene regulation, result from the binding of one RNA molecule to another, so-called target RNA molecule. This has led to recent interest in prediction of the secondary structure formed from interacting molecules. In this paper, we provide a brief overview of methods, applications, and challenges in computational prediction of nucleic acid secondary structure, both for single strands and for interacting strands."},
{"Title": "Algorithms for distributional and adversarial pipelined filter ordering problems", "URL": "https://dl.acm.org/doi/10.1145/1497290.1497300", "Full Abstract": "Pipelined filter ordering is a central problem in database query optimization. The problem is to determine the optimal order in which to apply a given set of commutative filters (predicates) to a set of elements (the tuples of a relation), so as to find, as efficiently as possible, the tuples that satisfy all of the filters. Optimization of pipelined filter ordering has recently received renewed attention in the context of environments such as the Web, continuous high-speed data streams, and sensor networks. Pipelined filter ordering problems are also studied in areas such as fault detection and machine learning under names such as learning with attribute costs, minimum-sum set cover, and satisficing search. We present algorithms for two natural extensions of the classical pipelined filter ordering problem: (1) a"},
{"Title": "Algorithmic Bioprocesses", "URL": "https://dl.acm.org/doi/book/10.5555/1642728", "Full Abstract": "A fundamental understanding of algorithmic bioprocesses is key to learning how information processing occurs in nature at the cell level. The field is concerned with the interactions between computer science on the one hand and biology, chemistry, and DNA-oriented nanoscience on the other. In particular, this book offers a comprehensive overview of research into algorithmic self-assembly, RNA folding, the algorithmic foundations for biochemical reactions, and the algorithmic nature of developmental processes. The editors of the book invited 36 chapters, written by the leading researchers in this area, and their contributions include detailed tutorials on the main topics, surveys of the state of the art in research, experimental results, and discussions of specific research goals. The main subjects addressed are sequence discovery, generation, and analysis; nanoconstructions and self-assembly; membrane computing; formal models and analysis; process calculi and automata; biochemical reactions; and other topics from natural computing, including molecular evolution, regulation of gene expression, light-based computing, cellular automata, realistic modelling of biological systems, and evolutionary computing. This subject is inherently interdisciplinary, and this book will be of value to researchers in computer science and biology who study the impact of the exciting mutual interaction between our understanding of bioprocesses and our understanding of computation."},
{"Title": "NP-Completeness of the Direct Energy Barrier Problem without Pseudoknots", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-10604-0_11", "Full Abstract": "Knowledge of energy barriers between pairs of secondary structures for a given DNA or RNA molecule is useful, both in understanding RNA function in biological settings and in design of programmed molecular systems. Current heuristics are not guaranteed to find the exact energy barrier, raising the question whether the energy barrier can be calculated efficiently. In this paper, we study the computational complexity of a simple formulation of the energy barrier problem, in which each base pair contributes an energy of 1 and only base pairs in the initial and final structures may be used on a folding pathway from initial to final structure. We show that this problem is NP-complete."},
{"Title": "Revenue monotonicity in deterministic, dominant-strategy combinatorial auctions", "URL": "https://dl.acm.org/doi/10.1016/j.artint.2010.08.005", "Full Abstract": "In combinatorial auctions using VCG, a seller can sometimes increase revenue by dropping bidders. In this paper we investigate the extent to which this counterintuitive phenomenon can also occur under other deterministic, dominant-strategy combinatorial auction mechanisms. Our main result is that such failures of 'revenue monotonicity' can occur under any such mechanism that is weakly maximal-meaning roughly that it chooses allocations that cannot be augmented to cause a losing bidder to win without hurting winning bidders-and that allows bidders to express arbitrary known single-minded preferences. We also give a set of other impossibility results as corollaries, concerning revenue when the set of goods changes, false-name-proofness, and the core."},
{"Title": "NP-completeness of the energy barrier problem without pseudoknots and temporary arcs", "URL": "https://dl.acm.org/doi/10.1007/s11047-010-9239-4", "Full Abstract": "Knowledge of energy barriers between pairs of secondary structures for a given DNA or RNA molecule is useful, both in understanding RNA function in biological settings and in design of programmed molecular systems. Current heuristics are not guaranteed to find the exact energy barrier, raising the question whether the energy barrier can be calculated efficiently. In this paper, we study the computational complexity of a simple formulation of the energy barrier problem, in which each base pair contributes an energy of 1 and only base pairs in the initial and final structures may be used on a folding pathway from initial to final structure. We show that this problem is NP-complete."},
{"Title": "Efficient codon optimization with motif engineering", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-25011-8_27", "Full Abstract": "It is now common to add protein coding genes into cloning vectors for expression within non-native host organisms. Codon optimization supports translational efficiency of the desired protein product, by exchanging codons which are rarely found in the host organism with more frequently observed codons. Motif engineering, such as removal of restriction enzyme recognition sites or addition of immuno-stimulatory elements, is also often necessary. We present an algorithm for optimizing codon bias of a gene with respect to a well motivated measure of bias, while simultaneously performing motif engineering. The measure is the previously studied codon adaptation index, which favors the use, in the gene to be optimized, of the most abundant codons found in the host genome. We demonstrate the efficiency and effectiveness of our algorithm on the GENCODE dataset and provide a guarantee that the solution found is always optimal."},
{"Title": "Less haste, less waste", "URL": "https://dl.acm.org/doi/10.5555/2042033.2042044", "Full Abstract": "We study the potential for molecule recycling in chemical reaction systems and their DNA strand displacement realizations. Recycling happens when a product of one reaction is a reactant in a later reaction. Recycling has the benefits of reducing consumption, or waste, of molecules and of avoiding fuel depletion. We present a binary counter that recycles molecules efficiently while incurring just a moderate slowdown compared to alternative counters that do not recycle strands. This counter is an"},
{"Title": "Two-sided matching with partial information", "URL": "https://dl.acm.org/doi/10.1145/2492002.2482607", "Full Abstract": "The traditional model of two-sided matching assumes that all agents fully know their own preferences. As markets grow large, however, it becomes impractical for agents to precisely assess their rankings over all agents on the other side of the market. We propose a novel model of two-sided matching in which agents are endowed with known partially ordered preferences and unknown true preferences drawn from known distributions consistent with the partial order. The true preferences are learned through interviews, revealing the pairwise rankings among all interviewed agents, performed according to a centralized interview policy, i.e., an algorithm that adaptively schedules interviews. Our goal is for the policy to guarantee both stability and optimality for a given side of the market, with respect to the underlying true preferences of the agents. As interviews are costly, we seek a policy that minimizes the number of interviews. We introduce three minimization objectives: (very weak) dominance, which minimizes the number of interviews for any underlying true preference profile; Pareto optimality, which guarantees that no other policy dominates the given policy; and optimality in expectation with respect to the preference distribution. We formulate our problem as a Markov decision process, implying an algorithm for computing an optimal-in-expectation policy in time polynomial in the number of possible preference orderings (and thus exponential in the size of the input). We then derive structural properties of dominant policies which we call optimality certificates. We show that computing a minimum optimality certificate is NP-hard, suggesting that optimal-in-expectation and/or Pareto optimal policies could be NP-hard to compute. Finally, we restrict attention to a setting in which agents on one side of the market have the same partially ordered preferences (but potentially distinct underlying true preferences), and in which agents must interview before matching. In this restricted setting, we show how to leverage the idea of minimum optimality certificates to design a computationally efficient interview-minimizing policy. This policy works without knowledge of the distributions and is dominant (and so is also Pareto optimal and optimal-in-expectation)."},
{"Title": "Reasoning about optimal stable matchings under partial information", "URL": "https://dl.acm.org/doi/10.1145/2600057.2602884", "Full Abstract": "We study two-sided matching markets in which participants are initially endowed with partial preference orderings, lacking precise information about their true, strictly ordered list of preferences. We wish to reason about matchings that are stable with respect to agents' true preferences, and which are furthermore optimal for one given side of the market. We present three main results. First, one can decide in polynomial time whether there exists a matching that is stable and optimal under all strict preference orders that refine the given partial orders, and can construct this matching in polynomial time if it does exist. We show, however, that deciding whether a given pair of agents are matched in all or no such optimal stable matchings is co-NP-complete, even under quite severe restrictions on preferences. Finally, we describe a polynomial-time algorithm that decides, given a matching that is stable under the partial preference orderings, whether that matching is stable and optimal for one side of the market under some refinement of the partial orders."},
{"Title": "On Design and Analysis of Chemical Reaction Network Algorithms", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-94812-6_1", "Full Abstract": "The fields of DNA computing, molecular programming and DNA nanotechnology offer exciting new possibilities for organizing and manipulating matter at the nanoscale, and prompt us to think about computation in creative new ways. Molecules reacting in a test tube change state, and counts of molecules can in principle be used to simulate counter machines, all in a highly distributed, asynchronous and stochastic manner. In this talk I’ll give some background on models of molecular programming, focusing on Stochastic Chemical Reaction Networks, and describe some beautiful results and open problems pertaining to this model of computing."},
{"Title": "Two-sided matching with partial information", "URL": "https://dl.acm.org/doi/10.1145/2482540.2482607", "Full Abstract": "The traditional model of two-sided matching assumes that all agents fully know their own preferences. As markets grow large, however, it becomes impractical for agents to precisely assess their rankings over all agents on the other side of the market. We propose a novel model of two-sided matching in which agents are endowed with known partially ordered preferences and unknown true preferences drawn from known distributions consistent with the partial order. The true preferences are learned through interviews, revealing the pairwise rankings among all interviewed agents, performed according to a centralized interview policy, i.e., an algorithm that adaptively schedules interviews. Our goal is for the policy to guarantee both stability and optimality for a given side of the market, with respect to the underlying true preferences of the agents. As interviews are costly, we seek a policy that minimizes the number of interviews. We introduce three minimization objectives: (very weak) dominance, which minimizes the number of interviews for any underlying true preference profile; Pareto optimality, which guarantees that no other policy dominates the given policy; and optimality in expectation with respect to the preference distribution. We formulate our problem as a Markov decision process, implying an algorithm for computing an optimal-in-expectation policy in time polynomial in the number of possible preference orderings (and thus exponential in the size of the input). We then derive structural properties of dominant policies which we call optimality certificates. We show that computing a minimum optimality certificate is NP-hard, suggesting that optimal-in-expectation and/or Pareto optimal policies could be NP-hard to compute. Finally, we restrict attention to a setting in which agents on one side of the market have the same partially ordered preferences (but potentially distinct underlying true preferences), and in which agents must interview before matching. In this restricted setting, we show how to leverage the idea of minimum optimality certificates to design a computationally efficient interview-minimizing policy. This policy works without knowledge of the distributions and is dominant (and so is also Pareto optimal and optimal-in-expectation)."},
{"Title": "Isometric Hamming embeddings of weighted graphs", "URL": "https://dl.acm.org/doi/10.1016/j.dam.2023.02.005", "Full Abstract": "A mapping α : V ( G ) → V ( H ) from the vertex set of one graph G to another graph H is an"},
{"Title": "Predicting DNA kinetics with a truncated continuous-time Markov chain method", "URL": "https://dl.acm.org/doi/10.1016/j.compbiolchem.2023.107837", "Full Abstract": "Predicting the kinetics of reactions involving nucleic acid strands is a fundamental task in biology and biotechnology. Reaction kinetics can be modeled as an elementary step continuous-time Markov chain, where states correspond to secondary structures and transitions correspond to base pair formation and breakage. Since the number of states in the Markov chain could be large, rates are determined by estimating the mean first passage time from sampled trajectories. As a result, the cost of kinetic predictions becomes prohibitively expensive for rare events with extremely long trajectories. Also problematic are scenarios where multiple predictions are needed for the same reaction, e.g., under different environmental conditions, or when calibrating model parameters, because a new set of trajectories is needed multiple times. We propose a new method, called pathway elaboration, to handle these scenarios. Pathway elaboration builds a truncated continuous-time Markov chain through both biased and unbiased sampling. The resulting Markov chain has moderate state space size, so matrix methods can efficiently compute reaction rates, even for rare events. Also the transition rates of the truncated Markov chain can easily be adapted when model or environmental parameters are perturbed, making model calibration feasible. We illustrate the utility of pathway elaboration on toehold-mediated strand displacement reactions, show that it well-approximates trajectory-based predictions of unbiased elementary step models on a wide range of reaction types for which such predictions are feasible, and demonstrate that it performs better than alternative truncation-based approaches that are applicable for mean first passage time estimation. Finally, in a small study, we use pathway elaboration to optimize the Metropolis kinetic model of Multistrand, an elementary step simulator, showing that the optimized parameters greatly improve reaction rate predictions. Our framework and dataset are available at"},
{"Title": "Factorization and pseudofactorization of weighted graphs", "URL": "https://dl.acm.org/doi/10.1016/j.dam.2023.04.019", "Full Abstract": "For unweighted graphs, finding isometric embeddings of a graph G is closely related to decompositions of G into Cartesian products of smaller graphs. When G is isomorphic to a Cartesian graph product, we call the factors of this product a factorization of G. When G is isomorphic to an isometric subgraph of a Cartesian graph product, we call those factors a pseudofactorization of G. Prior work has shown that an unweighted graph’s pseudofactorization can be used to generate a canonical isometric embedding into a product of the smallest possible pseudofactors. However, for arbitrary weighted graphs, which represent a richer variety of metric spaces, methods for finding isometric embeddings or determining their existence remain elusive, and indeed pseudofactorization and factorization have not previously been extended to this context. In this work, we address the problem of finding the factorization and pseudofactorization of a weighted graph G, where G satisfies the property that every edge constitutes a shortest path between its endpoints. We term such graphs minimal graphs, noting that every graph can be made minimal by removing edges not affecting its path metric. We generalize pseudofactorization and factorization to minimal graphs and develop new proof techniques that extend the previously proposed algorithms due to Graham and Winkler (1985) and Feder (1992) for pseudofactorization and factorization of unweighted graphs. We show that any n-vertex, m-edge graph with positive integer edge weights can be factored in O ( m 2 ) time, plus the time to find all pairs shortest paths (APSP) distances in a weighted graph, resulting in an overall running time of O ( m 2 + n 2 log log n ) time. We also show that a pseudofactorization for such a graph can be computed in O ( m n ) time, plus the time to solve APSP, resulting in an O ( m n + n 2 log log n ) running time."},
{"Title": "AlignOT: An Optimal Transport Based Algorithm for Fast 3D Alignment With Applications to Cryogenic Electron Microscopy Density Maps", "URL": "https://dl.acm.org/doi/10.1109/TCBB.2023.3327633", "Full Abstract": "Aligning electron density maps from Cryogenic electron microscopy (cryo-EM) is a first key step for studying multiple conformations of a biomolecule. As this step remains costly and challenging, with standard alignment tools being potentially stuck in local minima, we propose here a new procedure, called <italic>AlignOT</italic>, which relies on the use of computational optimal transport (OT) to align EM maps in 3D space. By embedding a fast estimation of OT maps within a stochastic gradient descent algorithm, our method searches for a rotation that minimizes the Wasserstein distance between two maps, represented as point clouds. We quantify the impact of various parameters on the precision and accuracy of the alignment, and show that <italic>AlignOT</italic> can outperform the standard local alignment methods, with an increased range of rotation angles leading to proper alignment. We further benchmark <italic>AlignOT</italic> on various pairs of experimental maps, which account for different types of conformational heterogeneities and geometric properties. As our experiments show good performance, we anticipate that our method can be broadly applied to align 3D EM maps."},
{"Title": "Interactive Theorem Proving and Program Development", "URL": "https://dl.acm.org/doi/book/10.5555/993954", "Full Abstract": "No abstract available."},
{"Title": "Visualizing Geometrical Statements with GeoView", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2004.09.013", "Full Abstract": "No abstract available."},
{"Title": "A structured approach to proving compiler optimizations based on dataflow analysis", "URL": "https://dl.acm.org/doi/10.1007/11617990_5", "Full Abstract": "This paper reports on the correctness proof of compiler optimizations based on data-flow analysis. We formulate the optimizations and analyses as instances of a general framework for data-flow analyses and transformations, and prove that the optimizations preserve the behavior of the compiled programs. This development is a part of a larger effort of certifying an optimizing compiler by proving semantic equivalence between source and compiled code."},
{"Title": "Filters on coinductive streams, an application to eratosthenes' sieve", "URL": "https://dl.acm.org/doi/10.1007/11417170_9", "Full Abstract": "We present the formal description of an algorithm to filter values from an infinite steam using a type theory based prover. The key aspect is that filters are partial co-recursive functions and we solve the problem of expressing partiality. We then show how to prove properties of this filter algorithm and we study an application computing the stream of all prime numbers."},
{"Title": "Dependent Types, Theorem Proving, and Applications for a Verifying Compiler", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-69149-5_19", "Full Abstract": "One approach to Prof. Hoare's challenge is to view the development of verified software from the perspective of interactive theorem provers. This idea is not new and many medium-scale software systems have been developed and verified in this manner. Developments based on HOL, ACL2, or PVS have already been described and advocated and our position stands on the same line: most powerful (higher-order) theorem proving systems already contain a programming language, programs can be developed and the correctness of these programs can be specified and verified, they can then be compiled into traditional executable code. In this sense, we already have a small scale example of a verification aware programming language."},
{"Title": "Affine functions and series with co-inductive real numbers", "URL": "https://dl.acm.org/doi/10.1017/S0960129506005809", "Full Abstract": "We extend the work of A. Ciaffaglione and P. di Gianantonio on the mechanical verification of algorithms for exact computation on real numbers, using infinite streams of digits implemented as a co-inductive type. Four aspects are studied. The first concerns the proof that digit streams correspond to axiomatised real numbers when they are already present in the proof system. The second re-visits the definition of an addition function, looking at techniques to let the proof search engine perform the effective construction of an algorithm that is correct by construction. The third concerns the definition of a function to compute affine formulas with positive rational coefficients. This is an example where we need to combine co-recursion and recursion. Finally, the fourth aspect concerns the definition of a function to compute series, with an application on the series that is used to compute Euler's number"},
{"Title": "Inductive and Coinductive Components of Corecursive Functions in Coq", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2008.05.018", "Full Abstract": "In Constructive Type Theory, recursive and corecursive definitions are subject to syntactic restrictions which guarantee termination for recursive functions and productivity for corecursive functions. However, many terminating and productive functions do not pass the syntactic tests. Bove proposed in her thesis an elegant reformulation of the method of accessibility predicates that widens the range of terminative recursive functions formalisable in Constructive Type Theory. In this paper, we pursue the same goal for productive corecursive functions. Notably, our method of formalisation of coinductive definitions of productive functions in Coq requires not only the use of ad-hoc predicates, but also a systematic algorithm that separates the inductive and coinductive parts of functions."},
{"Title": "Fixed point semantics and partial recursion in Coq", "URL": "https://dl.acm.org/doi/10.1145/1389449.1389461", "Full Abstract": "We propose to use the Knaster-Tarski least fixed point theorem as a basis to define recursive functions in the Calculus of Inductive Constructions. This widens the class of functions that can be modelled in type-theory based theorem proving tools to potentially nonterminating functions. This is only possible if we extend the logical framework by adding some axioms of classical logic.We claim that the extended framework makes it possible to reason about terminating or non-terminating computations and we show that extraction can also be extended to handle the new functions"},
{"Title": "Canonical Big Operators", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-71067-7_11", "Full Abstract": "In this paper, we present an approach to describe uniformly iterated \"big\" operations, like $\\sum_{i=0^n f(i)$ or max"},
{"Title": "A Short Presentation of Coq", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-71067-7_3", "Full Abstract": "The Coq proof assistant has been developed at INRIA, Ecole Normale Supérieure de Lyon, and University of Paris South for more than twenty years [6]. Its theoretical foundation is known as the \"Calculus of Inductive Constructions\" [4,5]. Versions of the system were distributed regularly from 1989 (version 4.10). The current revision is 8.1 and a revision 8.2 is about to come out. This 8th generation was started in 2004, at the time when a radical change in syntax was enforced and a textbook [2] was published. A more complete historical overview, provided by G. Huet and C. Paulin-Mohring, is available in the book foreword."},
{"Title": "Using Structural Recursion for Corecursion", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02444-3_14", "Full Abstract": "We propose a (limited) solution to the problem of constructing stream values defined by recursive equations that do not respect the guardedness condition. The guardedness condition is imposed on definitions of corecursive functions in Coq, AGDA, and other higher-order proof assistants. In this paper, we concentrate in particular on those non-guarded equations where recursive calls appear under functions. We use a correspondence between streams and functions over natural numbers to show that some classes of non-guarded definitions can be modelled through the encoding as structural recursive functions. In practice, this work extends the class of stream values that can be defined in a constructive type theory-based theorem prover with inductive and coinductive types, structural recursion and guarded corecursion."},
{"Title": "From Semantics to Computer Science", "URL": "https://dl.acm.org/doi/book/10.5555/1658177", "Full Abstract": "Gilles Kahn was one of the most influential figures in the development of computer science and information technology, not only in Europe but throughout the world. This volume of articles by several leading computer scientists serves as a fitting memorial to Kahn's achievements and reflects the broad range of subjects to which he contributed through his scientific research and his work at INRIA, the French National Institute for Research in Computer Science and Control. The editors also reflect upon the future of computing: how it will develop as a subject in itself and how it will affect other disciplines, from biology and medical informatics, to web and networks in general. Its breadth of coverage, topicality, originality and depth of contribution, make this book a stimulating read for all those interested in the future development of information technology."},
{"Title": "Formal study of plane delaunay triangulation", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-14052-5_16", "Full Abstract": "This article presents the formal proof of correctness for a plane Delaunay triangulation algorithm. It consists in repeating a sequence of edge flippings from an initial triangulation until the Delaunay property is achieved. To describe triangulations, we rely on a combinatorial hypermap specification framework we have been developing for years. We embed hypermaps in the plane by attaching coordinates to elements in a consistent way. We then describe what are legal and illegal Delaunay edges and a flipping operation which we show preserves hypermap, triangulation, and embedding invariants. To prove the termination of the algorithm, we use a generic approach expressing that any non-cyclic relation is well-founded when working on a finite set."},
{"Title": "Interactive Theorem Proving and Program Development", "URL": "https://dl.acm.org/doi/book/10.5555/1965123", "Full Abstract": "A practical introduction to the development of proofs and certified programs using Coq. An invaluable tool for researchers, students, and engineers interested in formal methods and the development of zero-fault software."},
{"Title": "A coq-based library for interactive and automated theorem proving in plane geometry", "URL": "https://dl.acm.org/doi/10.5555/2029365.2029401", "Full Abstract": "In this article, we present the development of a library of formal proofs for theorem proving in plane geometry in a pedagogical context. We use the Coq proof assistant. This library includes the basic geometric notions to state theorems and provides a database of theorems to construct interactive proofs more easily. It is an extension of the library of F. Guilhot for interactive theorem proving at the level of high-school geometry, where we eliminate redundant axioms and give formalizations for the geometric concepts using a vector approach. We also enrich this library by offering an automated deduction method which can be used as a complement to interactive proof. For that purpose, we integrate the formalization of the area method which was developed by J. Narboux in Coq."},
{"Title": "A Combination of a Dynamic Geometry Software With a Proof Assistant for Interactive Formal Proofs", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2012.06.005", "Full Abstract": "This paper presents an interface for geometry proving. It is a combination of a dynamic geometry software, Geogebra Geogebra development team, Introduction to GeoGebra. http://www.geogebra.org/book/intro-en/ with a proof assistant, Coq Coq development team, The Coq proof assitant reference manual. http://coq.inria.fr/refman/. Thanks to the features of Geogebra, users can create and manipulate geometric constructions, they discover conjectures and interactively build formal proofs with the support of Coq. Our system allows users to construct fully traditional proofs in the same style as the ones in high school. For each step of proving, we provide a set of applicable rules verified in Coq for users, we also provide tactics in Coq by which minor steps of reasoning are solved automatically."},
{"Title": "A machine-checked proof of the odd order theorem", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-39634-2_14", "Full Abstract": "This paper reports on a six-year collaborative effort that culminated in a complete formalization of a proof of the Feit-Thompson Odd Order Theorem in the Coq proof assistant. The formalized proof is constructive, and relies on nothing but the axioms and rules of the foundational framework implemented by Coq. To support the formalization, we developed a comprehensive set of reusable libraries of formalized mathematics, including results in finite group theory, linear algebra, Galois theory, and the theories of the real and complex algebraic numbers."},
{"Title": "Fixed Precision Patterns for the Formal Verification of Mathematical Constant Approximations", "URL": "https://dl.acm.org/doi/10.1145/2676724.2693172", "Full Abstract": "We describe two approaches for the computation of mathematical constant approximations inside interactive theorem provers. These two approaches share the same basis of fixed point computation and differ only in the way the proofs of correctness of the approximations are described. The first approach performs interval computations, while the second approach relies on bounding errors, for example with the help of derivatives. As an illustration, we show how to describe good approximations of the logarithm function and we compute -- to a precision of a million decimals inside the proof system, with a guarantee that all digits up to the millionth decimal are correct. All these experiments are performed with the Coq system, but most of the steps should apply to any interactive theorem prover."},
{"Title": "Formal proofs of transcendence for e and pi as an application of multivariate and symmetric polynomials", "URL": "https://dl.acm.org/doi/10.1145/2854065.2854072", "Full Abstract": "We describe the formalisation in Coq of a proof that the numbers `e` and `pi` are transcendental. This proof lies at the interface of two domains of mathematics that are often considered separately: calculus (real and elementary complex analysis) and algebra. For the work on calculus, we rely on the Coquelicot library and for the work on algebra, we rely on the Mathematical Components library. Moreover, some of the elements of our formalized proof originate in the more ancient library for real numbers included in the Coq distribution. The case of `pi` relies extensively on properties of multivariate polynomials and this experiment was also an occasion to put to test a newly developed library for these multivariate polynomials."},
{"Title": "Distant Decimals of $$\\pi $$ź", "URL": "https://dl.acm.org/doi/10.1007/s10817-017-9444-2", "Full Abstract": "We describe how to compute very far decimals of $$\\pi $$ź and how to provide formal guarantees that the decimals we compute are correct. In particular, we report on an experiment where 1 million decimals of $$\\pi $$ź and the billionth hexadecimal (without the preceding ones) have been computed in a formally verified way. Three methods have been studied, the first one relying on a spigot formula to obtain at a reasonable cost only one distant digit (more precisely a hexadecimal digit, because the numeration basis is 16) and the other two relying on arithmetic---geometric means. All proofs and computations can be made inside the Coq system. We detail the new formalized material that was necessary for this achievement and the techniques employed to guarantee the accuracy of the computed digits, in spite of the necessity to work with fixed precision numerical computation."},
{"Title": "Decision procedures for queues with integer constraints", "URL": "https://dl.acm.org/doi/10.1007/11590156_18", "Full Abstract": "Queues are a widely used data structure in programming languages. They also provide an important synchronization mechanism in modeling distributed protocols. In this paper we extend the theory of queues with a length function that maps a queue to its size, resulting in a combined theory of queues and Presburger arithmetic. This extension provides a natural but tight coupling between the two theories, and hence the general Nelson-Oppen combination method for decision procedures is not applicable. We present a decision procedure for the quantifier-free theory and a quantifier elimination procedure for the first-order theory that can remove a block of existential quantifiers in one step."},
{"Title": "What's decidable about arrays?", "URL": "https://dl.acm.org/doi/10.1007/11609773_28", "Full Abstract": "Motivated by applications to program verification, we study a decision procedure for satisfiability in an expressive fragment of a theory of arrays, which is parameterized by the theories of the array elements. The decision procedure reduces satisfiability of a formula of the fragment to satisfiability of an equisatisfiable quantifier-free formula in the combined theory of equality with uninterpreted functions (EUF), Presburger arithmetic, and the element theories. This fragment allows a constrained use of universal quantification, so that one quantifier alternation is allowed, with some syntactic restrictions. It allows expressing, for example, that an assertion holds for all elements in a given index range, that two arrays are equal in a given range, or that an array is sorted. We demonstrate its expressiveness through applications to verification of sorting algorithms and parameterized systems. We also prove that satisfiability is undecidable for several natural extensions to the fragment. Finally, we describe our implementation in the"},
{"Title": "Efficient strongly relational polyhedral analysis", "URL": "https://dl.acm.org/doi/10.1007/11609773_8", "Full Abstract": "Polyhedral analysis infers invariant linear equalities and inequalities of imperative programs. However, the exponential complexity of polyhedral operations such as image computation and convex hull limits the applicability of polyhedral analysis. Weakly relational domains such as intervals and octagons address the scalability issue by considering polyhedra whose constraints are drawn from a restricted, user-specified class. On the other hand, these domains rely solely on candidate expressions provided by the user. Therefore, they often fail to produce strong invariants."},
{"Title": "Fixed point iteration for computing the time elapse operator", "URL": "https://dl.acm.org/doi/10.1007/11730637_40", "Full Abstract": "We investigate techniques for automatically generating symbolic approximations to the time solution of a system of differential equations. This is an important primitive operation for the safety analysis of continuous and hybrid systems. In this paper we design a"},
{"Title": "On efficient distributed deadlock avoidance for real-time and embedded systems", "URL": "https://dl.acm.org/doi/10.5555/1898953.1899066", "Full Abstract": "Thread allocation is an important problem in distributed real-time and embedded (DRE) systems. A thread allocation policy that is too liberal may cause deadlock, while a policy that is too conservative limits potential parallelism, thus wasting resources. However, achieving (globally) optimal thread utilization, while avoiding deadlock, has been proven impractical in distributed systems: it requires too much communication between components."},
{"Title": "Decision procedures for term algebras with integer constraints", "URL": "https://dl.acm.org/doi/10.5555/1196027.1196032", "Full Abstract": "Term algebras can model recursive data structures which are widely used in programming languages. To verify programs we must be able to reason about these structures. However, as programming languages often involve multiple data domains, in program verification decision procedures for a single theory are usually not applicable. An important class of mixed constraints consists of combinations of data structures with integer constraints on the size of data structures. Such constraints can express memory safety properties such as absence of memory overflow and out-of-bound array access, which are crucial for program correctness. In this paper we extend the theory of term algebras with the length function which maps a term to its size, resulting in a combined theory of term algebras and Presburger arithmetic. This arithmetic extension provides a natural but tight coupling between the two theories, and hence the general purpose combination methods like Nelson-Op-pen combination are not applicable. We present decision procedures for quantifier-free theories in structures with an infinite constant domain and with a finite constant domain. We also present a quantifier elimination procedure for the extended first-order theory that can remove a block of existential quantifiers in one step."},
{"Title": "Efficient distributed deadlock avoidance with liveness guarantees", "URL": "https://dl.acm.org/doi/10.1145/1176887.1176891", "Full Abstract": "We present a deadlock avoidance algorithm for distributed systems that guarantees liveness. Deadlock avoidance in distributed systems is a hard problem and general solutions are considered impractical due to the high communication overhead. In previous work, however, we showed that practical solutions exist when all possible sequences of resource requests are known a priori in the form of call graphs; in this case protocols can be constructed that perform safe resource allocation based on local data only, that is, no communication between components is required. While avoiding deadlock, those protocols, however, did not avoid starvation: they guaranteed that some process could always make progress, but did not guarantee that every individual process would always eventually terminate.In this paper we present a resource allocation mechanism that avoids deadlock and guarantees absence of starvation, without undue loss of concurrency. The only assumption we make is that the local scheduler is fair. We prove the correctness of the algorithm and show how it can be implemented efficiently."},
{"Title": "Proving ATL* properties of infinite-state systems", "URL": "https://dl.acm.org/doi/10.1007/11921240_17", "Full Abstract": "Alternating temporal logic (atl*) was introduced to prove properties of multi-agent systems in which the agents have different objectives and may collaborate to achieve them. Examples include (distributed) controlled systems, security protocols, and contract-signing protocols. Proving atl* properties over finite-state systems was shown decidable by Alur et al., and a model checker for the sublanguage atl implemented in mocha."},
{"Title": "Verification constraint problems with strengthening", "URL": "https://dl.acm.org/doi/10.1007/11921240_3", "Full Abstract": "The deductive method reduces verification of safety properties of programs to, first, proposing inductive assertions and, second, proving the validity of the resulting set of first-order verification conditions. We discuss the transition from"},
{"Title": "Distributed priority inheritance for real-time and embedded systems", "URL": "https://dl.acm.org/doi/10.1007/11945529_9", "Full Abstract": "We study the problem of priority inversion in distributed real-time and embedded systems and propose a solution based on a distributed version of the priority inheritance protocol (PIP). Previous approaches to priority inversions in distributed systems use variations of the priority ceiling protocol (PCP), originally designed for centralized systems as a modification of PIP that also prevents deadlock. PCP, however, requires maintaining a global view of the acquired resources, which in distributed systems leads to high communication overhead."},
{"Title": "A family of distributed deadlock avoidance protocols and their reachable state spaces", "URL": "https://dl.acm.org/doi/10.5555/1759394.1759413", "Full Abstract": "We study resource management in distributed systems. Incorrect handling of resources may lead to deadlocks, missed deadlines, priority inversions, and other forms of incorrect behavior or degraded performance. While in centralized systems deadlock avoidance is commonly used to ensure correct and efficient resource allocation, distributed deadlock avoidance is harder, and general solutions are considered impractical due to the high communication overhead. However, solutions that use only operations on local data exist if some static information about the possible sequences of remote invocations is known."},
{"Title": "Verifying Balanced Trees", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-72734-7_26", "Full Abstract": "Balanced search trees provide guaranteed worst-case time performance and hence they form a very important class of data structures. However, the self-balancing ability comes at a price; balanced trees are more complex than their unbalanced counterparts both in terms of data structure themselves and related manipulation operations. In this paper we present a framework to model balanced trees in decidable first-order theories of term algebras with Presburger arithmetic. In this framework, a theory of term algebras (i.e., a theory of finite trees) is extended with Presburger arithmetic and with certain connecting functions that map terms (trees) to integers. Our framework is flexible in the sense that we can obtain a variety of decidable theories by tuning the connecting functions. By adding <em>maximal path</em>and <em>minimal path</em>functions, we obtain a theory of red-black trees in which the transition relation of tree self-balancing (rotation) operations is expressible. We then show how to reduce the verification problem of the red-black tree algorithm to constraint satisfiability problems in the extended theory."},
{"Title": "The Calculus of Computation", "URL": "https://dl.acm.org/doi/book/10.5555/1324777", "Full Abstract": "No abstract available."},
{"Title": "The reaction algebra", "URL": "https://dl.acm.org/doi/10.5555/1805839.1805872", "Full Abstract": "Event-pattern reactive programs are small programs that process an input stream of events to detect and act upon given temporal patterns. These programs are used in distributed systems to notify components when they must react."},
{"Title": "Constructing invariants for hybrid systems", "URL": "https://dl.acm.org/doi/10.1007/s10703-007-0046-1", "Full Abstract": "We present a new method for generating algebraic invariants of hybrid systems. The method reduces the invariant generation problem to a constraint solving problem using techniques from the theory of ideals over polynomial rings. Starting with a template invariant--a polynomial equality over the system variables with unknown coefficients--constraints are generated on the coefficients guaranteeing that the solutions are inductive invariants. To control the complexity of the constraint solving, several stronger conditions that imply inductiveness are proposed, thus allowing a trade-off between the complexity of the invariant generation process and the strength of the resulting invariants."},
{"Title": "Deductive verification of alternating systems", "URL": "https://dl.acm.org/doi/10.1007/s00165-008-0075-6", "Full Abstract": "Alternating systems are models of computer programs whose behavior is governed by the actions of multiple agents with, potentially, different goals. Examples include control systems, resource schedulers, security protocols, auctions and election mechanisms. Proving properties about such systems has emerged as an important new area of study in formal verification, with the development of logical frameworks such as the alternating temporal logic"},
{"Title": "Property-directed incremental invariant generation", "URL": "https://dl.acm.org/doi/10.1007/s00165-008-0080-9", "Full Abstract": "A fundamental method of analyzing a system such as a program or a circuit is invariance analysis, in which one proves that an assertion holds on all reachable states. Typically, the proof is performed via induction; however, an assertion, while invariant, may not be inductive (provable via induction). Invariant generation procedures construct auxiliary inductive assertions for strengthening the assertion to be inductive. We describe a general method of generating invariants that is incremental and property-directed. Rather than generating one large auxiliary inductive assertion, our method generates many simple assertions, each of which is inductive relative to those generated before it. Incremental generation is amenable to parallelization. Our method is also property-directed in that it generates inductive assertions that are relevant for strengthening the given assertion. We describe two instances of our method: a procedure for generating clausal invariants of finite-state systems and a procedure for generating affine inequalities of numerical infinite-state systems. We provide evidence that our method scales to checking safety properties of some large finite-state systems."},
{"Title": "The Logical Basis for Computer Programming", "URL": "https://dl.acm.org/doi/book/10.5555/1502271", "Full Abstract": "No abstract available."},
{"Title": "Temporal verification of reactive systems", "URL": "https://dl.acm.org/doi/10.5555/1880443.1880456", "Full Abstract": "No abstract available."},
{"Title": "The Calculus of Computation", "URL": "https://dl.acm.org/doi/book/10.5555/1951719", "Full Abstract": "Written with graduate and advanced undergraduate students in mind, this textbook introduces computational logic from the foundations of first-order logic to state-of-the-art decision procedures for arithmetic, data structures, and combination theories. The textbook also presents a logical approach to engineering correct software. Verification exercises are given to develop the reader's facility in specifying and verifying software using logic. The treatment of verification concludes with an introduction to the static analysis of software, an important component of modern verification systems. The final chapter outlines courses of further study."},
{"Title": "Advances in Knowledge Discovery and Data Mining", "URL": "https://dl.acm.org/doi/book/10.5555/2821283", "Full Abstract": "This two-volume set, LNAI 9077 + 9078, constitutes the refereed proceedings of the 19th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2015, held in Ho Chi Minh City, Vietnam, in May 2015.The proceedings contain 117 paper carefully reviewed and selected from 405 submissions. They have been organized in topical sections named: social networks and social media; classification; machine learning; applications; novel methods and algorithms; opinion mining and sentiment analysis; clustering; outlier and anomaly detection; mining uncertain and imprecise data; mining temporal and spatial data; feature extraction and selection; mining heterogeneous, high-dimensional, and sequential data; entity resolution and topic-modeling; itemset and high-performance data mining; and recommendations."},
{"Title": "Trends and Applications in Knowledge Discovery and Data Mining", "URL": "https://dl.acm.org/doi/book/10.5555/3002432", "Full Abstract": "This book constitutes the refereed proceedings at PAKDD Workshops 2015, held in conjunction with PAKDD, the 19th Pacific-Asia Conference on Knowledge Discovery and Data Mining in Ho Chi Minh City, Vietnam, in May 2015. The 23 revised papers presented were carefully reviewed and selected from 57 submissions. The workshops affiliated with PAKDD 2015 include: Pattern Mining and Application of Big Data (BigPMA), Quality Issues, Measures of Interestingness and Evaluation of data mining models (QIMIE), Data Analytics for Evidence-based Healthcare (DAEBH), Vietnamese Language and Speech Processing (VLSP)."},
{"Title": "Efficient program transformations for resilient parallel computation via randomization (preliminary version)", "URL": "https://dl.acm.org/doi/10.1145/129712.129742", "Full Abstract": "In this paper, we address the problem of automatically transforming"},
{"Title": "Resilient parallel computing on unreliable parallel machines", "URL": "https://dl.acm.org/doi/10.5555/165475.165497", "Full Abstract": "No abstract available."},
{"Title": "Parallel processing on networks of workstations", "URL": "https://dl.acm.org/doi/10.5555/876885.880024", "Full Abstract": "Abstract: One of the most sought after software innovation of this decade is the construction of systems using off-the-shelf-workstations that actually deliver and even surpass, the power and reliability of supercomputers. Using completely novel techniques: eager scheduling, evasive memory layouts and dispersed data management it is possible to build an execution environment for parallel programs on workstation networks. These techniques were originally developed in a theoretical framework for an abstract machine which models a shared memory asynchronous multiprocessor. The network of workstations platform presents an inherently asynchronous environment for the execution of our parallel program. This gives rise to substantial problems of correctness of the computation and of proper automatic load balancing of the work amongst the processors, so that a slow processor will not hold up the total computation. A limiting case of asynchrony is when a processor becomes infinitely slow, i.e. fails. Our methodology copes with all these problems, as well as with memory failures. An interesting feature of this system is that it is neither a fault-tolerant system extended for parallel processing nor is it parallel processing system extended for fault tolerance. The same novel mechanisms ensure both properties."},
{"Title": "CALYPSO", "URL": "https://dl.acm.org/doi/10.5555/822081.823036", "Full Abstract": "The importance of adapting networks of workstations for use as parallel processing platforms is well established. However current solutions do not always address important issues that exist in real networks. External factors like the sharing of resources, unpredictable behavior of the network and failures, are present in multiuser networks and must be addressed. CALYPSO is a prototype software system for writing and executing parallel programs on non-dedicated platforms, based on COTS networked workstations operating systems, and compilers. Among notable properties of the system are: (1) simple programming paradigm incorporating shared memory constructs and separating the programming and the execution parallelism, (2) transparent utilization of unreliable shared resources by providing dynamic load balancing and fault tolerance, and (3) effective performance for large classes of coarse-grained computations. We present the system and report our initial experiments and performance results in settings that closely resemble the dynamic behavior of a \"real\" network. Under varying work-load conditions, resource availability and process failures, the efficiency of the test program we present ranged from 84% to 94% bench-marked against a sequential program."},
{"Title": "Modeling data-intensive reactive systems with relational transition systems", "URL": "https://dl.acm.org/doi/10.1007/s002360050041", "Full Abstract": "In this paper, the formalism of"},
{"Title": "Parallel Suffix--Prefix-Matching Algorithm and Applications", "URL": "https://dl.acm.org/doi/10.1137/S0097539792190157", "Full Abstract": "Our main result in this paper is a parallel algorithm for suffix--prefix- (s--p-) matching that has optimal speedup on a concurrent-read/concurrent-write parallel random-access machine (CRCW PRAM). Given a string of length $m$, the algorithm runs in time $O(\\log m)$ using $m/ \\log m$ processors. This algorithm is important because we utilize s--p matching as a fundamental building block to solve several pattern- and string-matching problems, such as the following: {1. string matching; 2. multitext/multipattern string matching; 3. multidimensional pattern matching; 4. pattern-occurrence detection; 5. on-line string matching. In particular, our techniques and algorithms are the first to preserve optimal speedup in the context of pattern matching in higher dimensions and are the only known ones to do so for dimensions $d > 2$."},
{"Title": "KnittingFactory: An Infrastructure for Distributed Web Applications", "URL": "https://dl.acm.org/doi/book/10.5555/890254", "Full Abstract": "While Java and applets have created a new perspective for Web applications, some problems are still unsolved. Among these are the question of how Java applets can find other members of the collaboration session, how to deal with the restrictions imposed by the Java security model, and how to overcome the inability of applets to communicate directly, even if they belong to the same distributed application. KnittingFactory addresses the problem of finding other members of a collaboration session by providing a distributed registry system where the search is performed within a Web browser without violating its security model; the problem of arbitrary placement of applications by providing the core functionality for downloading applets from an arbitrary node; and finally the problem of direct applet-applet communication by using the Java Remote Method Invocation mechanisms to give applets information on how their fellow applets can be reached. Two example applications validate this concept and demonstrate the ease of use of KnittingFactory."},
{"Title": "Pincer Search", "URL": "https://dl.acm.org/doi/10.5555/645338.650396", "Full Abstract": "No abstract available."},
{"Title": "Exploiting Application Tunability for Efficient, Predictable, Parallel Resource Management", "URL": "https://dl.acm.org/doi/book/10.5555/890298", "Full Abstract": "Parallel computing is becoming increasing central and mainstream, driven both by the widespread availability of commodity SMP and high-performance cluster platforms, as well as the growing use of parallelism in general-purpose applications such as image recognition, virtual reality, and media processing. In addition to performance requirements, the latter computations impose soft real-time constraints, necessitating em efficient, predictable parallel resource management. Unfortunately, traditional resource management approaches in both parallel and real-time systems are inadequate for meeting this objective; the parallel approaches focus primarily on improving application performance and/or system utilization at the cost of arbitrarily delaying a given application, while the real-time approaches are overly conservative sacrificing system utilization in order to meet application deadlines. In this paper, we propose a novel approach for increasing parallel system utilization while meeting application soft real-time deadlines. Our approach exploits the application tunability found in several general-purpose computations. Tunability refers to an application's ability to trade off resource requirements over time, while maintaining a desired level of output quality. In other words, a large allocation of resources in one stage of the computation's lifetime may compensate, in a parameterizable manner, for a smaller allocation in another stage. We first describe language extensions to support tunability in the Calypso programming system, a component of the MILAN metacomputing project, and evaluate their expressiveness using an image processing application. We then characterize the performance benefits of tunability, using a synthetic task system to systematically identify its benefits and shortcomings. Our results are very encouraging: application tunability is convenient to express, and can significantly improve parallel system utilization for computations with predictability requirements."},
{"Title": "Mechanisms for Just-in-Time Allocation of Resources to Adaptive Parallel Programs", "URL": "https://dl.acm.org/doi/10.5555/645608.661825", "Full Abstract": "Adaptive parallel computations-computations that can adapt to changes in resource availability and requirement- can effectively use networked machines because they dynamically expand as machines become available and dynamically acquire machines as needed. While most parallel programming systems provide the means to develop adaptive programs, they do not provide any functional interface to external resource management systems. Thus, no existing resource management system has the capability to manage resources on commodity system software, arbitrating the demands of multiple adaptive computations written using diverse programming environments.This paper presents a set of novel mechanisms that facilitate dynamic allocation of resources to adaptive parallel computations. The mechanisms are built on low-level features common to many programming systems, and unique in their ability to transparently manage multiple adaptive parallel programs that were not developed to have their resources managed by external systems. We also describe the design and the implementation of the initial prototype of ResourceBroker, a resource management system built to validate these mechanisms."},
{"Title": "Exploiting Application Tunability for Efficient, Predictable Parallel Resource Management", "URL": "https://dl.acm.org/doi/10.5555/645608.661833", "Full Abstract": "Parallel computing is becoming increasing central and mainstream, driven both by the widespread availability of commodity SMP and high-performance cluster platforms, as well as the growing use of parallelism in general-purpose applications such as image recognition, virtual reality, and media processing. In addition to performance requirements, the latter computations impose soft real-time constraints, necessitating efficient, predictable parallel resource management. In this paper, we propose a novel approach for increasing parallel system utilization while meeting application soft real-time deadlines. Our approach exploits the application tunability found in several general-purpose computations. Tunability refers to an application's ability to trade off resource requirements over time, while maintaining a desired level of output quality. We first describe language extensions to support tunability in the Calypso system, then characterize the performance benefits of tunability, using a synthetic task system to systematically identify its benefits. Our results show that application tunability is convenient to express and can significantly improve parallel system utilization for computations with predictability requirements."},
{"Title": "Metacomputing with MILAN", "URL": "https://dl.acm.org/doi/10.5555/795690.797900", "Full Abstract": "The MILAN project, a joint effort involving Arizona State University and New York University, has produced and validated fundamental techniques for the realization of efficient, reliable, predictable virtual machines, that is, metacomputers, on top of environments that consist of an unreliable and dynamically changing set of machines. In addition to the techniques, the principal outcomes of the project include three parallel programming systems- Calypso, Chime, and Charlotte-which enable applications be developed for ideal, shared memory, parallel machines to execute on distributed platforms that are subject to failures, slowdowns, and changing resource availability. The lessons learned from the MILAN project are being used to design Computing Communities, a metacomputing frame-work for general computations."},
{"Title": "Charlotte", "URL": "https://dl.acm.org/doi/10.1016/S0167-739X%2899%2900009-6", "Full Abstract": "No abstract available."},
{"Title": "Exploiting Application Tunability for Efficient, Predictable Resource Management in Parallel and Distributed Systems", "URL": "https://dl.acm.org/doi/10.1006/jpdc.2000.1660", "Full Abstract": "Parallel and distributed computing is becoming increasingly mainstream, driven both by the widespread availability of commodity small-scale symmetric multiprocessors and high-performance cluster platforms, as well as the growing use of parallelism and distribution in networked applications such as image recognition, media processing, virtual reality, and telepresence. However, many of these applications impose soft timeliness and output quality constraints on top of the traditional performance requirements, necessitating efficient, predictable management of system resources. Existing techniques are inadequate to simultaneously support these twin requirements of efficiency and predictability. In this paper, we propose a novel approach for increasing system efficiency while meeting application timeliness and quality constraints. Our approach exploits the application tunability found in many general-purpose computations. Tunability refers to an application's ability to trade off resource requirements over several dimensions including time, quality, and resource type; the resulting flexibility enables the underlying resource management system to choose an application operating point best suited to available resource characteristics. We describe language and scheduler extensions to support tunability in the MILAN metacomputing environment and then systematically characterize performance benefits of tunability using a parameterizable task system. Our results show that application tunability is easily expressible and can significantly improve resource utilization."},
{"Title": "Automatic data and computation decomposition on distributed memory parallel computers", "URL": "https://dl.acm.org/doi/10.1145/509705.509706", "Full Abstract": "To exploit parallelism on shared memory parallel computers (SMPCs), it is natural to focus on decomposing the computation (mainly by distributing the iterations of the nested Do-Loops). In contrast, on distributed memory parallel computers (DMPCs), the decomposition of computation and the distribution of data must both be handled---in order to balance the computation load and to minimize the migration of data. We propose and validate experimentally a method for handling computations and data synergistically to minimize the overall execution time on DMPCs. The method is based on a number of novel techniques, also presented in this article. The core idea is to rank the \"importance\" of data arrays in a program and specify some of the dominant. The intuition is that the dominant arrays are the ones whose migration would be the most expensive. Using the correspondence between iteration space mapping vectors and distributed dimensions of the dominant data array in each nested Do-loop, allows us to design algorithms for determining data and computation decompositions at the same time. Based on data distribution, computation decomposition for each nested Do-loop is determined based on either the \"owner computes\" rule or the \"owner stores\" rule with respect to the dominant data array. If all temporal dependence relations across iteration partitions are regular, we use tiling to allow pipelining and the overlapping of computation and communication. However, in order to use tiling on DMPCs, we needed to extend the existing techniques for determining tiling vectors and tile sizes, as they were originally suited for SMPCs only. The overall method is illustrated on programs for the 2D heat equation, for the Gaussian elimination with pivoting, and for the 2D fast Fourier transform on a linear processor array and on a 2D processor grid."},
{"Title": "Pincer-Search", "URL": "https://dl.acm.org/doi/10.1109/TKDE.2002.1000342", "Full Abstract": "Discovering frequent itemsets is a key problem in important data mining applications, such as the discovery of association rules, strong rules, episodes, and minimal keys. Typical algorithms for solving this problem operate in a bottom-up, breadth-first search direction. The computation starts from frequent 1-itemsets (the minimum length frequent itemsets) and continues until all maximal (length) frequent itemsets are found. During the execution, every frequent itemset is explicitly considered. Such algorithms perform well when all maximal frequent itemsets are short. However, performance drastically deteriorates when some of the maximal frequent itemsets are long. We present a new algorithm which combines both the bottom-up and the top-down searches. The primary search direction is still bottom-up, but a restricted search is also conducted in the top-down direction. This search is used only for maintaining and updating a new data structure, the maximum frequent candidate set. It is used to prune early candidates that would be normally encountered in the bottom-up search. A very important characteristic of the algorithm is that it does not require explicit examination of every frequent itemset. Therefore, the algorithm performs well even when some maximal frequent itemsets are long. As its output, the algorithm produces the maximum frequent set, i.e., the set containing all maximal frequent itemsets, thus specifying immediately all frequent itemsets. We evaluate the performance of the algorithm using well-known synthetic benchmark databases, real-life census, and stock market databases. The improvement in performance can be up to several orders of magnitude, compared to the best previous algorithms."},
{"Title": "Detecting malicious network traffic using inverse distributions of packet contents", "URL": "https://dl.acm.org/doi/10.1145/1080173.1080176", "Full Abstract": "We study the problem of detecting malicious IP traffic in the network early, by analyzing the contents of packets. Existing systems look at packet contents as a bag of substrings and study characteristics of its"},
{"Title": "Sustaining moore's law in embedded computing through probabilistic and approximate design", "URL": "https://dl.acm.org/doi/10.1145/1629395.1629397", "Full Abstract": "The central theme of our work is the probabilistic and approximate design of embedded computing systems. This novel approach consists of two distinguishing aspects: (i) the design and implementation of embedded systems, using components which are susceptible to perturbations from various sources and (ii) a design methodology which consists of an exploration of a design space which characterizes the trade-off between quality of output and cost, to implement high performance and low energy embedded systems. In contrast with other work, our design methodology does not attempt to correct the errors introduced by components which are susceptible to perturbations, instead we design \"good enough\" systems. Our work has the potential to address challenges and impediments to Moore's law arising from material properties and manufacturing difficulties, which dictate that we shift from the current-day deterministic design paradigm to statistical and probabilistic designs of the future. In this paper, we provide a broad overview of our work on probabilistic and approximate design, present novel results in approximate arithmetic and its impact on digital signal processing algorithms, and sketch future directions for research."},
{"Title": "Optimizing energy to minimize errors in dataflow graphs using approximate adders", "URL": "https://dl.acm.org/doi/10.1145/1878921.1878948", "Full Abstract": "Approximate arithmetic is a promising, new approach to low-energy designs while tackling reliability issues. We present a method to optimally distribute a given energy budget among adders in a dataflow graph so as to minimize expected errors. The method is based on new formal mathematical models and algorithms, which quantitatively characterize the relative importance of the adders in a circuit. We demonstrate this method on a"},
{"Title": "An approach to energy-error tradeoffs in approximate ripple carry adders", "URL": "https://dl.acm.org/doi/10.5555/2016802.2016853", "Full Abstract": "Given a 16-bit or 32-bit overclocked ripple-carry adder, we minimize error by allocating multiple supply voltages to the gates. We solve the error minimization problem for a fixed energy budget using a binned geometric program solution (BGPS). A solution found via BGPS outperforms the two best prior approaches, uniform voltage scaling and biased voltage scaling, reducing error by as much as a factor of 2.58X and by a median of 1.58X in 90nm transistor technology."},
{"Title": "Some Perspectives on Complexity-Based Cryptography", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-89255-7_4", "Full Abstract": "In the 1940's, Shannon applied his information theory to build a mathematical foundation for classical cryptography which studies how information can be securely encrypted and communicated. In the internet age, Turing's theory of computation has been summoned to augment Shannon's model and create new frameworks, under which numerous cryptographic applications have blossomed. Fundamental concepts, such as ``information\" and ``knowledge transfer\", often need to be re-examined and reformulated. The amalgamation process is still on-going in view of the many unsolved security issues. In this talk we give a brief overview of the background, and discuss some of the recent developments in complexity-based cryptography. We also raise some open questions and explore directions for future work."},
{"Title": "A note on the feasibility of generalised universal composability†", "URL": "https://dl.acm.org/doi/10.1017/S0960129508007330", "Full Abstract": "In this paper we study (interpret) the precise composability guarantee of the generalised universal composability (GUC) feasibility with global setups that was proposed in the recent paper Canetti"},
{"Title": "A note on universal composable zero-knowledge in the common reference string model", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2008.10.027", "Full Abstract": "Pass observed that universal composable zero-knowledge (UCZK) protocols in the common reference string (CRS) model lose deniability that is a natural security property and implication of the ZK functionality in accordance with the UC framework. An open problem (or, natural query) raised in the literature is: are there any other essential security properties, other than the well-known deniability property, that could be lost by UCZK in the CRS model, in comparison with the ZK functionality in accordance with the UC framework? In this work, we answer this open question (or, natural query), by showing that when running concurrently with other protocols UCZK in the CRS model can lose proof of knowledge (POK) property that is very essential and core security implication of the ZK functionality. This is demonstrated by concrete attack against naturally existing UCZK protocols in the CRS model. Then, motivated by our attack, we make further clarifications of the underlying reasons beneath the concrete attack, and investigate the precise security guarantee of UC with CRS."},
{"Title": "Communication Complexity and Its Applications", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02270-8_2", "Full Abstract": "For any function f(x, y), its communication complexity is the minimum number of bits needed to be exchanged between two parties holding integers x and y respectively. Invented thirty years ago, communication complexity has been a central research area in theoretical computer science with rich applications to algorithmic problems. In this talk, we give an overview of computational complexity, high-lighting several notable recent results and the diverse mathematical techniques needed for deriving such results."},
{"Title": "Deniable internet key exchange", "URL": "https://dl.acm.org/doi/10.5555/1894302.1894328", "Full Abstract": "In this work, we develop a family of non-malleable and deniable Diffie-Hellman key-exchange (DHKE) protocols, named deniable Internet keyexchange (DIKE). The newly developed DIKE protocols are of conceptual clarity, provide much remarkable privacy protection to protocol participants, and are of highly practical (online) efficiency."},
{"Title": "On the Quantum Query Complexity of Local Search in Two and Three Dimensions", "URL": "https://dl.acm.org/doi/10.1007/s00453-008-9170-6", "Full Abstract": "The quantum query complexity of searching for local optima has been a subject of much interest in the recent literature. For the"},
{"Title": "Concurrent knowledge extraction in the public-key model", "URL": "https://dl.acm.org/doi/10.5555/1880918.1880994", "Full Abstract": "Knowledge extraction is a fundamental notion, modeling machine possession of values (witnesses) in a computational complexity sense and enabling one to argue about the internal state of a party in a protocol without probing its internal secret state. However, when transactions are concurrent (e.g., over the Internet) with players possessing public-keys (as is common in cryptography), assuring that entities \"know\" what they claim to know, where adversaries may be well coordinated across different transactions, turns out to be much more subtle and in need of re-examination. Here, we investigate how to formally treat knowledge possession by parties (with registered public-keys) interacting over the Internet. Stated more technically, we look into the relative power of the notion of \"concurrent knowledge-extraction\" (CKE) in the concurrent zero-knowledge (CZK) bare public-key (BPK) model where statements being proven can be dynamically and adaptively chosen by the prover."},
{"Title": "Tight Approximation Ratio of a General Greedy Splitting Algorithm for the Minimum ", "URL": "https://dl.acm.org/doi/10.5555/1966796.1966799", "Full Abstract": "For an edge-weighted connected undirected graph, the minimum"},
{"Title": "Tight Approximation Ratio of a General Greedy Splitting Algorithm for the Minimum k-Way Cut Problem", "URL": "https://dl.acm.org/doi/10.5555/3118745.3118910", "Full Abstract": "For an edge-weighted connected undirected graph, the minimum k-way cut problem is to find a subset of edges of minimum total weight whose removal separates the graph into k connected components. The problem is NP-hard when k is part of the input and W[1]-hard when k is taken as a parameter."},
{"Title": "Computationally-Fair group and identity-based key-exchange", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-29952-0_26", "Full Abstract": "In this work, we re-examine some fundamental group key-exchange and identity-based key-exchange protocols, specifically the Burmester-Desmedet group key-exchange protocol [7] (referred to as the BD-protocol) and the Chen-Kudla identity-based key-exchange protocol [9] (referred to as the CK-protocol). We identify some new attacks on these protocols, showing in particular that these protocols are not computationally fair. Specifically, with our attacks, an adversary can do the following damages:"},
{"Title": "Quantum computing", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-29952-0_7", "Full Abstract": "In recent years, the scientific world has seen much excitement over the development of quantum computing, and the ever increasing possibility of building real quantum computers. What's the advantage of quantum computing? What are the secrets in the atoms that could potentially unleash such enormous power, to be used for computing and information processing? In this talk, we will take a look at quantum computing, and make the case that we are witnessing a great science in the making."},
{"Title": "Online/Offline Signatures for Low-Power Devices", "URL": "https://dl.acm.org/doi/10.1109/TIFS.2012.2232653", "Full Abstract": "When digital signature is applied on low-power devices, like smart cards, wireless sensors and RFID tags, some specific properties, e.g., better offline storage, more modular and flexible deployment, are desired. To meet these needs, a new variant of the Fiat–Shamir transformation for digital signatures, referred to as $\\Gamma$ -transformation, is introduced and formalized in this work. Following this new transformation approach, some new signature schemes (referred to as $\\Gamma$-signatures) are presented and discussed. In particular, it is shown that the $\\Gamma$-signatures for discrete logarithm problem (DLP) developed in this work combine, in essence, the advantages of both Schnorr's signature and the digital signature standard (DSS), while saving from the disadvantages of them both."},
{"Title": "Graph Coloring Applied to Secure Computation in Non-Abelian Groups", "URL": "https://dl.acm.org/doi/10.1007/s00145-011-9104-3", "Full Abstract": "We study the natural problem of secure"},
{"Title": "OAKE", "URL": "https://dl.acm.org/doi/10.1145/2508859.2516695", "Full Abstract": "Cryptographic algorithm standards play an important role both to the practice of information security and to cryptography theory research. Among them, the KEA and OPACITY (KEA/OPACITY, in short) protocols, and the MQV and HMQV ((H)MQV, in short) protocols, are a family of implicitly authenticated Diffie-Hellman key-exchange (IA-DHKE) protocols that are among the most efficient authenticated key-exchange protocols known and are widely standardized. In this work, from some new design insights, we develop a new family of practical IA-DHKE protocols, referred to as OAKE (standing for \"optimal authenticated key-exchange\" in brief). We show that the OAKE protocol family combines, in essence, the advantages of both (H)MQV and KEA/OPACITY, while saving from or alleviating the disadvantages of them both."},
{"Title": "Privacy-Preserving Authenticated Key-Exchange Over Internet", "URL": "https://dl.acm.org/doi/10.1109/TIFS.2013.2293457", "Full Abstract": "Key-exchange, in particular Diffie–Hellman key-exchange (DHKE), is among the core cryptographic mechanisms for ensuring network security. For key-exchange over the Internet, both security and privacy are desired. In this paper, we develop a family of privacy-preserving authenticated DHKE protocols named deniable Internet key-exchange (DIKE), both in the traditional PKI setting and in the identity-based setting. The newly developed DIKE protocols are of conceptual clarity and practical (online) efficiency. They provide useful privacy protection to both protocol participants, and add novelty and new value to the IKE standard. To the best of our knowledge, our protocols are the first provably secure DHKE protocols that additionally enjoy all the following privacy protection advantages: 1) forward deniability, actually concurrent non-malleable statistical zero-knowledge, for both protocol participants simultaneously; 2) the session transcript and session-key can be generated merely from DH-exponents (together with some public values), which thus cannot be traced to the pair of protocol participants; and 3) exchanged messages do not bear peer's identity, and do not explicitly bear player role information."},
{"Title": "An ", "URL": "https://dl.acm.org/doi/10.5555/2722129.2722137", "Full Abstract": "In this paper, we introduce a novel approach for reducing the"},
{"Title": "Interdisciplinarity: A View from Theory of Computation", "URL": "https://dl.acm.org/doi/10.1145/2820468.2820472", "Full Abstract": "Increasingly, the concepts and methods of computer science are being recognized as a source of great intellectual interest, injecting fresh ideas into other scientific disciplines. Through discourses and collaborations, exciting multidisciplinary areas are blossoming. We illustrate this phenomenon from the viewpoint of Theory of Computation."},
{"Title": "Concurrent Knowledge Extraction in Public-Key Models", "URL": "https://dl.acm.org/doi/10.1007/s00145-014-9191-z", "Full Abstract": "Knowledge extraction is a fundamental notion, modeling machine possession of values (witnesses) in a computational complexity sense and enabling one to argue about the internal state of a party in a protocol without probing its internal secret state. However, when transactions are concurrent, say over the Internet, with players possessing public keys (as is common in cryptography), assuring that entities \"know\" what they claim to know, where adversaries may be well coordinated across different transactions, turns out to be much more subtle and in need of re-examination. In such settings, mixing the public-key structure as part of the language and statements is a natural adversarial strategy. Here, we investigate how to formally treat knowledge possession by parties interacting concurrently in the public-key model. More technically, we look into the relative power of the notion of \"concurrent knowledge extraction\" (CKE) for concurrent zero knowledge (CZK) in the bare public-key (BPK) model, where the language and statements being proved can be dynamically and adaptively chosen by the prover and may be possibly based on verifiers' public keys. By concrete attacks against some existing natural protocols, we first show that concurrent soundness and normal arguments of knowledge do not guarantee concurrent verifier security in the public-key setting. Here, roughly speaking, concurrent verifier security says that the malicious concurrent prover should \"know\" all the witnesses to all the possibly public-key-related statements adaptively chosen and successfully proved in the concurrent sessions. These concrete attacks serve as a good motivation for understanding \"possession of knowledge\" for concurrent transactions with registered public keys, i.e., the subtleties of concurrent knowledge extraction in the public-key model. This motivates us to introduce and formalize the notion of CKE, along with clarifications of various subtleties. Two implementations are then presented for constant-round concurrently knowledge extractable concurrent zero-knowledge (CZK---CKE) argument for $$\\mathcal {NP$$NP in the BPK model: One protocol is generic and based on standard polynomial-time assumptions, whereas the other protocol is computationally efficient and employs complexity leveraging in a novel way. Both protocols can be practically instantiated for some specific number-theoretic languages without going through general $$\\mathcal {NP$$NP-reductions. Of independent interest are the discussions about the subtleties surrounding the fundamental structure of Feige---Shamir zero knowledge in the BPK model."},
{"Title": "Dominant-Strategy versus Bayesian Multi-item Auctions", "URL": "https://dl.acm.org/doi/10.1145/3033274.3085120", "Full Abstract": "We address two related unanswered questions in maximum revenue multi-item auctions. Is dominant-strategy implementation equivalent to the semantically less stringent Bayesian one (as in the case of Myerson's 1-item auction)? Can one find explicit solutions for non-trivial families of multi-item auctions (as in the 1-item case)? In this paper, we present such natural families whose explicit solutions exhibit a revenue gap between the two implementations. More precisely, consider the"},
{"Title": "Towards data-algorithm dependent generalization", "URL": "https://dl.acm.org/doi/10.5555/3666122.3669611", "Full Abstract": "One of the major open problems in machine learning is to characterize generalization in the overparameterized regime, where most traditional generalization bounds become inconsistent even for overparameterized linear regression [46]. In many scenarios, this failure can be attributed to obscuring the crucial interplay between the training algorithm and the underlying data distribution. This paper demonstrate that the generalization behavior of overparameterized model should be analyzed in a both data-relevant and algorithm-relevant manner. To make a formal characterization, We introduce a notion called data-algorithm compatibility, which considers the generalization behavior of the entire data-dependent training trajectory, instead of traditional last-iterate analysis. We validate our claim by studying the setting of solving overparameterized linear regression with gradient descent. Specifically, we perform a data-dependent trajectory analysis and derive a sufficient condition for compatibility in such a setting. Our theoretical results demonstrate that if we take early stopping iterates into consideration, generalization can hold with significantly weaker restrictions on the problem instance than the previous last-iterate analysis."},
{"Title": "Post-WIMP user interfaces", "URL": "https://dl.acm.org/doi/10.1145/253671.253708", "Full Abstract": "Copyright © 1997 author."},
{"Title": "The history of computer graphics standards development", "URL": "https://dl.acm.org/doi/10.1145/279389.279434", "Full Abstract": "In keeping with the retrospective theme of this issue of"},
{"Title": "The shape of things to come", "URL": "https://dl.acm.org/doi/10.1145/279389.279446", "Full Abstract": "Copyright © 1998 Author."},
{"Title": "Look Ma! four hands! new models for interacting with 3D environments", "URL": "https://dl.acm.org/doi/10.1145/280953.281554", "Full Abstract": "No abstract available."},
{"Title": "Granularity in the design of interactive illustrations", "URL": "https://dl.acm.org/doi/10.1145/299649.299794", "Full Abstract": "We describe some issues in designing and building educational Java applets for an introductory computer graphics course. The design problem involves balancing educational goals of building intuition about fundamental concepts in a domain against heterogeneity both in subject material and in student backgrounds. We present our design approach for resolving these forces --- fine-grained units addressing small concepts --- and discuss its effects on other areas including hypertext structure, interface design, and software engineering."},
{"Title": "Scene graph APIs", "URL": "https://dl.acm.org/doi/10.1145/311625.311927", "Full Abstract": "No abstract available."},
{"Title": "Education", "URL": "https://dl.acm.org/doi/10.1145/345966.346038", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Immersive virtual reality for visualizing flow through an artery", "URL": "https://dl.acm.org/doi/10.5555/375213.375297", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Immersive Virtual Reality for Visualizing Flow Through an Artery", "URL": "https://dl.acm.org/doi/10.5555/832272.833917", "Full Abstract": "We present an immersive system for exploring numerically simulated flow data through a model of a coronary artery graft. This tightly-coupled interdisciplinary project is aimed at understanding how to reduce the failure rate of these grafts. The visualization system provides a mechanism for exploring the effect of changes to the geometry, to the flow, and for exploring potential sources of future lesions. The system uses gestural and voice interactions exclusively, moving away from more traditional windows/icons/menus/point-and-click (WIMP) interfaces.We present an example session using the system and discuss our experiences developing, testing, and using it. We describe some of the interaction and rendering techniques that we experimented with and describe their level of success. Our experience suggests that systems like this are exciting to clinical researchers, but conclusive evidence of their value is not yet available."},
{"Title": "Immersive VR for Scientific Visualization", "URL": "https://dl.acm.org/doi/10.1109/38.888006", "Full Abstract": "Immersive virtual reality can provide powerful techniques for scientific visualization. The research agenda for the technology sketched here offers a progress report, a hope, and a call to action."},
{"Title": "User interfaces", "URL": "https://dl.acm.org/doi/10.1145/365181.365192", "Full Abstract": "Copyright © 2001 ACM."},
{"Title": "Immersive Electronic Books for Surgical Training", "URL": "https://dl.acm.org/doi/10.1109/MMUL.2005.48", "Full Abstract": "Immersive electronic books (IEBooks) for surgical training will let surgeons explore previous surgical procedures in 3D. The authors describe the techniques and tools for creating IEBook."},
{"Title": "Visualization Research Problems in Next-Generation Educational Software", "URL": "https://dl.acm.org/doi/10.1109/MCG.2005.118", "Full Abstract": "The dream of universal access to high-quality, personalized educational content available both synchronously and asynchronously remains unrealized. For more than four decades, many have said that information technology (IT) would be a key technology in realizing this dream by helping to produce compelling and individualized educational content, the means for delivering it, and effective feedback and assessment mechanisms. Yet today the most visible impact of IT is simple uses of the Web as a delivery mechanism for text and images, as well as for some interactive Java applets, and as a communications medium through blogs and wikis."},
{"Title": "Next-generation educational software", "URL": "https://dl.acm.org/doi/10.1145/1281500.1281543", "Full Abstract": "The dream of universal access to high-quality, personalized educational content that is available both synchronously and asynchronously remains unrealized. For more than four decades, it has been said that information technology would be a key enabling technology for making this dream a reality by providing the ability to produce compelling and individualized content, the means for delivering it, and effective feedback and assessment mechanisms. Although IT has certainly had some impact, it has become a cliché to note that education is the last field to take systematic advantage of IT. There have been some notable successes of innovative software (e.g., the graphing calculator, the Geometer's Sketchpad, and the World Wide Web as an information-storage and -delivery vehicle), but we continue to teach-and students continue to learn-in ways that are virtually unchanged since the invention of the blackboard."},
{"Title": "Applications and Issues in Pen-Centric Computing", "URL": "https://dl.acm.org/doi/10.1109/MMUL.2008.82", "Full Abstract": "As part of the rapidly evolving field of designing more natural user interfaces for multimedia information, pen-centric computing refuses to disappear. As a quite natural and universal interface modality, it presents many challenges. In this article, the pen-centric computing group at Brown University, led by Andries Van Dam, surveys the many prototypes they have designed and implemented, and discuss the research issues in the field still to be explored."},
{"Title": "NuSys", "URL": "https://dl.acm.org/doi/10.1145/3103010.3121045", "Full Abstract": "Knowledge workers consume and annotate digital documents such as PDF files, videos, images and text notes - in some cases collaboratively - to form mental models and gain insight. An abundance of software solutions and utilities that were designed to assist users in stages of this process but not in the process as a whole, which makes knowledge work with documents unnecessarily inefficient. In this paper, we introduce ideas on how to streamline common knowledge worker tasks, such as collaboratively searching, gathering and freely arranging fragments of various media documents to gain understanding and then transforming emergent insights into interactive structured visualizations. Furthermore, we present NuSys, an integrated development environment (IDE) specialized for document-centric workflows, that implements the core of these ideas."},
{"Title": "Reflections on an introductory CS course, CS15, at Brown University", "URL": "https://dl.acm.org/doi/10.1145/3284639", "Full Abstract": "Copyright © 2018 ACM."},
{"Title": "Reflections on a Half-Century of Hypertext", "URL": "https://dl.acm.org/doi/10.1145/3342220.3344782", "Full Abstract": "2019 marks not only the 30th anniversary of the falling of the Berlin Wall, but also the 50th anniversaries of equally momentous events of 1968-1969 in the US and elsewhere. Martin Luther King and Robert Kennedy were assassinated. Hippie \"flower power\" and the closely related anti-Vietnam war movement were socio-political revolutions. In Europe, 2019 marks the 100th anniversary of the end of the \"war to end all wars\" and the 75th anniversary of D-Day. Counterpointing this societal turmoil, technology gave us hope. Neil Armstrong and Buzz Aldrin walked on the moon. Doug Engelbart and his team presented the \"Mother of All Demos\" of NLS at the '68 Fall Joint Computer Conference. Ivan Sutherland's pioneering Sketchpad (that demo'd interactive graphics in 1963) and Engelbart's NLS demo were two landmark events that were early examples of interactive computing in an era of batch computation. Interactive computing on time-sharing systems, combined with microminiaturization, would lead more than a decade later to the birth of the personal computer. It caused a revolution in the dominant model of computing that was centered on large mainframes and minicomputers used for science and engineering, finance and commerce. Interactive computing based on computer graphics and its use in hypermedia systems characterizes most of my research career. In 2019, it is difficult to remember the impact that interaction-based information structuring and sharing had on society; it certainly shaped my research career. In this presentation, I will reflect on the development of five decades of hypermedia systems and will demo three systems that have been highlights of my journey in hyperland. First, I'll show our FRESS hypertext system (still running 50-year old assembly code!), with the database of poetry used by a class of English students in 1976 in what is arguably the first online scholarly community. Next, I will demo our TAG (Touch Art Gallery) used by the Nobel Foundation a few years ago for a traveling exhibition on Alfred Nobel and all the Nobel Laureates. Finally, I'll interweave the hypertext-centric parts of my talk with some source material stored in an unbounded 2D workspace, using our current hypermedia system Dash, which is still under development and in an early but already useful state. These systems will be presented in the context of the research trends that led, ultimately, to the interconnected society in which we live. All of us working on our first hypertext systems in the '60s understood the potential of this technology. What I did not predict is that 50 years later the revolution in human-centered computing would remain far too unfinished in terms of its positive societal impact. Indeed, that impact and utility are increasingly in jeopardy from a variety of forces, both economic and political. I will close with some thoughts on both deliberately designed and unanticipated societal issues of social media that I feel we technologists must urgently help address."},
{"Title": "Dash", "URL": "https://dl.acm.org/doi/10.1145/3372923.3404807", "Full Abstract": "Popular application suites, as well as specialized apps, are designed for workflows in which users focus on a single task for extended periods of time. These application silos slow down the many other workflows that require users to move with agility between tasks in a single working session. This is particularly true for creative people who have personalized patterns of gathering, organizing, and presenting information from a variety of sources. Moreover, each application comes with its own learning curve and data model, restricting users seeking to extend their workflows and in some cases, losing data through poor data transferring mechanisms such as clipboard copy and paste."},
{"Title": "50 Years of Changes–How to Brace Yourself!", "URL": "https://dl.acm.org/doi/10.1145/3587422.3597996", "Full Abstract": "Having lived through the 50 years of changes, the panelists attempt to put them in calibrated perspective, not merely for the sake of fond reminiscence–and fun!–but as a guide to those who face the looming future changes."},
{"Title": "Using behavioral data to identify interviewer fabrication in surveys", "URL": "https://dl.acm.org/doi/10.1145/2470654.2481404", "Full Abstract": "Surveys conducted by human interviewers are one of the principal means of gathering data from all over the world, but the quality of this data can be threatened by interviewer fabrication. In this paper, we investigate a new approach to detecting interviewer fabrication automatically. We instrument electronic data collection software to record logs of low-level behavioral data and show that supervised classification, when applied to features extracted from these logs, can identify interviewer fabrication with an accuracy of up to 96%. We show that even when interviewers know that our approach is being used, have some knowledge of how it works, and are incentivized to avoid detection, it can still achieve an accuracy of 86%. We also demonstrate the robustness of our approach to a moderate amount of label noise and provide practical recommendations, based on empirical evidence, on how much data is needed for our approach to be effective."},
{"Title": "Selling in Exclusive Markets", "URL": "https://dl.acm.org/doi/10.1145/2465769.2465772", "Full Abstract": "We consider prior-free benchmarks in non-matroid settings. In particular, we show that a very desirable benchmark proposed by Hartline and Roughgarden is too strong, in the sense that no truthful mechanism can compete with it even in a very simple non-matroid setting where there are two exclusive markets and the seller can only sell to agents in one of them. On the other hand, we show that there is a mechanism that competes with a symmetrized version of this benchmark. We further investigate the more traditional best fixed price profit benchmark and show that there are mechanisms that compete with it in any downward-closed settings."},
{"Title": "On revenue maximization for agents with costly information acquisition", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-39212-2_43", "Full Abstract": "A prevalent assumption in traditional mechanism design is that buyers know their precise value for an item; however, this assumption is rarely true in practice. In most settings, buyers can \"deliberate\", i.e., spend money or time, in order improve their estimate of an item's value. It is known that the deliberative setting is fundamentally different than the classical one, and desirable properties of a mechanism such as equilibria, revenue maximization, or truthfulness, may no longer hold."},
{"Title": "Approximate revenue maximization in interdependent value settings", "URL": "https://dl.acm.org/doi/10.1145/2600057.2602858", "Full Abstract": "We study revenue maximization in settings where agents' values are interdependent: each agent receives a signal drawn from a correlated distribution and agents' values are functions of all of the signals. We introduce a variant of the generalized VCG auction with reserve prices and random admission, and show that this auction gives a constant approximation to the optimal expected revenue in matroid environments. Our results do not require any assumptions on the signal distributions, however, they require the value functions to satisfy a standard single-crossing property and a concavity-type condition."},
{"Title": "Convergence of Position Auctions under Myopic Best-Response Dynamics", "URL": "https://dl.acm.org/doi/10.1145/2632226", "Full Abstract": "We study the dynamics of multiround position auctions, considering both the case of exogenous click-through rates and the case in which click-through rates are determined by an endogenous consumer search process. In both contexts, we demonstrate that dynamic position auctions converge to their associated static, envy-free equilibria. Furthermore, convergence is efficient, and the entry of low-quality advertisers does not slow convergence. Because our approach predominantly relies on assumptions common in the sponsored search literature, our results suggest that dynamic position auctions converge more generally."},
{"Title": "On a competitive secretary problem", "URL": "https://dl.acm.org/doi/10.5555/2887007.2887138", "Full Abstract": "Consider a scenario in which there are multiple employers competing to hire the best possible employee. How does the competition between the employers affect their hiring strategies or their ability to hire one of the best possible candidates? In this paper, we address this question by studying a generalization of the classical secretary problem from optimal stopping theory: a set of ranked employers compete to hire from the same random stream of employees, and each employer wishes to hire the best candidate in the bunch. We show how to derive subgame-perfect Nash equilibrium strategies in this game and analyze the impact the competition has on the quality of the hires as a function of the rank of the employer. We present numerical results from simulations of these strategies."},
{"Title": "Simple pricing schemes for consumers with evolving values", "URL": "https://dl.acm.org/doi/10.5555/2884435.2884536", "Full Abstract": "We consider a pricing problem where a buyer is interested in purchasing/using a good, such as an app or music or software, repeatedly over time. The consumer discovers his value for the good only as he uses it, and the value evolves with each use. Optimizing for the seller's revenue in such dynamic settings is a complex problem and requires assumptions about how the buyer behaves before learning his future value(s), and in particular, how he reacts to risk. We explore the performance of a class of pricing mechanisms that are extremely simple for both the buyer and the seller to use: the buyer reacts to prices myopically without worrying about how his value evolves in the future; the seller needs to optimize for revenue over a space of only two parameters, and can do so without knowing the buyer's risk profile or fine details of the value evolution process. We present simple-versus-optimal type results, namely that under certain assumptions, simple pricing mechanisms of the above form are approximately optimal"},
{"Title": "The FedEx Problem", "URL": "https://dl.acm.org/doi/10.1145/2940716.2940752", "Full Abstract": "Consider the pricing problem faced by FedEx. Each customer has a package to ship, a deadline $d$ by which he needs his package to arrive, and a value $v$ for a guarantee that the package will arrive by his deadline. FedEx can (and does) offer a number of different shipping options in order to extract more revenue from their customers. In this paper, we solve the optimal (revenue-maximizing) auction problem for the single-agent version of this problem. Our paper adds to the relatively short list of multi-parameter settings for which a closed-form solution is known."},
{"Title": "A Prior-Independent Revenue-Maximizing Auction for Multiple Additive Bidders", "URL": "https://dl.acm.org/doi/10.1007/978-3-662-54110-4_12", "Full Abstract": "Recent work by Babaioff et al.ï ź[1], Yaoï ź[30], and Cai et al.ï ź[7] shows how to construct an approximately optimal auction for additive bidders, given access to the priors from which the bidders' values are drawn. In this paper, building on the single sample approach of Dhangwatnotai et al.ï ź[15], we show how the auctioneer can obtain approximately optimal expected revenue in this setting without knowing the priors, as long as the item distributions are regular."},
{"Title": "Stability of service under time-of-use pricing", "URL": "https://dl.acm.org/doi/10.1145/3055399.3055455", "Full Abstract": "We consider time-of-use pricing as a technique for matching supply and demand of temporal resources with the goal of maximizing social welfare. Relevant examples include energy, computing resources on a cloud computing platform, and charging stations for electric vehicles, among many others. A client/job in this setting has a window of time during which he needs service, and a particular value for obtaining it. We assume a stochastic model for demand, where each job materializes with some probability via an independent Bernoulli trial. Given a per-time-unit pricing of resources, any realized job will first try to get served by the cheapest available resource in its window and, failing that, will try to find service at the next cheapest available resource, and so on. Thus, the natural stochastic fluctuations in demand have the potential to lead to cascading overload events. Our main result shows that setting prices so as to optimally handle the"},
{"Title": "A simply exponential upper bound on the maximum number of stable matchings", "URL": "https://dl.acm.org/doi/10.1145/3188745.3188848", "Full Abstract": "Stable matching is a classical combinatorial problem that has been the subject of intense theoretical and empirical study since its introduction in 1962 in a seminal paper by Gale and Shapley. In this paper, we provide a new upper bound on"},
{"Title": "Energy Equilibria in Proof-of-Work Mining", "URL": "https://dl.acm.org/doi/10.1145/3328526.3329630", "Full Abstract": "The Bitcoin protocol induces miners, through monetary rewards, to expend energy in order to add blocks to the chain. We show that, when energy costs are substantial and taken into account, counterintuitive and unintended strategic behavior results: In a simple bounded-horizon setting with two identical miners there is a unique pure symmetric equilibrium in which both miners first \"slow down\" in order to decrease the crypto complexity and then take advantage of this decrease. If miners have different energy efficiencies and are restricted to choose the same hash rate for many epochs, there is a unique pure equilibrium in which miners either participate at low levels that depend in intricate ways on all the other miners' efficiencies, or choose to abstain from mining if their efficiency is too low. In the general setting in which miners can adapt their hash rates over time, we show that, unless the number of miners is very small, the only possible pure equilibria are rather chaotic, with miners quitting and starting again periodically --- or there is no pure equilibrium at all. We discuss the implications of these results for the stability of proof-of-work protocols."},
{"Title": "Combinatorial Auctions with Interdependent Valuations", "URL": "https://dl.acm.org/doi/10.1145/3328526.3329759", "Full Abstract": "No abstract available."},
{"Title": "An improved approximation algorithm for TSP in the half integral case", "URL": "https://dl.acm.org/doi/10.1145/3357713.3384273", "Full Abstract": "We design a 1.49993-approximation algorithm for the metric traveling salesperson problem (TSP) for instances in which an optimal solution to the subtour linear programming relaxation is half-integral. These instances received significant attention over the last decade due to a conjecture of Schalekamp, Williamson and van Zuylen stating that half-integral LP solutions have the largest integrality gap over all fractional solutions. So, if the conjecture of Schalekamp et al. holds true, our result shows that the integrality gap of the subtour polytope is bounded away from 3/2."},
{"Title": "A (slightly) improved approximation algorithm for metric TSP", "URL": "https://dl.acm.org/doi/10.1145/3406325.3451009", "Full Abstract": "For some > 10"},
{"Title": "An improved approximation algorithm for the minimum ", "URL": "https://dl.acm.org/doi/10.1145/3519935.3520062", "Full Abstract": "We give a randomized 1+5.06/√"},
{"Title": "Combinatorial Auctions with Interdependent Valuations", "URL": "https://dl.acm.org/doi/10.1287/moor.2023.1371", "Full Abstract": "We study combinatorial auctions with interdependent valuations, where each agent"},
{"Title": "A Deterministic Better-than-3/2 Approximation Algorithm for Metric TSP", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-32726-1_19", "Full Abstract": "We show that the max entropy algorithm can be derandomized (with respect to a particular objective function) to give a deterministic"},
{"Title": "Non-Adaptive Matroid Prophet Inequalities", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-71033-9_22", "Full Abstract": "We investigate non-adaptive algorithms for matroid prophet inequalities. Matroid prophet inequalities have been considered resolved since 2012 when [KW12] introduced thresholds that guarantee a tight 2-approximation to the prophet; however, this algorithm is adaptive. Other approaches of [CHMS10] and [FSZ16] have used non-adaptive thresholds with a feasibility restriction on the items that can be taken; however, this translates to adaptively changing an item’s threshold to infinity when it cannot be taken with respect to the additional feasibility constraint, hence the algorithm is not truly non-adaptive. A major application of prophet inequalities is in auction design, where non-adaptive prices possess a significant advantage: they convert to order-oblivious posted pricings, and are essential for translating a prophet inequality into a truthful mechanism for multi-dimensional buyers. The existing matroid prophet inequalities do not suffice for this application. We present the first non-adaptive constant-factor prophet inequality for graphic matroids."},
{"Title": "The STATEMATE semantics of statecharts", "URL": "https://dl.acm.org/doi/10.1145/235321.235322", "Full Abstract": "We describe the semantics of statecharts as implemented in the STATEMATE system. This was the first executable semantics defined for the language and has been in use for almost a decade. In terms of the controversy around whether changes made in a given step should take effect in the current step or in the next one, this semantics adopts the latter approach."},
{"Title": "STATEMATE", "URL": "https://dl.acm.org/doi/10.5555/567003.567014", "Full Abstract": "This paper provides an overview of the STATEMATE system, constructed over the past several years by the authors and their colleagues at Ad Cad Ltd., the R & D subsidiary of i-Logix, Inc. STATEMATE is a set of tools, with a heavy graphical orientation, intended for the specification, analysis, design, and documentation of large and complex reactive systems, such as real-time embedded systems, control and communication systems, and interactive software or hardware. It enables a user to prepare, analyze, and debug diagrammatic, yet precise, descriptions of the system under development from three interrelated points of view, capturing structure, functionality, and behavior. These views are represented by three graphical languages, the most intricate of which is the language of statecharts [4], used to depict reactive behavior over time. In addition to the used of statecharts, the main novelty of STATEMATE is in the fact that it \"understands\" the entire descriptions perfectly, to the point of being able to analyze them for crucial dynamic properties, to carry out rigorous executions and simulations of the described system, and to create running code automatically. These features are invaluable when it comes to the quality and reliability of the final outcome."},
{"Title": "Network flow and generalized path compression", "URL": "https://dl.acm.org/doi/10.1145/800135.804394", "Full Abstract": "An O(EVlog"},
{"Title": "Container problem in bi-rotator graphs", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-30501-9_34", "Full Abstract": "In this paper, we give an algorithm for the container problem in bi-rotator graphs. The solution achieves some fault tolerance such as file distribution based information dispersal technique. The algorithm is of polynomial order of"},
{"Title": "Interactive specification of structured designs", "URL": "https://dl.acm.org/doi/10.5555/800095.803100", "Full Abstract": "In"},
{"Title": "Protein Explorer", "URL": "https://dl.acm.org/doi/10.1145/1048935.1050166", "Full Abstract": "We are developing the 'Protein Explorer' system, a petaflops special-purpose computer system for molecular dynamics simulations. The Protein Explorer is a PC cluster equipped with special-purpose engines that calculate nonbonded interactions between atoms, which is the most time-consuming part of the simulations. A dedicated LSI 'MDGRAPE-3 chip' performs these force calculations at a speed of 165 gigaflops or higher. The system will have 6,144 MDGRAPE-3 chips to achieve a nominal peak performance of one petaflop. The system will be completed in 2006. In this paper, we describe the project plans and the architecture of the Protein Explorer."},
{"Title": "An optimization method based on chaotic immune evolutionary algorithm", "URL": "https://dl.acm.org/doi/10.1007/11539117_125", "Full Abstract": "Immune Evolutionary Algorithm (IEA) is proposed on the shortages of evolution algorithm and biological immune mechanism. According to the characteristics of chaos, a novel Chaotic Immune Evolutionary Algorithm (CIEA) is presented which introduces chaos to IEA. The algorithm has the merits of chaos, immunity and evolutionary algorithm. It can ensure the ability of global search and local search and enhance the performances of the algorithm. At last, we analyze the efficiency of the algorithm with two typical optimization problems. The analysis result shows that CIEA converges quickly and effectively avoids the inherent problem that the evolution algorithm traps in immature convergence, so CIEA is an effective way to solve complex optimization problem."},
{"Title": "Detecting, tracking and interacting with people in a public space", "URL": "https://dl.acm.org/doi/10.1145/1647314.1647330", "Full Abstract": "We have built a system that engages naive users in an audio-visual interaction with a computer in an unconstrained public space. We combine audio source localization techniques with face detection algorithms to detect and track the user throughout a large lobby. The sensors we use are an ad-hoc microphone array and a PTZ camera. To engage the user, the PTZ camera turns and points at sounds made by people passing by. From this simple pointing of a camera, the user is made aware that the system has acknowledged their presence. To further engage the user, we develop a face classification method that identifies and then greets previously seen users. The user can interact with the system through a simple hot-spot based gesture interface. To make the user interactions with the system feel natural, we utilize reconfigurable hardware, achieving a visual response time of less than 100ms. We rely heavily on machine learning methods to make our system self-calibrating and adaptive."},
{"Title": "FPMR", "URL": "https://dl.acm.org/doi/10.1145/1723112.1723129", "Full Abstract": "Machine learning and data mining are gaining increasing attentions of the computing society. FPGA provides a highly parallel, low power, and flexible hardware platform for this domain, while the difficulty of programming FPGA greatly limits its prevalence. MapReduce is a parallel programming framework that could easily utilize inherent parallelism in algorithms. In this paper, we describe FPMR, a MapReduce framework on FPGA, which provides programming abstraction, hardware architecture, and basic building blocks to developers."},
{"Title": "FPGA and GPU implementation of large scale SpMV", "URL": "https://dl.acm.org/doi/10.1109/SASP.2010.5521144", "Full Abstract": "Sparse matrix-vector multiplication (SpMV) is a fundamental operation for many applications. Many studies have been done to implement the SpMV on different platforms, while few work focused on the very large scale datasets with millions of dimensions. This paper addresses the challenges of implementing large scale SpMV with FPGA and GPU in the application of web link graph analysis. In the FPGA implementation, we designed the task partition and memory hierarchy according to the analysis of datasets scale and their access pattern. In the GPU implementation, we designed a fast and scalable SpMV routine with three passes, using a modified Compressed Sparse Row format. Results show that FPGA and GPU implementation achieves about 29x and 30x speedup on a StratixII EP2S180 FPGA and Radeon 5870 Graphic Card respectively compared with a Phenom 9550 CPU."},
{"Title": "Efficient PageRank and SpMV Computation on AMD GPUs", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2010.17", "Full Abstract": "Google's famous PageRank algorithm is widely used to determine the importance of web pages in search engines. Given the large number of web pages on the World Wide Web, efficient computation of PageRank becomes a challenging problem. We accelerated the power method for computing PageRank on AMD GPUs. The core component of the power method is the Sparse Matrix-Vector Multiplication (SpMV). Its performance is largely determined by the characteristics of the sparse matrix, such as sparseness and distribution of non-zero values. Based on careful analysis on the web linkage matrices, we design a fast and scalable SpMV routine with three passes, using a modified Compressed Sparse Row format. Our PageRank computation achieves 15x speedup on a Radeon 5870 Graphic Card compared with a PhenomII 965 CPU at 3.4GHz. Our method can easily adapt to large scale data sets. We also compare the performance of the same method on the OpenCL platform with our low-level implementation."},
{"Title": "Making Human Connectome Faster", "URL": "https://dl.acm.org/doi/10.1109/ICPADS.2010.105", "Full Abstract": "The research on complex Brain Networks plays a vital role in understanding the connectivity patterns of the human brain and disease-related alterations. Recent studies have suggested a noninvasive way to model and analyze human brain networks by using multi-modal imaging and graph theoretical approaches. Both the construction and analysis of the Brain Networks require tremendous computation. As a result, most current studies of the Brain Networks are focused on a coarse scale based on Brain Regions. Networks on this scale usually consist around 100 nodes. The more accurate and meticulous voxel-base Brain Networks, on the other hand, may consist 20K to 100K nodes. In response to the difficulties of analyzing large-scale networks, we propose an acceleration framework for voxel-base Brain Network Analysis based on Graphics Processing Unit (GPU). Our GPU implementations of Brain Network construction and modularity achieve 24x and 80x speedup respectively, compared with single-core CPU. Our work makes the processing time affordable to analyze multiple large-scale Brain Networks."},
{"Title": "FPGA accelerated parallel sparse matrix factorization for circuit simulations", "URL": "https://dl.acm.org/doi/10.5555/1987535.1987577", "Full Abstract": "Sparse matrix factorization is a critical step for the circuit simulation problem, since it is time consuming and computed repeatedly in the flow of circuit simulation. To accelerate the factorization of sparse matrices, a parallel CPU+FPGA based architecture is proposed in this paper. While the preprocessing of the matrix is implemented on CPU, the parallelism of numeric factorization is explored by processing several columns of the sparse matrix simultaneously on a set of processing elements (PE) in FPGA. To cater for the requirements of circuit simulation, we also modified the Gilbert/Peierls (G/P) algorithm and considered the scalability of our architecture. Experimental results on circuit matrices from the University of Florida Sparse Matrix Collection show that our architecture achieves speedup of 0.5x-5.36x compared with the CPU KLU results."},
{"Title": "A lattice boltzmann simulation of hemodynamics in a patient-specific aortic coarctation model", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-36961-2_3", "Full Abstract": "In this paper, we propose a system to determine the pressure gradient at rest in the aorta. We developed a technique to efficiently initialize a regular simulation grid from a patient-specific aortic triangulated model. On this grid we employ the lattice Boltzmann method to resolve the characteristic fluid flow through the vessel. The inflow rates, as measured physiologically, are imposed providing accurate pulsatile flow. The simulation required a resolution of at least 20 microns to ensure a convergence of the pressure calculation. HARVEY, a large-scale parallel code, was run on the IBM Blue Gene/Q supercomputer to model the flow at this high resolution. We analyze and evaluate the strengths and weaknesses of our system."},
{"Title": "Massively Parallel Model of Evolutionary Game Dynamics", "URL": "https://dl.acm.org/doi/10.1109/SC.Companion.2012.307", "Full Abstract": "To study the emergence of cooperative behavior, we have developed a scalable parallel framework. An important aspect is the amount of history that each agent can keep. When six memory steps are taken into account, the strategy space spans 2^4096 potential strategies, requiring large populations of agents. We introduce a multi-level decomposition method that allows us to exploit both multi-node and thread-level parallel scaling while minimizing the communication overhead. We present the following contributions: (1) A production run modeling up to six memory steps for populations consisting of up to 10^18 agents, making this study one of the largest yet undertaken. (2) Results exhibiting near perfect weak scaling and 82% strong scaling efficiency up to 262,144 processors of the IBM Blue Gene/P supercomputer and 16,384 processors of the Blue Gene/Q. Our framework marks an important step in the study of game dynamics with potential applications in fields ranging from biology to economics and sociology."},
{"Title": "The Price of Routing Unsplittable Flow", "URL": "https://dl.acm.org/doi/10.1137/070702370", "Full Abstract": "In this paper we study the “price of anarchy\" for the general class of (weighted and unweighted) atomic “congestion games\" with the sum of players' costs as the objective function. We show that for linear resource cost functions the price of anarchy is exactly $\\frac{3 + \\sqrt{5{2 \\approx 2.618$ for weighted congestion games and exactly $2.5$ for unweighted congestion games. We show that for resource cost functions that are polynomials of degree $d$ the price of anarchy is $d^{\\Theta(d)$. Our results also hold for mixed strategies. In particular, these results apply to atomic routing games where the traffic demand from a source to a destination must be satisfied by choosing a single path between source and destination."},
{"Title": "If multi-agent learning is the answer, what is the question?", "URL": "https://dl.acm.org/doi/10.1016/j.artint.2006.02.006", "Full Abstract": "The area of learning in multi-agent systems is today one of the most fertile grounds for interaction between game theory and artificial intelligence. We focus on the foundational questions in this interdisciplinary area, and identify several distinct agendas that ought to, we argue, be separated. The goal of this article is to start a discussion in the research community that will result in firmer foundations for the area."},
{"Title": "The Alternating Decision Tree Learning Algorithm", "URL": "https://dl.acm.org/doi/10.5555/645528.657623", "Full Abstract": "No abstract available."},
{"Title": "Estimating a mixture of two product distributions", "URL": "https://dl.acm.org/doi/10.1145/307400.307412", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "An adaptive version of the boost by majority algorithm", "URL": "https://dl.acm.org/doi/10.1145/307400.307419", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Large Margin Classification Using the Perceptron Algorithm", "URL": "https://dl.acm.org/doi/10.1023/A%3A1007662407062", "Full Abstract": "We introduce and analyze a new algorithm for linear classification which combines Rosenblatt‘s perceptron algorithm with Helmbold and Warmuth‘s leave-one-out method. Like Vapnik‘s maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik‘s algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort."},
{"Title": "Continuous Drifting Games", "URL": "https://dl.acm.org/doi/10.5555/648299.755171", "Full Abstract": "No abstract available."},
{"Title": "Parameterized verification by probabilistic abstraction", "URL": "https://dl.acm.org/doi/10.5555/1754809.1754817", "Full Abstract": "The paper studies automatic verification of liveness properties with probability 1 over parameterized programs that include probabilistic transitions, and proposes two novel approaches to the problem. The first approach is based on a Planner that occasionally determines the outcome of a finite sequence of \"random\" choices, while the other random choices are performed non-deterministically. Using a Planner, a probabilistic protocol can be treated just like a nonprobabilistic one and verified as such. The second approach is based on γ-fairness, a notion of fairness that is sound and complete for verifying simple temporal properties (whose only temporal operators are ⋄ and □) over finite-state systems. The paper presents a symbolic model checker based on γ-fairness.We then show how the network invariant approach can be adapted to accommodate probabilistic protocols. The utility of the Planner approach is demonstrated on a probabilistic mutual exclusion protocol. The utility of the approach of γ-fairness with network invariants is demonstrated on Lehman and Rabin's Courteous Philosophers algorithm."},
{"Title": "Demo", "URL": "https://dl.acm.org/doi/10.1145/3300061.3343372", "Full Abstract": "Playing Android games with Windows x86 PCs is now popular, and the common solution is to use mobile emulators built with the AOVB (Android-x86 On VirtualBox) architecture. Nevertheless, running heavy 3D Android games on AOVB incurs considerable overhead of full virtualization, thus often leading to unsatisfactory smoothness. To tackle this issue, we present DAOW, a commercial game-oriented Android emulator implementing the idea of direct Android emulation, which eliminates the overhead of full virtualization by providing foreign Android binaries with direct access to the domestic PC hardware through Windows kernel interfaces. In this demo, we will demonstrate that DAOW essentially outperforms traditional AOVB-based emulators in terms of running smoothness, game startup time, and memory usage."},
{"Title": "The Online Set Cover Problem", "URL": "https://dl.acm.org/doi/10.1137/060661946", "Full Abstract": "Let $X=\\{1,2,\\ldots,n\\$ be a ground set of $n$ elements, and let ${\\cal S$ be a family of subsets of $X$, $|{\\cal S|=m$, with a positive cost $c_S$ associated with each $S\\in{\\cal S$. Consider the following online version of the set cover problem, described as a game between an algorithm and an adversary. An adversary gives elements to the algorithm from $X$ one by one. Once a new element is given, the algorithm has to cover it by some set of ${\\cal S$ containing it. We assume that the elements of $X$ and the members of ${\\cal S$ are known in advance to the algorithm; however, the set $X'\\subseteq X$ of elements given by the adversary is not known in advance to the algorithm. (In general, $X'$ may be a strict subset of $X$.) The objective is to minimize the total cost of the sets chosen by the algorithm. Let ${\\cal C$ denote the family of sets in ${\\cal S$ that the algorithm chooses. At the end of the game the adversary also produces (offline) a family of sets ${\\cal C_{OPT$ that covers $X'$. The performance of the algorithm is the ratio between the cost of ${\\cal C$ and the cost of ${\\cal C_{OPT$. The maximum ratio, taken over all input sequences, is the"},
{"Title": "Boosting as a metaphor for algorithm design", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-45193-8_75", "Full Abstract": "Although some algorithms are better than others on average, there is rarely a best algorithm for a given problem. Instead, different algorithms often perform well on different problem instances. Not surprisingly, this phenomenon is most pronounced among algorithms for solving"},
{"Title": "HOW TO USE EXPERT ADVICE", "URL": "https://dl.acm.org/doi/book/10.5555/902707", "Full Abstract": "We analyze algorithms that predict a binary value by combining the predictions of several prediction strategies, called `experts'. Our analysis is for worst-case situations, i.e., we make no assumptions about the way the sequence of bits to be predicted is generated. We measure the performance of the algorithm by the difference between the expected number of mistakes it makes on the bit sequence and the expected number of mistakes made by the best expert on this sequence, where the expectation is taken with respect to the randomization in the predictions. We show that the minimum achievable difference is on the order of the square root of the number of mistakes of the best expert, and we give efficient algorithms that achieve this. Our upper and lower bounds have matching leading constants in most cases. We then show how this leads to certain kinds of pattern recognition/learning algorithms with performance bounds that improve on the best results currently known in this context. We also extend our analysis to the case in which log loss is used instead of the expected number of mistakes."},
{"Title": "A decision-theoretic generalization of on-line learning and an application to boosting", "URL": "https://dl.acm.org/doi/10.5555/646943.712093", "Full Abstract": "No abstract available."},
{"Title": "Learning to model sequences generated by switching distributions", "URL": "https://dl.acm.org/doi/10.1145/225298.225303", "Full Abstract": "Copyright © 1995 ACM."},
{"Title": "Boosting a weak learning algorithm by majority", "URL": "https://dl.acm.org/doi/10.1006/inco.1995.1136", "Full Abstract": "No abstract available."},
{"Title": "Gambling in a rigged casino", "URL": "https://dl.acm.org/doi/10.5555/795662.796294", "Full Abstract": "In the multi-armed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the expected per-round payoff of our algorithm approaches that of the best arm at the rate O(T/sup -1/3/), and we give an improved rate of convergence when the best arm has fairly low payoff. We also consider a setting in which the player has a team of \"experts\" advising him on which arm to play; here, we give a strategy that will guarantee expected payoff close to that of the best expert. Finally, we apply our result to the problem of learning to play an unknown repeated matrix game against an all-powerful adversary."},
{"Title": "Efficient algorithms for learning to play repeated games against computationally bounded adversaries", "URL": "https://dl.acm.org/doi/10.5555/795662.796296", "Full Abstract": "We examine the problem of learning to play various games optimally against resource-bounded adversaries, with an explicit emphasis on the computational efficiency of the learning algorithm. We are especially interested in providing efficient algorithms for games other than penny-matching (in which payoff is received for matching the adversary's action in the current round), and for adversaries other than the classically studied finite automata. In particular, we examine games and adversaries for which the learning algorithm's past actions may strongly affect the adversary's future willingness to \"cooperate\" (that is, permit high payoff), and therefore require carefully planned actions on the part of the learning algorithm. For example, in the game we call contract, both sides play O or 1 on each round, but our side receives payoff only if we play 1 in synchrony with the adversary; unlike penny-matching, playing O in synchrony with the adversary pays nothing. The name of the game is derived from the example of signing a contract, which becomes valid only if both parties sign (play 1)."},
{"Title": "Sets, algorithms, and independence", "URL": "https://dl.acm.org/doi/book/10.5555/905799", "Full Abstract": "No abstract available."},
{"Title": "Parameterized Verification with Automatically Computed Inductive Assertions", "URL": "https://dl.acm.org/doi/10.5555/647770.734120", "Full Abstract": "The paper presents a method, called the method of verification by invisible invariants, for the automatic verification of a large class of parameterized systems. The method is based on the automatic calculation of candidate inductive assertions and checking for their inductiveness, using symbolic model-checking techniques for both tasks. First, we show how to use model-checking techniques over finite (and small) instances of the parameterized system in order to derive candidates for invariant assertions. Next, we show that the premises of the standard deductive INV rule for proving invariance properties can be automatically resolved by finite-state (BDD-based) methods with no need for interactive theorem proving. Combining the automatic computation of invariants with the automatic resolution of the VCs (verification conditions) yields a (necessarily) incomplete but fully automatic sound method for verifying large classes of parameterized systems. The generated invariants can be transferred to the VC-validation phase without ever been examined by the user, which explains why we refer to them as \"invisible\". The efficacy of the method is demonstrated by automatic verification of diverse parameterized systems in a fully automatic and efficient manner."},
{"Title": "Vernier: Accurate and Fast Acoustic Motion Tracking Using Mobile Devices", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2018.8486365", "Full Abstract": "Acoustic motion tracking has been viewed as a promising user interaction technique in many scenarios such as Virtual Reality (VR), Smart Appliance, video gaming, etc. Existing acoustic motion tracking approaches, however, suffer from long window of accumulated signal and time-consuming signal processing. Consequently, they are inherently difficult to achieve both high accuracy and low delay. We propose Vernier, an efficient and accurate acoustic tracking method on commodity mobile devices. In the heart of Vernier lies a novel method to efficiently and accurately derive phase change and thus moving distance. Vernier significantly reduces the tracking delay/overhead by removing the complicated frequency analysis and long window of signal accumulation, while keeping a high tracking accuracy. We implement Vernier on Android, and evaluate its performance on COTS mobile devices including Samsung Galaxy S7 and Sony L50t. Evaluation results show that Vernier outperforms previous approaches with a tracking error less than 4 mm. The tracking speed achieves 3&#x00D7;improvement to existing phase based approaches and 10&#x00D7;to Doppler Effect based approaches. Vernier is also validated in applications like controlling and drawing, and we believe it is generally applicable in many real applications."},
{"Title": "Accounting for memory bank contention and delay in high-bandwidth multiprocessors", "URL": "https://dl.acm.org/doi/10.1145/215399.215425", "Full Abstract": "Copyright © 1995 ACM."},
{"Title": "Asynchrony versus bulk-synchrony in QRQW PRAM models", "URL": "https://dl.acm.org/doi/10.1145/248052.248084", "Full Abstract": "No abstract available."},
{"Title": "Boosting a weak learning algorithm by majority", "URL": "https://dl.acm.org/doi/10.5555/92571.92640", "Full Abstract": "No abstract available."},
{"Title": "Unsupervised learning of distributions on binary vectors using two layer networks", "URL": "https://dl.acm.org/doi/10.5555/2986916.2987028", "Full Abstract": "We study a particular type of Boltzmann machine with a bipartite graph structure called a harmonium. Our interest is in using such a machine to model a probability distribution on binary input vectors. We analyze the class of probability distributions that can be modeled by such machines, showing that for each"},
{"Title": "An improved boosting algorithm and its implications on learning complexity", "URL": "https://dl.acm.org/doi/10.1145/130385.130429", "Full Abstract": "In this work we present some improvements and extensions to previous work on boosting weak learners [Sch90, Fre90]. Our main result is an improvement of the boosting-by-majority algorithm. One implication of the performance of this algorithm is that if a concept class can be learned in the PAC model to within some fixed error smaller than 1/2, then it can be learned to within an arbitrarily small error ε > 0 with time complexity 0((1/ε)(log 1/ε)"},
{"Title": "Information, prediction, and query by committee", "URL": "https://dl.acm.org/doi/10.5555/2987061.2987121", "Full Abstract": "We analyze the \"query by committee\" algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of thresholded smooth functions."},
{"Title": "Information, Prediction, and Query by Committee", "URL": "https://dl.acm.org/doi/10.5555/645753.668253", "Full Abstract": "No abstract available."},
{"Title": "Data filtering and distribution modeling algorithms for machine learning", "URL": "https://dl.acm.org/doi/book/10.5555/193105", "Full Abstract": "This thesis is concerned with the analysis of algorithms for machine learning. The main focus is on the role of the distribution of the examples used for learning. Chapters 2 and 3 are concerned with algorithms for learning concepts from random examples. Briefly, the goal of the learner is to observe a set of labeled instances and generate a hypothesis that approximates the rule that maps the instances to their labels."},
{"Title": "Efficient learning of typical finite automata from random walks", "URL": "https://dl.acm.org/doi/10.1145/167088.167191", "Full Abstract": "Copyright © 1993 ACM."},
{"Title": "How to use expert advice", "URL": "https://dl.acm.org/doi/10.1145/167088.167198", "Full Abstract": "Copyright © 1993 ACM."},
{"Title": "Translation Validation", "URL": "https://dl.acm.org/doi/10.5555/646005.673739", "Full Abstract": "Translation validation is an alternative to the verification of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verification), each individual translation (i.e. a run of the compiler) is followed by a validation phase which verifies that the target code produced on this run correctly implements the submitted source program. In order to be a practical alternative to compiler verification, a key feature of this validation is its full automation."},
{"Title": "Scalable Industry Data Access Control in RFID-Enabled Supply Chain", "URL": "https://dl.acm.org/doi/10.1109/TNET.2016.2536626", "Full Abstract": "By attaching RFID tags to products, supply chain participants can identify products and create product data to record the product particulars in transit. Participants along the supply chain share their product data to enable information exchange and support critical decisions in production operations. Such an information sharing essentially requires a data access control mechanism when the product data relate to sensitive business issues. However, existing access control solutions are ill-suited to the RFID-enabled supply chain, as they are not scalable in handling a huge number of tags, introduce vulnerability to the product data, and perform poorly to support privilege revocation of product data. We present a new scalable industry data access control system that addresses these limitations. Our system provides an item-level data access control mechanism that defines and enforces access policies based on both the participants’ role attributes and the products’ RFID tag attributes. Our system further provides an item-level privilege revocation mechanism by allowing the participants to delegate encryption updates in revocation operation without disclosing the underlying data contents. We design a new updatable encryption scheme and integrate it with ciphertext policy-attribute-based encryption to implement the key components of our system."},
{"Title": "STPP", "URL": "https://dl.acm.org/doi/10.1109/TNET.2016.2590996", "Full Abstract": "Many object localization applications need the relative locations of a set of objects as oppose to their absolute locations. Although many schemes for object localization using radio frequency identification RFID tags have been proposed, they mostly focus on absolute object localization and are not suitable for relative object localization because of large error margins and the special hardware that they require. In this paper, we propose an approach called spatial-temporal phase profiling STPP to RFID-based relative object localization. The basic idea of STPP is that by moving a reader over a set of tags during which the reader continuously interrogating the tags, for each tag, the reader obtains a sequence of RF phase values, which we call a phase profile, from the tag’s responses over time. By analyzing the spatial-temporal dynamics in the phase profiles, STPP can calculate the spatial ordering among the tags. In comparison with prior absolute object localization schemes, STPP requires neither dedicated infrastructure nor special hardware. We implemented STPP and evaluated its performance in two real-world applications: locating misplaced books in a library and determining the baggage order in an airport. The experimental results show that STPP achieves about 84% ordering accuracy for misplaced books and 95% ordering accuracy for baggage handling. We further leverage the controllable reader antenna and upgrade STPP to infer the spacing between each pair of tags. The result shows that STPP could achieve promising performance on distance ranging."},
{"Title": "Leaders Election Without a Conflict Resolution Rule - Fast and Efficient Randomized Simulations among CRCW PRAMs", "URL": "https://dl.acm.org/doi/book/10.5555/902284", "Full Abstract": "We study the question of fast leaders election on TOLERANT, a CRCW PRAM model which tolerates concurrent write but does not support symmetry breaking. We give a randomized simulation of MAXIMUM (a very strong CRCW PRAM) on TOLERANT. The simulation is optimal, reliable, and runs in nearly doubly logarithmic time and linear space. This is the first simulation which is fast, optimal {\\it and space-efficient, and therefore grants true comparison of algorithms running on different CRCW PRAMs. Moreover, it implies that the memory to which concurrent read or concurrent write are assumed should {\\it never be more than linear-the rest of the memory can always be addressed under the EREW convention. The techniques presented in this paper tackle fundamental difficulties in the design of fast parallel algorithms."},
{"Title": "Artificial intelligence techniques in Prolog", "URL": "https://dl.acm.org/doi/book/10.5555/181710", "Full Abstract": "No abstract available."},
{"Title": "Towards more hardware-friendly deep learning", "URL": "https://dl.acm.org/doi/10.1145/3149166.3149171", "Full Abstract": "No abstract available."},
{"Title": "A closer look at memorization in deep networks", "URL": "https://dl.acm.org/doi/10.5555/3305381.3305406", "Full Abstract": "We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization."},
{"Title": "Sharp minima can generalize for deep nets", "URL": "https://dl.acm.org/doi/10.5555/3305381.3305487", "Full Abstract": "Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties."},
{"Title": "Multi-way, multilingual neural machine translation", "URL": "https://dl.acm.org/doi/10.1016/j.csl.2016.10.006", "Full Abstract": "The first attention-based neural-MT for multi-way, multilingual translation is proposed.Multi-way multilingual model is tested on more than 8 languages (En, Fr, Cz, De, Ru, Fi, Tr and Uz).It achieves the translation quality comparable to single-pair NMTs with less parameters.Single attention mechanism supports to align between multiple pairs and directions.Outperforms conventional SMT system on low-resource translation tasks. We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multi-way, multilingual model on ten language pairs from WMT15 simultaneously and observe clear performance improvements over models trained on only one language pair. We empirically evaluate the proposed model on low-resource language translation tasks. In particular, we observe that the proposed multilingual model outperforms strong conventional statistical machine translation systems on Turkish-English and Uzbek-English by incorporating the resources of other language pairs."},
{"Title": "On integrating a language model into neural machine translation", "URL": "https://dl.acm.org/doi/10.1016/j.csl.2017.01.014", "Full Abstract": "Recent advances in end-to-end neural machine translation models have achieved promising results on high-resource language pairs such as En Fr and En De. One of the major factor behind these successes is the availability of high quality parallel corpora. We explore two strategies on leveraging abundant amount of monolingual data for neural machine translation. We observe improvements by both combining scores from neural language model trained only on target monolingual data with neural machine translation model and fusing hidden-states of these two models. We obtain up to 2 BLEU improvement over hierarchical and phrase-based baseline on low-resource language pair, Turkish English. Our method was initially motivated towards tasks with less parallel data, but we also show that it extends to high resource languages such as Cs En and De En translation tasks, where we obtain 0.39 and 0.47 BLEU improvements over the neural machine translation baselines, respectively."},
{"Title": "Variational walkback", "URL": "https://dl.acm.org/doi/10.5555/3294996.3295193", "Full Abstract": "We propose a novel method to"},
{"Title": "GibbsNet", "URL": "https://dl.acm.org/doi/10.5555/3295222.3295262", "Full Abstract": "Directed latent variable models that formulate the joint distribution as"},
{"Title": "Verifying Liveness Properties of Reactive Systems (Tutorial Abstract)", "URL": "https://dl.acm.org/doi/10.5555/646883.710937", "Full Abstract": "No abstract available."},
{"Title": "Learning Resource Management Specifications in Smartphones", "URL": "https://dl.acm.org/doi/10.1109/ICPADS.2015.21", "Full Abstract": "Over the past few years we have observed a phenomenal growth of smartphones. Smartphones are equipped with various hardware and software resources such as Bluetooth, camera and gravity sensors. If these resources are not managed appropriately, it may cause severe problems such as battery drains and system crashes. However, the specifications of resource management are usually implicit. In this paper, we investigate the problem of mining resource management specifications from off-the-shelf apps. Our key insight is that if a set of operations to a resource are frequently performed in a specific order, it must contain the specifications of how to manage the resource. We design a tool named Automatic Resource Specification Miner (ARSM), to automatically extract resource management specifications in smartphones. In our experiments, ARSM can mine tens of rules from 100 top rated Android apps within six hours. Our work is orthogonal to existing studies on diagnosing smartphone apps. With the resource management specifications discovered, ARSM can help them pinpoint more bugs in apps."},
{"Title": "Incentives for Mobile Crowd Sensing: A Survey", "URL": "https://dl.acm.org/doi/10.1109/COMST.2015.2415528", "Full Abstract": "Recent years have witnessed the fast proliferation of mobile devices (e.g., smartphones and wearable devices) in people's lives. In addition, these devices possess powerful computation and communication capabilities and are equipped with various built-in functional sensors. The large quantity and advanced functionalities of mobile devices have created a new interface between human beings and environments. Many mobile crowd sensing applications have thus been designed which recruit normal users to contribute their resources for sensing tasks. To guarantee good performance of such applications, it's essential to recruit sufficient participants. Thus, how to effectively and efficiently motivate normal users draws growing attention in the research community. This paper surveys diverse strategies that are proposed in the literature to provide incentives for stimulating users to participate in mobile crowd sensing applications. The incentives are divided into three categories: entertainment, service, and money. Entertainment means that sensing tasks are turned into playable games to attract participants. Incentives of service exchanging are inspired by the principle of mutual benefits. Monetary incentives give participants payments for their contributions. We describe literature works of each type comprehensively and summarize them in a compact form. Further challenges and promising future directions concerning incentive mechanism design are also discussed."},
{"Title": "A Video Scrambling Technique Based On Space Filling Curves", "URL": "https://dl.acm.org/doi/10.5555/646752.704721", "Full Abstract": "In this paper we propose a video scrambling technique which scans a picture stored in a frame buffer along a pseudo-random space filling curve. We describe several efficient methods for generating cryptographically strong curves, and show that they actually decrease the bandwidth required to transmit the picture."},
{"Title": "Simple and Efficient Election Algorithms for Anonymous Networks", "URL": "https://dl.acm.org/doi/10.5555/645946.675002", "Full Abstract": "No abstract available."},
{"Title": "On parallel hashing and integer sorting", "URL": "https://dl.acm.org/doi/book/10.5555/99306", "Full Abstract": "No abstract available."},
{"Title": "Batch normalized recurrent neural networks", "URL": "https://dl.acm.org/doi/10.1109/ICASSP.2016.7472159", "Full Abstract": "Recurrent Neural Networks (RNNs) are powerful models for sequential data that have the potential to learn long-term dependencies. However, they are computationally expensive to train and difficult to parallelize. Recent work has shown that normalizing intermediate representations of neural networks can significantly improve convergence rates in feed-forward neural networks [1]. In particular, batch normalization, which uses mini-batch statistics to standardize features, was shown to significantly reduce training time. In this paper, we investigate how batch normalization can be applied to RNNs. We show for both a speech recognition task and language modeling that the way we apply batch normalization leads to a faster convergence of the training criterion but doesn't seem to improve the generalization performance."},
{"Title": "End-to-end attention-based large vocabulary speech recognition", "URL": "https://dl.acm.org/doi/10.1109/ICASSP.2016.7472618", "Full Abstract": "Many state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are hybrids of neural networks and Hidden Markov Models (HMMs). Recently, more direct end-to-end methods have been investigated, in which neural architectures were trained to model sequences of characters [1,2]. To our knowledge, all these approaches relied on Connectionist Temporal Classification [3] modules. We investigate an alternative method for sequence modelling based on an attention mechanism that allows a Recurrent Neural Network (RNN) to learn alignments between sequences of input frames and output labels. We show how this setup can be applied to LVCSR by integrating the decoding RNN with an n-gram language model and by speeding up its operation by constraining selections made by the attention mechanism and by reducing the source sequence lengths by pooling information over time. Recognition accuracies similar to other HMM-free RNN-based approaches are reported for the Wall Street Journal corpus."},
{"Title": "Unitary evolution recurrent neural networks", "URL": "https://dl.acm.org/doi/10.5555/3045390.3045509", "Full Abstract": "Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of"},
{"Title": "Deconstructing the ladder network architecture", "URL": "https://dl.acm.org/doi/10.5555/3045390.3045640", "Full Abstract": "The Ladder Network is a recent new approach to semi-supervised learning that turned out to be very successful. While showing impressive performance, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. This paper presents an extensive experimental investigation of variants of the Ladder Network in which we replaced or removed individual components to learn about their relative importance. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connections, followed by the application of noise, and the choice of what we refer to as the 'combinator function'. As the number of labeled training examples increases, the lateral connections and the reconstruction criterion become less important, with most of the generalization improvement coming from the injection of noise in each layer. Finally, we introduce a combinator function that reduces test error rates on Permutation-Invariant MNIST to 0.57% for the supervised setting, and to 0.97% and 1.0% for semi-supervised settings with 1000 and 100 labeled examples, respectively."},
{"Title": "Bidirectional Helmholtz machines", "URL": "https://dl.acm.org/doi/10.5555/3045390.3045655", "Full Abstract": "Efficient unsupervised training and inference in deep generative models remains a challenging problem. One basic approach, called Helmholtz machine or Variational Autoencoder, involves training a top-down directed generative model together with a bottom-up auxiliary model used for approximate inference. Recent results indicate that better generative models can be obtained with better approximate inference procedures. Instead of improving the inference procedure, we here propose a new model, the bidirectional Helmholtz machine, which guarantees that the top-down and bottom-up distributions can efficiently invert each other. We achieve this by interpreting both the top-down and the bottom-up directed models as approximate inference distributions and by defining the model distribution to be the geometric mean of these two. We present a lower-bound for the likelihood of this model and we show that optimizing this bound regularizes the model so that the Bhattacharyya distance between the bottom-up and top-down approximate distributions is minimized. This approach results in state of the art generative models which prefer significantly deeper architectures while it allows for orders of magnitude more efficient likelihood estimation."},
{"Title": "Noisy activation functions", "URL": "https://dl.acm.org/doi/10.5555/3045390.3045712", "Full Abstract": "Common nonlinear activation functions used in neural networks can cause training difficulties due to the saturation behavior of the activation function, which may hide dependencies that are not visible to vanilla-SGD (using first order gradients only). Gating mechanisms that use softly saturating activation functions to emulate the discrete switching of digital logic circuits are good examples of this. We propose to exploit the injection of appropriate noise so that the gradients may flow easily, even if the noiseless application of the activation function would yield zero gradients. Large noise will dominate the noise-free gradient and allow stochastic gradient descent to explore more. By adding noise only to the problematic parts of the activation function, we allow the optimization procedure to explore the boundary between the degenerate (saturating) and the well-behaved parts of the activation function. We also establish connections to simulated annealing, when the amount of noise is annealed down, making it easier to optimize hard objective functions. We find experimentally that replacing such saturating activation functions by noisy variants helps optimization in many contexts, yielding state-of-the-art or competitive results on different datasets and task, especially when training seems to be the most difficult, e.g., when curriculum learning is necessary to obtain good results."},
{"Title": "HeMIS: Hetero-Modal Image Segmentation", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-46723-8_54", "Full Abstract": "We introduce a deep learning image segmentation framework that is extremely robust to missing imaging modalities. Instead of attempting to impute or synthesize missing data, the proposed approach learns, for each modality, an embedding of the input image into a single latent vector space for which arithmetic operations (such as taking the mean) are well defined. Points in that space, which are averaged over modalities available at inference time, can then be further processed to yield the desired segmentation. As such, any combinatorial subset of available modalities can be provided as input, without having to learn a combinatorial number of imputation models. Evaluated on two neurological MRI datasets (brain tumors and MS lesions), the approach yields state-of-the-art segmentation results when provided with all modalities; moreover, its performance degrades remarkably gracefully when modalities are removed, significantly more so than alternative mean-filling or other synthesis approaches."},
{"Title": "Deep Learning", "URL": "https://dl.acm.org/doi/book/10.5555/3086952", "Full Abstract": "\"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.\" -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors."},
{"Title": "Towards Network-level Efficiency for Cloud Storage Services", "URL": "https://dl.acm.org/doi/10.1145/2663716.2663747", "Full Abstract": "Cloud storage services such as Dropbox, Google Drive, and Microsoft OneDrive provide users with a convenient and reliable way to store and share data from anywhere, on any device, and at any time. The cornerstone of these services is the"},
{"Title": "Location, Localization, and Localizability", "URL": "https://dl.acm.org/doi/book/10.5555/2765658", "Full Abstract": "This book reflects up-to-date research, fundamental theories, and key techniques of of wireless localization technology and error-controlling techniques. It also presents and discusses the issue of localizability, as well as privacy issues associated with LBS. This book encompasses the significant and quickly growing area of wireless localization technology. It presents comprehensive and up-to-date research in both fundamental theories and key techniques of network localization. In addition to localization approaches, it also is the first book to address the issue of localizability. The privacy issue of LBS technology is also discussed."},
{"Title": "Measurement and analysis on the packet delivery performance in a large-scale sensor network", "URL": "https://dl.acm.org/doi/10.1109/TNET.2013.2288646", "Full Abstract": "Understanding the packet delivery performance of a wireless sensor network (WSN) is critical for improving system performance and exploring future developments and applications of WSN techniques. In spite of many empirical measurements in the literature, we still lack in-depth understanding on how and to what extent different factors contribute to the overall packet losses for a complete stack of protocols at large scale. Specifically, very little is known about: 1) when, where, and under what kind of circumstances packet losses occur; 2) why packets are lost. As a step toward addressing those issues, we deploy a large-scale WSN and design a measurement system for retrieving important system metrics. We propose MAP, a step-by-step methodology to identify the losses, extract system events, and perform spatial-temporal correlation analysis by employing a carefully examined causal graph. MAP enables us to get a closer look at the root causes of packet losses in a low-power ad hoc network. This study validates some earlier conjectures on WSNs and reveals some new findings. The quantitative results also shed lights for future large-scale WSN deployments."},
{"Title": "On the delay performance in a large-scale wireless sensor network", "URL": "https://dl.acm.org/doi/10.1109/TNET.2013.2296331", "Full Abstract": "We present a comprehensive delay performance measurement and analysis in a large-scale wireless sensor network. We build a lightweight delay measurement system and present a robust method to calculate the per-packet delay. We show that the method can identify incorrect delays and recover them with a bounded error. Through analysis of delay and other system metrics, we seek to answer the following fundamental questions: What are the spatial and temporal characteristics of delay performance in a real network? What are the most important impacting factors, and is there any practical model to capture those factors? What are the implications to protocol designs? In this paper, we identify important factors from the data trace and show that the important factors are not necessarily the same with those in the Internet. Furthermore, we propose a delay model to capture those factors. We revisit several prevalent protocol designs such as Collection Tree Protocol, opportunistic routing, and Dynamic Switching-based Forwarding and show that our model and analysis are useful to practical protocol designs."},
{"Title": "Mobility Increases Localizability", "URL": "https://dl.acm.org/doi/10.1145/2676430", "Full Abstract": "Wireless indoor positioning has been extensively studied for the past 2 decades and continuously attracted growing research efforts in mobile computing context. As the integration of multiple inertial sensors (e.g., accelerometer, gyroscope, and magnetometer) to nowadays smartphones in recent years, human-centric mobility sensing is emerging and coming into vogue. Mobility information, as a new dimension in addition to wireless signals, can benefit localization in a number of ways, since location and mobility are by nature related in the physical world. In this article, we survey this new trend of mobility enhancing smartphone-based indoor localization. Specifically, we first study how to measure human mobility: what types of sensors we can use and what types of mobility information we can acquire. Next, we discuss how mobility assists localization with respect to enhancing location accuracy, decreasing deployment cost, and enriching location context. Moreover, considering the quality and cost of smartphone built-in sensors, handling measurement errors is essential and accordingly investigated. Combining existing work and our own working experiences, we emphasize the principles and conduct comparative study of the mainstream technologies. Finally, we conclude this survey by addressing future research directions and opportunities in this new and largely open area."},
{"Title": "New Results on Semantical Non-Monotonic Reasoning", "URL": "https://dl.acm.org/doi/10.5555/648097.748257", "Full Abstract": "No abstract available."},
{"Title": "Generative adversarial nets", "URL": "https://dl.acm.org/doi/10.5555/2969033.2969125", "Full Abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model"},
{"Title": "On the number of linear regions of deep neural networks", "URL": "https://dl.acm.org/doi/10.5555/2969033.2969153", "Full Abstract": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers."},
{"Title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "URL": "https://dl.acm.org/doi/10.5555/2969033.2969154", "Full Abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance."},
{"Title": "How transferable are features in deep neural networks?", "URL": "https://dl.acm.org/doi/10.5555/2969033.2969197", "Full Abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be"},
{"Title": "Using recurrent neural networks for slot filling in spoken language understanding", "URL": "https://dl.acm.org/doi/10.5555/2817174.2817185", "Full Abstract": "Semantic slot filling is one of the most challenging problems in spoken language understanding (SLU). In this paper, we propose to use recurrent neural networks (RNNs) for this task, and present several novel architectures designed to efficiently model past and future temporal dependencies. Specifically, we implemented and compared several important RNN architectures, including Elman, Jordan, and hybrid variants. To facilitate reproducibility, we implemented these networks with the publicly available Theano neural network toolkit and completed experiments on the well-known airline travel information system (ATIS) benchmark. In addition, we compared the approaches on two custom SLU data sets from the entertainment and movies domains. Our results show that the RNN-based models outperform the conditional random field (CRF) baseline by 2% in absolute error reduction on the ATIS benchmark. We improve the state-of-the-art by 0.5% in the Entertainment domain, and 6.7% for the movies domain."},
{"Title": "Challenges in representation learning", "URL": "https://dl.acm.org/doi/10.1016/j.neunet.2014.09.005", "Full Abstract": "The ICML 2013 Workshop on Challenges in Representation Learning11http://deeplearning.net/icml2013-workshop-competition. focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions."},
{"Title": "Editorial introduction to the Neural Networks special issue on Deep Learning of Representations", "URL": "https://dl.acm.org/doi/10.1016/j.neunet.2014.12.006", "Full Abstract": "No abstract available."},
{"Title": "Wonder", "URL": "https://dl.acm.org/doi/10.1109/DCOSS.2014.28", "Full Abstract": "Efficient tag identification is fundamentally required in large-scale RFID systems. Tag signal collision degrades identification efficiency as tag IDs involved in collision cannot be decoded. The situation becomes even worse in large-scale RFID systems when tag cardinality booms. Existing anti-collision protocols focus on either reducing collision probability or adopting spread spectrum techniques. Unfortunately, the former approach cannot resolve collision radically and the latter one occupies extra bandwidth resources. To address these issues, we propose to resolve tag collision using orthogonal Walsh code, in which tags map their IDs to a group of Walsh codes and transmit them sequentially. The reader can retrieve tag IDs by inverse mapping even under collision circumstances. We further design a new efficient tag identification protocol, Wonder, which reduces identification time without spreading the bandwidth. We conduct extensive simulations to examine its effectiveness and the results show that our protocol significantly improves identification efficiency over previous anti-collision protocols."},
{"Title": "BOND", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2014.48", "Full Abstract": "In a large-scale wireless sensor network, thousands of sensor nodes periodically generate and forward data back to the sink. In our recent outdoor deployment, we observe that some bottleneck nodes can greatly determine other nodes' data collection ratio, and thus affect the whole network performance. To figure out the importance of a node in data collection, the manager needs to understand the interactive behaviors among the parent and child nodes. To address this issue, we present a management tool BOND (Bottleneck Node Detector). We introduce the concept of Node Dependence to characterize how much a node relies on each of its parent nodes. BOND models the routing process as a Hidden Markov Model, and uses a machine learning approach to learn the state transition probabilities in this model based on the observed traces. BOND utilizes Node Dependence to explore the hidden bottleneck nodes in the network. Moreover, we can predict how adding or removing the sensor nodes would impact the data flow, thus avoid data loss and flow congestion in redeployment. We implement our tool on real hardware and deploy it in an outdoor system. Our extensive experiments show that BOND infers the Node Dependence with an average accuracy of more than 85%."},
{"Title": "Aggregation Capacity of Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TC.2012.230", "Full Abstract": "A critical function of wireless sensor networks (WSNs) is data gathering. One is often only interested in collecting a specific function of the sensor measurements at a sink node, rather than downloading all the raw data from all the sensors. In this paper, we study the capacity of computing and transporting the specific functions of sensor measurements to the sink node, called aggregation capacity, for WSNs. We focus on random WSNs that can be classified into two types: random extended WSN and random dense WSN. All existing results about aggregation capacity are studied for dense WSNs, including random cases and arbitrary cases, under the protocol model (ProM) or physical model (PhyM). In this paper, we propose the first aggregation capacity scaling laws for random extended WSNs. We point out that unlike random dense WSNs, for random extended WSNs, the assumption made in ProM and PhyM that each successful transmission can sustain a constant rate is over-optimistic and unpractical due to transmit power limitation. We derive the first result on aggregation capacity for random extended WSNs under the generalized physical model. Particularly, we prove that, for the type-sensitive divisible perfectly compressible functions and type-threshold divisible perfectly compressible functions, the aggregation capacities for random extended WSNs with ${\\mbi {n$ nodes are of order $\\Thetab ({{{({\\bf log {\\mbi {n)^{ - {\\alphab \\over {\\bf 2 - {\\bf 1)$ and $\\Theta ({{{{{({\\bf log {\\mbi {n)^{ - \\alphab /{\\bf 2 \\over {{\\bf log{\\bf log {\\mbi {n)$, respectively, where $\\alphab \\gt {\\bf 2$ denotes the power attenuation exponent in the generalized physical model. Furthermore, we improve the aggregation throughput for general divisible perfectly compressible functions to $\\Omegab ({{{({\\bf log {\\mbi {n)^{ - {\\alphab \\over {\\bf 2)$ by choosing $\\Thetab ({\\bf log\\; {\\mbi {n)$ sensors from a small region (relative to the whole region) as sink nodes."},
{"Title": "Enhancing Visibility of Network Performance in Large-Scale Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2014.49", "Full Abstract": "Being embedded in the physical world, wireless sensor networks (WSNs) present a wide range of failures, due to environment conditions, hardware limitations and software uncertainties, and so on. Once deployed, the interactivity of a WSN greatly decreases, which leads to limited visibility of network performance for managers to investigate sensor behaviors. Existing evidence-based approaches aim to explain particular network symptoms based on expert knowledge and heuristic experiences, which degrade diagnosis accuracy and perform unreliably. These diagnosis models define a limited group of network failures, emphasizing on expert knowledge too much, and thus fail to be adopted to different applications. In this work, we propose VN2, a novel tool to enhance the visibility of network performance. VN2 quantifies a node's state in terms of variation of 43 metrics, and trains a representative matrix of network exceptions with Non-negative Matrix Factorization (NMF) model. With this matrix, when a new network state coming up, VN2 automatically attributes abnormal symptoms to one or more root causes. We implement VN2 on test bed and real system traces. Experimental results show that VN2 models network exceptions involving small subsets of root causes, and the interpretation of root causes help us understand network behaviors in details."},
{"Title": "Generic Composite Counting in RFID Systems", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2014.67", "Full Abstract": "Counting the number of RFID tags is a fundamental issue and has a wide range of applications in RFID systems. Most existing protocols, however, only apply to the scenario where a single reader counts the number of tags covered by its radio, or at most the union of tags covered by multiple readers. They are unable to achieve more complex counting objectives, i.e., counting the number of tags in a composite set expression such as (S_1 big cup S_2) - (S_3 big cap S_4). This type of counting has realistic significance since it provides more diversity than existing counting scenario, and can be applied in various applications. In this paper, we formally introduce the RFID composite counting problem, which aims at counting the tags in arbitrary set expression. We obtain strong lower bounds on the communication cost of composite counting. We then propose a generic Composite Counting Framework (CCF) that provides estimates for any set expression with desired accuracy. The communication cost of CCF is proved to be within a small factor from the optimal. We build a prototype system for CCF using USRP software defined radio and Intel WISP computational tags. Also, extensive simulations are conducted to evaluate the performance of CCF. The experimental results show that CCF is generic, accurate and time-efficient."},
{"Title": "Naive kinematics", "URL": "https://dl.acm.org/doi/10.5555/1625135.1625220", "Full Abstract": "Ways in which physical objects interact are explored, and in particular the concept of freedom is analysed. Intuitively, the fit between two shapes in a given spatial configuration is a statement about how much one shape needs to mutilated in order to be made identical to the other. The freedom of one object with respect to another specifies what motions the First object can go through without the second one moving. The formulations, termed naive kinematics, are compared to work that was done in the kinematics of machinery in the 10th century and that has since been somewhat neglected."},
{"Title": "Ten requirements for a theory of change", "URL": "https://dl.acm.org/doi/10.1007/BF03037081", "Full Abstract": "In order to focus the discussion on temporal reasoning in Artificial Intelligence, we propose a list of requirements that a rigorous theory of time and change must meet. These requirements refer to the desired expressiveness of the formalism (such as representing continuous change) and to potential pitfalls (such as the"},
{"Title": "Deep learning of representations", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-39593-2_1", "Full Abstract": "Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges."},
{"Title": "Scaling Up Spike-and-Slab Models for Unsupervised Feature Learning", "URL": "https://dl.acm.org/doi/10.1109/TPAMI.2012.273", "Full Abstract": "We describe the use of two spike-and-slab models for modeling real-valued data, with an emphasis on their applications to object recognition. The first model, which we call spike-and-slab sparse coding (S3C), is a preexisting model for which we introduce a faster approximate inference algorithm. We introduce a deep variant of S3C, which we call the partially directed deep Boltzmann machine (PD-DBM) and extend our S3C inference algorithm for use on this model. We describe learning procedures for each. We demonstrate that our inference procedure for S3C enables scaling the model to unprecedented large problem sizes, and demonstrate that using S3C as a feature extractor results in very good object recognition performance, particularly when the number of labeled examples is low. We show that the PD-DBM generates better samples than its shallow counterpart, and that unlike DBMs or DBNs, the PD-DBM may be trained successfully without greedy layerwise training."},
{"Title": "Representation Learning", "URL": "https://dl.acm.org/doi/10.1109/TPAMI.2013.50", "Full Abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning."},
{"Title": "Multi-prediction deep Boltzmann machines", "URL": "https://dl.acm.org/doi/10.5555/2999611.2999673", "Full Abstract": "We introduce the"},
{"Title": "Generalized denoising auto-encoders as generative models", "URL": "https://dl.acm.org/doi/10.5555/2999611.2999712", "Full Abstract": "Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty)."},
{"Title": "Stochastic ratio matching of RBMs for sparse high-dimensional inputs", "URL": "https://dl.acm.org/doi/10.5555/2999611.2999761", "Full Abstract": "Sparse high-dimensional data vectors are common in many application domains where a very large number of rarely non-zero features can be devised. Unfortunately, this creates a computational bottleneck for unsupervised feature learning algorithms such as those based on auto-encoders and RBMs, because they involve a reconstruction step where the whole input vector is predicted from the current feature values. An algorithm was recently developed to successfully handle the case of auto-encoders, based on an importance sampling scheme stochastically selecting which input elements to actually reconstruct during training for each particular example. To generalize this idea to RBMs, we propose a stochastic ratio-matching algorithm that inherits all the computational advantages and unbiasedness of the importance sampling scheme. We show that stochastic ratio matching is a good estimator, allowing the approach to beat the state-of-the-art on two bag-of-word text classification benchmarks (20 Newsgroups and RCV1), while keeping computational cost linear in the number of non-zeros."},
{"Title": "Combining modality specific deep neural networks for emotion recognition in video", "URL": "https://dl.acm.org/doi/10.1145/2522848.2531745", "Full Abstract": "In this paper we present the techniques used for the University of Montréal's team submissions to the 2013 Emotion Recognition in the Wild Challenge. The challenge is to classify the emotions expressed by the primary human subject in short video clips extracted from feature length movies. This involves the analysis of video clips of acted scenes lasting approximately one-two seconds, including the audio track which may contain human voices as well as background music. Our approach combines multiple deep neural networks for different data modalities, including: (1) a deep convolutional neural network for the analysis of facial expressions within video frames; (2) a deep belief net to capture audio information; (3) a deep autoencoder to model the spatio-temporal information produced by the human actions depicted within the entire scene; and (4) a shallow network architecture focused on extracted features of the mouth of the primary human subject in the scene. We discuss each of these techniques, their performance characteristics and different strategies to aggregate their predictions. Our best single model was a convolutional neural network trained to predict emotions from static frames using two large data sets, the Toronto Face Database and our own set of faces images harvested from Google image search, followed by a per frame aggregation strategy that used the challenge training data. This yielded a test set accuracy of 35.58%. Using our best strategy for aggregating our top performing models into a single predictor we were able to produce an accuracy of 41.03% on the challenge test set. These compare favorably to the challenge baseline test set accuracy of 27.56%."},
{"Title": "On the synthesis of a reactive module", "URL": "https://dl.acm.org/doi/10.1145/75277.75293", "Full Abstract": "We consider the synthesis of a reactive module with input"},
{"Title": "Sea depth measurement with restricted floating sensors", "URL": "https://dl.acm.org/doi/10.1145/2512448", "Full Abstract": "Sea depth monitoring is a critical task for ensuring safe operation of harbors. Traditional schemes largely rely on labor-intensive work and expensive hardware. This study explores the possibility of deploying networked sensors on the surface of the sea, measuring and reporting the sea depth of given areas. We propose a Restricted Floating Sensors (RFS) model in which sensor nodes are anchored to the sea bottom, floating within a restricted area. Distinguished from traditional stationary or mobile sensor networks, the RFS network consists of sensor nodes with restricted mobility. We construct the network model and elaborate the corresponding localization problem. We show that by locating such RFS sensors, the sea depth can be estimated without the help of any extra ranging devices. A prototype system with 25 Telos sensor nodes is deployed to validate this design. We also examine the efficiency and scalability of this design through large-scale simulations."},
{"Title": "Does Wireless Sensor Network Scale? A Measurement Study on GreenOrbs", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2012.216", "Full Abstract": "Sensor networks are deemed suitable for large-scale deployments in the wild for a variety of applications. In spite of the remarkable efforts the community put to build the sensor systems, an essential question still remains unclear at the system level, motivating us to explore the answer from a point of real-world deployment view. Does the wireless sensor network really scale? We present findings from a large-scale operating sensor network system, GreenOrbs, with up to 330 nodes deployed in the forest. We instrument such an operating network throughout the protocol stack and present observations across layers in the network. Based on our findings from the system measurement, we propose and make initial efforts to validate three conjectures that give potential guidelines for future designs of large-scale sensor networks. 1) A small portion of nodes bottlenecks the entire network, and most of the existing network indicators may not accurately capture them. 2) The network dynamics mainly come from the inherent concurrency of network operations instead of environment changes. 3) The environment, although the dynamics are not as significant as we assumed, has an unpredictable impact on the sensor network. We suggest that an event-based routing structure can be trained and thus better adapted to the wild environment when building a large-scale sensor network."},
{"Title": "Understanding Routing Dynamics in a Large-Scale Wireless Sensor Network", "URL": "https://dl.acm.org/doi/10.1109/MASS.2013.56", "Full Abstract": "Routing dynamics are intrinsic characteristics of operational wireless sensor networks (WSNs). We present the measurement and analysis results for routing dynamics in a large-scale WSN. We seek to answer several fundamental questions: How dynamically are current routing protocols performing? What causes routing dynamics? What is the impact of routing dynamics? Answers to the above questions are critical to understanding the interactions among multiple network elements, evaluating protocol design strategies, and improving system performances. However, measurements in large-scale WSNs are challenging due to the lack of dedicated log information (be analogous to configuration files, syslog messages used in Internet). We propose an approach to identify the routing dynamics based on limited information and correlate them with system events to find out the root causes. The key findings of our study include: 1) parent change events mainly affect local nodes, i.e. They do not cause routing instability on far-away nodes, 2) environment dynamics and routing loops have large impact on routing, 3) small portion of parent changes might not be necessary, while a large portion of parent changes are effective in improving network performance."},
{"Title": "End-to-End Delay Measurement in Wireless Sensor Networks without Synchronization", "URL": "https://dl.acm.org/doi/10.1109/MASS.2013.71", "Full Abstract": "The deployment of large scale Wireless Sensor Networks generally needs network management and measurement solutions. End-to-end delay is one of the most important metrics in assessing the network performance. Many efforts have been devoted to measuring the end-to-end delay efficiently and precisely. Unfortunately, existing approaches often require sensor nodes to be tightly time synchronized which is costly in resource limited sensor network. We propose a novel scheme that can measure the end-to-end delay for each packet to the granularity of tens of microseconds without clock synchronization. Through extensive experiments on our testbed, we examine the effectiveness of our approach. The results show that our scheme achieves high performance with low overhead. We also present observations about the network states by tracking and analyzing the end-to-end delay data."},
{"Title": "SmokeGrenade", "URL": "https://dl.acm.org/doi/10.1109/MASS.2013.73", "Full Abstract": "Leveraging a wireless multi-path channel as a source of common randomness, a number of key generation methods have been proposed according to information-theory security. However, by taking the advantages of node's mobility, existing schemes usually have low generation rate or low entropy. To overcome this limitation, we present a key generation protocol with known Artificial Interference, named Smoke Grenade, a new physical-layer approach for secret key generations in a narrowband fading channel. Our scheme utilizes artificial interference to contribute to the change of the measured values on channel states. The theoretical analysis shows that the key generation rate rises with the increment of the interference power. Particularly, the achievable key rate of Smoke Grenade achieves at least four times better than that of traditional key generation schemes when the average interference power is normalized to 1. Simulation results also show that Smoke Grenade has a higher generation rate and entropy compared with some known state-of-the-art approaches."},
{"Title": "Survival of the Fittest", "URL": "https://dl.acm.org/doi/10.1109/MASS.2013.79", "Full Abstract": "Data dissemination is a building block of wireless sensor networks (WSNs). In order to guarantee the reliability, many existing works rely on a negotiation scheme, making senders and receivers negotiate the schedule of transmissions through a three-way handshake procedure. According to our observation, however, negotiation incurs long dissemination time and seriously defers the network wide convergence. On the other hand, the flooding approach, which is conventionally considered to be inefficient and energy-consuming, may facilitate data dissemination if appropriately designed. This motivates us to pursue a delicate tradeoff between negotiation and flooding in the data dissemination process. In this paper, we propose SurF (Survival of the Fittest), a data dissemination protocol which selectively adopts negotiation and leverages flooding opportunistically. How to capture and utilize the opportunities when negotiation should be used is a challenging issue. SurF incorporates a time-reliability model to estimate the time efficiencies of the two schemes (flooding vs. negotiation) and dynamically selects the fittest one to facilitate the dissemination process. We implement SurF based on TinyOS 2.1.1 and evaluate its performance with 40 TelosB nodes. The results show that SurF, while retaining the dissemination reliability, reduces the dissemination time by 40% in average, compared with the state-of-the-art protocols."},
{"Title": "Contextual tag inference", "URL": "https://dl.acm.org/doi/10.1145/2037676.2037689", "Full Abstract": "This article examines the use of two kinds of context to improve the results of content-based music taggers: the relationships between tags and between the clips of songs that are tagged. We show that users agree more on tags applied to clips temporally “closer” to one another; that conditional restricted Boltzmann machine models of tags can more accurately predict related tags when they take context into account; and that when training data is “smoothed” using context, support vector machines can better rank these clips according to the original, unsmoothed tags and do this more accurately than three standard multi-label classifiers."},
{"Title": "Shallow vs. deep sum-product networks", "URL": "https://dl.acm.org/doi/10.5555/2986459.2986534", "Full Abstract": "We investigate the representational power of sum-product networks (computation networks analogous to neural networks, but whose individual units compute either products or weighted sums), through a theoretical analysis that compares deep (multiple hidden layers) vs. shallow (one hidden layer) architectures. We prove there exist families of functions that can be represented much more efficiently with a deep network than with a shallow one, i.e. with substantially fewer hidden units. Such results were not available until now, and contribute to motivate recent research involving learning of deep sum-product networks, and more generally motivate research in Deep Learning."},
{"Title": "The manifold tangent classifier", "URL": "https://dl.acm.org/doi/10.5555/2986459.2986715", "Full Abstract": "We combine three important ideas present in previous work for building classifiers: the semi-supervised hypothesis (the input distribution contains information about the classifier), the unsupervised manifold hypothesis (data density concentrates near low-dimensional manifolds), and the manifold hypothesis for classification (different classes correspond to disjoint manifolds separated by low density). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold. Record-breaking classification results are obtained."},
{"Title": "On tracking the partition function", "URL": "https://dl.acm.org/doi/10.5555/2986459.2986738", "Full Abstract": "Markov Random Fields (MRFs) have proven very powerful both as density estimators and feature extractors for classification. However, their use is often limited by an inability to estimate the partition function"},
{"Title": "Algorithms for hyper-parameter optimization", "URL": "https://dl.acm.org/doi/10.5555/2986459.2986743", "Full Abstract": "Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models"},
{"Title": "Random search for hyper-parameter optimization", "URL": "https://dl.acm.org/doi/10.5555/2188385.2188395", "Full Abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms."},
{"Title": "Random search for hyper-parameter optimization", "URL": "https://dl.acm.org/doi/10.5555/2503308.2188395", "Full Abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms."},
{"Title": "Learning algorithms for the classification restricted Boltzmann machine", "URL": "https://dl.acm.org/doi/10.5555/2188385.2188407", "Full Abstract": "Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning."},
{"Title": "FLIGHT", "URL": "https://dl.acm.org/doi/10.1145/2348543.2348584", "Full Abstract": "In this paper, we propose a novel clock calibration approach called FLIGHT, which leverages the fact that the fluorescent light intensity changes with a stable period that equals half of the alternating current's. By tuning to the light emitted from indoor fluorescent lamps, FLIGHT can intelligently extract the light period information and achieve network wide time calibration by referring to such a common time reference. We address a series of practical challenges and implement FLIGHT in TelosB motes. We conduct comprehensive experiments using a 12-node test-bed in both static and mobile environments. Over one-week measurement suggests that compared with existing technologies, FLIGHT can achieve tightly synchronized time with low energy consumption."},
{"Title": "Clock calibration using fluorescent lighting", "URL": "https://dl.acm.org/doi/10.1145/2348543.2348611", "Full Abstract": "In this demo, we propose a novel clock calibration approach called FLIGHT, which leverages the fact that the fluorescent light intensity changes with a stable period that equals half of the alternating current's. By tuning to the light emitted from indoor fluorescent lamps, FLIGHT can intelligently extract the light period information and achieve network wide time calibration by referring to such a common time reference. We address a series of practical challenges and implement FLIGHT in TelosB motes. In this demonstration, we will show that by taking advantage of the stability of the AC frequency, the detected light intensity, even from different lamps, exhibits a consistent and stable period. FLIGHT can achieve tightly synchronized time with low energy consumption. In addition, since FLIGHT is independent to the network message exchange, time synchronization can be retained even when the network is temporarily disconnected. Such characteristics particularly suit various mobility-enabled scenarios."},
{"Title": "Distributed Coverage in Wireless Ad Hoc and Sensor Networks by Topological Graph Approaches", "URL": "https://dl.acm.org/doi/10.1109/TC.2011.149", "Full Abstract": "Coverage problem is a fundamental issue in wireless ad hoc and sensor networks. Previous techniques for coverage scheduling often require accurate location information or range measurements, which cannot be easily obtained in resource-limited ad hoc and sensor networks. Recently, a method based on algebraic topology is proposed to achieve coverage verification using only connectivity information. The topological method sheds some light on the issue of location-free coverage. Unfortunately, the needs of centralized computation and rigorous restriction on sensing and communication ranges greatly limit the applicability in practical large-scale distributed sensor networks. In this work, we make the first attempt toward establishing a graph theoretical framework for connectivity-based coverage with configurable coverage granularity. We propose a novel coverage criterion and scheduling method based on cycle partition. Our method is able to construct a sparse coverage set in a distributed manner, using purely connectivity information. Compared with existing methods, our design has a particular advantage, which permits us to configure or adjust the quality of coverage by adequately exploiting diverse sensing ranges and specific requirements of different applications. We formally prove the correctness and evaluate the effectiveness of our approach through extensive simulations and comparisons with the state-of-the-art approaches."},
{"Title": "Observation vs statistics", "URL": "https://dl.acm.org/doi/10.1109/MASS.2012.6502548", "Full Abstract": "We investigate efficient channel learning and opportunity utilization problem in cognitive radio networks (CRN). We find that the sensing order of multiple channels and channel accessing policy play a critical role in designing effective and efficient scheme to maximize the throughput. Leveraging this important finding, we propose a near optimal online channel access policy. We prove that, our policy can converge to an optimal point in a guaranteed probability. Further, we design a computational efficient channel access policy, integrating optimal stopping theory and multi-armed bandit policy effectively. The computational complexity is reduced from O(K NK) to O(K), where N is the number of channels, and K is the maximum number of sensing/probing times in each procedure. Our simulation results validate our policy, showing at least 40% performance improvement over statistically optimal but fixed policy."},
{"Title": "On the Delay Performance Analysis in a Large-Scale Wireless Sensor Network", "URL": "https://dl.acm.org/doi/10.1109/RTSS.2012.81", "Full Abstract": "We present a comprehensive delay performance measurement and analysis in an operational large-scale urban wireless sensor network. We build a light-weight delay measurement system in such a network and present a robust method to calculate per-packet delay. Through analysis of delay and system metrics, we seek to answer the following fundamental questions: what are the spatial and temporal characteristics of delay performance in a real network? what are the most important impacting factors and is there any practical model to capture those factors? what are the implications to protocol design? In this paper, we explore the important factors from the data in presence of various metrics and randomness, and show that the important factors are not necessarily the same with that in Internet. Further, we propose a delay model to capture those factors and validate it in the network. We revisit several prevalent protocol designs such as Collection Tree Protocol, opportunistic routing and Dynamic Switching based Forwarding, and show the implications to protocol designs."},
{"Title": "It is Not Just a Matter of Time", "URL": "https://dl.acm.org/doi/10.1109/RTSS.2012.84", "Full Abstract": "Emergency navigation is an emerging application of wireless sensor networks with significant research and social values. In order to ensure the safety and timeliness of navigation for the users, most of the existing works model navigation as a path-planning problem and adopt different metrics, such as the shortest route, the minimum exposure path, and the maximum safe distance. Without sufficient consideration of the dynamics of danger, the existing approaches are likely to cause users to move back and forth during navigation, known as oscillation. Frequent oscillations inevitably result in the user remaining in danger for a longer period of time, amplification the user's panic, and eventual decrease in the chances of survival. In this paper we take users' oscillations in the dynamic environments into account and quantify the local success rate of navigation using a metric called ENO (Expected Number of Oscillations). We then propose OPEN, an oscillation-free navigation approach that minimizes the probability of oscillation and guarantees the success rate of emergency navigation. We implement OPEN and evaluate its performance through test-bed experiments and extensive simulations. The results demonstrate that OPEN outperforms the current state-of-the-arts approaches with respect to user safety and navigation efficiency."},
{"Title": "Slow, decorrelated features for pretraining complex cell-like networks", "URL": "https://dl.acm.org/doi/10.5555/2984093.2984105", "Full Abstract": "We introduce a new type of neural network activation function based on recent physiological rate models for complex cells in visual area V1. A single-hidden-layer neural network of this kind of model achieves 1.50% error on MNIST. We also introduce an existing criterion for learning slow, decorrelated features as a pretraining strategy for image models. This pretraining strategy results in orientation-selective features, similar to the receptive fields of complex cells. With this pretraining, the same single-hidden-layer model achieves 1.34% error, even though the pretraining sample distribution is very different from the fine-tuning distribution. To implement this pretraining strategy, we derive a fast algorithm for online learning of decorrelated features such that each iteration of the algorithm runs in linear time with respect to the number of features."},
{"Title": "An infinite factor model hierarchy via a noisy-or mechanism", "URL": "https://dl.acm.org/doi/10.5555/2984093.2984139", "Full Abstract": "The Indian Buffet Process is a Bayesian nonparametric approach that models objects as arising from an infinite number of latent factors. Here we extend the latent factor model framework to two or more unbounded layers of latent factors. From a generative perspective, each layer defines a conditional"},
{"Title": "Why Does Unsupervised Pre-training Help Deep Learning?", "URL": "https://dl.acm.org/doi/10.5555/1756006.1756025", "Full Abstract": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training."},
{"Title": "Word representations", "URL": "https://dl.acm.org/doi/10.5555/1858681.1858721", "Full Abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/"},
{"Title": "Deep belief networks are compact universal approximators", "URL": "https://dl.acm.org/doi/10.1162/neco.2010.08-09-1081", "Full Abstract": "Deep belief networks (DBN) are generative models with many layers of hidden causal variables, recently introduced by Hinton, Osindero, and Teh (2006), along with a greedy layer-wise unsupervised learning algorithm. Building on Le Roux and Bengio (2008) and Sutskever and Hinton (2008), we show that deep but narrow generative networks do not require more parameters than shallow ones to achieve universal approximation. Exploiting the proof technique, we prove that deep but narrow feedforward neural networks with sigmoidal units can represent any Boolean expression."},
{"Title": "Tractable multivariate binary density estimation and the restricted boltzmann forest", "URL": "https://dl.acm.org/doi/10.1162/NECO_a_00014", "Full Abstract": "We investigate the problem of estimating the density function of multivariate binary data. In particular, we focus on models for which computing the estimated probability of any data point is tractable. In such a setting, previous work has mostly concentrated on mixture modeling approaches. We argue that for the problem of tractable density estimation, the restricted Boltzmann machine (RBM) provides a competitive framework for multivariate binary density modeling. With this in mind, we also generalize the RBM framework and present the restricted Boltzmann forest (RBForest), which replaces the binary variables in the hidden layer of RBMs with groups of tree-structured binary variables. This extension allows us to obtain models that have more modeling capacity but remain tractable. In experiments on several data sets, we demonstrate the competitiveness of this approach and study some of its properties."},
{"Title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "URL": "https://dl.acm.org/doi/10.5555/1756006.1953039", "Full Abstract": "We explore an original strategy for building deep networks, based on stacking layers of"},
{"Title": "Suitability of V1 energy models for object classification", "URL": "https://dl.acm.org/doi/10.1162/NECO_a_00084", "Full Abstract": "Simulations of cortical computation have often focused on networks built from simplified neuron models similar to rate models hypothesized for V1 simple cells. However, physiological research has revealed that even V1 simple cells have surprising complexity. Our computational simulations explore the effect of this complexity on the visual system's ability to solve simple tasks, such as the categorization of shapes and digits, after learning from a limited number of examples. We use recently proposed high-throughput methodology to explore what axes of modeling complexity are useful in these categorization tasks. We find that complex cell rate models learn to categorize objects better than simple cell models, and without incurring extra computational expense. We find that the squaring of linear filter responses leads to better performance. We find that several other components of physiologically derived models do not yield better performance."},
{"Title": "Compilation of Nonprocedural Specifications into Computer Programs", "URL": "https://dl.acm.org/doi/10.1109/TSE.1983.236736", "Full Abstract": "The paper describes the compilation of a program specification, written in the very high level nonprocedural MODEL language, into an object, PL/1 or Cobol, procedural language program. Nonprocedural programming languages are descriptive and devoid of procedural controls. They are therefore easier to use and require less programming skills than procedural languages. The MODEL language is briefly presented and illustrated followed by a description of the compilation process. An important early phase in the compilation is the representation of the specification by a dependency graph, denoted as array graph, which expresses the data flow interdependencies between statements. Two classes of algorithms which utilize this graph are next described. The first class checks various completeness, nonambiguity, and consistency aspects of the specification. Upon detecting any problems, the system attempts some automatic correcting measures which are reported to the user, or alternately, when no corrections appear as reasonable, it reports the error and solicits a modification from the user. The second class of algorithms produces an intermediate design of an object program in a language independent form. Finally, PL/1 or Cobol code is generated."},
{"Title": "Multicast Throughput for Hybrid Wireless Networks under Gaussian Channel Model", "URL": "https://dl.acm.org/doi/10.1109/TMC.2010.206", "Full Abstract": "We study the multicast capacity for hybrid wireless networks consisting of ordinary ad hoc nodes and base stations under Gaussian Channel model, which generalizes both the unicast and broadcast capacities for hybrid wireless networks. Assume that all ordinary ad hoc nodes transmit at a constant power P, and the power decays along the path, with attenuation exponent \\alpha >2. The data rate of a transmission is determined by the Signal to Interference plus Noise Ratio (SINR) at the receiver as B \\log (1 + {\\rm SINR). The ordinary ad hoc nodes are placed in the square region {\\cal A(a) of area a according to a Poisson point process of intensity n/a. Then, m additional base stations (BSs) acting as the relaying communication gateways are placed regularly in the region {\\cal A(a ), and are connected by a high-bandwidth wired network. Let a=n and a=1, we construct the hybrid extended network (HEN) and hybrid dense network (HDN), respectively. We choose randomly and independently n_s ordinary ad hoc nodes to be the sources of multicast sessions. We assume that each multicast session has n_d randomly chosen terminals. Three broad categories of multicast strategies are proposed. The first one is the hybrid strategy, i.e., the multihop scheme with BS-supported, which further consists of two types of strategies called connectivity strategy and percolation strategy, respectively. The second one is the ordinary ad hoc strategy, i.e., the multihop scheme without any BS-supported. The third one is the classical BS-based strategy under which any communication between two ordinary ad hoc nodes is relayed by some specific BSs. According to the different scenarios in terms of m, n, and n_d, we select the optimal scheme from the three categories of strategies, and derive the achievable multicast throughput based on the optimal decision."},
{"Title": "Consensus and Mutual Exclusion in a Multiple Access Channel", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2010.161", "Full Abstract": "We consider deterministic feasibility and time complexity of two fundamental tasks in distributed computing: consensus and mutual exclusion. Processes have different labels and communicate through a multiple access channel. The adversary wakes up some processes in possibly different rounds. In any round, every awake process either listens or transmits. The message of a process i is heard by all other awake processes, if i is the only process to transmit in a given round. If more than one process transmits simultaneously, there is a collision and no message is heard. We consider three characteristics that may or may not exist in the channel: collision detection (listening processes can distinguish collision from silence), the availability of a global clock showing the round number, and the knowledge of the number n of all processes. If none of the above three characteristics is available in the channel, we prove that consensus and mutual exclusion are infeasible; if at least one of them is available, both tasks are feasible, and we study their time complexity. Collision detection is shown to cause an exponential gap in complexity: if it is available, both tasks can be performed in time logarithmic in n, which is optimal, and without collision detection both tasks require linear time. We then investigate both consensus and mutual exclusion in the absence of collision detection, but under alternative presence of the two other features. With global clock, we give an algorithm whose time complexity linearly depends on n and on the wake-up time, and an algorithm whose complexity does not depend on the wake-up time and differs from the linear lower bound only by a factor O(\\log^2 n). If n is known, we also show an algorithm whose complexity differs from the linear lower bound only by a factor O(\\log^2 n)."},
{"Title": "Sampling based algorithms for quantile computation in sensor networks", "URL": "https://dl.acm.org/doi/10.1145/1989323.1989401", "Full Abstract": "We study the problem of computing approximate quantiles in large-scale sensor networks communication-efficiently, a problem previously studied by Greenwald and Khana [12] and Shrivastava et al [21]. Their algorithms have a total communication cost of O(k log"},
{"Title": "Visualizing anomalies in sensor networks", "URL": "https://dl.acm.org/doi/10.1145/2018436.2018521", "Full Abstract": "Diagnosing a large-scale sensor network is a crucial but challenging task due to the spatiotemporally dynamic network behaviors of sensor nodes. In this demo, we present Sensor Anomaly Visualization Engine (SAVE), an integrated system that tackles the sensor network diagnosis problem using both visualization and anomaly detection analytics to guide the user quickly and accurately diagnose sensor network failures. Temporal expansion model, correlation graphs and dynamic projection views are proposed to effectively interpret the topological, correlational and dimensional sensor data dynamics and their anomalies. Through a real-world large-scale wireless sensor network deployment (GreenOrbs), we demonstrate that SAVE is able to help better locate the problem and further identify the root cause of major sensor network failures."},
{"Title": "Cardinality Estimation for Large-Scale RFID Systems", "URL": "https://dl.acm.org/doi/10.5555/2015837.2015858", "Full Abstract": "Counting the number of RFID tags (cardinality) is a fundamental problem for large-scale RFID systems. Not only does it satisfy some real application requirements, it also acts as an important aid for RFID identification. Due to the extremely long processing time, slotted ALOHA-based or tree-based arbitration protocols are often impractical for many applications, because tags are usually attached to moving objects and they may have left the readers interrogation region before being counted. Recently, estimation schemes have been proposed to count the approximate number of tags. Most of them, however, suffer from two scalability problems: time inefficiency and multiple-reading. Without resolving these problems, large-scale RFID systems cannot easily apply the estimation scheme as well as the corresponding identification. In this paper, we present the Lottery Frame (LoF) estimation scheme, which can achieve high accuracy, low latency, and scalability. LoF estimates the tag numbers by utilizing the collision information. We show the significant advantages, e.g., high accuracy, short processing time, and low overhead, of the proposed LoF scheme through analysis and simulations."},
{"Title": "On multicast throughput scaling of hybrid wireless networks with general node density", "URL": "https://dl.acm.org/doi/10.1016/j.comnet.2011.06.029", "Full Abstract": "In this paper, we consider hybrid wireless networks with a general node density @l@?[1,n], where n ad hoc nodes are uniformly distributed and m base stations (BSs) are regularly placed in a square region A(n,A)=1,Ax1,A with A@?[1,n]. We focus on multicast sessions in which each ad hoc node as a user chooses randomly d ad hoc nodes as its destinations. Specifically, when d=1 (or d=n-1), a multicast session is essentially a unicast (or broadcast) session. We study the asymptotic multicast throughput for such a hybrid wireless network according to different cases in terms of m@?[1,n] and d@?[1,n], as n->~. To be specific, we design two types of multicast schemes, called hybrid scheme and BS-based scheme, respectively. For the hybrid scheme, there are two alternative routing backbones: sparse backbones and dense backbones. Particularly, according to different regimes of the node density @l=nA, we derive the thresholds in terms of m and d. Depending on these thresholds, we determine which scheme is preferred for the better performance of network throughput."},
{"Title": "Nonlocal estimation of manifold structure", "URL": "https://dl.acm.org/doi/10.5555/2527326.2527327", "Full Abstract": "We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation invites an exploration of nonlocal manifold learning algorithms that attempt to discover shared structure in the tangent planes at different positions. A training criterion for such an algorithm is proposed, and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data on learning handwritten character image rotations, where local nonparametric methods fail."},
{"Title": "Greedy layer-wise training of deep networks", "URL": "https://dl.acm.org/doi/10.5555/2976456.2976476", "Full Abstract": "Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization."},
{"Title": "An empirical evaluation of deep architectures on problems with many factors of variation", "URL": "https://dl.acm.org/doi/10.1145/1273496.1273556", "Full Abstract": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks."},
{"Title": "Augmented functional time series representation and forecasting with Gaussian processes", "URL": "https://dl.acm.org/doi/10.5555/2981562.2981596", "Full Abstract": "We introduce a functional representation of time series which allows forecasts to be performed over an unspecified horizon with progressively-revealed information sets. By virtue of using Gaussian processes, a complete covariance matrix between forecasts at several time-steps is available. This information is put to use in an application to actively trade price spreads between commodity futures contracts. The approach delivers impressive out-of-sample risk-adjusted returns after transaction costs on a portfolio of 30 spreads."},
{"Title": "Learning the 2-D topology of images", "URL": "https://dl.acm.org/doi/10.5555/2981562.2981668", "Full Abstract": "We study the following question: is the two-dimensional structure of images a very strong prior or is it something that can be learned with a few examples of natural images? If someone gave us a learning task involving images for which the two-dimensional topology of pixels was not known, could we discover it automatically and exploit it? For example suppose that the pixels had been permuted in a fixed but unknown way, could we recover the relative two-dimensional location of pixels on images? The surprising result presented here is that not only the answer is yes, but that about as few as a thousand images are enough to approximately recover the relative locations of about a thousand pixels. This is achieved using a manifold learning algorithm applied to pixels associated with a measure of distributional similarity between pixel intensities. We compare different topology-extraction approaches and show how having the two-dimensional topology can be exploited."},
{"Title": "Topmoumoute online natural gradient algorithm", "URL": "https://dl.acm.org/doi/10.5555/2981562.2981669", "Full Abstract": "Guided by the goal of obtaining an optimization algorithm that is both fast and yields good generalization, we study the descent direction maximizing the decrease in generalization error or the probability of not increasing generalization error. The surprising result is that from both the Bayesian and frequentist perspectives this can yield the natural gradient direction. Although that direction can be very expensive to compute we develop an efficient, general, online approximation to the natural gradient descent which is suited to large scale problems. We report experimental results showing much faster convergence in computation time and in number of iterations with TONGA (Topmoumoute Online natural Gradient Algorithm) than with stochastic gradient descent, even on very large datasets."},
{"Title": "Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model", "URL": "https://dl.acm.org/doi/10.1109/TNN.2007.912312", "Full Abstract": "Previous work on statistical language modeling has shown that it is possible to train a feedforward neural network to approximate probabilities over sequences of words, resulting in significant error reduction when compared to standard baseline models based on n-grams. However, training the neural network model with the maximum-likelihood criterion requires computations proportional to the number of words in the vocabulary. In this paper, we introduce adaptive importance sampling as a way to accelerate training of the model. The idea is to use an adaptive n-gram model to track the conditional distributions produced by the neural network. We show that a very significant speedup can be obtained on standard problems."},
{"Title": "Simple Programs and Their Decision Problems", "URL": "https://dl.acm.org/doi/10.5555/646231.682062", "Full Abstract": "No abstract available."},
{"Title": "Distributed Coverage in Wireless Ad Hoc and Sensor Networks by Topological Graph Approaches", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2010.9", "Full Abstract": "Coverage problem is a fundamental issue in wireless ad hoc and sensor networks. Previous techniques for coverage scheduling often require accurate location information or range measurements, which cannot be easily obtained in resource-limited ad hoc and sensor networks. Recently, a method based on algebraic topology has been proposed to achieve coverage verification using only connectivity information. The topological method sheds some light on the issue of location-free coverage. Unfortunately, the needs of centralized computation and rigorous restriction on sensing and communication ranges greatly limit the applicability in practical large-scale distributed sensor networks. In this work, we make the first attempt towards establishing a graph theoretical framework for connectivity-based coverage with configurable coverage granularity. We propose a novel coverage criterion and scheduling method based on cycle partition. Our method is able to construct a sparse coverage set in a distributed manner, using purely connectivity information. Compared with existing methods, our design has a particular advantage, which permits us to configure or adjust the quality of coverage by adequately exploiting diverse sensing ranges and specific requirements of different applications. We formally prove the correctness and evaluate the effectiveness of our approach through extensive simulations and comparisons with the state-of-the-art approaches."},
{"Title": "TSS", "URL": "https://dl.acm.org/doi/10.1109/TC.2010.81", "Full Abstract": "Previous multikeyword search in DHT-based P2P systems often relies on multiple single keyword search operations, suffering from unacceptable traffic cost and poor accuracy. Precomputing term-set-based index can significantly reduce the cost but needs exponentially growing index size. Based on our observations that 1) queries are typically short and 2) users usually have limited interests, we propose a novel index pruning method, called TSS. By solely publishing the most relevant term sets from documents on the peers, TSS provides comparable search performance with a centralized solution, while the index size is reduced from exponential to the scale of O(nlog(n)). We evaluate this design through comprehensive trace-driven simulations using the TREC WT10G data collection and the query log of a major commercial search engine."},
{"Title": "Multicast capacity of wireless ad hoc networks under Gaussian channel model", "URL": "https://dl.acm.org/doi/10.1109/TNET.2009.2037431", "Full Abstract": "We study the multicast capacity of large-scale random extended multihop wireless networks, where a number of wireless nodes are randomly located in a square region with side length"},
{"Title": "Passive diagnosis for wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1109/TNET.2009.2037497", "Full Abstract": "Network diagnosis, an essential research topic for traditional networking systems, has not received much attention for wireless sensor networks (WSNs). Existing sensor debugging tools like sympathy or EmStar rely heavily on an add-in protocol that generates and reports a large amount of status information from individual sensor nodes, introducing network overhead to the resource constrained and usually traffic-sensitive sensor network. We report our initial attempt at providing a lightweight network diagnosis mechanism for sensor networks. We further propose PAD, a probabilistic diagnosis approach for inferring the root causes of abnormal phenomena. PAD employs a packet marking scheme for efficiently constructing and dynamically maintaining the inference model. Our approach does not incur additional traffic overhead for collecting desired information. Instead, we introduce a probabilistic inference model that encodes internal dependencies among different network elements for online diagnosis of an operational sensor network system. Such a model is capable of additively reasoning root causes based on passively observed symptoms. We implement the PAD prototype in our sea monitoring sensor network test-bed. We also examine the efficiency and scalability of this design through extensive trace-driven simulations."},
{"Title": "Long-term large-scale sensing in the forest", "URL": "https://dl.acm.org/doi/10.1007/s11704-010-0123-2", "Full Abstract": "We introduce GreenOrbs, our recent effort in exploring the fundamental challenges and future direction of long-term large-scale applications of wireless sensor networks. An integrated framework with more than 1000 sensors has been implemented, including the indoor test bed, prototype systems, and forest deployment: comprehensively supporting research, development, and forestry applications."},
{"Title": "Oceansense", "URL": "https://dl.acm.org/doi/10.1145/1854219.1854223", "Full Abstract": "In this project, we explore the possibility of deploying networked sensors on the Ocean's surface, to monitor depth and temperature, as well as other valuable environmental parameters. Sea depth monitoring is a critical task to ensure the safe operation of harbors. Traditional schemes largely rely on labor-intensive work and expensive hardware. We present a new solution for measuring the sea depth with Restricted Floating Sensors. To address the problem of node localization on the changeable sea environment, we propose Perpendicular Intersection (PI), a novel mobile-assisted localization scheme. In the OceanSense project, we propose the concept of passive diagnosis as well as the PAD approach which is both lightweight and adaptive to network dynamics. The OceanSense system has been working for over 16 months and provides large amounts of valuable data about the sea."},
{"Title": "Guest Introduction", "URL": "https://dl.acm.org/doi/10.1023/A%3A1013921901994", "Full Abstract": "No abstract available."},
{"Title": "Model Selection for Small Sample Regression", "URL": "https://dl.acm.org/doi/10.1023/A%3A1013943418833", "Full Abstract": "Model selection is an important ingredient of many machine learning algorithms, in particular when the sample size in small, in order to strike the right trade-off between overfitting and underfitting. Previous classical results for linear regression are based on an asymptotic analysis. We present a new penalization method for performing model selection for regression that is appropriate even for small samples. Our penalization is based on an accurate estimator of the ratio of the expected training error and the expected generalization error, in terms of the expected eigenvalues of the input covariance matrix."},
{"Title": "Kernel Matching Pursuit", "URL": "https://dl.acm.org/doi/10.1023/A%3A1013955821559", "Full Abstract": "Matching Pursuit algorithms learn a function that is a weighted sum of basis functions, by sequentially appending functions to an initially empty basis, to approximate a target function in the least-squares sense. We show how matching pursuit can be extended to use non-squared error loss functions, and how it can be used to build kernel-based solutions to machine learning problems, while keeping control of the sparsity of the solution. We present a version of the algorithm that makes an optimal choice of both the next basis and the weights of all the previously chosen bases. Finally, links to boosting algorithms and RBF training procedures, as well as an extensive experimental comparison with SVMs for classification are given, showing comparable results with typically much sparser models."},
{"Title": "Robust regression with asymmetric heavy-tail noise distributions", "URL": "https://dl.acm.org/doi/10.1162/08997660260293300", "Full Abstract": "In the presence of a heavy-tail noise distribution, regression becomes much more difficult. Traditional robust regression methods assume that the noise distribution is symmetric, and they downweight the influence of so-called outliers. When the noise distribution is asymmetric, these methods yield biased regression estimators. Motivated by data-mining problems for the insurance industry, we propose a new approach to robust regression tailored to deal with asymmetric noise distribution. The main idea is to learn most of the parameters of the model using conditional quantile estimators (which are biased but robust estimators of the regression) and to learn a few remaining parameters to combine and correct these estimators, to minimize the average squared error in an unbiased way. Theoretical analysis and experiments show the clear advantages of the approach. Results are on artificial data as well as insurance data, using both linear and neural network predictors."},
{"Title": "A neural probabilistic language model", "URL": "https://dl.acm.org/doi/10.5555/944919.944966", "Full Abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the"},
{"Title": "Extensions to metric based model selection", "URL": "https://dl.acm.org/doi/10.5555/944919.944970", "Full Abstract": "Metric-based methods have recently been introduced for model selection and regularization, often yielding very significant improvements over the alternatives tried (including cross-validation). All these methods require unlabeled data over which to compare functions and detect gross differences in behavior away from the training points. We introduce three new extensions of the metric model selection methods and apply them to feature selection. The first extension takes advantage of the particular case of time-series data in which the task involves prediction with a horizon"},
{"Title": "Bias learning, knowledge sharing", "URL": "https://dl.acm.org/doi/10.1109/TNN.2003.810608", "Full Abstract": "Biasing properly the hypothesis space of a learner has been shown to improve generalization performance. Methods for achieving this goal have been proposed, that range from designing and introducing a bias into a learner to automatically learning the bias. Multitask learning methods fall into the latter category. When several related tasks derived from the same domain are available, these methods use the domain-related knowledge coded in the training examples of all the tasks as a source of bias. We extend some of the ideas presented in this field and describe a new approach that identifies a family of hypotheses, represented by a manifold in hypothesis space, that embodies domain-related knowledge. This family is learned using training examples sampled from a group of related tasks. Learning models trained on these tasks are only allowed to select hypotheses that belong to the family. We show that the new approach encompasses a large variety of families which can be learned. A statistical analysis on a class of related tasks is performed that shows significantly improved performances when using this approach."},
{"Title": "Permutation Graphs and Transitive Graphs", "URL": "https://dl.acm.org/doi/10.1145/321707.321710", "Full Abstract": "Copyright © 1972 ACM."},
{"Title": "Axiomatic approach to total correctness of programs", "URL": "https://dl.acm.org/doi/10.1007/BF00288637", "Full Abstract": "We present here an axiomatic approach which enables one to prove by formal methods that his program is \"totally correct\" (i.e., it terminates and is logically correct--does what it is supposed to do). The approach is similar to Hoare's approach [3] for proving that a program is \"partially correct\" (i.e., that whenever it terminates it produces correct results). Our extension to Hoare's method lies in the possibility of proving both correctness and termination by one unified formalism. One can choose to prove total correctness by a single step, or by incremental proof steps, each step establishing more properties of the program."},
{"Title": "A complete axiomatic system for proving deductions about recursive programs", "URL": "https://dl.acm.org/doi/10.1145/800105.803415", "Full Abstract": "Denoting a version of Hoare's system for proving partial correctness of recursive programs by"},
{"Title": "BloomCast", "URL": "https://dl.acm.org/doi/10.1109/CCGRID.2009.50", "Full Abstract": "Efficient and effective full-text retrieval in unstructured peer-to-peer networks remains a challenge in the research community. First, it is difficult, if not impossible, for unstructured P2P search protocols to effectively locate items with guaranteed recall rate. Second, existing schemes to improve search successful rate often rely on replicating a large number of item replicas across the wide area network, incurring a large amount of communication and storage cost. In this paper we propose BloomCast, an efficient and effective full-text retrieval scheme, in unstructured P2P networks. BloomCast is effective because it guarantees perfect recall rate with high probability. It is efficient because the overall communication cost of full-text search is reduced below a formal bound. Furthermore, by casting Bloom Filters instead of the raw documents across the network, BloomCast significantly reduces the communication cost and storage cost for replication. We demonstrate the power of BloomCast design through both mathematical proof and comprehensive simulations. Results show that BloomCast outperforms existing schemes in terms of both recall rate and communication cost."},
{"Title": "Fine-grained boundary recognition in wireless ad hoc and sensor networks by topological methods", "URL": "https://dl.acm.org/doi/10.1145/1530748.1530767", "Full Abstract": "Location-free boundary recognition is crucial and critical for many fundamental network functionalities in wireless ad hoc and sensor networks. Previous designs, often coarse-grained, fail to accurately locate boundaries, especially when small holes exist. To address this issue, we propose a fine-grained boundary recognition approach using connectivity information only. This algorithm accurately discovers inner and outer boundary cycles without using location information. To the best of our knowledge, this is the first design being able to determinately locate all hole boundaries no matter how small the holes are. Also, this distributed algorithm does not rely on high node density. We formally prove the correctness of our design, and evaluate its effectiveness through extensive simulations."},
{"Title": "Performance of Bulk Data Dissemination in Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02085-8_26", "Full Abstract": "Wireless sensor networks (WSNs) have recently gained a great deal of attention as a topic of research, with a wide range of applications being explored. Bulk data dissemination is a basic building block for sensor network applications. The problem of designing efficient bulk data dissemination protocols has been addressed in a number of recent studies. The problem of mathematically analyzing the performance of these protocols, however, has not been addressed sufficiently in the literature. In this work, we show a way of analyzing mathematically the performance of bulk data dissemination protocols in WSNs. Our model can be applied to general networks by use of the shortest propagation path. Our model is accurate by considering topological information, impact of contention, and impact of pipelining. We validate the analytical results through detailed simulations, and we find the analytical results fit well with the simulation results. Further, we demonstrate that the analytical results can be used to aid protocol design for performance optimizations, e.g., page size tuning for shortening the completion time."},
{"Title": "Multicast Throughput of Hybrid Wireless Networks Under Gaussian Channel Model", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2009.18", "Full Abstract": "We study the multicast capacity for hybrid wireless networks consisting of ordinary wireless nodes and base stations under Gaussian Channel model, which generalizes both the unicast capacity and broadcast capacity for hybrid wireless networks. We simply consider the hybrid extended network, where the ordinary wireless nodes are placed in the square region An with side-length sqrt n according to a Poisson point process with unit intensity. In addition, $m$ additional base stations (BSs) serving as the relay gateway are placed regularly in the region An and they are connected by a high-bandwidth wired network. Three broad categories of multicast strategies are proposed in this paper. According to the different scenarios in terms of m, n and n_d, we select the optimal scheme from the three categories of strategies, and derive the achievable multicast throughput based on the optimal decision."},
{"Title": "Joint Throughput Optimization for Wireless Mesh Networks", "URL": "https://dl.acm.org/doi/10.1109/TMC.2008.160", "Full Abstract": "In this paper, we address the problem of joint channel assignment, link scheduling, and routing for throughput optimization in wireless networks with multiradios and multichannels. We mathematically formulate this problem by taking into account the interference, the number of available radios the set of usable channels, and other resource constraints at nodes. We also consider the possible combining of several consecutive channels into one so that a network interface card (NIC) can use the channel with larger range of frequencies and thus improve the channel capacity. Furthermore, we consider several interference models and assume a general yet practical network model in which two nodes may still not communicate directly even if one is within the transmission range of the other. We designed efficient algorithm for throughput (or fairness) optimization by finding flow routing, scheduling of transmissions, and dynamic channel assignment and combining. We show that the performance, fairness and throughput, achieved by our method is within a constant factor of the optimum. Our model also can deal with the situation when each node will charge a certain amount for relaying data to a neighboring node and each flow has a budget constraint. Our extensive evaluation shows that our algorithm can effectively exploit the number of channels and radios. In addition, it shows that combining multiple channels and assigning them to a single user at some time slots indeed increases the maximum throughput of the system compared to assigning a single channel."},
{"Title": "Convolutional networks for images, speech, and time series", "URL": "https://dl.acm.org/doi/10.5555/303568.303704", "Full Abstract": "No abstract available."},
{"Title": "Pattern recognition", "URL": "https://dl.acm.org/doi/10.5555/303568.303907", "Full Abstract": "No abstract available."},
{"Title": "Object Recognition with Gradient-Based Learning", "URL": "https://dl.acm.org/doi/10.5555/646469.691875", "Full Abstract": "Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations."},
{"Title": "Binary Pseudowavelets and Applications to Bilevel Image Processing", "URL": "https://dl.acm.org/doi/10.5555/789086.789621", "Full Abstract": "This paper shows the existance of binary pseudowavelets, bases on the binary domain that exhibit some of the properties of wavelets, such as multiresolution reconstruction and compact support. The binary pseudowavelets are defined on % n (binary vectors of length n) and are operated upon with the binary operators logical and and exclusive or. The forward transform, or analysis, is the decomposition of a binary vector into its constituant binary pseudowavelets. Binary pseudowavelets allow multiresolution, progressive reconstruction of binary vectors by using progressively more coefficients in the inverse transform. Binary pseudowavelets bases, being sparse matrices, also provide for fast transforms; moreover pseudowavelets rely on hardware-friendly operations for efficient software and hardware implementation."},
{"Title": "Stochastic learning of strategic equilibria for auctions", "URL": "https://dl.acm.org/doi/10.1162/089976699300016412", "Full Abstract": "No abstract available."},
{"Title": "Inference for the generalization error", "URL": "https://dl.acm.org/doi/10.5555/3009657.3009701", "Full Abstract": "In order to to compare learning algorithms, experimental results reported in the machine learning litterature often use statistical tests of significance. Unfortunately, most of these tests do not take into account the variability due to the choice of training set. We perform a theoretical investigation of the variance of the cross-validation estimate of the generalization error that takes into account the variability due to the choice of training sets. This allows us to propose two new ways to estimate this variance. We show, via simulations, that these new statistics perform well relative to the statistics considered by Dietterich (Dietterich, 1998)."},
{"Title": "Modeling high-dimensional discrete data with multi-layer neural networks", "URL": "https://dl.acm.org/doi/10.5555/3009657.3009714", "Full Abstract": "The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially. In this paper we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow only at most as the square of the number of variables, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions. The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units. The connectivity of the neural network can be pruned by using dependency tests between the variables. Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks, and show that significant improvements can be obtained by pruning the network."},
{"Title": "Incorporating second-order functional knowledge for better option pricing", "URL": "https://dl.acm.org/doi/10.5555/3008751.3008817", "Full Abstract": "Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in two of its arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of continuous functions with these and other properties. We apply this new class of functions to the task of modeling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the"},
{"Title": "A neural probabilistic language model", "URL": "https://dl.acm.org/doi/10.5555/3008751.3008881", "Full Abstract": "A goal of statistical language modeling is to learn the joint probability function of sequences of words. This is intrinsically difficult because of the curse of dimensionality: we propose to fight it with its own weapons. In the proposed approach one learns simultaneously (1) a distributed representation for each word (i.e. a similarity between words) along with (2) the probability function for word sequences, expressed with these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar to words forming an already seen sentence. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach very significantly improves on a state-of-the-art trigram model."},
{"Title": "Taking on the curse of dimensionality in joint distributions using neural networks", "URL": "https://dl.acm.org/doi/10.1109/72.846725", "Full Abstract": "The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially. We propose an architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow at most as the square of the number of variables, using a multilayer neural network to represent the joint distribution of the variables as the product of conditional distributions. The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units. The connectivity of the neural network can be pruned by using dependency tests between the variables (thus reducing significantly the number of parameters). Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks and show that significant improvements can be obtained by pruning the network"},
{"Title": "Collaborative query processing among heterogeneous sensor networks", "URL": "https://dl.acm.org/doi/10.1145/1374699.1374705", "Full Abstract": "Demands on better interacting with physical world require an effective and comprehensive collaboration mechanism among multiple heterogeneous sensor networks. Previous works mainly focus on improving each single and specific sensor network, thus fail to address this newly emerged issue. In this paper, we study the issue of collaborative query processing among multiple heterogeneous sensor networks and formulate it into an optimization problem with respect to energy efficiency, called EE-QPS. To the best of our knowledge, we are the first one considering the collaborative query processing among heterogeneous sensor networks. By utilizing the implications among sensor networks, we design a heuristic approach named IAP to resolve EE-QPS. The experimental results validate our scheme and show that IAP achieves optimized energy efficiency under various environments."},
{"Title": "Design of a stabilizing AQM controller for large-delay networks based on internal model control", "URL": "https://dl.acm.org/doi/10.1016/j.comcom.2007.12.023", "Full Abstract": "We focus on the problem of the stability of congestion control for networks with large round-trip communication delays. Nearly all the existing AQM schemes neglect the impact large communication delay has on system behavior, such as stability, robustness and convergence. The drastic queue oscillations in large delay networks of PI, REM and DC-AQM decrease link utilization and introduce avoidable delay jitter. To address this problem, we propose a robust IMC-PID congestion controller based on the internal model control principle to restrict the negative impact of the stability caused by large delay. Simulation results demonstrate that the integrated performance of our proposed scheme outperforms others as communication delay increases, and achieves high link utilization and small delay jitter."},
{"Title": "A robust proportional controller for AQM based on optimized second-order system model", "URL": "https://dl.acm.org/doi/10.1016/j.comcom.2008.03.013", "Full Abstract": "Active Queue Management (AQM) is an effective mechanism to improve the performance of end-to-end congestion control. However, existing AQM schemes are sensitive to network traffic changes. In this paper, we propose a novel AQM algorithm based, for the first time, on the optimized second-order system model, called Adaptive Optimized Proportional Controller (AOPC). AOPC measures the latest packet loss ratio, and uses it as a complement to queue length in order to dynamically adjust packet drop probability. Through using TCP throughput model, AOPC is capable of detaching from the number of TCP sessions N and insensitive to various network conditions. The parameter tuning rule is in compliance with the optimized second-order system model which has a small overshoot and fast convergence speed. We comprehensively evaluate the performances of AOPC through extensive simulations using NS2 simulator, and contrast it with previous approaches such as REM, PI, PID, PIP, and LRED. Simulation results demonstrate that AOPC is more responsive to time-varying network conditions than other algorithms, and obtains the best tradeoff between utilization and delay."},
{"Title": "Quality of Trilateration", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2008.59", "Full Abstract": "The proliferation of wireless and mobile devices has fostered the demand of context aware applications. Location is one of the most significant contexts. Multilateration, as a basic building block of localization, however, have not yet overcome the challenges of (1) poor ranging measurement; (2) dynamic and noisy environments; (3) fluctuations in wireless communications. Hence, they often suffer poor accuracy and can hardly be employed in practical applications. In this study, we propose Quality of Trilateration (QoT) that quantifies the geometric relationship of objects and the ranging noise. Based on QoT, we design a confidence based iterative localization scheme, in which nodes dynamically select trilaterations with the highest quality for localization. To validate this design, a wireless sensor network prototype is deployed and results show that QoT well represents trilateration accuracy, and the proposed scheme significantly improve localization performances."},
{"Title": "RCT", "URL": "https://dl.acm.org/doi/10.1016/j.future.2007.12.002", "Full Abstract": "Resource discovery is of great importance in grid environments. Most of existing approaches treat all resources equally without any categorizing mechanism. We propose, Resource Category Tree (RCT), which organizes resources based on their characteristics represented by primary attributes (PA). RCT adopts a structure of distributed AVL tree, with each node representing a specific range of PA values. Though RCT adopts a hierarchical structure, it does not require nodes in higher levels maintain more information than those in lower levels, which makes RCT highly scalable. RCT is featured by self-organization, load-aware self-adaptation and fault tolerance. Based on RCT, commonly used queries, such as range queries and multi-attribute queries, are well supported. We conduct performance evaluations through comprehensive simulations."},
{"Title": "Training methods for adaptive boosting of neural networks", "URL": "https://dl.acm.org/doi/10.5555/302528.302765", "Full Abstract": "No abstract available."},
{"Title": "An input output HMM architecture", "URL": "https://dl.acm.org/doi/10.5555/2998687.2998740", "Full Abstract": "We introduce a recurrent architecture having a modular structure and we formulate a training procedure based on the EM algorithm. The resulting model has similarities to hidden Markov models, but supports recurrent networks processing style and allows to exploit the supervised learning paradigm while using maximum likelihood estimation."},
{"Title": "Diffusion of credit in Markovian models", "URL": "https://dl.acm.org/doi/10.5555/2998687.2998756", "Full Abstract": "This paper studies the problem of diffusion in Markovian models, such as hidden Markov models (HMMs) and how it makes very difficult the task of learning of long-term dependencies in sequences. Using results from Markov chain theory, we show that the problem of diffusion is reduced if the transition probabilities approach 0 or 1. Under this condition, standard HMMs have very limited modeling capabilities, but input/output HMMs can still perform interesting computations."},
{"Title": "Convergence properties of the K-means algorithms", "URL": "https://dl.acm.org/doi/10.5555/2998687.2998760", "Full Abstract": "This paper studies the convergence properties of the well known K-Means clustering algorithm. The K-Means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the EM algorithm to this hard threshold case. We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm."},
{"Title": "Learning long-term dependencies with gradient descent is difficult", "URL": "https://dl.acm.org/doi/10.1109/72.279181", "Full Abstract": "Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered"},
{"Title": "Diffusion of context and credit information in Markovian models", "URL": "https://dl.acm.org/doi/10.5555/1622620.1622628", "Full Abstract": "This paper studies the problem of ergodicity of transition probability matrices in Markovian models, such as hidden Markov models (HMMs), and how it makes very difficult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, we show that this problem of diffusion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approaches based on continuous optimization, such as gradient descent and the Baum-Welch algorithm."},
{"Title": "LeRec: a NN/HMM hybrid for on-line handwriting recognition", "URL": "https://dl.acm.org/doi/10.1162/neco.1995.7.6.1289", "Full Abstract": "We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network that can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors."},
{"Title": "Recurrent neural networks for missing or asynchronous data", "URL": "https://dl.acm.org/doi/10.5555/2998828.2998884", "Full Abstract": "In this paper we propose recurrent neural networks with feedback into the input units for handling two types of data analysis problems. On the one hand, this scheme can be used for static data when some of the input variables are missing. On the other hand, it can also be used for sequential data, when some of the input variables are missing or are available at different frequencies. Unlike in the case of probabilistic models (e.g. Gaussian) of the missing variables, the network does not attempt to model the distribution of the missing variables given the observed variables. Instead it is a more \"discriminant\" approach that fills in the missing variables for the sole purpose of minimizing a learning criterion (e.g., to minimize an output error)."},
{"Title": "Hierarchical recurrent neural networks for long-term dependencies", "URL": "https://dl.acm.org/doi/10.5555/2998828.2998898", "Full Abstract": "We have already shown that extracting long-term dependencies from sequential data is difficult, both for determimstic dynamical systems such as recurrent networks, and probabilistic models such as hidden Markov models (HMMs) or input/output hidden Markov models (IOHMMs). In practice, to avoid this problem, researchers have used domain specific a-priori knowledge to give meaning to the hidden or state variables representing past context. In this paper, we propose to use a more general type of a-priori knowledge, namely that the temporal dependencies are structured hierarchically. This implies that long-term dependencies are represented by variables with a long time scale. This principle is applied to a recurrent network which includes delays and multiple time scales. Experiments confirm the advantages of such structures. A similar approach is proposed for HMMs and IOHMMs."},
{"Title": "Input-output HMMs for sequence processing", "URL": "https://dl.acm.org/doi/10.1109/72.536317", "Full Abstract": "We consider problems of sequence processing and propose a solution based on a discrete-state model in order to represent past context. We introduce a recurrent connectionist architecture having a modular structure that associates a subnetwork to each state. The model has a statistical interpretation we call input-output hidden Markov model (IOHMM). It can be trained by the estimation-maximization (EM) or generalized EM (GEM) algorithms, considering state trajectories as missing data, which decouples temporal credit assignment and actual parameter estimation. The model presents similarities to hidden Markov models (HMMs), but allows us to map input sequences to output sequences, using the same processing style as recurrent neural networks. IOHMMs are trained using a more discriminant learning paradigm than HMMs, while potentially taking advantage of the EM algorithm. We demonstrate that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem. Experimental results are presented for the seven Tomita grammars, showing that these adaptive models can attain excellent generalization"},
{"Title": "Multi-task learning for stock selection", "URL": "https://dl.acm.org/doi/10.5555/2998981.2999114", "Full Abstract": "Artificial Neural Networks can be used to predict future returns of stocks in order to take financial decisions. Should one build a separate network for each stock or share the same network for all the stocks? In this paper we also explore other alternatives, in which some layers are shared and others are not shared. When the prediction of future returns for different stocks are viewed as different tasks, sharing some parameters across stocks is a form of multi-task learning. In a series of experiments with Canadian stocks, we obtain yearly returns that are more than 14% above various benchmarks."},
{"Title": "Mining Frequent Trajectory Patterns for Activity Monitoring Using Radio Frequency Tag Arrays", "URL": "https://dl.acm.org/doi/10.1109/PERCOM.2007.23", "Full Abstract": "Activity monitoring, a crucial task in many applications, is often conducted expensively using video cameras. Also, effectively monitoring a large field by analyzing images from multiple cameras remains a challenging problem. In this paper, we introduce a novel application of the recently developed RFID technology: using RF tag arrays for activity monitoring, where data mining techniques play a critical role. The RFID technology provides an economically attractive solution due to the low cost of RF tags and readers. Another novelty of this design is that the tracking objects do not need to attach any transmitters or receivers, such as tags or readers. By developing a practical fault-tolerant method, we offset the noise of RF tag data and mine frequent trajectory patterns as models of regular activities. Our empirical study using real RFID systems and data sets verifies the feasibility and the effectiveness of our design."},
{"Title": "Underground structure monitoring with wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1145/1236360.1236370", "Full Abstract": "Environment monitoring in coal mines is an important application of wireless sensor networks (WSNs) that has commercial potential. We discuss the design of a Structure-Aware Self-Adaptive WSN system, SASA. By regulating the mesh sensor network deployment and formulating a collaborative mechanism based on a regular beacon strategy, SASA is able to rapidly detect structure variations caused by underground collapses. A prototype is deployed with 27 Mica2 motes. We present our implementation experiences as well as the experimental results. To better evaluate the scalability and reliability of SASA, we also conduct a large-scale trace-driven simulation based on real data collected from the experiments."},
{"Title": "A Stern-based Collusion-Secure Software Watermarking Algorithm and Its Implementation", "URL": "https://dl.acm.org/doi/10.1109/MUE.2007.53", "Full Abstract": "Stern algorithm is a robust static software watermarking scheme. It can resist many common attacks except collusive attacks. In this paper we propose an improved Stern algorithm, make it has the ability of resisting collusive attacks and simplify its implementation at the same time. Furthermore, we describe the implementation of our algorithm and the issues that arise when targeting MSIL. We then validated it by experiments against a variety of attacks."},
{"Title": "S-Club: an overlay-based efficient service discovery mechanism in CROWN Grid", "URL": "https://dl.acm.org/doi/10.1007/s10115-006-0045-3", "Full Abstract": "Information service plays a key role in grid system, handles resource discovery and management process. Employing existing information service architectures suffers from poor scalability, long search response time, and large traffic overhead. In this paper, we propose a service club mechanism, called S-Club, for efficient service discovery. In S-Club, an overlay based on existing Grid Information Service (GIS) mesh network of CROWN is built, so that GISs are organized as service clubs. Each club serves for a certain type of service while each GIS may join one or more clubs. S-Club is adopted in our CROWN Grid and the performance of S-Club is evaluated by comprehensive simulations. The results show that S-Club scheme significantly improves search performance and outperforms existing approaches."},
{"Title": "EOS", "URL": "https://dl.acm.org/doi/10.1145/1242572.1242803", "Full Abstract": "In this paper, we present the design and implementation of our expertise oriented search system, EOS http://www.arnetminer.net. EOS is a researcher social network system. It has gathered information about a half-million computer science researchers from the Web and constructed a social network among the researchers through their co-authorship. In particular, the relationship in the social network information is used in both ranking experts for a given topic and searching for associations between researchers. Our experimental results demonstrate that the proposed methods for expert finding and association search in a social network are both more effective and efficient than the baseline methods."},
{"Title": "Iso-Map", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2007.115", "Full Abstract": "Contour mapping is a crucial part of many wireless sensor network applications. Many efforts have been made to avoid collecting data from all the sensors in the network and producing maps at the sink, which is proven to be inefficient. The existing approaches (often aggregation based), however, suffer from heavy transmission traffic and incur large computational overheads on each sensor node. We propose Iso-Map, an energy-efficient protocol for contour mapping, which builds contour maps based solely on the reports collected from intelligently selected \"isoline nodes\" in wireless sensor networks. Iso-Map achieves high-quality contour mapping while significantly reducing the generated traffic from O(n) to O( n ), where n is the total number of sensor nodes in the field. The per-node computation overhead is also restrained as a constant. We conduct comprehensive trace-driven simulations to verify this protocol, and demonstrate that Iso-Map outperforms the previous approaches in the sense that it produces contour maps of high fidelity with significantly reduced energy cost."},
{"Title": "Use of multi-layered networks for coding speech with phonetic features", "URL": "https://dl.acm.org/doi/10.5555/2969735.2969761", "Full Abstract": "Preliminary results on speaker-independant speech recognition are reported. A method that combines expertise on neural networks with expertise on speech recognition is used to build the recognition systems. For transient sounds, event-driven property extractors with variable resolution in the time and frequency domains are used. For sonorant speech, a model of the human auditory system is preferred to FFT as a front-end module."},
{"Title": "Data-driven execution of multi-layered networks for automatic speech recognition", "URL": "https://dl.acm.org/doi/10.5555/2887965.2888095", "Full Abstract": "A set of Multi-Layered Networks (MLN) for Automatic Speech Recognition (ASR) is proposed. Such a set allows the integration of information extracted with variable resolution in the time and frequency domains and to keep the number of links between nodes of the networks small in order to allow significant generalization during learning with a reasonable training set size. Subsets of networks can be executed depending on preconditions based on descriptions of the time evolution of signal energies allowing spectral properties that are significant in different acoustic situations to be learned."},
{"Title": "Speaker independent speech recognition with neural networks and speech knowledge", "URL": "https://dl.acm.org/doi/10.5555/2969830.2969857", "Full Abstract": "We attempt to combine neural networks with knowledge from speech science to build a speaker independent speech recognition system. This knowledge is utilized in designing the preprocessing, input coding, output coding, output supervision and architectural constraints. To handle the temporal aspect of speech we combine delays, copies of activations of hidden and output units at the input level, and Back-Propagation for Sequences (BPS), a learning algorithm for networks with local self-loops. This strategy is demonstrated in several experiments, in particular a nasal discrimination task for which the application of a speech theory hypothesis dramatically improved generalization."},
{"Title": "A neural network to detect homologies in proteins", "URL": "https://dl.acm.org/doi/10.5555/2969830.2969882", "Full Abstract": "In order to detect the presence and location of immunoglobulin (Ig) domains from amino acid sequences we built a system based on a neural network with one hidden layer trained with back propagation. The program was designed to efficiently identify proteins exhibiting such domains, characterized by a few localized conserved regions and a low overall homology. When the National Biomedical Research Foundation (NBRF) NEW protein sequence database was scanned to evaluate the program's performance, we obtained very low rates of false negatives coupled with a moderate rate of false positives."},
{"Title": "Programmable execution of multi-layered networks for automatic speech recognition", "URL": "https://dl.acm.org/doi/10.1145/63342.63345", "Full Abstract": "A set of Multi-Layered Networks allows the integration of information extracted with variable resolution in the time and frequency domains and to keep the number of links between nodes of the networks small for significant generalization during learning with a reasonable training set size."},
{"Title": "On the generalization capability of multi-layered networks in the extraction of speech properties", "URL": "https://dl.acm.org/doi/10.5555/1623891.1623999", "Full Abstract": "The paper describes a speech coding system based on an ear model followed by a set of Multi-Layer Networks (MLN). MLNs are trained to learn how to recognize articulatory features like the place and manner of articulation. Experiments are performed on 10 English vowels showing a recognition rate higher than 95% for new speakers. When features are used for recognition, comparable results are obtained for vowels and diphthongs not used for training and pronounced by new speakers. This suggests that MLNs suitably fed by the data computed by an ear model have good generalization capabilities over new speakers and new sounds."},
{"Title": "Use of multilayer networks for the recognition of phonetic features and phonemes", "URL": "https://dl.acm.org/doi/10.1111/j.1467-8640.1989.tb00323.x", "Full Abstract": "No abstract available."},
{"Title": "Use of multi-layered networks for coding speech with phonetic features", "URL": "https://dl.acm.org/doi/10.5555/89851.89877", "Full Abstract": "No abstract available."},
{"Title": "Phonetically-based multi-layered neural networks for classification", "URL": "https://dl.acm.org/doi/10.1016/0167-6393%2890%2990041-7", "Full Abstract": "No abstract available."},
{"Title": "Speaker independent speech recognition with neural networks and speech knowledge", "URL": "https://dl.acm.org/doi/10.5555/109230.109257", "Full Abstract": "No abstract available."},
{"Title": "S-Club", "URL": "https://dl.acm.org/doi/10.1109/ICEBE.2005.108", "Full Abstract": "Information service plays a key role in Grid system, handles resource discovery and management process. Employing existing information service architectures suffers from poor scalability, long search response time, and large traffic overhead. In this paper, we propose a service club mechanism, called S-Club, for efficient service discovery. In S-Club, an overlay based on existing GIS mesh network of CROWN is built, so that GISs are organized as service clubs. Each club serves for a certain type of service while each GIS may join one or more clubs. S-Club is adopted in RLDS as the information service in CROWN Grid and the performance of S-Club is evaluated by comprehensive simulations. The results show that S-Club scheme significantly improves search performance and outperforms existing approaches."},
{"Title": "Early experience of remote and hot service deployment with trustworthiness in CROWN grid", "URL": "https://dl.acm.org/doi/10.1007/11573937_33", "Full Abstract": "CROWN Grid aims to empower in-depth integration of resources and cooperation of researchers nationwide and worldwide. In such a distributed environment, to facilitate adoption of services, remote and hot service deployment is highly desirable. Furthermore, when the deployer and the target container are from different domains, great security challenges arise when a service is deployed to the remote container. In this paper, we present ROST, an original scheme of Remote & hOt Service deployment with Trustworthiness. By dynamically updating runtime environment configurations, ROST avoids restarting the runtime system during deployment. Moreover, we adopt trust negotiation in ROST to assure the security of service deployment. We conduct experiments in a real grid environment, and evaluate ROST comprehensively."},
{"Title": "Dynamic Layer Management in Superpeer Architectures", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2005.137", "Full Abstract": "Superpeer unstructured P2P systems have been found to be very effective by dividing the peers into two layers, superlayer and leaf-layer, in which message flooding is only conducted among superlayer and all leaf-peers are represented by corresponding superpeers. However, current superpeer systems do not employ any effective layer management schemes, so the transient and low-capacity peers are allowed to act as superpeers. Moreover, the lack of an appropriate size ratio maintenance mechanism on superlayer to leaf-layer makes the system's search performance far from being optimal. We present one workload model aimed at reducing the weighted overhead of a network. Using our proposed workload model, a network can determine an optimal layer size ratio between leaf-layer and superlayer. We then propose a Dynamic Layer Management algorithm, DLM, which can maintain an optimal layer size ratio and adaptively elect and adjust peers between superlayer and leaf-layer. DLM is completely distributed in the sense that each peer decides to be a superpeer or a leaf-peer independently without global knowledge. DLM could effectively help a superpeer P2P system maintain the optimal layer size ratio and designate peers with relatively long lifetime and large capacities as superpeers, and the peers with short lifetime and low capacities as leaf-peers under highly dynamic network situations. We demonstrate that the quality of a superpeer system is significantly improved under the DLM scheme by comprehensive simulations."},
{"Title": "Access Control Policy Negotiation for Remote Hot-deployed Grid Services", "URL": "https://dl.acm.org/doi/10.1109/E-SCIENCE.2005.11", "Full Abstract": "Service grid is a widely distributed environment, where service deployers and containers may be located in different autonomous domains. In such cases, different from traditional scenarios such as J2EE applications, the access control policy should not be determined by a deployer or a container only. Existing grid application deployment solutions do not address this unique requirement. In this paper, we propose a general approach, namely CROWN.ST, an access control policy negotiation solution for remote hot-deployment of grid services in CROWN (China R&D Environment Over Wide-area Network). Based on an access control policy language derived from non-recursive stratified Datalog with constraints, we design the negotiation procedure and three types of meta-policies. We implement a CROWN.ST prototype and evaluate our design by comprehensive experiments."},
{"Title": "OpenSPACE", "URL": "https://dl.acm.org/doi/10.1109/E-SCIENCE.2005.62", "Full Abstract": "Our key project, CROWN (China Research and Development Environment Over Wide-area Network), aims to empower in-depth integration of resources and cooperation of researchers nationwide and worldwide using grid technologies. It adopts service oriented architecture. Current service grids do not consider the separation of services and underlying resources, potentially causing low job processing efficiency and resource utilization. In this paper, we propose a novel architecture for grid systems, called Open Service Provisioning and Consuming Environment (OpenSPACE). OpenSPACE is adopted by CROWN and is evaluated by prototype implementation based on real applications."},
{"Title": "Effectively Utilizing Global Cluster Memory for Large Data-Intensive Parallel Programs", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2006.10", "Full Abstract": "Large scientific parallel applications demand large amounts of memory space. Current parallel computing platforms schedule jobs without fully knowing their memory requirements. This leads to uneven memory allocation in which some nodes are overloaded. This, in turn, leads to disk paging, which is extremely expensive in the context of scientific parallel computing. To solve this problem, we propose a new peer-to-peer solution called Parallel Network RAM. This approach avoids the use of disk, better utilizes available RAM resources, and will allow larger problems to be solved while reducing the computational, communication, and synchronization overhead typically involved in parallel applications. We proposed several different Parallel Network RAM designs and evaluated the performance of each under different conditions. We discovered that different designs are appropriate in different situations."},
{"Title": "A design of overlay anonymous multicast protocol", "URL": "https://dl.acm.org/doi/10.5555/1898953.1898982", "Full Abstract": "Multicast services are demanded by a variety of applications. Many applications require anonymity during their communication. However, there has been very little work on anonymous multicasting and such services are not available yet. Since there are fundamental differences between multicast and unicast, the solutions proposed for anonymity in unicast communications cannot be directly applied to multicast applications. In this paper we define the anonymous multicast system, and propose a mutual anonymous multicast (MAM) protocol including the design of a unicast mutual anonymity protocol and construction and optimization of an anonymous multicast tree. MAM is self organizing and completely distributed. We define the attack model in an anonymous multicast system and analyze the anonymity degree. We also evaluate the performance of MAM by simulations."},
{"Title": "A distributed paging RAM grid system for wide-area memory sharing", "URL": "https://dl.acm.org/doi/10.5555/1898953.1899020", "Full Abstract": "Memory-intensive applications often suffer from the poor performance of disk swapping when memory is inadequate. Remote memory sharing schemes, which provide a remote memory that is faster than the local hard disk, are able to improve the performance of such applications. Due to the limitation of being applicable within single clusters only, however, most of the previous remote memory mechanisms, such as the network memory scheme, fail to be extendable into a large scale, distributed, heterogeneous, and dynamic environment. In this work, we propose a service-oriented grid memory sharing scheme, Distributed Paging RAM Grid (DPRG). We study the properties and criteria of large scale memory sharing, and then design major operations and optimizations to fit the usage of grid systems. We collect trace from our grid environment, and evaluate DPRG through comprehensive trace-driven simulations. Results show that DPRG significantly outperforms existing remote memory sharing schemes and supports grid computing applications effectively."},
{"Title": "Optimizing overlay topology by reducing cut vertices", "URL": "https://dl.acm.org/doi/10.1145/1378191.1378213", "Full Abstract": "Overlay networks provide base infrastructures for many areas including multimedia streaming and content distributions. Since most overlay networks are highly decentralized and self-organized, cut vertices may exist in such systems due to the lack of centralized management. A cut vertex is defined as a network node whose removal increases the number of network components. Failure of these nodes can break an overlay into a large number of disconnected components and greatly downgrade the upper layer services like media streaming. We propose here a distributed mechanism, CAM, which efficiently detects the cut vertices before they fail and neutralizes them into normal overlay nodes with slight overhead so that the possibility of network decomposition is minimized after they fail. We prove the correctness of this algorithm and evaluate the performance of our design through trace driven simulations."},
{"Title": "Resource management and organization in CROWN grid", "URL": "https://dl.acm.org/doi/10.1145/1146847.1146857", "Full Abstract": "The main goal of the key project, CROWN Grid, is to empower in-depth integration of resources and cooperation of researchers nationwide and worldwide. CROWN project was started late 2003 and we have successfully released CROWN 2.0 in November of 2005. In this paper, we introduce the resource management and organization in the CROWN grid."},
{"Title": "Dubious feedback", "URL": "https://dl.acm.org/doi/10.1145/1146847.1146897", "Full Abstract": "Reputation based trust management is increasingly popular in providing a quantitative measurement for peers choosing reliable resources and trusted cooperators in a decentralized Peer-to-Peer (P2P) environment. However, existing approaches do little regarding the validation of a peer's reputation, that is, it is challenging to guarantee the validation and accuracy of computing a reputation value due to malicious denigration or overpraising. In this work, we first investigate the impact of this problem. We then propose TruthRep approach, which encourages peers to provide honest feedback by involving the quality of their evaluations of others into computing reputations. We outline the challenging issues of this design, and present preliminary experimental results."},
{"Title": "Contour map matching for event detection in sensor networks", "URL": "https://dl.acm.org/doi/10.1145/1142473.1142491", "Full Abstract": "Many sensor network applications, such as object tracking and disaster monitoring, require effective techniques for event detection. In this paper, we propose a novel event detection mechanism based on matching the contour maps of in-network sensory data distribution. Our key observation is that events in sensor networks can be abstracted into spatio-temporal patterns of sensory data and that pattern matching can be done efficiently through contour map matching. Therefore, we propose simple SQL extensions to allow users to specify common types of events as patterns in contour maps and study energy-efficient techniques of contour map construction and maintenance for our pattern-based event detection. Our experiments with synthetic workloads derived from a real-world coal mine surveillance application validate the effectiveness and efficiency of our approach."},
{"Title": "Mutual anonymous overlay multicast", "URL": "https://dl.acm.org/doi/10.1016/j.jpdc.2006.04.002", "Full Abstract": "Multicast services are demanded by a variety of applications. Many applications require anonymity during their communication. However, there has been very little work on anonymous multicasting and such services are not available yet. Due to the fundamental differences between multicast and unicast, the solutions proposed for anonymity in unicast communications cannot be directly applied to multicast applications. In this paper we define the anonymous multicast system, and propose a mutual anonymous multicast (MAM) protocol including the design of a unicast mutual anonymity protocol and construction and optimization of an anonymous multicast tree. MAM is self-organizing and completely distributed. We define the attack model in an anonymous multicast system and analyze the anonymity degree. We also evaluate the performance of MAM by comprehensive simulations."},
{"Title": "DiCAS", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2006.137", "Full Abstract": "Peer-to-peer networks are widely criticized for their inefficient flooding search mechanism. Distributed Hash Table (DHT) algorithms have been proposed to improve the search efficiency by mapping the index of a file to a unique peer based on predefined hash functions. However, the tight coupling between indices and hosting peers incurs high maintenance cost in a highly dynamic network. To properly balance the tradeoff between the costs of indexing and searching, we propose the distributed caching and adaptive search (DiCAS) algorithm, where indices are passively cached in a group of peers based on a predefined hash function. Guided by the same function, adaptive search selectively forwards queries to \"matched” peers with a high probability of caching the desired indices. The search cost is reduced due to shrunk searching space. Different from the DHT solutions, distributed caching loosely maps the index of a file to a group of peers in a passive fashion, which saves the cost of updating indices. Our simulation study shows that the DiCAS protocol can significantly reduce the network search traffic with the help of small cache space contributed by each individual peer."},
{"Title": "Improving Query Response Delivery Quality in Peer-to-Peer Systems", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2006.157", "Full Abstract": "Unstructured peer-to-peer (P2P) system is the prevalent model in today's P2P systems. In such systems, a response is sent along the same path that carried the incoming query message. To guarantee the anonymity of the requestor, no requestor information is included in the response message, and each node in the query's incoming path only knows its direct neighbors who sent the query request to it. This mechanism introduces response loss when any one node or connection in the path fails, which is a common occurrence in the P2P system due to its dynamic feature. In this paper, we address the response loss problem and show that peers' oscillation can cause up to a 35 percent response loss in an unstructured P2P system. We also present three techniques to alleviate this problem: the redundant response delivery (RRD) scheme as a proactive approach, the adaptive response delivery (ARD) scheme as a reactive approach, and the extended adaptive response delivery scheme to render ARD to function in an unstructured P2P system with limited or no flooding-based search mechanism. We have evaluated our techniques in a large-scale network simulation. With limited traffic overhead, all three techniques reduce response loss rate by more than 65 percent and are fully distributed. We have designed our techniques to be simple to develop and implement in existing P2P systems."},
{"Title": "Rumor Riding", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2006.320195", "Full Abstract": "Although anonymizing Peer-to-Peer (P2P) systems often incurs extra costs in terms of transfer efficiency, many systems tryto mask the identities of their users for privacy considerations. Existing anonymity approaches are mainly path-based: peershave to pre-construct an anonymous path before transmission. The overhead of maintaining and updating such paths is significantlyhigh. In this paper, we propose Rumor Riding (RR), a lightweight mutual anonymity protocol for decentralized P2P systems.RR employs a random walk scheme which frees initiating peers from the heavy load of path construction. Compared with previousRSA-based anonymity approaches, RR also takes advantage of lower cryptographic overhead by mainly utilizing a symmetric cryptographicalgorithm to achieve anonymity. We demonstrate the effectiveness of this design through trace-driven simulations. The analyticaland experimental results show that RR is more efficient than existing protocols. We also discuss our early implementationexperiences with the RR prototype."},
{"Title": "Prioritized Overlay Multicast in Mobile Ad Hoc Environments", "URL": "https://dl.acm.org/doi/10.1109/MC.2004.1266298", "Full Abstract": "Many proposed routing protocols for manets require nodes to maintain and update complicatedroute information, which incurs significant overhead when groups have different priorities.To address this problem, some researchers have begun focusing on application-layer, or overlay, multicast in which an overlay network forms a virtual network consisting of only member nodesatop the physical infrastructure. The authors propose a prototype of prioritized overlay multicast for manets in which participating nodes can carry out multiple functions and thus be associated with more than one overlay tree."},
{"Title": "A Distributed Approach to Solving Overlay Mismatching Problem", "URL": "https://dl.acm.org/doi/10.5555/977400.977974", "Full Abstract": "In unstructured peer-to-peer (P2P) systems, the mechanism of a peer randomly joining and leaving a P2P network causes topology mismatching between the P2P logical overlay network and the physical underlying network, causing a large volume of redundant traffic in the Internet. In order to alleviate the mismatching problem, we propose Adaptive Connection Establishment (ACE), an algorithm of building an overlay multicast tree among each source node and the peers within a certain diameter from the source peer, and further optimizing the neighbor connections that are not on the tree, while retaining the search scope. Our simulation study shows that this approach can effectively solve the mismatching problem and significantly reduce P2P traffic. We further study the tradeoffs between the topology optimization rate and the information exchange overhead by changing the diameter used to build the tree."},
{"Title": "Distributed Caching and Adaptive Search in Multilayer P2P Networks", "URL": "https://dl.acm.org/doi/10.5555/977400.977981", "Full Abstract": "To improve the scalability of Gnutella-like unstructured Peer-to-Peer (P2P) networks, a uniform index caching (UIC) mechanism was suggested in some earlier work. In UIC, query results are cached in all peers along the inverse query path such that the same query of other peers can be replied from their nearby-cached results. However, our experiments show that the UIC method causes a large amount of duplicated and unnecessary caching of items among neighboring peers. Aiming at improving the search efficiency, we propose a distributed caching mechanism which distributes the cache results among neighboring peers. Furthermore, based on the distributed caching mechanism, an adaptive search approach is built which selectively forwards the query to the peers with a high probability of providing the desired cache results. All the enhancements above are defined in a protocol called DistributedCaching and Adaptive Search (DiCAS). In the DiCAS enhanced Gnutella network, all the peers are logically divided into multiple layers, with the character that all the peers in the same layer have the same group ID. The query flooding is restricted in one layer with the matched group ID. Our simulation study shows that, with the help of the index caching and search space division, the DiCAS protocol can significantly reduce the network search traffic in unstructured P2P systems without degrading query success rate."},
{"Title": "Dynamic Layer Management in Super-Peer Architectures", "URL": "https://dl.acm.org/doi/10.5555/1018425.1020268", "Full Abstract": "The emerging peer-to-peer (P2P) model has recently gained a significant attention due to its high potential of sharing various resources among networked users. Super-peer unstructured P2P systems have been found very effective by dividing the peers into two layers, super-layer and leaf-layer, in which message flooding is only conducted among super-layer. However, current super-peer systems do not employ any effective layer management schemes, which means the transient and low-capacity peers are allowed to act as super-peers. Moreover, the lack of an appropriate size ratio maintenance mechanism on super-layer to leaf-layer makes the systemýs search performance far from being optimal. We propose a Dynamic Layer Management algorithm, DLM, which can maintain the optimal layer size ratio, and adaptively adjust peers between super-layer and leaf-layer. DLM is completely distributed in the sense that each peer decides to be a super-peer or a leaf peer independently without the global knowledge. DLM could effectively help a super-peer P2P system maintain the optimal layer size ratio, and designate peers with relatively long lifetime and large capacities as super-peers, and the peers with short lifetime and low capacities as leaf-peers under highly dynamic network situations. We demonstrate that the quality of a super-peer system is significantly improved under DLM scheme by comprehensive simulations."},
{"Title": "Parallel Network RAM", "URL": "https://dl.acm.org/doi/10.5555/1018425.1020306", "Full Abstract": "Large scientific parallel applications demand large amounts of memory space. Current parallel computing platforms schedule jobs without fully knowing their memory requirements. This leads to uneven memory allocation in which some nodes are overloaded. This, in turn, leads to disk paging, which is extremely expensive in the context of scientific parallel computing. To solve this problem, we propose a new peer-to-peer solution called Parallel Network RAM. This approach avoids the use of disk and better utilizes available RAM resources. This approach will allow larger problems to be solved while reducing the computational, communication and synchronization overhead typically involved in parallel applications."},
{"Title": "Efficient Peer-to-Peer Overlay Construction", "URL": "https://dl.acm.org/doi/10.1109/CEC-EAST.2004.39", "Full Abstract": "In unstructured peer-to-peer (P2P) systems, the mechanism of a peer randomly joining and leaving a P2P network causes topology mismatch between the P2P logical overlay network and the physical underlying network, causing a large volume of redundant traffic in the Internet. In order to alleviate the mismatching problem, we introduce several distributed algorithms to optimize the overlay, while retaining the search scope. Our simulation study shows that this approach can effectively solve the mismatch problem and significantly reduce P2P traffic and response time."},
{"Title": "Reliable Response Delivery in Peer-to-Peer Systems", "URL": "https://dl.acm.org/doi/10.5555/1032659.1034228", "Full Abstract": "Unstructured peer-to-peer (P2P) system is the prevalent model in today's P2P system. In such systems, a response is sent along the same path that carried the incoming query message. To guarantee the anonymity of the requestor, no requestor information is included in the query message and each node in the query incoming path only knows its immeadiate neighbors who sent the query request to it. This mechanism introduces response loss when any one node or connection in the path fails, which is a general case in the P2P system due to its dynamic nature. In this paper, we aim at addressing the response loss problem and present three techniques to alleviate this problem: redundant response delivery (RRD) scheme as a proactive approach, adaptive response delivery (ARD) scheme as a reactive approach and extended adaptive response delivery to render ARD to function in an unstructured P2P system with limited or no flooding based search mechanism. With limited traffic overhead, all three techniques reduces response loss rate by more than 65% and they are all fully distributed."},
{"Title": "LANDMARC", "URL": "https://dl.acm.org/doi/10.1023/B%3AWINE.0000044029.06344.dd", "Full Abstract": "Growing convergence among mobile computing devices and embedded technology sparks the development and deployment of \"context-aware\" applications, where location is the most essential context. In this paper we present LANDMARC, a location sensing prototype system that uses Radio Frequency Identification (RFID) technology for locating objects inside buildings. The major advantage of LANDMARC is that it improves the overall accuracy of locating objects by utilizing the concept of reference tags. Based on experimental analysis, we demonstrate that active RFID is a viable and cost-effective candidate for indoor location sensing. Although RFID is not designed for indoor location sensing, we point out three major features that should be added to make RFID technologies competitive in this new and growing market."},
{"Title": "Location Awareness in Unstructured Peer-to-Peer Systems", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2005.21", "Full Abstract": "Peer-to-Peer (P2P) computing has emerged as a popular model aiming at further utilizing Internet information and resources. However, the mechanism of peers randomly choosing logical neighbors without any knowledge about underlying physical topology can cause a serious topology mismatch between the P2P overlay network and the physical underlying network. The topology mismatch problem brings great stress in the Internet infrastructure. It greatly limits the performance gain from various search or routing techniques. Meanwhile, due to the inefficient overlay topology, the flooding-based search mechanisms cause a large volume of unnecessary traffic. Aiming at alleviating the mismatching problem and reducing the unnecessary traffic, we propose a location-aware topology matching (LTM) technique. LTM builds an efficient overlay by disconnecting slow connections and choosing physically closer nodes as logical neighbors while still retaining the search scope and reducing response time for queries. LTM is scalable and completely distributed in the sense that it does not require any global knowledge of the whole overlay network. The effectiveness of LTM is demonstrated through simulation studies."},
{"Title": "A Mutual Anonymous Peer-to-Peer Protocol Design", "URL": "https://dl.acm.org/doi/10.1109/IPDPS.2005.49", "Full Abstract": "Peer-to-Peer (P2P) computing has become a popular application model because of its easy resource sharing pattern and powerful query search scheme. However, decentralized P2P architecture is scarcely seen to deploy the anonymity on its peers. In this paper, we propose a mutual anonymity protocol, called Secret-sharing-based Mutual Anonymity Protocol (SSMP),for decentralized P2P systems. SSMP employs Shamirs' secret sharing scheme to allow peers to issue queries and responders to deliver requested files anonymously. Compared with pre-vious designs, SSMP achieves mutual anonymity in P2P systems with a high degree of anonymity and a low cryptography processing overhead. We evaluate SSMP by comprehensive simulations."},
{"Title": "Provide Privacy for Mobile P2P Systems", "URL": "https://dl.acm.org/doi/10.1109/ICDCSW.2005.114", "Full Abstract": "Nowadays privacy and anonymity have become an increasing requirement in wireless networks. However, current mobile peer-to-peer architectures have not taken into account anonymity, especially the mutual anonymity between two nodes. In this paper, we propose a mutual anonymity protocol, calledSecret-sharing-based Mutual Anonymity Protocol (SMA), for mobile P2P networks. Our simulation results show that SMA achieves mutual anonymity in mobile P2P networks with a low cryptography processing overhead."},
{"Title": "TruGrid", "URL": "https://dl.acm.org/doi/10.1109/ICDCSW.2005.136", "Full Abstract": "In typical distributed environments, there are two parties: consumers and providers. Consumers have computational jobs while may lack of computational resources. Providers have relatively underutilized resources. With the rapid advancement in high-speed networks, how to enable the effective and secure interaction between consumers and providers has become increasingly important. Because of the distributed ownership of resources, however, it is a very challenging problem. We propose the TruGrid, a Self-sustaining Trustworthy Grid, to solve it. The TruGrid is a novel loosely connected grid system, which guarantees the incentive of every participant and therefore it is a self-sustaining system. We also identify two security challenges arising in the TruGrid, i.e., free-rider and boaster. The proposed trustworthiness management and the penalty model successfully overcome the challenges. Simulation results demonstrate that the TruGrid is a promising actuator to enable the interaction between autonomous consumers and providers."},
{"Title": "Access Control in Peer-to-Peer Collaborative Systems", "URL": "https://dl.acm.org/doi/10.1109/ICDCSW.2005.29", "Full Abstract": "As an emerging model of communication and computation, peer-to-peer networking represents a fully distributed, cooperative network design, and has recently gained significant acceptance. Peer groups share the properties of peer-to-peer overlay network, including full decentralization, symmetric abilities, and dynamism, which make security problems more complicated. In this paper, we propose a fine-grained and attribute-based access control framework forpeer-to-peer systems. This design employs a novel policy model which extends role-based trust management language RT to satisfy security requirements of peer groups. Intend for a pure decentralized model without centralized server, our framework presents distributed delegation authorization mechanism which could avoid single point of failure. We also introduce our implementation experience."},
{"Title": "Efficient Information Service Management Using Service Club in CROWN Grid", "URL": "https://dl.acm.org/doi/10.1109/SCC.2005.48", "Full Abstract": "The main goal of our key project, CROWN Grid, is to empower in-depth integration of resources and cooperation of researchers nationwide and worldwide. In CROWN, Information Service is the kernel part which handles resource discovery and management process. Employing existing information service architectures suffers from poor scalability, long search response time, and large traffic overhead. In this paper, we propose a service club mechanism, called S-Club, for efficient service discovery. In S-Club, an overlay based on existing GIS mesh network of CROWN is built, so that GISs are organized as service clubs. Each club serves for a certain type of service while each GIS may join one or more clubs. Performance of S-Club is evaluated by comprehensive simulations. Simulation results show that S-Club scheme significantly improves search performance and outperforms existing approaches."},
{"Title": "Semantic sensor net", "URL": "https://dl.acm.org/doi/10.1007/11534310_119", "Full Abstract": "Existing approaches for sensor networks suffer from a number of critical drawbacks. First, homogeneous deployments have been commonly assumed, but in practice multiple deployments of sensor nets and heterogeneity of sensor networks are a serious problem. Second, existing approaches are very application-dependent and engineering-oriented. Third, there has been little standard available for WSNs. These drawbacks have significantly limited the further development of sensor networks. To overcome these critical drawbacks, we propose an extensive framework: Semantic Sensor Net (SSN). In brief, a semantic sensor net is a heterogeneous sensor network which enables dynamic tagging of semantic information to sensory data to allow more efficient and systematic monitoring and handling of the environmental dynamics to provide demanded services."},
{"Title": "A random walk based anonymous peer-to-peer protocol design", "URL": "https://dl.acm.org/doi/10.1007/11534310_17", "Full Abstract": "Anonymity has been one of the most challenging issues in Ad Hoc environment such as P2P systems. In this paper, we propose an anonymous protocol called"},
{"Title": "LANDMARC", "URL": "https://dl.acm.org/doi/10.5555/826025.826389", "Full Abstract": "Growing convergence among mobile computing devices and embedded technology sparks the development and deployment of \"context-aware\" applications, where location is the most essential context. In this paper we present LANDMARC, a location sensing prototype systemthat uses Radio Frequency Identification (RFID) technology for locating objects inside buildings. The major advantage of LANDMARC is that it improves the overall accuracy of locating objects by utilizing the concept of reference tags. Based on experimental analysis, wedemonstrate that active RFID is a viable and cost-effective candidate for indoor location sensing. Although RFID is not designed for indoor location sensing, we point out three major features that should be added to make RFID technologies competitive in this new and growing market."},
{"Title": "On scalable and locality-aware web document sharing", "URL": "https://dl.acm.org/doi/10.1016/S0743-7315%2803%2900096-0", "Full Abstract": "We propose a scalable Web document sharing infrastructure model called Browsers-Aware Proxy Server. In this design, a proxy server connecting to a group of networked clients maintains an index file of data objects of all clients' browser caches. If a user request misses in its local browser cache and the proxy cache, the browsers-aware proxy server will search the index file attempting to find it in another client's browser cache before sending the request to an upper level proxy or the Web server. If such a request does hit in a remote client, this client will directly forward the data object to the requesting client; or the proxy server fetches the data object from this client and then forwards it to the requesting client. The contributions of this caching model are twofold. First, we show that the amount of sharable data in browser caches is significant and can be utilized for document sharing among clients to improve Web caching performance and scalability. Second, the browsers-aware model can effectively and further improve Web prefetching performance. The browsers-aware model and its supported prefetching technique build a strong locality-aware Internet environment to make Web accesses fast with low communication costs. Conducting trace-driven simulations, we show the effectiveness of the browsers-aware model, and its unique advantages to facilitate prefetching."},
{"Title": "Overlay topology optimization and security studies in peer-to-peer systems", "URL": "https://dl.acm.org/doi/book/10.5555/1048510", "Full Abstract": "Current and future Internet and distributed systems rely on both centralized client-server model and decentralized peer-to-peer (P2P) model. P2P model is an emerging technology aiming to effectively utilize and manage increasingly large and globally distributed information and computing resources, complementing the available client-server services. In order to truly adopt the P2P model for deploying large-scale Internet applications, and timely merge this model as an indispensable component in the main stream of distributed computing technology, we must address several major technical challenges including the efficiency of overlay networks, cost-effective P2P information search, and privacy and security protection of peers. This dissertation focuses on addressing two critical issues. The first issue is topology mismatch problem between P2P overlay networks and the underlying physical networks in unstructured P2P systems. Addressing topology mismatch problem can fundamentally improve overall search performance of P2P systems. We demonstrate the seriousness of the topology mismatch problem, and define an optimal overlay problem that is proved to be a NP-hard problem. We then develop several effective schemes and algorithms to alleviate the topology mismatch problem. Our proposed algorithms are completely distributed, scalable and effective. Simulation studies show that the total traffic and response time of the queries can be significantly reduced by these schemes without shrinking the search scope. The second issue is overlay distributed denial-of-service (DDoS) attack in P2P systems. Most previous security techniques protect networks from network-layer DDoS attacks, but cannot be applied to overlay DDoS attacks. We propose a distributed and scalable method, DD-POLICE, to detect malicious nodes in order to defend P2P systems from overlay flooding-based DDoS attacks. We show the effectiveness of DD-POLICE by simulation studies and implementation on Gnutella 0.6 protocols. We believe that widely employing these proposed approaches will make P2P systems more scalable and robust."},
{"Title": "Improving Unstructured Peer-to-Peer Systems by Adaptive Connection Establishment", "URL": "https://dl.acm.org/doi/10.1109/TC.2005.146", "Full Abstract": "In unstructured peer-to-peer (P2P) systems, the mechanism of a peer randomly joining and leaving a P2P network causes a topology mismatch between the P2P logical overlay network and the physical underlying network, incurring a large volume of redundant traffic in the Internet. In order to alleviate the topology mismatch problem, we propose Adaptive Connection Establishment (ACE), an algorithm for building an overlay multicast tree among each source node and the peers within a certain diameter from the source peer and further optimizing the neighbor connections that are not on the tree while retaining the search scope. Our simulation study shows that this approach can effectively solve the mismatch problem and significantly reduce P2P traffic. We further study the trade-offs between the topology optimization rate and the information exchange overhead by changing the diameter used to build the tree."},
{"Title": "Ad-UDDI", "URL": "https://dl.acm.org/doi/10.1007/11607380_6", "Full Abstract": "In SOA (Service Oriented Architecture), web service providers use service registries to publish services and requestors use registries to find them. The major current service registry specifications, UDDI (Universal Description, Discovery and Integration), has the following drawbacks. First, it replicates all public service publications in all UBR (Universal Business Registry) nodes, which is not scalable and efficient, and second, it collects service information in a passive manner, which means it waits for service publication, updating or discovery request passively and thus cannot guarantee the real-time validity of the services information. In this paper, we propose an active and distributed UDDI architecture called Ad-UDDI, which extends and organizes the private or semi-private UDDIs based on industry classifications. Further, Ad-UDDI adopts an active monitoring mechanism, so that service information can be updated automatically and the service requestors may find the latest service information conveniently. We evaluate Ad-UDDI by comprehensive simulations and experimental results show that it outperforms existing approaches significantly."},
{"Title": "IPR", "URL": "https://dl.acm.org/doi/10.1109/WI.2005.84", "Full Abstract": "Inter-organizational business processes usually require complex and time-consuming interactions between partners than simple interactions supported by WSDL. Automated reconciliation is essential to enable dynamic inter-organizational business collaboration. To the best of our knowledge, however, there is not a practical automated reconciliation algorithm available. In this paper, we propose a practical automated reconciliation algorithm, called IPR (Interaction Process Reconciliation) based on Petri Net, which is able to effectively facilitate dynamic interactions among trading partners in a peer-to-peer fashion. We implement a prototype IPR server in our lab, and evaluate our design by comprehensive experiments. Results show that IPR significantly outperforms existing approaches in terms of matching success rate, response time, and matching efficiency."},
{"Title": "Approaching Optimal Peer-to-Peer Overlays", "URL": "https://dl.acm.org/doi/10.5555/1097871.1098192", "Full Abstract": "In unstructured peer-to-peer (P2P) systems, there exists a serious topology mismatch problem between physical and logical network. We first analyze the relationship between the property of the overlay and the corresponding message duplications incurred by queries in a given overlay, and prove that computing an optimal overlay with global knowledge is an NP-hard problem. Motivated by the analysis results, we design a distributed overlay optimization algorithm, THANCS, to attack topology mismatch. We demonstrate its performance by comprehensive simulations in dynamic environments. The proposed THANCS has three major strengths. First, it does not need any global knowledge. Second, its optimization convergent speed is fast. Third, it is orthogonal with other types of advanced search approaches."},
{"Title": "Virtual Surrounding Face Geocasting with Guaranteed Message Delivery for Ad Hoc and Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2006.320213", "Full Abstract": "Geocasting in wireless sensor networks and ad hoc networks is the delivery of a message from a source to all the nodes ina given geographical region. The objectives of a geocasting protocol are two-folds: guaranteed message delivery and low transmissioncost. Most of the existing protocols do not guarantee message delivery, and those that do incur a high transmission costs.In this paper, we introduce the idea of a Virtual Surrounding Face (VSF), and present a geocasting protocol based on VSF.By using mathematical analyses and simulation studies, we show that the proposed protocol guarantees message delivery andhas a significant lower transmission cost than the existing approaches."},
{"Title": "Grid and Cooperative Computing  GCC 2006", "URL": "https://dl.acm.org/doi/book/10.5555/1209748", "Full Abstract": "No abstract available."},
{"Title": "RCT", "URL": "https://dl.acm.org/doi/10.5555/1191828.1192513", "Full Abstract": "Computational resource discovery is of great importance in grid environments. Existing approaches do not consider the characteristics of application resource requirements. We propose, Resource Category Tree (RCT), which organizes computational resources based on their characteristics represented by primary attributes (PA). RCT adopts a structure of AVL tree, with each node representing a specific range of PA values. Though RCT adopts a hierarchical structure, it does not require nodes in higher levels maintain more information than those in lower levels, which makes RCT highly scalable. RCT is featured by self-organization, load-aware self-adaptation and fault tolerance. Based on RCT, commonly used queries, such as range queries and multi-attribute queries, are well supported. We conduct performance evaluations through comprehensive simulations."},
{"Title": "Dynamic Key-Updating", "URL": "https://dl.acm.org/doi/10.1109/PERCOM.2007.13", "Full Abstract": "The objective of private authentication for Radio Frequency Identification (RFID) systems is to allow valid readers to explicitly authenticate their dominated tags without leaking tags' private information. To achieve this goal, RFID tags issue encrypted authentication messages to the RFID reader, and the reader searches the key space to locate the tags. Due to the lack of efficient key updating algorithms, previous schemes are vulnerable to many active attacks, especially the compromising attack. In this paper, we propose a Strong and lightweight RFID Private Authentication protocol, SPA. By designing a novel key updating method, we achieve the forward secrecy in SPA with an efficient key search algorithm. We also show that, compared with existing designs, SPA is able to effectively defend against both passive and active attacks, including compromising attacks. Through prototype implementation, we observe that SPA is practical and scalable in current RFID infrastructures."},
{"Title": "A neural network to detect homologies in proteins", "URL": "https://dl.acm.org/doi/10.5555/109230.109282", "Full Abstract": "No abstract available."},
{"Title": "Artificial neural networks and their application to sequence recognition", "URL": "https://dl.acm.org/doi/book/10.5555/171735", "Full Abstract": "No abstract available."},
{"Title": "Artificial neural networks and their application to sequence recognition", "URL": "https://dl.acm.org/doi/book/10.5555/919790", "Full Abstract": "This thesis studies the introduction of a priori structure into the design of learning systems based on artificial neural networks applied to sequence recognition, in particular to phoneme recognition in continuous speech. Because we are interested in sequence analysis, algorithms for training recurrent networks are studied and an original algorithm for constrained recurrent networks is proposed and test results are reported. We also discuss the integration of connectionist models with other analysis tools that have been shown to be useful for sequences, such as dynamic programming and hidden Markov models. We introduce an original algorithm to perform global optimization of a neural network/hidden Markov model hybrid, and show how to perform such a global optimization on all the parameters of the system. Finally, we consider some alternatives to sigmoid networks: Radial Basis Functions, and a method for searching for better learning rules using a priori knowledge and optimization algorithms."},
{"Title": "Neural network", "URL": "https://dl.acm.org/doi/10.5555/2986916.2986938", "Full Abstract": "The subject of this paper is the integration of multi-layered Artificial Neural Networks (ANN) with probability density functions such as Gaussian mixtures found in continuous density Hidden Markov Models (HMM). In the first part of this paper we present an ANN/HMM hybrid in which all the parameters of the system are simultaneously optimized with respect to a single criterion. In the second part of this paper, we study the relationship between the density of the inputs of the network and the density of the outputs of the networks. A few experiments are presented to explore how to perform density estimation with ANNs."},
{"Title": "Learning the dynamic nature of speech with back-propagation for sequences", "URL": "https://dl.acm.org/doi/10.1016/0167-8655%2892%2990035-X", "Full Abstract": "No abstract available."},
{"Title": "Phonetically motivated acoustic parameters for continuous speech recognition using neural networks", "URL": "https://dl.acm.org/doi/10.1016/0167-6393%2892%2990020-8", "Full Abstract": "No abstract available."},
{"Title": "Global optimization of a neural network-hidden Markov model hybrid", "URL": "https://dl.acm.org/doi/10.1109/72.125866", "Full Abstract": "The integration of multilayered and recurrent artificial neural networks (ANNs) with hidden Markov models (HMMs) is addressed. ANNs are suitable for approximating functions that compute new acoustic parameters, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported"},
{"Title": "Credit assignment through time", "URL": "https://dl.acm.org/doi/10.5555/2987189.2987199", "Full Abstract": "Learning to recognize or predict sequences using long-term context has many applications. However, practical and theoretical problems are found in training recurrent neural networks to perform tasks in which input/output dependencies span long intervals. Starting from a mathematical analysis of the problem, we consider and compare alternative algorithms and architectures on tasks for which the span of the input/output dependencies can be controlled. Results on the new algorithms show performance qualitatively superior to that obtained with backpropagation."},
{"Title": "Globally trained handwritten word recognizer using spatial representation, convolutional neural networks and hidden Markov models", "URL": "https://dl.acm.org/doi/10.5555/2987189.2987307", "Full Abstract": "We introduce a new approach for on-line recognition of handwritten words written in unconstrained mixed style. The preprocessor performs a word-level normalization by fitting a model of the word structure using the EM algorithm. Words are then coded into low resolution \"annotated images\" where each pixel contains information about trajectory direction and curvature. The recognizer is a convolution network which can be spatially replicated. From the network output, a hidden Markov model produces word scores. The entire system is globally trained to minimize word-level errors."},
{"Title": "Non-Threshold based Event Detection for 3D Environment Monitoring in Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2007.123", "Full Abstract": "Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy- efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records."},
{"Title": "ROST", "URL": "https://dl.acm.org/doi/10.1016/j.future.2007.01.004", "Full Abstract": "The main goal of our key project, the CROWN Grid, is to empower in-depth integration of resources and the cooperation of researchers nationwide and worldwide. CROWN exploits a service-oriented architecture based on OGSA. In CROWN, remote service deployment is highly desirable. To the best of our knowledge, however, there is no successful solution to ensure the enabling remote and hot service deployment in grid systems. Traditionally, remote deployment is supported in a cold fashion, which results in many disadvantages, such as low efficiency. Moreover, since the deployer and the target container may be in different domains, great security challenges arise when a service is deployed to a remote container. In this paper, we present ROST, an original scheme of Remote and hOt Service deployment with Trustworthiness. By dynamically updating runtime environment configurations, ROST avoids restarting the runtime system during deployment. In addition, we include trust negotiation in ROST, which greatly increases the flexibility and security of the CROWN Grid. ROST has been successfully implemented. We conduct comprehensive experiments with real applications, and the results show that ROST is viable and significantly improves the service efficiency and quality of CROWN. We believe that the wide deployment of ROST would also benefit other grid systems."},
{"Title": "Building a Scalable Bipartite P2P Overlay Network", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2007.1059", "Full Abstract": "Peer-to-Peer (P2P) model, being widely adopted in today’s Internet computing, suffers from the problem of topology mismatch between the overlay networks and the underlying physical network. Traditional topology optimization techniques identify physical closer nodes to connect as overlay neighbors, but could significantly shrink the search scope. Recent efforts have been made to address the mismatch problem without sacrificing search scope, but they either need time synchronization among peers or have a low convergent speed. In this paper, we propose a scalable bipartite overlay (SBO) scheme to optimize the overlay topology by identifying and replacing the mismatched connections. In SBO, we employ an efficient strategy for distributing optimization tasks in peers with different colors. We conducted comprehensive simulations to evaluate this design. The results show that SBO achieves approximately 85% reduction on traffic cost and about 60% reduction on query response time. Our comparisons with previous approaches to address the topology mismatch problem have shown that SBO can achieve fast convergent speed without the need of time synchronization among peers."},
{"Title": "Rendered path", "URL": "https://dl.acm.org/doi/10.1145/1287853.1287861", "Full Abstract": "Sensor positioning is a crucial part of many location-dependent applications that utilize wireless sensor networks (WSNs). Current localization approaches can be divided into two groups: range-based and range-free. Due to the high costs and critical as-sumptions, the range-based schemes are often impractical for WSNs. The existing range-free schemes, on the other hand, suffer from poor accuracy and low scalability. Without the help of a large number of uniformly deployed seed nodes, those schemes fail in anisotropic WSNs with possible holes. To address this issue, we propose the Rendered Path (REP) protocol. To the best of our knowledge, REP is the only range-free protocol for locating sen-sors with constant number of seeds in anisotropic sensor net-works."},
{"Title": "Defending P2Ps from Overlay Flooding-based DDoS", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2007.31", "Full Abstract": "flooding-based search mechanism is often used in unstructured P2P systems. Although a flooding-based search mechanism is simple and easy to implement, it is vulnerable to overlay distributed denial-of-service (DDoS) attacks. Most previous security techniques protect networks from network-layer DDoS attacks, but cannot be applied to overlay DDoS attacks. Overlay flooding-based DDoS attacks can be more damaging in that a small number of messages are inherently propagated to consume a large amount of bandwidth and computation resources. We propose a distributed and scalable method, DD-POLICE, to detect malicious nodes in order to defend P2P systems from overlay flooding-based DDoS attacks. We show the effectiveness of DD-POLICE by comprehensive simulation studies. We believe that deploying DD-POLICE will make P2P systems more scalable and robust."},
{"Title": "Difficulty-aware Hybrid Search in Peer-to-Peer Networks", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2007.35", "Full Abstract": "By combining an unstructured protocol with a DHT-based global index, hybrid Peer-to-Peer (P2P) improves search efficiency in terms of query recall and response time. The key challenge in hybrid search is to estimate the number of peers that can answer a given query. Existing approaches assume that such a number can be directly obtained by computing item popularity. In this work, we show that such an assumption is not always valid, and previous designs cannot distinguish whether items related to a query are distributed in many peers or are in a few peers. To address this issue, we propose QRank, a difficulty-aware hybrid search, which ranks queries by weighting keywords based on term frequency. Using rank values, QRank selects proper search strategies for queries. We conduct comprehensive trace-driven simulations to evaluate this design. Results show that QRank significantly improves the search quality as well as reducing system traffic cost compared with existing approaches."},
{"Title": "VIRE", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2007.84", "Full Abstract": "RFID technologies are gaining much attention as they are attractive solutions to many application domains. Localization based on active RFID technologies provides a much needed added-value to further expand the application domain. LANDMARC was the first attempt using active RFID for indoor location sensing with satisfactory results. However, the LANDMARC approach suffers from two drawbacks. First, it does not work well in a closed area with severe radio signal multi-path effects. Second, to further improve the localization accuracy, more reference tags are needed which is costly and may trigger the RF interference phenomenon. The proposed VIRE approach can overcome the above drawbacks without additional cost. Based on the concept of virtual reference tags, a proximity map is maintained by each reader. An elimination algorithm is used to eliminate those unlikely locations to reduce the estimation error. Our experimental results show that the new method consistently enhances the precision of indoor localization from 17 to 73 percent over the LANDMARC approach at different tag locations in different environments."},
{"Title": "Indexable PLA for efficient similarity search", "URL": "https://dl.acm.org/doi/10.5555/1325851.1325903", "Full Abstract": "Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the \"dimensionality curse\". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only"},
{"Title": "Gradient Boundary Detection for Time Series Snapshot Construction in Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2007.1057", "Full Abstract": "In many applications of sensor networks, the sink needs to keep track of the history of sensed data of a monitored region for scientific analysis or supporting historical queries. We call these historical data as a time series of value distributions, or snapshots. Obviously, to build the time series snapshots by requiring all the sensors to transmit their data to the sink periodically is not energy-efficient. In this paper, we introduce the idea of gradient boundary, and propose a gradient boundary detection (GBD) algorithm to construct these time series snapshots of a monitored region. In GBD, a monitored region is partitioned into a set of sub-regions and all sensed data in one sub-region are within a predefined value range, namely gradient interval. Sensors located on the boundaries of the sub-regions are required to transmit the data to the sink, and then the sink recovers all sub-regions to construct snapshots of the monitored area. In this process, only the boundary sensors transmit their data, and therefore, energy consumption is greatly reduced. The simulation results show that GBD is able to build snapshots with a comparable accuracy and has up to 40% of energy saving compared with the existing approaches for large gradient intervals."},
{"Title": "Scalable Live Streaming Service Based on Interoverlay Optimization", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2007.70708", "Full Abstract": "In order to provide scalable live-streaming services, we propose an Inter-Overlay Optimization scheme, IOO. Instead of selecting better paths in the same overlay, IOO constructs efficient paths using peers in different overlays, so as to (i) improve global resource utilization of P2P streaming networks; (ii) assign resources based on their locality and delay; (iii) guarantee streaming service quality by using the nearest peers, even when such peers might belong to different overlays; and (iv) balance the load among the group (streaming overlay) members. We compare the performance of IOO with existing approaches through trace driven simulations. Results show that IOO outperforms previous schemes in terms of resource utilization and the QoS of streaming services. IOO scheme has been implemented in an Internet based live streaming system, called AnySee. AnySee was successfully released in the summer of 2004 in CERNET of China. Over 60,000 users enjoy massive entertainment programs, including TV programs, movies, and academic conferences videos."},
{"Title": "Cardinality Estimation for Large-scale RFID Systems", "URL": "https://dl.acm.org/doi/10.1109/PERCOM.2008.77", "Full Abstract": "Counting or estimating the number of tags is crucial for large-scale RFID systems. The use of multiple readers was recently proposed to improve the efficiency and effectiveness in reading RFID tags. Due to the long processing time, tag identification based counting schemes are often impractical, especially when tags are attached to moving objects. The existing estimation based schemes, on the other hand, suffer from the multiple-reading problem. To address this issue, we propose the Lottery Frame (LoF) scheme, a replicate-insensitive estimation protocol, that is able to eliminate multiple-readings. We show the high accuracy, short processing time and low overhead of the proposed LoF scheme through analysis and simulations."},
{"Title": "Robust and efficient aggregate query processing in wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1007/s11036-008-0052-6", "Full Abstract": "Wireless sensor networks have been widely used in many applications, such as soil temperature monitoring for plant growth and abnormal event detection of industrial parameters. Among these applications, aggregate queries, such as SUM, COUNT, AVERAGE, MIN and MAX are often used to collect statistical data. Due to the low quality sensing devices or random environmental disturbances, sensor data are often noisy. Hence, the idea of moving average, which computes the average over consecutive aggregate data, is introduced to offset the effect. The high link loss rate, however, makes the result after averaging still inaccurate. To address this issue, we propose a PCM-based data transmission scheme to \"make up\" the possibly lost data. Specifically, we focus on obtaining robust aggregate results under high link loss rate. In order to reduce the communication traffic that dominates the energy consumption of the sensor network, we also design an intelligent path selection algorithm for our scheme. Our extensive simulation results have shown that this technique outperforms its counterparts under various sensor network conditions."},
{"Title": "Efficient multi-keyword search over p2p web", "URL": "https://dl.acm.org/doi/10.1145/1367497.1367631", "Full Abstract": "Current search mechanisms of DHT-based P2P systems can well handle a single keyword search problem. Other than single keyword search, multi-keyword search is quite popular and useful in many real applications. Simply using the solution for single keyword search will require distributed intersection/union operations in wide area networks, leading to unacceptable traffic cost. As it is well known that Bloom Filter (BF) is effective in reducing traffic, we would like to use BF encoding to handle multi-keyword search."},
{"Title": "Self-monitoring for sensor networks", "URL": "https://dl.acm.org/doi/10.1145/1374618.1374675", "Full Abstract": "Local monitoring is an effective mechanism for the security of wireless sensor networks (WSNs). Existing schemes assume the existence of sufficient number of active nodes to carry out monitoring operations. Such an assumption, however, is often difficult for a large scale sensor network. In this work, we focus on designing an efficient scheme integrated with good self-monitoring capability as well as providing an infrastructure for various security protocols using local monitoring. To the best of our knowledge, we are the first to present the formal study on finding optimized self-monitoring topology for WSNs. We show the problem is NP-complete even under the unit disk graph (UDG) model, and give the upper bound on the approximation ratio. We further propose two distributed polynomial algorithms with provable approximation ratio to address this issue. Through comprehensive simulations, we evaluate the effectiveness of this design."},
{"Title": "Reading Checks with Multilayer Graph Transformer Networks", "URL": "https://dl.acm.org/doi/10.5555/844378.844449", "Full Abstract": "No abstract available."},
{"Title": "Global Training of Document Processing Systems Using Graph Transformer Networks", "URL": "https://dl.acm.org/doi/10.5555/794189.794462", "Full Abstract": "We propose a new machine learning paradigm called Graph Transformer Networks that extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure.A complete check reading system based on these concepts is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provide record accuracy on business and personal checks. It is presently deployed commercially and reads million of checks per month."},
{"Title": "AdaBoosting Neural Networks", "URL": "https://dl.acm.org/doi/10.5555/646257.685554", "Full Abstract": "No abstract available."},
{"Title": "Shared context probabilistic transducers", "URL": "https://dl.acm.org/doi/10.5555/3008904.3008962", "Full Abstract": "Recently, a model for supervised learning of probabilistic transducers represented by suffix trees was introduced. However, this algorithm tends to build very large trees, requiring very large amounts of computer memory. In this paper, we propose a new, more compact, transducer model in which one shares the parameters of distributions associated to contexts yielding similar conditional output distributions. We illustrate the advantages of the proposed algorithm with comparative experiments on inducing a noun phrase recognizer."},
{"Title": "Training methods for adaptive boosting of neural networks", "URL": "https://dl.acm.org/doi/10.5555/3008904.3008996", "Full Abstract": "\"Boosting\" is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is"},
{"Title": "The Z-Coder Adaptive Binary Coder", "URL": "https://dl.acm.org/doi/10.5555/874052.874882", "Full Abstract": "No abstract available."},
{"Title": "A Memory-Efficient Adaptive Huffman Coding Algorthm for Very Large Sets of Symbols", "URL": "https://dl.acm.org/doi/10.5555/874052.874937", "Full Abstract": "No abstract available."},
{"Title": "Browsing through High Quality Document Images with DjVu", "URL": "https://dl.acm.org/doi/10.5555/582987.785943", "Full Abstract": "No abstract available."},
{"Title": "Shared context probabilistic transducers", "URL": "https://dl.acm.org/doi/10.5555/302528.302730", "Full Abstract": "No abstract available."},
{"Title": "SOLONet", "URL": "https://dl.acm.org/doi/10.1007/s11276-006-0728-4", "Full Abstract": "Overlay networks have made it easy to implement multicast functionality in MANETs. Their flexibility to adapt to different environments has helped in their steady growth. Overlay multicast trees that are built using location information account for node mobility and have a low latency. However, the performance gains of such trees are offset by the overhead involved in distributing and maintaining precise location information. As the degree of (location) accuracy increases, the performance improves but the overhead required to store and broadcast this information also increases. In this paper, we present SOLONet, a design to build a sub-optimal location aided overlay multicast tree, where location updates of each member node are event based. Unlike several other approaches, SOLONet doesn't require every packet to carry location information or each node maintain location information of every other node or carrying out expensive location broadcast for each node. Our simulation results indicate that SOLONet is scalable and its sub-optimal tree performs very similar to an overlay tree built by using precise location information. SOLONet strikes a good balance between the advantages of using location information (for building efficient overlay multicast trees) versus the cost of maintaining and distributing location information of every member nodes."},
{"Title": "On the Reliability of Large-Scale Distributed Systems   A Topological View", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2008.9", "Full Abstract": "In large-scale, self-organized and distributed systems, such as peer-to-peer (P2P) overlays and wireless sensor networks (WSN), a small proportion of nodes are likely to be more critical to the system's reliability than the others. This paper focuses on detecting cut vertices so that we can either neutralize or protect these critical nodes. Detection of cut vertices is trivial if the global knowledge of the whole system is known but it is very challenging when the global knowledge is missing. In this paper, we propose a completely distributed scheme where every single node can determine whether it is a cut vertex or not. In addition, our design can also confine the detection overhead to a constant instead of being proportional to the size of a network. The correctness of this algorithm is theoretically proved and a number of performance measures are verified through trace driven simulations."},
{"Title": "Capacity of large scale wireless networks under Gaussian channel model", "URL": "https://dl.acm.org/doi/10.1145/1409944.1409962", "Full Abstract": "In this paper, we study the multicast capacity of a large scale random wireless network. We simply consider the extended multihop network, where a number of wireless nodes"},
{"Title": "A Two-Hop Solution to Solving Topology Mismatch", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2008.24", "Full Abstract": "The efficiency of Peer-to-Peer (P2P) systems is largely dependent on the overlay constructions. Due to the random selection of logical neighbors, there is serious topology mismatch problem between the overlay and the physical topologies in many P2P systems. Such mismatching causes unnecessary query message duplications on both overlay and IP level, as well as increase query response time. In this research, we define the optimal overlay problem, and prove its NP-hardness. To address this issue, we propose a distributed overlay optimization algorithm, THANCS, and evaluate its effectiveness through trace driven simulations. The proposed THANCS has four major strengths. First, it does not need any global knowledge. Second, its optimization convergent speed is fast. Third, it is orthogonal to other types of advanced search approaches. Fourth, it reduces both the traffic cost and the search latency."},
{"Title": "Passive diagnosis for wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1145/1460412.1460424", "Full Abstract": "Network diagnosis, an essential research topic for traditional networking systems, has not received much attention for wireless sensor networks. Existing sensor debugging tools like sympathy or EmStar rely heavily on an add-in protocol that generates and reports a large amount of status information from individual sen-sor nodes, introducing network overhead to a resource constrained and usually traffic sensitive sensor network. We report in this study our initial attempt at providing a light-weight network diag-nosis mechanism for sensor networks. We propose PAD, a prob-abilistic diagnosis approach for inferring the root causes of ab-normal phenomena. PAD employs a packet marking algorithm for efficiently constructing and dynamically maintaining the inference model. Our approach does not incur additional traffic overhead for collecting desired information. Instead, we introduce a prob-abilistic inference model which encodes internal dependencies among different network elements, for online diagnosis of an operational sensor network system. Such a model is capable of additively reasoning root causes based on passively observed symptoms. We implement the PAD design in our sea monitoring sensor network test-bed and validate its effectiveness. We further evaluate the efficiency and scalability of this design through ex-tensive trace-driven simulations."},
{"Title": "Perpendicular Intersection", "URL": "https://dl.acm.org/doi/10.1109/RTSS.2008.16", "Full Abstract": "Existing localization approaches are divided into two groups: range-based and range-free. The range-free schemes often suffer from poor accuracy and low scalability, while the range-based localization approaches heavily depend on extra hardware capabilities or rely on the absolute RSSI (received signal strength indicator) values, far from practical. In this work, we propose a mobile-assisted localization scheme called PerpendicularIntersection (PI), setting a dedicate tradeoff between range-free and range-based approaches. Instead of directly mapping RSSI values into physical distances, by contrasting RSSI values from the mobile beacon to a sensor node, PI utilizes the geometric relationship of perpendicular intersection to compute node positions. We have implemented the prototype of PI with 100 TelosBmotes. Through comprehensive experiments, we show that PI achieves high accuracy and low overhead, significantly outperforming the existing range-based and the mobile-assisted localization schemes."},
{"Title": "MDS", "URL": "https://dl.acm.org/doi/10.1109/RTSS.2008.26", "Full Abstract": "Geographical hash table (GHT) has been widely used to provide energy efficiency for data-centric stor-age in wireless sensor networks. Such a mechanism, however, suffers from high communication cost when we apply multi-dimensional event search in the net-work. In this work, we present MDS, a flexible, com-plete, and efficient multi-dimensional search mecha-nism atop traditional GHT based data-centric storage architecture. MDS utilizes bloom filters to reduce the communication cost of in-network intersection and union operations for multi-dimensional queries in wireless sensor networks. This scheme can be easily extended to support multi-dimensional range queries. Our mathematical analysis indicates the optimal set-tings for the bloom filters that maximize the traffic sav-ings according to the information popularities. We conduct comprehensive simulations to evaluate our design. Results show that MDS achieves significant performance improvement in terms of energy consump-tions and thus improves the applicability of the multi-dimensional search over the GHT based data-centric storage in sensor networks."},
{"Title": "Nonthreshold-Based Event Detection for 3D Environment Monitoring in Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TKDE.2008.114", "Full Abstract": "Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy-efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records."},
{"Title": "Difficulty-Aware Hybrid Search in Peer-to-Peer Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2008.72", "Full Abstract": "By combining an unstructured protocol with a DHT-based index, hybrid Peer-to-Peer (P2P) improves search efficiency in terms of query recall and response time. The key challenge in hybrid search is to estimate the number of peers that can answer a given query. Existing approaches assume that such a number can be directly obtained by computing item popularity. In this work, we show that such an assumption is not always valid, and previous designs cannot distinguish whether items related to a query are distributed in many peers or are in a few peers. To address this issue, we propose QRank, a difficulty-aware hybrid search, which ranks queries by weighting keywords based on term frequency. Using rank values, QRank selects proper search strategies for queries. We conduct comprehensive trace-driven simulations to evaluate this design. Results show that QRank significantly improves the search quality as well as reducing system traffic cost compared with existing approaches."},
{"Title": "MOCUS: moving object counting using ultrasonic sensor networks", "URL": "https://dl.acm.org/doi/10.1504/IJSNET.2008.016462", "Full Abstract": "Counting the number of moving objects in a given area has many practical applications. By investigating a series of state-of-the-art technologies, we propose a Moving Object Counting approach using Ultrasonic Sensor networks (MOCUS). In MOCUS, we deploy a network of three-node ultrasound sensor clusters, with each cluster having one ultrasound transmitting node and two ultrasound receiving nodes. Such three-node sensor clusters can successfully offset interference problems and accurately detect the direction of moving objects. In order to cover a wide area, MOCUS employs multiple sensor clusters, forming a wireless sensor network. To alleviate the impact of object moving velocity, shape of objects and distinguish closely tied multiple objects, we introduce intra-cluster analysis and inter-cluster cooperation techniques. We deploy a MOCUS prototype in our lab and evaluate the design through extensive experiments."},
{"Title": "Popularity adaptive search in hybrid P2P systems", "URL": "https://dl.acm.org/doi/10.1016/j.jpdc.2008.09.004", "Full Abstract": "In a hybrid peer-to-peer (P2P) system, flooding and DHT are both employed for content locating. The decision to use flooding or DHT largely depends on the population of desired data. Previous works either use local information only, or do not consider dynamic factors of P2P systems. In this paper, we propose a Popularity Adaptive Search method for Hybrid (PASH) protocol. By dynamically estimating the content popularity, PASH properly selects search methods so as to efficiently saves query traffic cost and response time. We evaluate PASH through synthetic and trace-driven simulations. The results show that PASH outperforms existing approaches and it also scales well."},
{"Title": "Virtual surrounding face geocasting in wireless ad hoc and sensor networks", "URL": "https://dl.acm.org/doi/10.1109/TNET.2008.927251", "Full Abstract": "Geocasting in wireless sensor and ad hoc networks means delivering a message from a source node to all the nodes in a given geographical region. The objectives of a geocasting protocol are two-fold: guaranteed message delivery and low transmission cost. Most of the existing protocols do not guarantee message de-livery, and those that do, incur high transmission costs."},
{"Title": "Randomizing RFID private authentication", "URL": "https://dl.acm.org/doi/10.1109/PERCOM.2009.4912773", "Full Abstract": "Privacy protection is increasingly important during authentications in Radio Frequency Identification (RFID) systems. In order to achieve high-speed authentication in large-scale RFID systems, researchers propose tree-based approaches, in which any pair of tags share a number of key components. Such designs, being efficient, often fail to achieve forward secrecy and resistance to attacks, such as compromising and desynchronization. Indeed, these attacks may still take effect even after a tag successfully finishes the authentication and key-updating procedure. To address the issue, we propose a lightweight RFID private authentication protocol, RWP, based on the random walk concept. RWP also provides the forward security and temporal resistance to the tracking attack. The analysis results show that RWP effectively enhances the security protection for RFID private authentication, and increases the authentication efficiency from O(logN) to O(1)."},
{"Title": "Semantic Sensor Net: an extensible framework", "URL": "https://dl.acm.org/doi/10.1504/IJAHUC.2009.024518", "Full Abstract": "Existing approaches for sensor networks suffer from a number of serious drawbacks, including assumption of homogeneous sensor nodes, application-dependency, engineering-orientation, and lack of interoperability. To overcome these drawbacks, we propose an extensive framework: Semantic Sensor Net (SSN). It is a framework catering for heterogeneous sensor networks, which enables dynamic tagging of semantic information to sensory data to allow more efficient and systematic monitoring and handling of environmental dynamics to provide diverse services. Semantics refers to the important meaning of sensory data, sensor nodes and application requirements. Essential semantics enables integration, exchange, and reuse of sensory data across various applications."},
{"Title": "Underground coal mine monitoring with wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1145/1498915.1498916", "Full Abstract": "Environment monitoring in coal mines is an important application of wireless sensor networks (WSNs) that has commercial potential. We discuss the design of a Structure-Aware Self-Adaptive WSN system, SASA. By regulating the mesh sensor network deployment and formulating a collaborative mechanism based on a regular beacon strategy, SASA is able to rapidly detect structure variations caused by underground collapses. We further develop a sound and robust mechanism for efficiently handling queries under instable circumstances. A prototype is deployed with 27 mica2 motes in a real coal mine. We present our implementation experiences as well as the experimental results. To better evaluate the scalability and reliability of SASA, we also conduct a large-scale trace-driven simulation based on real data collected from the experiments."},
{"Title": "Boosting Neural Networks", "URL": "https://dl.acm.org/doi/10.1162/089976600300015178", "Full Abstract": "Boosting is a general method for improving the performance of learning algorithms. A recently proposed boosting algorithm, AdaBoost, has been applied with great success to several benchmark machine learning problems using mainly decision trees as base classifiers. In this article we investigate whether AdaBoost also works as well with neural networks, and we discuss the advantages and drawbacks of different versions of the AdaBoost algorithm. In particular, we compare training methods based on sampling the training set and weighting the cost function. The results suggest that random resampling of the training data is not the main explanation of the success of the improvements brought by AdaBoost. This is in contrast to bagging, which directly aims at reducing variance and for which random resampling is essential to obtain the reduction in generalization error. Our system achieves about 1.4% error on a data set of on-line handwritten digits from more than 200 writers. A boosted multilayer network achieved 1.5% error on the UCI letters and 8.1% error on the UCI satellite data set, which is significantly better than boosted decision trees."},
{"Title": "Gradient-Based Optimization of Hyperparameters", "URL": "https://dl.acm.org/doi/10.1162/089976600300015187", "Full Abstract": "Many machine learning algorithms can be formulated as the minimization of a training criterion that involves a hyperparameter. This hyperparameter is usually chosen by trial and error with a model selection criterion. In this article we present a methodology to optimize several hyperparameters, based on the computation of the gradient of a model selection criterion with respect to the hyperparameters. In the case of a quadratic training criterion, the gradient of the selection criterion with respect to the hyperparameters is efficiently computed by backpropagating through a Cholesky decomposition. In the more general case, we show that the implicit function theorem can be used to derive a formula for the hyperparameter gradient involving second derivatives of the training criterion."},
{"Title": "Experiments on the application of IOHMMs to model financial returns series", "URL": "https://dl.acm.org/doi/10.1109/72.896800", "Full Abstract": "Input-output hidden Markov models (IOHMM) are conditional hidden Markov models in which the emission (and possibly the transition) probabilities can be conditioned on an input sequence. For example, these conditional distributions can be linear, logistic, or nonlinear (using for example multilayer neural networks). We compare the generalization performance of several models which are special cases of input-output hidden Markov models on financial time-series prediction tasks: an unconditional Gaussian, a conditional linear Gaussian, a mixture of Gaussians, a mixture of conditional linear Gaussians, a hidden Markov model, and various IOHMMs. The experiments compare these models on predicting the conditional density of returns of market and sector indices. Note that the unconditional Gaussian estimates the first moment with the historical average. The results show that, although for the first moment the historical average gives the best results, for the higher moments, the IOHMMs yielded significantly better performance, as estimated by the out-of-sample likelihood"},
{"Title": "A parallel mixture of SVMs for very large scale problems", "URL": "https://dl.acm.org/doi/10.5555/2980539.2980622", "Full Abstract": "Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least"},
{"Title": "K-local hyperplane and convex distance Nearest Neighbor algorithms", "URL": "https://dl.acm.org/doi/10.5555/2980539.2980666", "Full Abstract": "Guided by an initial idea of building a complex (non linear) decision surface with maximal"},
{"Title": "Estimating car insurance premia", "URL": "https://dl.acm.org/doi/10.5555/2980539.2980717", "Full Abstract": "Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers."},
{"Title": "Cost functions and model combination for VaR-based asset allocation using neural networks", "URL": "https://dl.acm.org/doi/10.1109/72.935098", "Full Abstract": "We introduce an asset-allocation framework based on the active control of the value-at-risk of the portfolio. Within this framework, we compare two paradigms for making the allocation using neural networks. The first one uses the network to make a forecast of asset behavior, in conjunction with a traditional mean-variance allocator for constructing the portfolio. The second paradigm uses the network to directly make the portfolio allocation decisions. We consider a method for performing soft input variable selection, and show its considerable utility. We use model combination (committee) methods to systematize the choice of hyperparameters during training. We show that committees using both paradigms are significantly outperforming the benchmark market performance"},
{"Title": "Manifold Parzen Windows", "URL": "https://dl.acm.org/doi/10.5555/2968618.2968724", "Full Abstract": "The similarity between objects is a fundamental element of many learning algorithms. Most non-parametric methods take this similarity to be fixed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies. We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigenvectors of regularized local covariance matrices. Experiments in density estimation show significant improvements with respect to Parzen density estimators. The density estimators can also be used within Bayes classifiers, yielding classification rates similar to SVMs and much superior to the Parzen classifier."},
{"Title": "A parallel mixture of SVMs for very large scale problems", "URL": "https://dl.acm.org/doi/10.1162/089976602753633402", "Full Abstract": "Support vector machines (SVMs) are the state-of-the-art models for many classification problems, but they suffer from the complexity of their training algorithm, which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundred thousand examples with SVMs. This article proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole data set. Experiments on a large benchmark data set (Forest) yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples). In addition, and surprisingly, a significant improvement in generalization was observed."},
{"Title": "Scaling Large Learning Problems with Hard Parallel Mixtures", "URL": "https://dl.acm.org/doi/10.5555/647230.719403", "Full Abstract": "A challenge for statistical learning is to deal with large data sets, e.g. in data mining. Popular learning algorithms such as Support Vector Machines have training time at least quadratic in the number of examples: they are hopeless to solve problems with a million examples. We propose a \"hard parallelizable mixture\" methodology which yields significantly reduced training time through modularization and parallelization: the training data is iteratively partitioned by a \"gater\" model in such a way that it becomes easy to learn an \"expert\" model separately in each region of the partition. Ap robabilistic extension and the use of a set of generative models allows representing the gater so that all pieces of the model are locally trained. For SVMs, time complexity appears empirically to locally grow linearly with the number of examples, while generalization performance can be enhanced. For the probabilistic version of the algorithm, the iterative algorithm provably goes down in a cost function that is an upper bound on the negative log-likelihood."},
{"Title": "On the reliability of large-scale distributed systems - A topological view", "URL": "https://dl.acm.org/doi/10.1016/j.comnet.2009.03.012", "Full Abstract": "In large-scale, self-organized distributed systems, such as peer-to-peer (P2P) overlays and wireless sensor networks (WSN), a small proportion of the nodes are likely to be more critical to the system's reliability than others. This paper focuses on detecting cut vertices so that we can either neutralize or protect these critical nodes. Detection of cut vertices is trivial if the global knowledge of the whole system is known but it is very challenging when the global knowledge is not available. In this paper, we propose a completely distributed scheme where every single node can determine whether it is a cut vertex or not. In addition, our design can also confine the detection overhead to a constant instead of being proportional to the size of a network. The correctness of this algorithm is theoretically proved and the key performance gains are measured and verified through trace-driven simulations."},
{"Title": "Canopy closure estimates with GreenOrbs", "URL": "https://dl.acm.org/doi/10.1145/1644038.1644049", "Full Abstract": "Motivated by the needs of precise forest inventory and real-time surveillance for ecosystem management, in this paper we present GreenOrbs [2], a wireless sensor network system and its application for canopy closure estimates. Both the hardware and software designs of GreenOrbs are tailored for sensing in wild environments without human supervision, including a firm weatherproof enclosure of sensor motes and a light-weight mechanism for node state monitoring and data collection. By incorporating a pre-deployment training process as well as a distributed calibration method, the estimates of canopy closure stay accurate and consistent against uncertain sensory data and dynamic environments. We have implemented a prototype system of GreenOrbs and carried out multiple rounds of deployments. The evaluation results demonstrate that GreenOrbs outperforms the conventional approaches for canopy closure estimates. Some early experiences are reported in this paper."},
{"Title": "Contour-cast", "URL": "https://dl.acm.org/doi/10.1109/ICPADS.2009.49", "Full Abstract": "Data dissemination and discovery is critical for ad-hoc wireless sensor networks. Most existing research depends on location information that is not always obtained easily, efficiently and accurately. We propose the concept of Contour-cast, a location-free data dissemination and discovery approach for large-scale wireless sensor networks. One important property of Contour-cast is that it does not depend on physical position or accurate localization services. The other advantage is that each node needs not maintain too much topology information. We evaluate Contour-cast thoroughly using metrics including data retrieval success ratio, storage cost, and load balance. Evaluation results show that Contour-cast can reach comparable functionalities and performance as the other approaches with physical location information."},
{"Title": "WormCircle", "URL": "https://dl.acm.org/doi/10.1109/ICPADS.2009.97", "Full Abstract": "Wormhole attack is a severe threat against wireless ad hoc and sensor networks. It can be launched without compromising any legitimate node or cryptographic mechanisms, and often serves as a stepping stone for many serious attacks. Most existing countermeasures often make critical assumptions or require specialized hardware devices in the network. Those assumptions and requirements limit the applicability of previous approaches. In this work, we explore the impact of wormhole attacks on network connectivity topologies, and develop a simple distributed method to detect wormholes, called WormCircle. WormCircle relies solely on local connectivity information without any requirements on special hardware devices or making any rigorous assumptions on network properties. We establish the correctness of this design in continuous geometric domains and extend it into discrete networks. We evaluate the effectiveness in randomly deployed sensor networks through extensive simulations."},
{"Title": "Rendered path", "URL": "https://dl.acm.org/doi/10.1109/TNET.2009.2024940", "Full Abstract": "Sensor positioning is a crucial part of many location-dependent applications that utilize wireless sensor networks (WSNs). Current localization approaches can be divided into two groups: range-based and range-free. Due to the high costs and critical assumptions, the range-based schemes are often impractical for WSNs. The existing range-free schemes, on the other hand, suffer from poor accuracy and low scalability. Without the help of a large number of uniformly deployed seed nodes, those schemes fail in anisotropic WSNs with possible holes. To address this issue, we propose the Rendered Path (REP) protocol. To the best of our knowledge, REP is the only range-free protocol for locating sensors with constant number of seeds in anisotropic sensor networks."},
{"Title": "Energy-Efficient Wake-Up Scheduling for Data Collection and Aggregation", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2009.45", "Full Abstract": "A sensor in wireless sensor networks (WSNs) periodically produces data as it monitors its vicinity. The basic operation in such a network is the systematic gathering (with or without in-network aggregation) and transmitting of sensed data to a base station for further processing. A key challenging question in WSNs is to schedule nodes' activities to reduce energy consumption. In this paper, we focus on designing energy-efficient protocols for low-data-rate WSNs, where sensors consume different energy in different radio states (transmitting, receiving, listening, sleeping, and being idle) and also consume energy for state transition. We use TDMA as the MAC layer protocol and schedule the sensor nodes with consecutive time slots at different radio states while reducing the number of state transitions. We prove that the energy consumption by our scheduling for homogeneous network is at most twice of the optimum and the timespan of our scheduling is at most a constant times of the optimum. The energy consumption by our scheduling for heterogeneous network is at most ?? (log R"},
{"Title": "Study on the energy efficiency biased on improved LEACH in wireless sensor networks", "URL": "https://dl.acm.org/doi/10.5555/1843971.1844068", "Full Abstract": "Energy efficiency and data reliability have always been the key problem in WSN. In this paper, firstly, a selection of cluster head algorithm is designed which is able to detect the energy change of nodes, rapidly ascertain new cluster head, and avoid excessive energy consumption between cluster head and cluster nodes. Secondly, differentiated service is introduced in our algorithm which makes achieve higher data reliability. Finally, evaluating our algorithm reliability and energy consumption, the simulation results show that it obtains higher data reliability, and simultaneously achieves lower energy consumption."},
{"Title": "Refresh", "URL": "https://dl.acm.org/doi/10.5555/1833515.1833640", "Full Abstract": "Privacy-Preserving Authentication (PPA) is crucial for Radio Frequency Identifcation (RFID)-enabled applications. Without appropriate formal privacy models, it is difficult for existing PPA schemes to explicitly prove their privacy. Even worse, RFID systems cannot discover potential security flaws that are vulnerable to new attacking patterns. Recently, researchers propose a formal model, termed as Strong Privacy, which strictly requires tags randomly generate their output. Adopting the Strong Privacy model, PPA schemes have to employ brute-force search in tags' authentications, which incurs unacceptable overhead and delay to large-scale RFID systems. Instead of adopting Strong Privacy, most PPA schemes improve the authentication efficiency at the cost of the privacy degradation. Due to the lack of proper formal models, it cannot be theoretically proven that the degraded PPA schemes can achieve acceptable privacy in practical RFID systems. To address these issues, we propose a weak privacy model, Refresh, for designing PPA schemes with high efficiency as well as acceptable privacy. Based on Refresh, we show that many well-known PPA schemes do not provide satisfied privacy protection, even though they achieve relatively high authentication efficiency. We further propose a Light-weight privAcy-preServing authenTication scheme, LAST, which can guarantee the privacy based on the Refresh model and realize"},
{"Title": "Beyond triangle inequality", "URL": "https://dl.acm.org/doi/10.5555/1833515.1833781", "Full Abstract": "Knowing accurate positions of nodes in wireless ad-hoc and sensor networks is essential for a wide range of pervasive and mobile applications. However, errors are inevitable in distance measurements and we observe that a small number of outliers can degrade localization accuracy drastically. To deal with noisy and outlier ranging results, triangle inequality is often employed in existing approaches. Our study shows that triangle inequality has a lot of limitations which make it far from accurate and reliable. In this study, we formally define the outlier detection problem for network localization and build a theoretical foundation to identify outliers based on graph embeddability and rigidity theory. Our analysis shows that the redundancy of distance measurements plays an important role. We then design a bilateration generic cycles based outlier detection algorithm, and examine its effectiveness and efficiency through a network prototype implementation of MicaZ motes as well as extensive simulations. The results shows that our design significantly improves the localization accuracy by wisely rejecting outliers."},
{"Title": "Understanding node localizability of wireless ad-hoc networks", "URL": "https://dl.acm.org/doi/10.5555/1833515.1833822", "Full Abstract": "Location awareness is highly critical for wireless ad-hoc and sensor networks. Many efforts have been made to solve the problem of whether or not a network can be localized. Nevertheless, based on the data collected from a working sensor network, it is observed that the network is NOT always entirely localizable. Theoretical analyses also suggest that, in most cases, it is unlikely that all nodes in a network are localizable, although a (large) portion of the nodes can be uniquely located. Existing studies merely examine whether or not a network is localizable as a whole; yet two fundamental questions remain unaddressed: First, given a network configuration, whether or not a specific node is localizable? Second, how many nodes in a network can be located and which are them? In this study, we analyze the limitation of previous works and propose a novel concept of node localizability. By deriving the necessary and sufficient conditions for node localizability, for the first time, it is possible to analyze how many nodes one can expect to locate in sparsely or moderately connected networks. To validate this design, we implement our solution on a real-world system and the experimental results show that node localizability provides useful guidelines for network deployment and other location-based services."},
{"Title": "DPLC", "URL": "https://dl.acm.org/doi/10.5555/1833515.1833825", "Full Abstract": "Previous packet length optimizations for sensor networks often employ a fixed optimal length scheme, while in this study we present DPLC, a Dynamic Packet Length Control scheme. To make DPLC more efficient in terms of channel utilization, we incorporate a lightweight and accurate link estimation method that captures both physical channel conditions and interferences. We further provide two easy-touse services, i.e., small message aggregation and large message fragmentation, to facilitate upper-layer application programming. The implementation of DPLC based on TinyOS 2.1 is lightweight, with respect to computation, memory, and header overhead. Our experiments using a real indoor testbed running CTP show that DPLC results in a 13% reduction in transmission overhead and a 41.8% reduction in energy consumption compared with the original protocol, and a 21% reduction in transmission overhead and a 15.1% reduction in energy consumption compared with simple aggregation schemes."},
{"Title": "Iso-Map", "URL": "https://dl.acm.org/doi/10.1109/TKDE.2009.157", "Full Abstract": "Contour mapping is a crucial part of many wireless sensor network applications. Many efforts have been made to avoid collecting data from all the sensors in the network and producing maps at the sink, which is proven to be inefficient. The existing approaches (often aggregation based), however, suffer from heavy transmission traffic and incur large computational overheads on each sensor node. We propose Iso-Map, an energy-efficient protocol for contour mapping, which builds contour maps based solely on the reports collected from intelligently selected “isoline nodes” in wireless sensor networks. Iso-Map achieves high-quality contour mapping while significantly reducing the generated traffic from O(n) to O(\\sqrt n), where n is the total number of sensor nodes in the field. The pernode computation overhead is also restrained as a constant. We conduct comprehensive trace-driven simulations to verify this protocol, and demonstrate that Iso-Map outperforms the previous approaches in the sense that it produces contour maps of high fidelity with significantly reduced energy cost."},
{"Title": "False Negative Problem of Counting Bloom Filter", "URL": "https://dl.acm.org/doi/10.1109/TKDE.2009.209", "Full Abstract": "Bloom filter is effective, space-efficient data structure for concisely representing a data set and supporting approximate membership queries. Traditionally, researchers often believe that it is possible that a Bloom filter returns a false positive, but it will never return a false negative under well-behaved operations. By investigating the mainstream variants, however, we observe that a Bloom filter does return false negatives in many scenarios. In this work, we show that the undetectable incorrect deletion of false positive items and detectable incorrect deletion of multiaddress items are two general causes of false negative in a Bloom filter. We then measure the potential and exposed false negatives theoretically and practically. Inspired by the fact that the potential false negatives are usually not fully exposed, we propose a novel Bloom filter scheme, which increases the ratio of bits set to a value larger than one without decreasing the ratio of bits set to zero. Mathematical analysis and comprehensive experiments show that this design can reduce the number of exposed false negatives as well as decrease the likelihood of false positives. To the best of our knowledge, this is the first work dealing with both the false positive and false negative problems of Bloom filter systematically when supporting standard usages of item insertion, query, and deletion operations."},
{"Title": "Quality of Trilateration", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2009.90", "Full Abstract": "The proliferation of wireless and mobile devices has fostered the demand for context-aware applications, in which location is one of the most significant contexts. Multilateration, as a basic building block of localization, however, has not yet overcome the challenges of 1) poor ranging measurements; 2) dynamic and noisy environments; and 3) fluctuations in wireless communications. Hence, multilateration-based approaches often suffer from poor accuracy and can hardly be employed in practical applications. In this study, we propose Quality of Trilateration (QoT) that quantifies the geometric relationship of objects and ranging noises. Based on QoT, we design a confidence-based iterative localization scheme, in which nodes dynamically select trilaterations with the highest quality for location computation. To validate this design, a prototype network based on wireless sensor motes is deployed and the results show that QoT well represents trilateration accuracy, and the proposed scheme significantly improves localization accuracy."},
{"Title": "Elon", "URL": "https://dl.acm.org/doi/10.1145/1811039.1811046", "Full Abstract": "We present a new mechanism called Elon for enabling efficient and long-term reprogramming in wireless sensor networks. Elon reduces the transferred code size significantly by introducing the concept of replaceable component. It avoids the cost of hardware reboot with a novel software reboot mechanism. Moreover, it significantly prolongs the reprogramming lifetime by avoiding flash writes for TelosB nodes. Experimental results show that Elon transfers up to 120--389 times less information than Deluge, and 18-42 times less information than Stream. The software reboot mechanism that Elon applies reduces the rebooting cost by 50.4%-53.87% in terms of beacon packets, and 56.83% in terms of unsynchronized nodes. In addition, Elon prolongs the reprogramming lifetime by a factor of 2.3."},
{"Title": "Inference for the Generalization Error", "URL": "https://dl.acm.org/doi/10.1023/A%3A1024068626366", "Full Abstract": "In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the"},
{"Title": "Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering", "URL": "https://dl.acm.org/doi/10.5555/2981345.2981368", "Full Abstract": "Several unsupervised learning algorithms based on an eigendecomposition provide either an embedding or a clustering only for given training points, with no straightforward extension for out-of-sample examples short of recomputing eigenvectors. This paper provides a unified framework for extending Local Linear Embedding (LLE), Isomap, Laplacian Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction) as well as for Spectral Clustering. This framework is based on seeing these algorithms as learning eigenfunctions of a data-dependent kernel. Numerical experiments show that the generalizations performed have a level of error comparable to the variability of the embedding algorithms due to the choice of training data."},
{"Title": "No unbiased estimator of the variance of K-fold cross-validation", "URL": "https://dl.acm.org/doi/10.5555/2981345.2981410", "Full Abstract": "Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances. In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation of uncertainty around the K-fold cross-validation estimator. The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation. An analysis based on the eigende-composition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as confirmed by numerical experiments."},
{"Title": "Unsupervised sense disambiguation using bilingual probabilistic models", "URL": "https://dl.acm.org/doi/10.3115/1218955.1218992", "Full Abstract": "We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora. The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. The second model, which we call the Concept model, is a hierarchical model that uses a concept latent variable to relate different language specific sense labels. We show that both models improve performance on the word sense disambiguation task over previous unsupervised approaches, with the Concept model showing the largest improvement. Furthermore, in learning the Concept model, as a by-product, we learn a sense inventory for the parallel language."},
{"Title": "Learning Eigenfunctions Links Spectral Embedding and Kernel PCA", "URL": "https://dl.acm.org/doi/10.1162/0899766041732396", "Full Abstract": "In this letter, we show a direct relation between spectral embedding methods and kernel principal components analysis and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density. Whereas spectral embedding methods provided only coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nyström formula) for multidimensional scaling (MDS), spectral clustering, Laplacian eigenmaps, locally linear embedding (LLE), and Isomap. The analysis provides, for all such spectral embedding methods, the definition of a loss function, whose empirical average is minimized by the traditional algorithms. The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn. Experiments with LLE, Isomap, spectral clustering, and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding."},
{"Title": "No Unbiased Estimator of the Variance of K-Fold Cross-Validation", "URL": "https://dl.acm.org/doi/10.5555/1005332.1044695", "Full Abstract": "Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied."},
{"Title": "Non-local manifold tangent learning", "URL": "https://dl.acm.org/doi/10.5555/2976040.2976057", "Full Abstract": "We claim and present arguments to the effect that a large class of manifold learning algorithms that are essentially local and can be framed as kernel learning algorithms will suffer from the curse of dimensionality, at the dimension of the true underlying manifold. This observation suggests to explore non-local manifold learning algorithms which attempt to discover shared structure in the tangent planes at different positions. A criterion for such an algorithm is proposed and experiments estimating a tangent plane prediction function are presented, showing its advantages with respect to local manifold learning algorithms: it is able to generalize very far from training data (on learning handwritten character image rotations), where a local non-parametric method fails."},
{"Title": "Semi-supervised learning by entropy minimization", "URL": "https://dl.acm.org/doi/10.5555/2976040.2976107", "Full Abstract": "We consider the semi-supervised learning problem, where a decision rule is to be learned from labeled and unlabeled data. In this framework, we motivate minimum entropy regularization, which enables to incorporate unlabeled data in the standard supervised learning. Our approach includes other approaches to the semi-supervised problem as particular or limiting cases. A series of experiments illustrates that the proposed solution benefits from unlabeled data. The method challenges mixture models when the data are sampled from the distribution class spanned by the generative model. The performances are definitely in favor of minimum entropy regularization when generative models are misspecified, and the weighting of unlabeled data provides robustness to the violation of the \"cluster assumption\". Finally, we also illustrate that the method can also be far superior to manifold learning in high dimension spaces."},
{"Title": "Brain inspired reinforcement learning", "URL": "https://dl.acm.org/doi/10.5555/2976040.2976182", "Full Abstract": "Successful application of reinforcement learning algorithms often involves considerable hand-crafting of the necessary non-linear features to reduce the complexity of the value functions and hence to promote convergence of the algorithm. In contrast, the human brain readily and autonomously finds the complex features when provided with sufficient training. Recent work in machine learning and neurophysiology has demonstrated the role of the basal ganglia and the frontal cortex in mammalian reinforcement learning. This paper develops and explores new reinforcement learning algorithms inspired by neurological evidence that provides potential new approaches to the feature construction problem. The algorithms are compared and evaluated on the Acrobot task."},
{"Title": "The curse of highly variable functions for local kernel machines", "URL": "https://dl.acm.org/doi/10.5555/2976248.2976262", "Full Abstract": "We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior - with similarity between examples expressed with a local kernel - are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semi-supervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at"},
{"Title": "Non-local manifold parzen windows", "URL": "https://dl.acm.org/doi/10.5555/2976248.2976263", "Full Abstract": "To escape from the curse of dimensionality, we claim that one can learn non-local functions, in the sense that the value and shape of the learned function at"},
{"Title": "Convex neural networks", "URL": "https://dl.acm.org/doi/10.5555/2976248.2976264", "Full Abstract": "Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors."},
{"Title": "The ", "URL": "https://dl.acm.org/doi/10.1007/11766247_42", "Full Abstract": "We describe a general method to transform a non-markovian sequential decision problem into a supervised learning problem using a"},
{"Title": "Why are long-term large-scale wireless sensor networks difficult", "URL": "https://dl.acm.org/doi/10.1145/1854219.1854224", "Full Abstract": "Wireless sensor networks in the past decade have achieved remarkable progress, while the real applications are still far from being long-term or large-scale. This paper presents GreenOrbs [1], the latest effort to explore the fundamental challenges of long-term large-scale wireless sensor networks. GreenOrbs supports a series of forestry applications. Based on the early experience with GreenOrbs, this paper further discusses the future research directions."},
{"Title": "Revisting Tag Collision Problem in RFID Systems", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2010.27", "Full Abstract": "In RFID systems, the reader is unable to discriminate concurrently reported IDs of tags from the overlapped signals, and a collision happens. Many algorithms for anticollision are proposed to improve the throughput and reduce the latency for tag identification. Existing anti-collision algorithms mainly employ CRC based collision detection functions for determining whether the collision happens. Generating CRC codes, however, requires complicated computations for both RF tags and readers, and hence incurs non-trivial time consumption, becoming the bottleneck. In this study, we design a Quick Collision Detection (QCD) scheme based on the bitwise complement function plus collision preamble, which significantly reduces the number of gates for computation and facilitates to simplify the IC design of RFID tags. The QCD scheme does not require any modification on upperlevel air protocols, so it can be seamlessly adopted by current anti-collision algorithms. Through comprehensive analysis and simulations, we show that QCD improves the identification efficiency by 40%."},
{"Title": "Multicast throughput for large scale cognitive networks", "URL": "https://dl.acm.org/doi/10.1007/s11276-010-0237-3", "Full Abstract": "In this paper, we focus on the achievable throughput of cognitive networks consisting of the primary ad hoc network (PaN) and the secondary ad hoc network (SaN). We construct PaN and SaN by placing nodes according to Poisson point processes of density"},
{"Title": "ETOC", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2010.5762755", "Full Abstract": "Accurate localization is crucial for wireless ad-hoc and sensor networks. Among the localization schemes, component-based approaches specialize in localization performance, which can properly conquer network sparseness and anchor sparseness. However, such design is sensitive to measurement errors. Existing robust localization methods focus on eliminating the positioning error of a single node. Indeed, a single node has two dimensions of freedom in 2D space and only suffers from one type of transformation: translation. As a rigid 2D structure, a component suffers from three possible transformations: translation, rotation, and reflection. A high degree of freedom brings about complicated cases of error productions and difficulties on error controlling. This study is the first work addressing how to deal with ranging noises for component- based methods. By exploiting a set of robust patterns, we present an Error-TOlerant Component-based algorithm (ETOC) that not only inherits the high-performance characteristic of component-based methods, but also achieves robustness of the result. We evaluate ETOC through a real-world sensor network consisting of 120 TelosB motes as well as extensive large-scale simulations. Experiment results show that, comparing with the-state-of-the-art designs, ETOC can work properly in sparse networks and provide more accurate localization results."},
{"Title": "Identification-free batch authentication for RFID tags", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2010.5762764", "Full Abstract": "Cardinality estimation and tag authentication are two major issues in large-scale Radio Frequency Identification (RFID) systems. While there exist both per-tag and probabilistic approaches for the cardinality estimation, the RFID-oriented authentication protocols are mainly per-tag based: the reader authenticates one tag at each time. For a batch of tags, current RFID systems have to identify them and then authenticate each tag sequentially, incurring large volume of authentication data and huge communication cost. We study the RFID batch authentication issue and propose the first probabilistic approach, termed as Single Echo based Batch Authentication (SEBA), to meet the requirement of prompt and reliable batch authentications in large scale RFID applications, e.g., the anti-counterfeiting solution. Without the need of identifying tags, SEBA provides a provable probabilistic guarantee that the percentage of potential counterfeit products is under the user-defined threshold. The experimental result demonstrates the effectiveness of SEBA in fast batch authentications and significant improvement compared to existing approaches."},
{"Title": "Exploring the hidden connectivity in urban vehicular networks", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2010.5762773", "Full Abstract": "The high mobility of VANET makes information exchange across the network excessively difficult. Traditional approaches designed for stationary networks are not applicable due to the high dynamics among the nodes. Applying the routing techniques tailored for general mobile networks inevitably brings huge traffic burden to the crowded urban VANET and leads to low efficiency. To make the information exchange fluent and efficient, we explore the unique features of the urban VANET. By exploring the invariants in the mobile network topology, we are able to efficiently manage the information on top of the \"intersection graph\" transformed from the underlying network of road segments in the urban area. Our approach can thus achieve efficient query dissemination and data retrieval on this information organization. We intensively investigate and analyze a trace that records the movement of more than 4000 taxies in the urban area of Shanghai City over several months. We grasp the key impact of the fundamental factors that affect the VANET behaviors and accordingly develop tailored techniques to maximize the performance of this design. Experimental results validate the effectiveness and efficiency of our design."},
{"Title": "Locating sensors in the wild", "URL": "https://dl.acm.org/doi/10.1145/1869983.1870012", "Full Abstract": "Localization is a fundamental issue of wireless sensor networks that has been extensively studied in the literature. The real-world experience from GreenOrbs, a sensor network system in the forest, shows that localization in the wild remains very challenging due to various interfering factors. In this paper we propose CDL, a Combined and Differentiated Localization approach. The central idea is that ranging quality is the key that determines the overall localization accuracy. In its unremitting pursuit of better ranging quality, CDL incorporates virtual-hop localization, local filtration, and ranging-quality aware calibration. We have implemented CDL and evaluated it by extensive experiments and simulations. The results demonstrate that CDL outperforms current state-of-art approaches with better accuracy, efficiency and consistent performance."},
{"Title": "LISTEN", "URL": "https://dl.acm.org/doi/10.1109/RTSS.2010.15", "Full Abstract": "Recent advances in the application field increasingly demand the use of wireless camera sensor networks (WCSNs), for which localization is a crucial task to enable various location-based services. Most of the existing localization approaches for WCSNs are essentially interactive, i.e. require the interaction among the nodes throughout the localization process. As a result, they are costly to realize in practice, vulnerable to sniffer attacks, inefficient in energy consumption and computation. In this paper we propose LISTEN, a non-interactive localization approach. Using LISTEN, every camera sensor node only needs to silently listen to the beacon signals from a mobile beacon node and capture a few images until determining its own location. We design the movement trajectory of the mobile beacon node, which guarantees to locate all the nodes successfully. We have implemented LISTEN and evaluated it through extensive experiments. The experimental results demonstrate that it is accurate, efficient, and suitable for WCSNs that consist of low-end camera sensors."},
{"Title": "Beyond trilateration", "URL": "https://dl.acm.org/doi/10.1109/TNET.2010.2049578", "Full Abstract": "The proliferation of wireless and mobile devices has fostered the demand of context-aware applications, in which location is often viewed as one of the most significant contexts. Classically, trilateration is widely employed for testing network localizability; even in many cases, it wrongly recognizes a localizable graph as nonlocalizable. In this study, we analyze the limitation of trilateration-based approaches and propose a novel approach that inherits the simplicity and efficiency of trilateration and, at the same time, improves the performance by identifying more localizable nodes. We prove the correctness and optimality of this design by showing that it is able to locally recognize all one-hop localizable nodes. To validate this approach, a prototype system with 60 wireless sensors is deployed. Intensive and large-scale simulations are further conducted to evaluate the scalability and efficiency of our design."},
{"Title": "Capacity and delay in mobile ad hoc networks under Gaussian channel model", "URL": "https://dl.acm.org/doi/10.1145/1923641.1923650", "Full Abstract": "We study the asymptotic delay and throughput in mobile ad hoc networks where η ad hoc nodes are distributed uniformly on a 2-D square (torus) region of area η. The communications between nodes are characterized by Gaussian Channel model, instead of the simplified protocol model or physical model. The mobility of nodes is characterized by two general broad classes of practical mobility models, i.e., hybrid random walk models and discrete random direction models, which generalize many mobility models used in the literature. Our results either fill the gap in this area or generalize a stream of milestone results on asymptotic capacity, delay, and the tradeoffs developed recently."},
{"Title": "Edge Self-Monitoring for Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2010.72", "Full Abstract": "Local monitoring is an effective mechanism for the security of wireless sensor networks (WSNs). Existing schemes assume the existence of sufficient number of active nodes to carry out monitoring operations. Such an assumption, however, is often difficult for a large-scale sensor network. In this work, we focus on designing an efficient scheme integrated with good self-monitoring capability as well as providing an infrastructure for various security protocols using local monitoring. To the best of our knowledge, we are the first to present the formal study on optimizing network topology for edge self-monitoring in WSNs. We show that the problem is NP-complete even under the unit disk graph (UDG) model and give the upper bound on the approximation ratio in various graph models. We provide polynomial-time approximation scheme (PTAS) algorithms for the problem in some specific graphs, for example, the monitoring-set-bounded graph. We further design two distributed polynomial algorithms with provable approximation ratio. Through comprehensive simulations, we evaluate the effectiveness of our design."},
{"Title": "Component-based localization in sparse wireless networks", "URL": "https://dl.acm.org/doi/10.1109/TNET.2010.2072965", "Full Abstract": "Localization is crucial for wireless ad hoc and sensor networks. As the distance-measurement ranges are often less than the communication ranges for many ranging systems, most communication-dense wireless networks are localization-sparse. Consequently, existing algorithms fail to provide accurate localization supports. In order to address this issue, by introducing the concept of component, we group nodes into components so that nodes are able to better share ranging and anchor knowledge. Operating on the granularity of components, our design, CALL, relaxes two essential restrictions in localization: the node ordering and the anchor distribution. Compared to previous designs, CALL is proven to be able to locate the same number of nodes using the least information. We evaluate the effectiveness of CALL through extensive simulations. The results show that CALL locates 90% nodes in a network with average degree 7.5 and 5% anchors, which outperforms the state-of-the-art design Sweeps by about 40%."},
{"Title": "Algorithms for local sensor synchronization", "URL": "https://dl.acm.org/doi/10.1109/ICDE.2011.5767841", "Full Abstract": "In a wireless sensor network (WSN), each sensor monitors environmental parameters, and reports its readings to a base station, possibly through other nodes. A sensor works in cycles, in each of which it stays active for a fixed duration, and then sleeps until the next cycle. The frequency of such cycles determines the portion of time that a sensor is active, and is the dominant factor on its battery life. The majority of existing work assumes globally synchronized WSN where all sensors have the same frequency. This leads to waste of battery power for applications that entail different accuracy of measurements, or environments where sensor readings have large variability. To overcome this problem, we propose LS, a query processing framework for locally synchronized WSN. We consider that each sensor ni has a distinct sampling frequency fi, which is determined by the application or environment requirements. The complication of LS is that ni has to wake up with a network frequency Fi≥fi, in order to forward messages of other sensors. Our goal is to minimize the sum of Fi without delaying packet transmissions. Specifically, given a routing tree, we first present a dynamic programming algorithm that computes the optimal network frequency of each sensor; then, we develop a heuristic for finding the best tree topology, if this is not fixed in advance."},
{"Title": "GreenOrbs", "URL": "https://dl.acm.org/doi/10.5555/1996686.1996737", "Full Abstract": "No abstract available."},
{"Title": "The temporal logic of programs", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1977.32", "Full Abstract": "A unified approach to program verification is suggested, which applies to both sequential and parallel programs. The main proof method suggested is that of temporal reasoning in which the time dependence of events is the basic concept. Two formal systems are presented for providing a basis for temporal reasoning. One forms a formalization of the method of intermittent assertions, while the other is an adaptation of the tense logic system Kb, and is particularly suitable for reasoning about concurrent programs."},
{"Title": "A proof method for cyclic programs", "URL": "https://dl.acm.org/doi/10.1007/BF00289074", "Full Abstract": "We consider the specification and verification of"},
{"Title": "The Temporal Semantics of Concurrent Programs", "URL": "https://dl.acm.org/doi/10.5555/647172.716123", "Full Abstract": "No abstract available."},
{"Title": "On the temporal analysis of fairness", "URL": "https://dl.acm.org/doi/10.1145/567446.567462", "Full Abstract": "The use of the temporal logic formalism for program reasoning is reviewed. Several aspects of responsiveness and fairness are analyzed, leading to the need for an additional temporal operator: the 'until' operator -U. Some general questions involving the 'until' operator are then discussed. It is shown that with the addition of this operator the temporal language becomes expressively complete. Then, two deductive systems DX and DUX are proved to be complete for the languages without and with the new operator respectively."},
{"Title": "A linear history semantics for distributed languages extended abstract", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1980.5", "Full Abstract": "A denotational semantics is given for a distributed language based on communication (CSP). The semantics uses linear sequences of communications to record computations; for any well formed program segment the semantics is a relation between attainable states and the communication sequences needed to attain these states. In binding two or more processes we match and merge the communication sequences assumed by each process to obtain a sequence and State of the combined process. The approach taken here is distinguished by relatively simple semantic domains and ordering."},
{"Title": "Further Results on Propositional Dynamic Logic of Nonregular Programs", "URL": "https://dl.acm.org/doi/10.5555/648063.747451", "Full Abstract": "No abstract available."},
{"Title": "Finite Models for Deterministic Propositional Dynamic Logic", "URL": "https://dl.acm.org/doi/10.5555/646235.682560", "Full Abstract": "No abstract available."},
{"Title": "Impartiality, Justice and Fairness", "URL": "https://dl.acm.org/doi/10.5555/646235.682695", "Full Abstract": "No abstract available."},
{"Title": "Propositional dynamic logic of context-free programs", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1981.38", "Full Abstract": "The borderline between decidable and undecidable Propositional Dynamic Logic (PDL) is sought when iterative programs represented by regular expressions are augmented with increasingly more complex recursive programs represented by context-free languages. The results in this paper and its companion [HPS] indicate that this line is extremely close to the original regular PDL. The main result of the present paper is: The validity problem for PDL with additional programs αΔ(β)γΔ for regular α, β and γ, defined as Uiαi; β; γi, is Π11-complete. One of the results of [HPS] shows that the single program AΔ(B) AΔ for atomic A and B is actually sufficient for obtaining Π11- completeness. However, the proofs of this paper use different techniques which seem to be worthwhile in their own right."},
{"Title": "Termination of probabilistic concurrent programs", "URL": "https://dl.acm.org/doi/10.1145/582153.582154", "Full Abstract": "The asynchronous execution behavior of several concurrent processes, which may use randomization, is studied. Viewing each process as a discrete Markov chain over the set of common execution states, we give necessary and sufficient conditions for the processes to converge almost surely to a given set of goal states, under any fair, but otherwise arbitrary schedule, provided that the state space is finite. (These conditions can be checked mechanically.) An interesting feature of the proof method is that it depends only on the topology of the transitions and not on the actual values of the probabilities. We also show that in our model synchronization protocols that use randomization are in certain cases no more powerful than deterministic protocols. This is demonstrated by (a) Proving lower bounds on the size of a shared variable necessary to ensure mutual exlusion and lockout-free behavior of the protocol; and (b) Showing that no fully symmetric 'randomized' protocol can ensure mutual exclusion and freedom from lockout."},
{"Title": "Is the interesting part of process logic uninteresting?", "URL": "https://dl.acm.org/doi/10.1145/582153.582189", "Full Abstract": "With the (necessary) condition that atomic programs in PL be binary, we present an algorithm for the translation of a PL formula X into a PDL program τ (X) such that a finite path satisfies X iff it belongs to τ (X). This reduction has two immediate corollaries: 1) validity in this PL can be tested by testing validity of formulas in PDL; 2) all finite-path program properties expressible in this PL are expressible in PDL.The translation, however, seems to be of non-elementary time complexity. The significance of the result to the search for natural and powerful logics of programs is discussed."},
{"Title": "Rendezvous with ADA", "URL": "https://dl.acm.org/doi/10.1145/3304133.3304152", "Full Abstract": "A fragment of ADA abstracting the communication and synchronization part is studied. An operational semantics for this fragment is given, emphasizing the justice and fairness aspects of the selection mechanisms. An appropriate notion of fairness is shown to be equivalent to the explicit entry-queues proposed in the reference manual. Proof rules for invariance and liveness properties are given and illustrated on an example. The proof rules are based on temporal logic."},
{"Title": "Representational power of restricted boltzmann machines and deep belief networks", "URL": "https://dl.acm.org/doi/10.1162/neco.2008.04-07-510", "Full Abstract": "Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs."},
{"Title": "Classification using discriminative restricted Boltzmann machines", "URL": "https://dl.acm.org/doi/10.1145/1390156.1390224", "Full Abstract": "Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting."},
{"Title": "Extracting and composing robust features with denoising autoencoders", "URL": "https://dl.acm.org/doi/10.1145/1390156.1390294", "Full Abstract": "Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite."},
{"Title": "Zero-data learning of new tasks", "URL": "https://dl.acm.org/doi/10.5555/1620163.1620172", "Full Abstract": "We introduce the problem of zero-data learning, where a model must generalize to classes or tasks for which no training data are available and only a description of the classes or tasks are provided. Zero-data learning is useful for problems where the set of classes to distinguish or tasks to solve is very large and is not entirely covered by the training data. The main contributions of this work lie in the presentation of a general formalization of zero-data learning, in an experimental analysis of its properties and in empirical evidence showing that generalization is possible and significant in this context. The experimental work of this paper addresses two classification problems of character recognition and a multitask ranking problem in the context of drug discovery. Finally, we conclude by discussing how this new framework could lead to a novel perspective on how to extend machine learning towards AI, where an agent can be given a specification for a learning problem before attempting to solve it (with very few or even zero examples)."},
{"Title": "Learning Deep Architectures for AI", "URL": "https://dl.acm.org/doi/10.1561/2200000006", "Full Abstract": "Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need"},
{"Title": "Quadratic features and deep architectures for chunking", "URL": "https://dl.acm.org/doi/10.5555/1620853.1620921", "Full Abstract": "We experiment with several chunking models. Deeper architectures achieve better generalization. Quadratic filters, a simplification of a theoretical model of V1 complex cells, reliably increase accuracy. In fact, logistic regression with quadratic filters outperforms a standard single hidden layer neural network. Adding quadratic filters to logistic regression is almost as effective as feature engineering. Despite predicting each output label independently, our model is competitive with ones that use previous decisions."},
{"Title": "Justifying and generalizing contrastive divergence", "URL": "https://dl.acm.org/doi/10.1162/neco.2008.11-07-647", "Full Abstract": "We study an expansion of the log likelihood in undirected graphical models such as the restricted Boltzmann machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector in RBMs). We are particularly interested in estimators of the gradient of the log likelihood obtained through this expansion. We show that its residual term converges to zero, justifying the use of a truncation---running only a short Gibbs chain, which is the main idea behind the contrastive divergence (CD) estimator of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked autoassociators. The derivation is not specific to the particular parametric forms used in RBMs and requires only convergence of the Gibbs chain. We present theoretical and empirical evidence linking the number of Gibbs steps"},
{"Title": "Exploring Strategies for Training Deep Neural Networks", "URL": "https://dl.acm.org/doi/10.5555/1577069.1577070", "Full Abstract": "Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms."},
{"Title": "Incorporating Functional Knowledge in Neural Networks", "URL": "https://dl.acm.org/doi/10.5555/1577069.1577111", "Full Abstract": "Incorporating prior knowledge of a particular task into the architecture of a learning algorithm can greatly improve generalization performance. We study here a case where we know that the function to be learned is non-decreasing in its two arguments and convex in one of them. For this purpose we propose a class of functions similar to multi-layer neural networks but (1) that has those properties, (2) is a universal approximator of Lipschitz functions with these and other properties. We apply this new class of functions to the task of modelling the price of call options. Experiments show improvements on regressing the price of call options using the new types of function classes that incorporate the"},
{"Title": "Curriculum learning", "URL": "https://dl.acm.org/doi/10.1145/1553374.1553380", "Full Abstract": "Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them \"curriculum learning\". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions)."},
{"Title": "Workshop summary: Workshop on learning feature hierarchies", "URL": "https://dl.acm.org/doi/10.1145/1553374.1553543", "Full Abstract": "No abstract available."},
{"Title": "A hybrid pareto mixture for conditional asymmetric fat-tailed distributions", "URL": "https://dl.acm.org/doi/10.1109/TNN.2009.2016339", "Full Abstract": "In many cases, we observe some variables"},
{"Title": "Learning Deep Architectures for AI", "URL": "https://dl.acm.org/doi/book/10.5555/1816498", "Full Abstract": "Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area."},
{"Title": "Link quality aware code dissemination in wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2011.6089086", "Full Abstract": "Wireless reprogramming is a crucial technique for software deployment in wireless sensor networks (WSNs). Code dissemination is a basic building block to enable wireless repro-gramming. We present ECD, an Efficient Code Dissemination protocol leveraging 1-hop link quality information. Compared to prior works, ECD has three salient features. First, it supports dynamically configurable packet sizes. By increasing the packet size for high PHY rate radios, it significantly improves the transmission efficiency. Second, it employs an accurate sender selection algorithm to mitigate transmission collisions and transmissions over poor links. Third, it employs a simple impact-based backoff timer design to shorten the time spent in coordinating multiple eligible senders so that the largest impact sender is most likely to transmit. We implement ECD based on TinyOS and evaluate its performance extensively. Testbed experiments show that ECD outperforms state-of-the-art protocols, Deluge and MNP, in terms of completion time and data traffic. (e.g., about 20% less traffic and 20 -- 30% shorter completion time compared to Deluge)."},
{"Title": "Sweep Coverage with Mobile Sensors", "URL": "https://dl.acm.org/doi/10.1109/TMC.2010.237", "Full Abstract": "Many efforts have been made for addressing coverage problems in sensor networks. They fall into two categories, full coverage and barrier coverage, featured as static coverage. In this work, we study a new coverage scenario, sweep coverage, which differs with the previous static coverage. In sweep coverage, we only need to monitor certain points of interest (POIs) periodically so the coverage at each POI is time-variant, and thus we are able to utilize a small number of mobile sensors to achieve sweep coverage among a much larger number of POIs. We investigate the definitions and model for sweep coverage. Given a set of POIs and their sweep period requirements, we prove that determining the minimum number of required sensors (min-sensor sweep-coverage problem) is NP-hard, and it cannot be approximated within a factor of 2. We propose a centralized algorithm with constant approximation ratio 3 for the min-sensor sweep-coverage problem. We further characterize the nonlocality of the problem and design a distributed sweep algorithm, DSWEEP, cooperating sensors to provide efficiency with the best effort. We conduct extensive simulations to study the performance of the proposed algorithms. Our simulations show that DSWEEP outperforms the randomized scheme in both effectiveness and efficiency."},
{"Title": "SenSpire OS", "URL": "https://dl.acm.org/doi/10.1109/TC.2011.58", "Full Abstract": "The development of a modern sensor network is difficult because of the long-term unattended operation mode, diverse application requirements, and stringent resource constraints. To address these issues, we present SenSpire OS, a predictable, flexible, and efficient operating system for wireless sensor networks. We improve system predictability by two-phase interrupt servicing and predictable thread synchronization; we achieve system flexibility by providing a hybrid model for both event-driven programming and multithreaded programming; we retain system efficiency by employing stack sharing and modular design. Moreover, we have designed a three-layer networking stack and an object-oriented programming language (CSpire) to enhance system usability and programming convenience. Having implemented SenSpire OS on three most commonly used sensor node platforms, we evaluate its performance extensively. Results show that SenSpire OS ensures predictable system performance, provides a flexible hybrid model for application programming, and is efficient in resource utilization."},
{"Title": "Topological detection on wormholes in wireless ad hoc and sensor networks", "URL": "https://dl.acm.org/doi/10.1109/TNET.2011.2163730", "Full Abstract": "Wormhole attack is a severe threat to wireless ad hoc and sensor networks. Most existing countermeasures either require specialized hardware devices or make strong assumptions on the network in order to capture the specific (partial) symptom induced by wormholes. Those requirements and assumptions limit the applicability of previous approaches. In this paper, we present our attempt to understand the impact and inevitable symptom of wormholes and develop distributed detection methods by making as few restrictions and assumptions as possible. We fundamentally analyze the wormhole problem using a topology methodology and propose an effective distributed approach, which relies solely on network connectivity information, without any requirements on special hardware devices or any rigorous assumptions on network properties. We formally prove the correctness of this design in continuous geometric domains and extend it into discrete domains. We evaluate its performance through extensive simulations."},
{"Title": "A Survey of Green Mobile Networks", "URL": "https://dl.acm.org/doi/10.1007/s11036-011-0316-4", "Full Abstract": "The explosive development of Information and Communication Technology (ICT) has significantly enlarged both the energy demands and the"},
{"Title": "BloomCast", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2011.168", "Full Abstract": "Efficient and effective full-text retrieval in unstructured peer-to-peer networks remains a challenge in the research community. First, it is difficult, if not impossible, for unstructured P2P systems to effectively locate items with guaranteed recall. Second, existing schemes to improve search success rate often rely on replicating a large number of item replicas across the wide area network, incurring a large amount of communication and storage costs. In this paper, we propose BloomCast, an efficient and effective full-text retrieval scheme, in unstructured P2P networks. By leveraging a hybrid P2P protocol, BloomCast replicates the items uniformly at random across the P2P networks, achieving a guaranteed recall at a communication cost of O(\\sqrt{N), where N is the size of the network. Furthermore, by casting Bloom Filters instead of the raw documents across the network, BloomCast significantly reduces the communication and storage costs for replication. We demonstrate the power of BloomCast design through both mathematical proof and comprehensive simulations based on the query logs from a major commercial search engine and NIST TREC WT10G data collection. Results show that BloomCast achieves an average query recall of 91 percent, which outperforms the existing WP algorithm by 18 percent, while BloomCast greatly reduces the search latency for query processing by 57 percent."},
{"Title": "Optimizing Bloom Filter Settings in Peer-to-Peer Multikeyword Searching", "URL": "https://dl.acm.org/doi/10.1109/TKDE.2011.14", "Full Abstract": "Peer-to-Peer multikeyword searching requires distributed intersection/union operations across wide area networks, raising a large amount of traffic cost. Existing schemes commonly utilize Bloom Filters (BFs) encoding to effectively reduce the traffic cost during the intersection/union operations. In this paper, we address the problem of optimizing the settings of a BF. We show, through mathematical proof, that the optimal setting of BF in terms of traffic cost is determined by the statistical information of the involved inverted lists, not the minimized false positive rate as claimed by previous studies. Through numerical analysis, we demonstrate how to obtain optimal settings. To better evaluate the performance of this design, we conduct comprehensive simulations on TREC WT10G test collection and query logs of a major commercial web search engine. Results show that our design significantly reduces the search traffic and latency of the existing approaches."},
{"Title": "ACM China Council", "URL": "https://dl.acm.org/doi/10.1145/2133806.2133807", "Full Abstract": "No abstract available."},
{"Title": "Scaling Laws of Multicast Capacity for Power-Constrained Wireless Networks under Gaussian Channel Model", "URL": "https://dl.acm.org/doi/10.1109/TC.2011.63", "Full Abstract": "We study the asymptotic networking-theoretic multicast capacity bounds for random extended networks (REN) under Gaussian channel model, in which all wireless nodes are individually power-constrained. During the transmission, the power decays along path with attenuation exponent \\alpha > 2. In REN, n nodes are randomly distributed in the square region of side length \\sqrt{n. There are n_s randomly and independently chosen multicast sessions. Each multicast session has n_d+1 randomly chosen terminals, including one source and n_d destinations. By effectively combining two types of routing and scheduling strategies, we analyze the asymptotic achievable throughput for all n_s=\\omega (1) and n_d. As a special case of our results, we show that for n_s=\\Theta (n), the per-session multicast capacity for REN is of order \\Theta ({1\\over \\sqrt{n_d n) when n_d=O({n\\over ({\\log n)^{\\alpha +1 ) and is of order \\Theta ({1\\over n_d \\cdot (\\log n)^{-{\\alpha \\over 2 ) when n_d=\\Omega ({n\\over \\log n )."},
{"Title": "Bulk data dissemination in wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1016/j.comnet.2012.04.007", "Full Abstract": "Wireless sensor networks (WSNs) have recently gained a great deal of attention as a topic of research, with a wide range of applications being explored. Bulk data dissemination is a basic building block for sensor network applications. The problem of designing efficient bulk data dissemination protocols has been addressed in a number of recent studies. The problem of accurately analyzing the performance of these protocols, however, has not been addressed sufficiently in the literature. In this work, we show a way of accurately analyzing the performance of bulk data dissemination protocols in WSNs. Our model can be applied to practical network topologies by use of the shortest propagation path. Our model is accurate by considering topological information, impact of contention, and impact of pipelining. We validate the analytical results through testbeds and detailed simulations. Results show that the analytical results fit well with the testbed results and simulation results. Further, we demonstrate that the analytical results can be used to aid protocol design for performance optimizations, e.g., page size tuning for shortening the completion time."},
{"Title": "Energy-Efficient Reverse Skyline Query Processing over Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TKDE.2011.64", "Full Abstract": "Reverse skyline query plays an important role in many sensing applications, such as environmental monitoring, habitat monitoring, and battlefield monitoring. Due to the limited power supplies of wireless sensor nodes, the existing centralized approaches, which do not consider energy efficiency, cannot be directly applied to the distributed sensor environment. In this paper, we investigate how to process reverse skyline queries energy efficiently in wireless sensor networks. Initially, we theoretically analyzed the properties of reverse skyline query and proposed a skyband-based approach to tackle the problem of reverse skyline query answering over wireless sensor networks. Then, an energy-efficient approach is proposed to minimize the communication cost among sensor nodes of evaluating range reverse skyline query. Moreover, optimization mechanisms to improve the performance of multiple reverse skylines are also discussed. Extensive experiments on both real-world data and synthetic data have demonstrated the efficiency and effectiveness of our proposed approaches with various experimental settings."},
{"Title": "Theory and network applications of balanced kautz tree structures", "URL": "https://dl.acm.org/doi/10.1145/2220352.2220355", "Full Abstract": "In order to improve scalability and to reduce the maintenance overhead for structured peer-to-peer (P2P) networks, researchers have proposed architectures based on several interconnection networks with a fixed-degree and a logarithmical diameter. Among existing fixed-degree interconnection networks, the Kautz digraph has many distinctive topological properties compared to others. It, however, requires that the number of peers have the some given values, determined by peer degree and network diameter. In practice, we cannot guarantee how many peers will join a P2P network at a given time, since a P2P network is typically dynamic with peers frequently entering and leaving. To address such an issue, we propose the balanced Kautz tree and Kautz ring structures. We further design a novel structured P2P system, called BAKE, based on the two structures that has the logarithmical diameter and constant degree, even the number of peers is an arbitrary value. By keeping a total ordering of peers and employing a robust locality-preserved resource placement strategy, resources that are similar in a single or multidimensional attributes space are stored on the same peer or neighboring peers. Through analysis and simulation, we show that BAKE achieves the optimal diameter and as good a connectivity as the Kautz digraph does (almost achieves the Moore bound), and supports the exact as well as the range queries efficiently. Indeed, the structures of balanced Kautz tree and Kautz ring we propose can also be applied to other interconnection networks after minimal modifications, for example, the de Bruijn digraph."},
{"Title": "Understanding Node Localizability of Wireless Ad Hoc and Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TMC.2011.122", "Full Abstract": "Location awareness is highly critical for wireless ad-hoc and sensor networks. Many efforts have been made to solve the problem of whether or not a network can be localized. Nevertheless, based on the data collected from a working sensor network, it is observed that the network is not always entirely localizable. Theoretical analyses also suggest that, in most cases, it is unlikely that all nodes in a network are localizable, although a (large) portion of the nodes can be uniquely located. Existing studies merely examine whether or not a network is localizable as a whole; yet two fundamental questions remain unaddressed: First, given a network configuration, whether or not a specific node is localizable? Second, how many nodes in a network can be located and which are them? In this study, we analyze the limitation of previous works and propose a novel concept of node localizability. By deriving the necessary and sufficient conditions for node localizability, for the first time, it is possible to analyze how many nodes one can expect to locate in sparsely or moderately connected networks. To validate this design, we implement our solution on a real-world system and the experimental results show that node localizability provides useful guidelines for network deployment and other location-based services."},
{"Title": "Locating in fingerprint space", "URL": "https://dl.acm.org/doi/10.1145/2348543.2348578", "Full Abstract": "Indoor localization is of great importance for a range of pervasive applications, attracting many research efforts in the past decades. Most radio-based solutions require a process of site survey, in which radio signatures of an interested area are annotated with their real recorded locations. Site survey involves intensive costs on manpower and time, limiting the applicable buildings of wireless localization worldwide. In this study, we investigate novel sensors integrated in modern mobile phones and leverage user motions to construct the radio map of a floor plan, which is previously obtained only by site survey. On this basis, we design LiFS, an indoor localization system based on off-the-shelf WiFi infrastructure and mobile phones. LiFS is deployed in an office building covering over 1600m"},
{"Title": "There Exit Decidable Context Free Propositional Dynamic Logics", "URL": "https://dl.acm.org/doi/10.5555/648064.747595", "Full Abstract": "No abstract available."},
{"Title": "Termination of Probabilistic Concurrent Program", "URL": "https://dl.acm.org/doi/10.1145/2166.357214", "Full Abstract": "Copyright © 1983 ACM."},
{"Title": "Symmetric and Economical Solutions to the Mutual Exclusion Problem in a Distributed System (Extended Abstract)", "URL": "https://dl.acm.org/doi/10.5555/646237.682887", "Full Abstract": "No abstract available."},
{"Title": "On the extremely fair treatment of probabilistic algorithms", "URL": "https://dl.acm.org/doi/10.1145/800061.808757", "Full Abstract": "A proof system based on linear temporal logic for the qualitative verification of concurrent probabilistic programs is proposed. The concept of extreme fairness is introduced as an approximation to the notion of probabilistic executions. The proof system proposed is shown to be relatively complete with respect to validity over all extremely fair computations. The proof methodology is demonstrated by proving correctness of a new probabilistic algorithm for solving the mutual exclusion problem ([CLP])."},
{"Title": "On the scope of static checking in definitional languages", "URL": "https://dl.acm.org/doi/10.1145/800171.809622", "Full Abstract": "The paper concerns the use in software development of a class of very high level languages characterized as"},
{"Title": "Temporal verification of carrier-sense local area network protocols", "URL": "https://dl.acm.org/doi/10.1145/800017.800516", "Full Abstract": "We examine local area network protocols and verify the correctness of two representative algorithms using temporal logic. We introduce an interval temporal logic that allows us to make assertions of the form “in the next k units, X holds.” This logic encodes intuitive arguments about contention protocols quite directly. We present two proofs of an Ethernet-like contention protocol, one using the interval temporal logic and one using classical temporal logic. We also verify a contention-free protocol using an invariant that seems to have wide applicability for such protocols."},
{"Title": "Verification of probabilistic programs", "URL": "https://dl.acm.org/doi/10.1137/0213021", "Full Abstract": "No abstract available."},
{"Title": "A linear-history semantics for languages for distributed programming", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2884%2990022-7", "Full Abstract": "No abstract available."},
{"Title": "A Hardware Implementation of the CSP Primitives and its Verification", "URL": "https://dl.acm.org/doi/10.5555/646238.683211", "Full Abstract": "No abstract available."},
{"Title": "Verification of multiprocess probabilistic protocols", "URL": "https://dl.acm.org/doi/10.1145/800222.806732", "Full Abstract": "A new probabilistic symmetric solution to the"},
{"Title": "Is the interesting part of process logic uninteresting? A translation from PL to PDL", "URL": "https://dl.acm.org/doi/10.1137/0213051", "Full Abstract": "No abstract available."},
{"Title": "Now you may compose temporal logic specifications", "URL": "https://dl.acm.org/doi/10.1145/800057.808665", "Full Abstract": "A compositional temporal logic proof system for the specification and verification of concurrent programs is presented. Versions of the system are developed for shared variables and communication based programming languages that include procedures."},
{"Title": "Checking that finite state concurrent programs satisfy their linear specification", "URL": "https://dl.acm.org/doi/10.1145/318593.318622", "Full Abstract": "We present an algorithm for checking satisfiability of a linear time temporal logic formula over a finite state concurrent program. The running time of the algorithm is exponential in the size of the formula but linear in the size of the checked program. The algorithm yields also a formal proof in case the formula is valid over the program. The algorithm has four versions that check satisfiability by unrestricted, impartial, just and fair computations of the given program."},
{"Title": "Domain adaptation for large-scale sentiment classification", "URL": "https://dl.acm.org/doi/10.5555/3104482.3104547", "Full Abstract": "The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classifiers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains."},
{"Title": "Contractive auto-encoders", "URL": "https://dl.acm.org/doi/10.5555/3104482.3104587", "Full Abstract": "We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining."},
{"Title": "Large-scale learning of embeddings with reconstruction sampling", "URL": "https://dl.acm.org/doi/10.5555/3104482.3104601", "Full Abstract": "In this paper, we present a novel method to speed up the learning of embeddings for large-scale learning tasks involving very sparse data, as is typically the case for Natural Language Processing tasks. Our speed-up method has been developed in the context of Denoising Auto-encoders, which are trained in a purely unsupervised way to capture the input distribution, and learn embeddings for words and text similar to earlier neural language models. The main contribution is a new method to approximate reconstruction error by a sampling procedure. We show how this approximation can be made to obtain an unbiased estimator of the training criterion, and we show how it can be leveraged to make learning much more computationally efficient. We demonstrate the effectiveness of this method on the Amazon and RCV1 NLP datasets. Instead of reducing vocabulary size to make learning practical, our method allows us to train using very large vocabularies. In particular, reconstruction sampling requires 22x less training time on the full Amazon dataset."},
{"Title": "Unsupervised models of images by spike-and-slab RBMs", "URL": "https://dl.acm.org/doi/10.5555/3104482.3104626", "Full Abstract": "The"},
{"Title": "Deep learning of representations for unsupervised and transfer learning", "URL": "https://dl.acm.org/doi/10.5555/3045796.3045800", "Full Abstract": "Deep learning algorithms seek to exploit the unknown structure in the input distribution in order to discover good representations, often at multiple levels, with higher-level learned features defined in terms of lower-level features. The objective is to make these higher-level representations more abstract, with their individual features more invariant to most of the variations that are typically present in the training distribution, while collectively preserving as much as possible of the information in the input. Ideally, we would like these representations to disentangle the unknown factors of variation that underlie the training distribution. Such unsupervised learning of representations can be exploited usefully under the hypothesis that the input distribution"},
{"Title": "Unsupervised and transfer learning challenge", "URL": "https://dl.acm.org/doi/10.5555/3045796.3045806", "Full Abstract": "Learning good representations from a large set of unlabeled data is a particularly challenging task. Recent work (see Bengio (2009) for a review) shows that training deep architectures is a good way to extract such representations, by extracting and disentangling gradually higher-level factors of variation characterizing the input distribution. In this paper, we describe different kinds of layers we trained for learning representations in the setting of the Unsupervised and Transfer Learning Challenge. The strategy of our team won the final phase of the challenge. It combined and stacked different one-layer unsupervised learning algorithms, adapted to each of the five datasets of the competition. This paper describes that strategy and the particular one-layer learning algorithms feeding a simple linear classifier with a tiny number of labeled training samples (1 to 64 per class)."},
{"Title": "Quickly generating representative samples from an rbm-derived process", "URL": "https://dl.acm.org/doi/10.1162/NECO_a_00158", "Full Abstract": "Two recently proposed learning algorithms, herding and fast persistent contrastive divergence (FPCD), share the following interesting characteristic: they exploit changes in the model parameters while sampling in order to escape modes and mix better during the sampling process that is part of the learning algorithm. We justify such approaches as ways to escape modes while keeping approximately the same asymptotic distribution of the Markov chain. In that spirit, we extend FPCD using an idea borrowed from Herding in order to obtain a pure sampling algorithm, which we call the rates-FPCD sampler. Interestingly, this sampler can improve the model as we collect more samples, since it optimizes a lower bound on the log likelihood of the training data. We provide empirical evidence that this new algorithm displays substantially better and more robust mixing than Gibbs sampling."},
{"Title": "Learning structured embeddings of knowledge bases", "URL": "https://dl.acm.org/doi/10.5555/2900423.2900470", "Full Abstract": "Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigid symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like natural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning methods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text."},
{"Title": "Higher order contractive auto-encoder", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-23783-6_41", "Full Abstract": "We propose a novel regularizer when training an auto-encoder for unsupervised feature extraction. We explicitly encourage the latent representation to contract the input space by regularizing the norm of the Jacobian (analytically) and the Hessian (stochastically) of the encoder's output with respect to its input, at the training points. While the penalty on the Jacobian's norm ensures robustness to tiny corruption of samples in the input space, constraining the norm of the Hessian extends this robustness when moving further away from the sample. From a manifold learning perspective, balancing this regularization with the auto-encoder's reconstruction objective yields a representation that varies most when moving along the data manifold in input space, and is most insensitive in directions orthogonal to the manifold. The second order regularization, using the Hessian, penalizes curvature, and thus favors smooth manifold. We show that our proposed technique, while remaining computationally efficient, yields representations that are significantly better suited for initializing deep architectures than previously proposed approaches, beating state-of-the-art performance on a number of datasets."},
{"Title": "Higher order contractive auto-encoder", "URL": "https://dl.acm.org/doi/10.5555/2034117.2034159", "Full Abstract": "We propose a novel regularizer when training an autoencoder for unsupervised feature extraction. We explicitly encourage the latent representation to contract the input space by regularizing the norm of the Jacobian (analytically) and the Hessian (stochastically) of the encoder's output with respect to its input, at the training points. While the penalty on the Jacobian's norm ensures robustness to tiny corruption of samples in the input space, constraining the norm of the Hessian extends this robustness when moving further away from the sample. From a manifold learning perspective, balancing this regularization with the auto-encoder's reconstruction objective yields a representation that varies most when moving along the data manifold in input space, and is most insensitive in directions orthogonal to the manifold. The second order regularization, using the Hessian, penalizes curvature, and thus favors smooth manifold. We show that our proposed technique, while remaining computationally efficient, yields representations that are significantly better suited for initializing deep architectures than previously proposed approaches, beating state-of-the-art performance on a number of datasets."},
{"Title": "On the expressive power of deep architectures", "URL": "https://dl.acm.org/doi/10.5555/2050236.2050237", "Full Abstract": "Deep architectures are families of functions corresponding to deep circuits. Deep Learning algorithms are based on parametrizing such circuits and tuning their parameters so as to approximately optimize some training objective. Whereas it was thought too difficult to train deep architectures, several successful algorithms have been proposed in recent years. We review some of the theoretical motivations for deep architectures, as well as some of their practical successes, and propose directions of investigations to address some of the remaining challenges."},
{"Title": "On the expressive power of deep architectures", "URL": "https://dl.acm.org/doi/10.5555/2050345.2050349", "Full Abstract": "Deep architectures are families of functions corresponding to deep circuits. Deep Learning algorithms are based on parametrizing such circuits and tuning their parameters so as to approximately optimize some training objective. Whereas it was thought too difficult to train deep architectures, several successful algorithms have been proposed in recent years. We review some of the theoretical motivations for deep architectures, as well as some of their practical successes, and propose directions of investigations to address some of the remaining challenges."},
{"Title": "Achieving Private, Scalable, and Precise Data Collection in Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/ICPADS.2012.13", "Full Abstract": "Wireless Sensor Networks (WSN) become increasingly popular to collect data over a large area. Given the collected data set, the network manager can extract various kinds of aggregate statistics from the set to characterize the physical space. On the collection of the data, three requirements should be imposed: (1) Privacy: as sensor nodes are source limited and often deployed in an open environment, the sensed data suffer from privacy vulnerabilities. Secure mechanism should be provided to protect data privacy, (2) Communication efficiency: collecting data from large-scale sensor networks often involves large-volume data generation and transmission, which may quickly consume the energy of the WSN. To prolong the lifetimes of the sensor nodes, the sensed data should be transmitted in lightweight manner, (3) Accuracy: the sensed data should be recovered accurately at the base station (BS) so that the manager can manipulate them freely to achieve any precise aggregate statistic he prefers. To satisfy these requirements, we propose two novel privacy-preserving data collection schemes based on compressive sensing techniques. Our schemes address the privacy, communication efficiency and accuracy issues simultaneously. Detailed theoretical analysis and simulation results confirm the high performance of the proposed schemes."},
{"Title": "Ad-hoc Anonymity", "URL": "https://dl.acm.org/doi/10.1109/ICPADS.2012.36", "Full Abstract": "Location-based Service (LBS) becomes increasingly important for wireless and mobile networks. In current LBS schemes, Service Providers (SPs) require users report their accurate locations, which can be illegally used by adversaries to infer sensitive information of users. Privacy disclosure raises serious concerns and limits the application of LBS. Previous solutions either rely on pre-installed centralized intermediary or assume cooperative users. Other distributed solutions, lacking of user obligation, only provide relatively poor anonymity. In this study, we propose a new solution of pseudonym change, which works for non-cooperative users and in the absence of intermediary. The basic idea behind our solution is ad-hoc anonymity. The proposed solution allows users to decide whether or not participating according to their own wills. In addition, artificially generated dummies mix up all the users who have participated in pseudonym changes at different times. Theoretical analysis demonstrates that asynchronous pseudonym change and dummy participation notably enhance privacy protection. We implement our solution on a real-world dataset of mobile phone users, which is collected at Boston, MA, and by the Reality Mining, MIT. The simulation results show that our approach significantly outperforms existing solutions."},
{"Title": "Noninteractive Localization of Wireless Camera Sensors with Mobile Beacon", "URL": "https://dl.acm.org/doi/10.1109/TMC.2011.278", "Full Abstract": "Recent advances in the application field increasingly demand the use of wireless camera sensor networks (WCSNs), for which localization is a crucial task to enable various location-based services. Most of the existing localization approaches for WCSNs are essentially interactive, i.e., require the interaction among the nodes throughout the localization process. As a result, they are costly to realize in practice, vulnerable to sniffer attacks, inefficient in energy consumption and computation. In this paper, we propose LISTEN, a noninteractive localization approach. Using LISTEN, every camera sensor node only needs to silently listen to the beacon signals from a mobile beacon node and capture a few images until determining its own location. We design the movement trajectory of the mobile beacon node, which guarantees to locate all the nodes successfully. We have implemented LISTEN and evaluated it through extensive experiments. Both the analytical and experimental results demonstrate that it is accurate, cost-efficient, and especially suitable for WCSNs that consist of low-end camera sensors."},
{"Title": "Localization of wireless sensor networks in the wild", "URL": "https://dl.acm.org/doi/10.1109/TNET.2012.2200906", "Full Abstract": "Localization is a fundamental issue of wireless sensor networks that has been extensively studied in the literature. Our real-world experience from GreenOrbs, a sensor network system deployed in a forest, shows that localization in the wild remains very challenging due to various interfering factors. In this paper, we propose CDL, a Combined and Differentiated Localization approach for localization that exploits the strength of range-free approaches and range-based approaches using received signal strength indicator (RSSI). A critical observation is that ranging quality greatly impacts the overall localization accuracy. To achieve a better ranging quality, our method CDL incorporates virtual-hop localization, local filtration, and ranging-quality aware calibration. We have implemented and evaluated CDL by extensive real-world experiments in GreenOrbs and large-scale simulations. Our experimental and simulation results demonstrate that CDL outperforms current state-of-art localization approaches with a more accurate and consistent performance. For example, the average location error using CDL in GreenOrbs system is 2.9 m, while the previous best method SISR has an average error of 4.6 m."},
{"Title": "Exploiting Ubiquitous Data Collection for Mobile Users in Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2012.92", "Full Abstract": "We study the ubiquitous data collection for mobile users in wireless sensor networks. People with handheld devices can easily interact with the network and collect data. We propose a novel approach for mobile users to collect the network-wide data. The routing structure of data collection is additively updated with the movement of the mobile user. With this approach, we only perform a limited modification to update the routing structure while the routing performance is bounded and controlled compared to the optimal performance. The proposed protocol is easy to implement. Our analysis shows that the proposed approach is scalable in maintenance overheads, performs efficiently in the routing performance, and provides continuous data delivery during the user movement. We implement the proposed protocol in a prototype system and test its feasibility and applicability by a 49-node testbed. We further conduct extensive simulations to examine the efficiency and scalability of our protocol with varied network settings."},
{"Title": "OFA", "URL": "https://dl.acm.org/doi/10.1016/j.comnet.2013.02.008", "Full Abstract": "Accurate self-localization is a key enabling technique for many pervasive applications. Existing approaches, most of which are multilateration based, often suffer ambiguities, resulting in huge localization errors. To address this problem, previous approaches discard those positioning results with possible flip ambiguities, trading the localization performance for the result robustness. However, the high false positive rate of flip prediction incorrectly rejects many reliable location estimates. By exploiting the characteristics of flip ambiguity, which causes either huge or zero error, we propose the concept of optimistic localization and design an algorithm, OFA, that employs a global consistency check and a location correction phase in the localization process. We analyze the performance gain and cost of OFA, and further evaluate this design through extensive simulations. The results show that OFA obtains robustness with extremely low performance cost, so as to reduce the requirement on the average degree from 25 to 10 for robustly localizing a network."},
{"Title": "Beyond triangle inequality", "URL": "https://dl.acm.org/doi/10.1145/2422966.2422983", "Full Abstract": "Knowing accurate positions of nodes in wireless ad hoc and sensor networks is essential for a wide range of pervasive and mobile applications. However, errors are inevitable in distance measurements and we observe that a small number of outliers can degrade localization accuracy drastically. To deal with noisy and outlier ranging results, triangle inequality, is often employed in existing approaches. Our study shows that triangle inequality has many limitations, which make it far from accurate and reliable. In this study, we formally define the outlier detection problem for network localization and build a theoretical foundation to identify outliers based on graph embeddability and rigidity theory. Our analysis shows that the redundancy of distance measurements plays an important role. We then design a bilateration generic cycles-based outlier detection algorithm, and examine its effectiveness and efficiency through a network prototype implementation of MicaZ motes as well as extensive simulations. The results show that our design significantly improves the localization accuracy by wisely rejecting outliers."},
{"Title": "Fine-Grained Location-Free Planarization in Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TMC.2012.57", "Full Abstract": "Extracting planar graph from network topologies is of great importance for efficient protocol design in wireless ad hoc and sensor networks. Previous techniques of planar topology extraction are often based on ideal assumptions, such as UDG communication model and accurate node location measurements. To make these protocols work effectively in practice, we need extract a planar topology in a location-free and distributed manner with small stretch factors. The planar topologies constructed by current location-free methods often have large stretch factors. In this paper, we present a fine-grained and location-free network planarization method under $(\\rho)$-quasi-UDG communication model with $(\\rho \\ge 1/\\sqrt{2)$. Compared with existing location-free planarization approaches, our method can extract a provably connected planar graph, called topological planar simplification (TPS), from the connectivity graph in a fine-grained manner using local connectivity information. We evaluate our design through extensive simulations and compare with the state-of-the-art approaches. The simulation results show that our method produces high-quality planar graphs with a small stretch factor in practical large-scale networks."},
{"Title": "Asymptotic throughput for large-scale wireless networks with general node density", "URL": "https://dl.acm.org/doi/10.1007/s11276-012-0485-5", "Full Abstract": "We study the asymptotic throughput for a large-scale wireless ad hoc network consisting of"},
{"Title": "Link Scheduling for Exploiting Spatial Reuse in Multihop MIMO Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2012.181", "Full Abstract": "Multiple-Input-Multiple-Output (MIMO) has great potential for enhancing the throughput of multihop wireless networks via spatial multiplexing or spatial reuse. Spatial reuse with Stream Control (SC) provides a considerable improvement of the network throughput over spatial multiplexing. The gain of spatial reuse, however, is still not fully exploited. There exist large numbers of additional data streams, which could be transmitted concurrently with those data streams scheduled by stream control at certain time slots and vicinities. In this paper, we address the issue of MIMO link scheduling to maximize the gain of spatial reuse and thus network throughput. We propose a Receiver-Oriented Interference Suppression model (ROIS), based on which we design both centralized and distributed link scheduling algorithms to fully exploit the gain of spatial reuse in multihop MIMO networks. Further, we address the traffic-aware link scheduling problem by injecting nonuniform traffic load into the network. Through theoretical analysis and comprehensive performance evaluation, we achieve the following results: 1) link scheduling based on ROIS achieves significant higher network throughput than that based on stream control, with any interference range, number of antennas, and average hop length of data flows. 2) The traffic-aware scheduling is enticingly complementary to the link scheduling based on ROIS model. Accordingly, the two scheduling schemes can be combined to further enhance the network throughput."},
{"Title": "Sensor Network Navigation without Locations", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2012.207", "Full Abstract": "We propose a pervasive usage of the sensor network infrastructure as a cyber-physical system for navigating internal users in locations of potential danger. Our proposed application differs from previous work in that they typically treat the sensor network as a media of data acquisition while in our navigation application, in-situ interactions between users and sensors become ubiquitous. In addition, human safety and time factors are critical to the success of our objective. Without any preknowledge of user and sensor locations, the design of an effective and efficient navigation protocol faces nontrivial challenges. We propose to embed a road map system in the sensor network without location information so as to provide users navigating routes with guaranteed safety. We accordingly design efficient road map updating mechanisms to rebuild the road map in the event of changes in dangerous areas. In this navigation system, each user only issues local queries to obtain their navigation route. The system is highly scalable for supporting multiple users simultaneously. We implement a prototype system with 36 TelosB motes to validate the effectiveness of this design. We further conduct comprehensive and large-scale simulations to examine the efficiency and scalability of the proposed approach under various environmental dynamics."},
{"Title": "Message in a Sealed Bottle", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2013.38", "Full Abstract": "Many proximity-based mobile social networks are developed to facilitate connections between any two people, or to help a user to find people with a matched profile within a certain distance. A challenging task in these applications is to protect the privacy of the participants' profiles and personal interests. In this paper, we design novel mechanisms, when given a preference-profile submitted by a user, that search persons with matching-profile in decentralized multi-hop mobile social networks. Our mechanisms also establish a secure communication channel between the initiator and matching users at the time when the matching user is found. Our rigorous analysis shows that our mechanism is privacy-preserving (no participants' profile and the submitted preference-profile are exposed), verifiable (both the initiator and the unmatched user cannot cheat each other to pretend to be matched), and efficient in both communication and computation. Extensive evaluations using real social network data, and actual system implementation on smart phones show that our mechanisms are significantly more efficient than existing solutions."},
{"Title": "MoLoc", "URL": "https://dl.acm.org/doi/10.1109/ICDCS.2013.41", "Full Abstract": "Indoor localization has enabled a great number of mobile and pervasive applications, attracting attentions from researchers worldwide. Most of current solutions rely on Received Signal Strength (RSS) of wireless signals as location fingerprint, to discriminate locations of interest. Fingerprint uniqueness with respect to locations is a basic requirement in these fingerprinting-based solutions. However, due to insufficient number of signal sources, temporal variations of wireless signals, and rich multipath effects, such requirement is not always met in complex indoor environments, which we refer to as fingerprint ambiguity. In this work, we explore the potential of leveraging user motion against fingerprint ambiguity. Our basic idea is that user motion patterns collected by built-in sensors of mobile phones add to the diversity built by RSS fingerprints. On this basis, we propose MoLoc, a motion-assisted localization scheme implemented on mobile phones. MoLoc can easily be integrated in existing localization systems by simply adding a motion database that is constructed automatically by crowdsourcing. We conducted experiments in a large office hall. The experiment results show that MoLoc doubles the localization accuracy achieved by the fingerprinting method, and limits the mean localization error to less than 1m."},
{"Title": "Informative counting", "URL": "https://dl.acm.org/doi/10.1145/2491288.2491299", "Full Abstract": "Many algorithms have been introduced to deterministically authenticate Radio Frequency Identification (RFID) tags, while little work has been done to address the scalability issue in batch authentications. Deterministic approaches verify them one by one, and the communication overhead and time cost grow linearly with increasing size of tags. We design a fine-grained batch authentication scheme, INformative Counting (INC), which achieves sublinear authentication time and communication cost in batch verifications. INC also provides authentication results with accurate estimates of the number of counterfeiting tags and genuine tags, while previous batch authentication methods merely provide 0/1 results indicating the existence of counterfeits. We conduct detailed theoretical analysis and extensive experiments to examine this design and the results show that INC significantly outperforms previous work in terms of effectiveness and efficiency."},
{"Title": "The Glory of the Past", "URL": "https://dl.acm.org/doi/10.5555/648065.747612", "Full Abstract": "No abstract available."},
{"Title": "Proving Termination of Prolog Programs", "URL": "https://dl.acm.org/doi/10.5555/648065.761168", "Full Abstract": "No abstract available."},
{"Title": "Linear and Branching Structures in the Semantics and Logics of Reactive Systems", "URL": "https://dl.acm.org/doi/10.5555/646239.683353", "Full Abstract": "No abstract available."},
{"Title": "Verification of multiprocess probabilistic protocols", "URL": "https://dl.acm.org/doi/10.1007/BF01843570", "Full Abstract": "No abstract available."},
{"Title": "A really abstract concurrent model and its temporal logic", "URL": "https://dl.acm.org/doi/10.1145/512644.512660", "Full Abstract": "In this paper we advance the radical notion that a computational model based on the <i>reals</i> provides a more abstract description of concurrent and reactive systems, than the conventional <i>integers</i> based behavioral model of execution <i>sequences.</i> The real model is studied in the setting of temporal logic, and we illustrate its advantages by providing a <i>fully abstract</i> temporal semantics for a simple concurrent language, and an example of verification of a concurrent program within the real temporal logic defined here. It is shown that, by imposing the crucial condition of <i>finite variability,</i> we achieve a balanced formalism that is insensitive to <i>finite</i> stuttering, but can recognize <i>infinite</i> stuttering, a distinction which is essential for obtaining a fully abstract semantics of non-terminating processes. Among other advantages, going into real-based semantics obviates the need for the controversial representation of concurrency by interleaving, and most of the associated fairness constraints."},
{"Title": "Applications of temporal logic to the specification and verification of reactive systems: a survey of current trends", "URL": "https://dl.acm.org/doi/10.5555/19518.19527", "Full Abstract": "No abstract available."},
{"Title": "Specification and implementation of concurrently accessed data structures: An abstract data type approach", "URL": "https://dl.acm.org/doi/10.5555/28220.28239", "Full Abstract": "No abstract available."},
{"Title": "Specification and Implementation of Concurrently Accessed Data Structures", "URL": "https://dl.acm.org/doi/10.5555/646503.696285", "Full Abstract": "No abstract available."},
{"Title": "Very High Level Concurrent Programming", "URL": "https://dl.acm.org/doi/10.1109/TSE.1987.233791", "Full Abstract": "Concurrent systems are typically large and complex, requiring long, development time and much labor. They are, therefore, prime candidates for simplification and automation of the design and programming process. Their major application areas include real time systems, operating systems and cooperative computation. New applications are emerging with the trends towards wide usage of personal computers connected in a network and towards use of parallel processing in supercomputer architectures."},
{"Title": "Applications of temporal logic to the specification of real time systems (extended abstract)", "URL": "https://dl.acm.org/doi/10.5555/52808.52812", "Full Abstract": "No abstract available."},
{"Title": "Applications of Temporal Logic to the Specification of Real-time Systems", "URL": "https://dl.acm.org/doi/10.5555/646841.706461", "Full Abstract": "No abstract available."},
{"Title": "A framework for the synthesis of reactive modules", "URL": "https://dl.acm.org/doi/10.5555/52824.52825", "Full Abstract": "No abstract available."},
{"Title": "A Framework for the Synthesis of Reactive Modules", "URL": "https://dl.acm.org/doi/10.5555/646724.702877", "Full Abstract": "No abstract available."},
{"Title": "Learning algorithms for the classification restricted Boltzmann machine", "URL": "https://dl.acm.org/doi/10.5555/2503308.2188407", "Full Abstract": "Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning."},
{"Title": "Detonation Classification from Acoustic Signature with the Restricted Boltzmann Machine", "URL": "https://dl.acm.org/doi/10.1111/j.1467-8640.2012.00419.x", "Full Abstract": "We compare the recently proposed Discriminative Restricted Boltzmann Machine (DRBM) to the classical Support Vector Machine (SVM) on a challenging classification task consisting in identifying weapon classes from audio signals. The three weapon classes considered in this work (mortar, rocket, and rocket-propelled grenade), are difficult to reliably classify with standard techniques because they tend to have similar acoustic signatures. In addition, specificities of the data available in this study make it challenging to rigorously compare classifiers, and we address methodological issues arising from this situation. Experiments show good classification accuracy that could make these techniques suitable for fielding on autonomous devices. DRBMs appear to yield better accuracy than SVMs, and are less sensitive to the choice of signal preprocessing and model hyperparameters. This last property is especially appealing in such a task where the lack of data makes model validation difficult."},
{"Title": "Large-scale feature learning with spike-and-slab sparse coding", "URL": "https://dl.acm.org/doi/10.5555/3042573.3042751", "Full Abstract": "We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call"},
{"Title": "A generative process for sampling contractive auto-encoders", "URL": "https://dl.acm.org/doi/10.5555/3042573.3042804", "Full Abstract": "The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error."},
{"Title": "Modeling temporal dependencies in high-dimensional sequences", "URL": "https://dl.acm.org/doi/10.5555/3042573.3042813", "Full Abstract": "We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription."},
{"Title": "Deep learning for NLP (without magic)", "URL": "https://dl.acm.org/doi/10.5555/2390500.2390505", "Full Abstract": "Machine learning is everywhere in today's NLP, but by and large machine learning amounts to numerical optimization of weights for human designed representations and features. The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks. This tutorial aims to cover the basic motivation, ideas, models and learning algorithms in deep learning for natural language processing. Recently, these methods have been shown to perform very well on various NLP tasks such as language modeling, POS tagging, named entity recognition, sentiment analysis and paraphrase detection, among others. The most attractive quality of these techniques is that they can perform well without any external hand-designed resources or time-intensive feature engineering. Despite these advantages, many researchers in NLP are not familiar with these methods. Our focus is on insight and understanding, using graphical illustrations and simple, intuitive derivations. The goal of the tutorial is to make the inner workings of these techniques transparent, intuitive and their results interpretable, rather than black boxes labeled \"magic here\"."},
{"Title": "Disentangling factors of variation for facial expression recognition", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-33783-3_58", "Full Abstract": "We propose a semi-supervised approach to solve the task of emotion recognition in 2D face images using recent ideas in deep learning for handling the factors of variation present in data. An emotion classification algorithm should be both robust to (1) remaining variations due to the pose of the face in the image after centering and alignment, (2) the identity or morphology of the face. In order to achieve this invariance, we propose to learn a hierarchy of features in which we gradually filter the factors of variation arising from both (1) and (2). We address (1) by using a multi-scale contractive convolutional network (CCNET) in order to obtain invariance to translations of the facial traits in the image. Using the feature representation produced by the CCNET, we train a"},
{"Title": "Learning deep physiological models of affect", "URL": "https://dl.acm.org/doi/10.1109/MCI.2013.2247823", "Full Abstract": "More than 15 years after the early studies in Affective Computing (AC), [1] the problem of detecting and modeling emotions in the context of human-computer interaction (HCI) remains complex and largely unexplored. The detection and modeling of emotion is, primarily, the study and use of artificial intelligence (AI) techniques for the construction of computational models of emotion. The key challenges one faces when attempting to model emotion [2] are inherent in the vague definitions and fuzzy boundaries of emotion, and in the modeling methodology followed. In this context, open research questions are still present in all key components of the modeling process. These include, first, the appropriateness of the modeling tool employed to map emotional manifestations and responses to annotated affective states; second, the processing of signals that express these manifestations (i.e., model input); and third, the way affective annotation (i.e., model output) is handled. This paper touches upon all three key components of an affective model (i.e., input, model, output) and introduces the use of deep learning (DL) [3], [4], [5] methodologies for affective modeling from multiple physiological signals."},
{"Title": "Better mixing via deep representations", "URL": "https://dl.acm.org/doi/10.5555/3042817.3042881", "Full Abstract": "It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples."},
{"Title": "On the difficulty of training recurrent neural networks", "URL": "https://dl.acm.org/doi/10.5555/3042817.3043083", "Full Abstract": "There are two widely known issues with properly training recurrent neural networks, the"},
{"Title": "Maxout networks", "URL": "https://dl.acm.org/doi/10.5555/3042817.3043084", "Full Abstract": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called"},
{"Title": "Modeling term dependencies with quantum language models for IR", "URL": "https://dl.acm.org/doi/10.1145/2484028.2484098", "Full Abstract": "Traditional information retrieval (IR) models use bag-of-words as the basic representation and assume that some form of independence holds between terms. Representing term dependencies and defining a scoring function capable of integrating such additional evidence is theoretically and practically challenging. Recently, Quantum Theory (QT) has been proposed as a possible, more general framework for IR. However, only a limited number of investigations have been made and the potential of QT has not been fully explored and tested. We develop a new, generalized Language Modeling approach for IR by adopting the probabilistic framework of QT. In particular, quantum probability could account for both single and compound terms at once without having to extend the term space artificially as in previous studies. This naturally allows us to avoid the weight-normalization problem, which arises in the current practice by mixing scores from matching compound terms and from matching single terms. Our model is the first practical application of quantum probability to show significant improvements over a robust bag-of-words baseline and achieves better performance on a stronger non bag-of-words baseline."},
{"Title": "STAGGER", "URL": "https://dl.acm.org/doi/10.1109/MASS.2013.83", "Full Abstract": "Channel utilization for wireless sensor networks is far from efficient, especially for convergecast in which multiple nodes are sending packets to a receiver. In this paper, we analyze the channel utilization when multiple nodes contend for the channel in convergecast and show that channel utilization can be improved by accumulating packets on each node. However, the number of accumulated packets should be carefully determined. Otherwise, the system performance may not be improved or even be degraded, e.g., incurring additional packet delay. Based on the analysis result, we present STAGGER to achieve channel utilization improvement while guarantee the worst case performance. We implement STAGGER in TinyOS 2.1 and evaluate its performance on TelosB nodes. STAGGER only uses local information to determine the number of accumulated packets without incurring additional overhead. It adopts CSMA at the low level and preserves its nice properties, e.g., fairness. The experimental results show that the design can significantly improve the per-hop throughput and reduce packet loss ratio under high traffic rate."},
{"Title": "Challenges, Designs, and Performances of Large-Scale Open-P2SP Content Distribution", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2012.252", "Full Abstract": "Content distribution on today's Internet operates primarily in two modes: server-based and peer-to-peer (P2P). To leverage the advantages of both modes while circumventing their key limitations, a third mode: peer-to-server/peer (P2SP) has emerged in recent years. Although P2SP can provide efficient hybrid server-P2P content distribution, P2SP generally works in a closed manner by only utilizing its private owned servers to accelerate its private organized peer swarms. Consequently, P2SP still has its limitations in both content abundance and server bandwidth. To this end, the fourth mode (or says a generalized mode of P2SP) has appeared as \"open-P2SP\" that integrates various third-party servers, contents, and data transfer protocols all over the Internet into a large, open, and federated P2SP platform. In this paper, based on a large-scale commercial open-P2SP system named \"QQXuanfeng\", we investigate the key challenging problems, practical designs and real-world performances of open-P2SP. Such \"white-box\" study of open-P2SP provides solid experiences and helpful heuristics to the designers of similar systems."},
{"Title": "Exploiting constructive interference for scalable flooding in wireless networks", "URL": "https://dl.acm.org/doi/10.1109/TNET.2013.2238951", "Full Abstract": "Constructive interference-based flooding (CIBF) is a latency-optimal flooding protocol, which can realize millisecond network flooding latency and submicrosecond time synchronization accuracy, require no network state information, and be adapted to topology changes. However, constructive interference (CI) has a precondition to function, i.e., the maximum temporal displacement Δ of concurrent packet transmissions should be less than a given hardware constrained threshold (e.g., 0.5 µs, for the IEEE 802.15.4 radio). In this paper, we derive the closed-form packet reception ratio (PRR) formula for CIBF and theoretically disclose that CIBF suffers the scalability problem. The packet reception performance of intermediate nodes degrades significantly as the density or the size of the network increases. We analytically show that CIBF has a PRR lower bound (94.5%) in the grid topology. Based on this observation, we propose the spine constructive interference-based flooding (SCIF) protocol for an arbitrary uniformly distributed topology. Extensive simulations show that SCIF floods the entire network much more reliably than the state-of-the-art Glossy protocol does in high-density or large-scale networks. We further explain the root cause of CI with waveform analysis, which is mainly examined in simulations and experiments."},
{"Title": "Finding Best and Worst k-Coverage Paths in Multihop Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2012.329", "Full Abstract": "Coverage is a fundamental problem in wireless sensor networks (WSNs). From both economic and applicable concerns, designers always would like to provide guaranteed QoS of coverage of WSNs. In this paper, we address two path-coverage problems in WSNs, maximum $(k)$-support path coverage (a.k.a. best case coverage) and minimum $(k)$-breach path coverage (a.k.a. worst case coverage), in which every point on the desired resultant path is covered by at least $(k)$ sensors simultaneously while optimizing certain objectives. We present two polynomial-time approaches to find optimal solutions for both maximum $(k)$-support coverage problem and minimum $(k)$-breach coverage problem. The time complexity of both algorithms are $(O(k^2n\\log n))$, where $(n)$ is the number of deployed sensor nodes and $(k)$ is the coverage degree. In addition, a number of properties of $(k)$th-nearest point Voronoi diagram are presented, which is new to the literature."},
{"Title": "D2", "URL": "https://dl.acm.org/doi/10.1109/RTSS.2013.28", "Full Abstract": "Detecting and diagnosing anomalies in networked embedded systems like sensor networks is a very difficult task, due to the variable workloads and severe resource constraints. We notice that most node-level debugging tools can provide detailed program information inside the node but fail to detect when and where a problem occurs in the network. On the other hand, most network-level diagnosis tools can effectively detect a problem from the network but fail to narrow down the problem within the node because they lack detailed program information. To close the gap, we propose D2, a new anomaly detection and diagnosis method by combining program profiling and symptom mining. D2 employs binary instrumentation to perform lightweight function count profiling. Based on the statistics, D2 uses PCA (Principal Component Analysis) based approach for automatically detecting network anomalies. Compared to previous methods, D2 is able to point programmers closer to the most likely causes by a novel approach combining statistical tests and program call graph analysis. We implement our method based on TinyOS 2.1.1 and evaluate its effectiveness by case studies in the development of a working sensor network. Results show that our method is effective for detecting and diagnosing problems in real-world sensor network systems, and at the same time, incurs an acceptable overhead."},
{"Title": "T-CloudDisk", "URL": "https://dl.acm.org/doi/10.1145/2541614.2541623", "Full Abstract": "Cloud storage services such as Dropbox have quickly gained enormous popularity in recent years. They offer users with convenient and reliable approaches to store and share data from anywhere, any device at anytime. However, they are still suffering from the \""},
{"Title": "Illuminations and the revelations", "URL": "https://dl.acm.org/doi/10.1145/2557968.2557974", "Full Abstract": "Wireless sensor networks (WSNs) are developed in an accelerated pace during the past years. Motivated by the need of closely monitoring the forest and urban environments and accurately measuring carbon sequestration and emission, we launched two long-term large-scale WSNs: \"GreenOrbs\" in 2009 and \"CitySee\" in 2011, respectively. This paper presents our recent advances and experience on how to implement the two WSNs with stable and high performance. Some research directions are also discussed based on our observations and measurements."},
{"Title": "On the Feasibility of Gradient-Based Data-Centric Routing Using Bloom Filters", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2013.11", "Full Abstract": "Gradient-based routing using Bloom filters is an effective mechanism to enable data-centric queries in multihop networks. A node compressively describes its data items as a Bloom filter, which is then diffused away to the other nodes with information decay. The Bloom filters form an information potential that eventually navigates queries to the source node by ascending the potential field. The existing designs of Bloom filters, however, have critical limitations with respect to the feasibility of gradient-based routing. The compressed routing entries appear to be noisy. Noise in unrelated routing entries is very likely to equal to even outweigh information in right routing entries, thus blinding a query to its desired destination. This work addresses the root cause of the mismatch between the ideal and the practical performance of gradient-based routing using Bloom filters. We first investigate the impact of decaying model on the effectiveness of routing entries, and then evaluate the negative impact of noise on routing decisions. Based on such analytical results, we derive the necessary and sufficient condition of feasible gradient-based routing using Bloom filters. Accordingly, we propose a receiver-oriented design of Bloom filters, called Wader, which satisfies the necessary and sufficient condition. The evaluation results demonstrate that Wader guarantees the correctness and efficiency of gradient-based routing with high probability."},
{"Title": "Localization-Oriented Network Adjustment in Wireless Ad Hoc and Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2013.17", "Full Abstract": "Localization is an enabling technique for many sensor network applications. Real-world deployments demonstrate that, in practice, a network is not always entirely localizable, leaving a certain number of theoretically nonlocalizable nodes. Previous studies mainly focus on how to tune network settings to make a network localizable. However, the existing methods are considered to be coarse-grained, since they equally deal with localizable and nonlocalizable nodes. Ignoring localizability induces unnecessary adjustments and accompanying costs. In this study, we propose a fine-grained approach, localizability-aided localization (LAL), which basically consists of three phases: node localizability testing, structure analysis, and network adjustment. LAL triggers a single round adjustment, after which some popular localization methods can be successfully carried out. Being aware of node localizability, all network adjustments made by LAL are purposefully selected. Experiment and simulation results show that LAL effectively guides the adjustment while makes it efficient in terms of the number of added edges and affected nodes."},
{"Title": "Elon", "URL": "https://dl.acm.org/doi/10.1145/2560017", "Full Abstract": "We present a new mechanism called Elon for enabling efficient and long-term reprogramming in wireless sensor networks. Elon reduces the transferred code size significantly by introducing the concept of"},
{"Title": "Big Data", "URL": "https://dl.acm.org/doi/10.1007/s11036-013-0489-0", "Full Abstract": "In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop. We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aim to provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions."},
{"Title": "QoF", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2013.98", "Full Abstract": "Due to its large scale and constrained communication radius, a wireless sensor network mostly relies on multi-hop transmissions to deliver a data packet along a sequence of nodes. It is of essential importance to measure the forwarding quality of multi-hop paths and such information shall be utilized in designing efficient routing strategies. Existing metrics like ETX, ETF mainly focus on quantifying the link performance in between the nodes while overlooking the forwarding capabilities inside the sensor nodes. The experience on manipulating GreenOrbs, a large-scale sensor network with 330 nodes, reveals that the quality of forwarding inside each sensor node is at the least an equally important factor that contributes to the path quality in data delivery. In this paper we propose QoF, Quality of Forwarding, a new metric which explores the performance in the gray zone inside a node left unattended in previous studies. By combining the QoF measurements within a node and over a link, we are able to comprehensively measure the intact path quality in designing efficient multi-hop routing protocols. We implement QoF and build a modified Collection Tree Protocol (CTP). We evaluate the data collection performance in a testbed consisting of 50 TelosB nodes, and compare it with the original CTP protocol. The experimental results show that our approach takes both transmission cost and forwarding reliability into consideration, thus achieving a high throughput for data collection."},
{"Title": "Robust Component-Based Localizationin Sparse Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2013.85", "Full Abstract": "Accurate localization is crucial for wireless ad-hoc and sensor networks. Among the localization schemes, component-based approaches specialize in localization performance. By grouping nodes into increasingly large rigid components, component-based localization algorithms can properly conquer network sparseness and anchor sparseness. However, such design is sensitive to measurement errors. Existing robust localization methods focus on eliminating the positioning error of a single node. Indeed, a single node has two dimensions of freedom in 2D space and only suffers from one type of transformation: translation. As a rigid 2D structure, a component suffers from three possible transformations: translation, rotation, and reflection. A high degree of freedom brings about complicated cases of error productions and difficulties on error controlling. This study is the first work addressing how to deal with ranging noises for component-based methods. By exploiting a set of robust patterns, we present an Error-TOlerant Component-based algorithm (ETOC) that not only inherits the high-performance characteristic of component-based methods, but also achieves robustness of the result. We evaluate ETOC through a real-world sensor network consisting of 120 TelosB motes as well as extensive large-scale simulations. Experiment results show that, comparing with the-state-of-the-art designs, ETOC can work properly in sparse networks and provide more accurate localization results."},
{"Title": "Towards Energy-Fairness in Asynchronous Duty-Cycling Sensor Networks", "URL": "https://dl.acm.org/doi/10.1145/2490256", "Full Abstract": "In this article, we investigate the problem of controlling node sleep intervals so as to achieve the min-max energy fairness in asynchronous duty-cycling sensor networks. We propose a mathematical model to describe the energy efficiency of such networks and observe that traditional sleep interval setting strategies, for example, operating sensor nodes with an identical sleep interval, or intuitive control heuristics, for example, greedily increasing sleep intervals of sensor nodes with high energy consumption rates, hardly perform well in practice. There is an urgent need to develop an efficient sleep interval control strategy for achieving fair and high energy efficiency. To this end, we theoretically formulate the Sleep Interval Control (SIC) problem and find out that it is a convex optimization problem. By utilizing the convex property, we decompose the original problem and propose a distributed algorithm, called GDSIC. In GDSIC, sensor nodes can tune sleep intervals through a local information exchange such that the maximum energy consumption rate of the network approaches to be minimized. The algorithm is self-adjustable to the traffic load variance and is able to serve as a unified framework for a variety of asynchronous duty-cycling MAC protocols. We implement our approach in a prototype system and test its feasibility and applicability on a 50-node testbed. We further conduct extensive trace-driven simulations to examine the efficiency and scalability of our algorithm with various settings."},
{"Title": "In transition from global to modular temporal reasoning about programs", "URL": "https://dl.acm.org/doi/10.5555/101969.101977", "Full Abstract": "No abstract available."},
{"Title": "On the development of reactive systems", "URL": "https://dl.acm.org/doi/10.5555/101969.101990", "Full Abstract": "No abstract available."},
{"Title": "Rooting UNITY", "URL": "https://dl.acm.org/doi/10.1145/75200.75202", "Full Abstract": "Copyright © 1989 Authors."},
{"Title": "On the Synthesis of an Asynchronous Reactive Module", "URL": "https://dl.acm.org/doi/10.5555/646243.681607", "Full Abstract": "No abstract available."},
{"Title": "Learning omega-Regular Languages from Queries and Counter-Examples (A Preliminary Report)", "URL": "https://dl.acm.org/doi/10.5555/647702.734324", "Full Abstract": "No abstract available."},
{"Title": "Proving partial order liveness properties", "URL": "https://dl.acm.org/doi/10.5555/90397.92361", "Full Abstract": "No abstract available."},
{"Title": "Proving Partial Order Liveness Properties", "URL": "https://dl.acm.org/doi/10.5555/646244.684386", "Full Abstract": "No abstract available."},
{"Title": "What regularized auto-encoders learn from the data-generating distribution", "URL": "https://dl.acm.org/doi/10.5555/2627435.2750359", "Full Abstract": "What do auto-encoders learn about the underlying data-generating distribution? Recent work suggests that some auto-encoder variants do a good job of capturing the local manifold structure of data. This paper clarifies some of these previous observations by showing that minimizing a particular form of regularized reconstruction error yields a reconstruction function that locally characterizes the shape of the data-generating density. We show that the auto-encoder captures the score (derivative of the log-density with respect to the input). It contradicts previous interpretations of reconstruction error as an energy function. Unlike previous results, the theorems provided here are completely generic and do not depend on the parameterization of the auto-encoder: they show what the auto-encoder would tend to if given enough capacity and examples. These results are for a contractive training criterion we show to be similar to the denoising auto-encoder training criterion with small corruption noise, but with contraction applied on the whole reconstruction function rather than just encoder. Similarly to score matching, one can consider the proposed training criterion as a convenient alternative to maximum likelihood because it does not involve a partition function. Finally, we show how an approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution, and this is confirmed in sampling experiments."},
{"Title": "A semantic matching energy function for learning with multi-relational data", "URL": "https://dl.acm.org/doi/10.1007/s10994-013-5363-6", "Full Abstract": "Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature as well as on data from a real-world knowledge base (WordNet). In addition, we present how our method can be applied to perform word-sense disambiguation in a context of open-text semantic parsing, where the goal is to learn to assign a structured meaning representation to almost any sentence of free text, demonstrating that it can scale up to tens of thousands of nodes and thousands of types of relation."},
{"Title": "Learning semantic representations of objects and their parts", "URL": "https://dl.acm.org/doi/10.5555/2583611.2583674", "Full Abstract": "Recently, large scale image annotation datasets have been collected with millions of images and thousands of possible annotations. Latent variable models, or embedding methods, that simultaneously learn semantic representations of object labels and image representations can provide tractable solutions on such tasks. In this work, we are interested in jointly learning representations both for the objects in an image, and the parts of those objects, because such deeper semantic representations could bring a leap forward in image retrieval or browsing. Despite the size of these datasets, the amount of annotated data for objects and parts can be costly and may not be available. In this paper, we propose to bypass this cost with a method able to learn to jointly label objects and parts without requiring exhaustively labeled data. We design a model architecture that can be trained under a proxy supervision obtained by combining standard image annotation (from ImageNet) with semantic part-based within-label relations (from WordNet). The model itself is designed to model both object image to object label similarities, and object label to object part label similarities in a single joint system. Experiments conducted on our combined data and a precisely annotated evaluation set demonstrate the usefulness of our approach."},
{"Title": "Communication with directed logic variables", "URL": "https://dl.acm.org/doi/10.1145/99583.99615", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "Deep generative stochastic networks trainable by backprop", "URL": "https://dl.acm.org/doi/10.5555/3044805.3044918", "Full Abstract": "We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining."},
{"Title": "Marginalized denoising auto-encoders for nonlinear representations", "URL": "https://dl.acm.org/doi/10.5555/3044805.3045057", "Full Abstract": "Denoising auto-encoders (DAEs) have been successfully used to learn new representations for a wide range of machine learning tasks. During training, DAEs make many passes over the training dataset and reconstruct it from partial corruption generated from a pre-specified corrupting distribution. This process learns robust representation, though at the expense of requiring many training epochs, in which the data is explicitly corrupted. In this paper we present the"},
{"Title": "Conditioning and time representation in long short-term memory networks", "URL": "https://dl.acm.org/doi/10.1007/s00422-013-0575-1", "Full Abstract": "Dopaminergic models based on the temporal-difference learning algorithm usually do not differentiate trace from delay conditioning. Instead, they use a fixed temporal representation of elapsed time since conditioned stimulus onset. Recently, a new model was proposed in which timing is learned within a long short-term memory (LSTM) artificial neural network representing the cerebral cortex (Rivest et al. in J Comput Neurosci 28(1):107---130, 2010 ). In this paper, that model's ability to reproduce and explain relevant data, as well as its ability to make interesting new predictions, are evaluated. The model reveals a strikingly different temporal representation between trace and delay conditioning since trace conditioning requires working memory to remember the past conditioned stimulus while delay conditioning does not. On the other hand, the model predicts no important difference in DA responses between those two conditions when trained on one conditioning paradigm and tested on the other. The model predicts that in trace conditioning, animal timing starts with the conditioned stimulus offset as opposed to its onset. In classical conditioning, it predicts that if the conditioned stimulus does not disappear after the reward, the animal may expect a second reward. Finally, the last simulation reveals that the buildup of activity of some units in the networks can adapt to new delays by adjusting their rate of integration. Most importantly, the paper shows that it is possible, with the proposed architecture, to acquire discharge patterns similar to those observed in dopaminergic neurons and in the cerebral cortex on those tasks simply by minimizing a predictive cost function."},
{"Title": "Deep learning and cultural evolution", "URL": "https://dl.acm.org/doi/10.1145/2598394.2598395", "Full Abstract": "We propose a theory and its first experimental tests, relating difficulty of learning in deep architectures to culture and language. The theory is articulated around the following hypotheses: learning in an individual human brain is hampered by the presence of effective local minima, particularly when it comes to learning higher-level abstractions, which are represented by the composition of many levels of representation, i.e., by deep architectures; a human brain can learn such high-level abstractions if guided by the signals produced by other humans, which act as hints for intermediate and higher-level abstractions; language and the recombination and optimization of mental concepts provide an efficient evolutionary recombination operator for this purpose. The theory is grounded in experimental observations of the difficulties of training deep artificial neural networks and an empirical test of the hypothesis regarding the need for guidance of intermediate concepts is demonstrated. This is done through a learning task on which all the tested machine learning algorithms failed, unless provided with hints about intermediate-level abstractions."},
{"Title": "Scaling up deep learning", "URL": "https://dl.acm.org/doi/10.1145/2623330.2630802", "Full Abstract": "Deep learning has rapidly moved from a marginal approach in the machine learning community less than ten years ago to one that has strong industrial impact, in particular for high-dimensional perceptual data such as speech and images, but also natural language. The demand for experts in deep learning is growing very fast (faster than we can graduate PhDs), thereby considerably increasing their market value. Deep learning is based on the idea of learning multiple levels of representation, with higher levels computed as a function of lower levels, and corresponding to more abstract concepts automatically discovered by the learner. Deep learning arose out of research on artificial neural networks and graphical models and the literature on that subject has considerably grown in recent years, culminating in the creation of a dedicated conference (ICLR). The tutorial will introduce some of the basic algorithms, both on the supervised and unsupervised sides, as well as discuss some of the guidelines for successfully using them in practice. Finally, it will introduce current research questions regarding the challenge of scaling up deep learning to much larger models that can successfully extract information from huge datasets."},
{"Title": "Learned-norm pooling for deep feedforward and recurrent neural networks", "URL": "https://dl.acm.org/doi/10.5555/3120260.3120295", "Full Abstract": "In this paper we propose and investigate a novel nonlinear unit, called"},
{"Title": "Iterative neural autoregressive distribution estimator (NADE-", "URL": "https://dl.acm.org/doi/10.5555/2968826.2968863", "Full Abstract": "Training of the neural autoregressive density estimator (NADE) can be viewed as doing one step of probabilistic inference on missing values in data. We propose a new model that extends this inference scheme to multiple steps, arguing that it is easier to learn to improve a reconstruction in"},
{"Title": "Learning concept embeddings for query expansion by quantum entropy minimization", "URL": "https://dl.acm.org/doi/10.5555/2892753.2892773", "Full Abstract": "In web search, users queries are formulated using only few terms and term-matching retrieval functions could fail at retrieving relevant documents. Given a user query, the technique of query expansion (QE) consists in selecting related terms that could enhance the likelihood of retrieving relevant documents. Selecting such expansion terms is challenging and requires a computational framework capable of encoding complex semantic relationships. In this paper, we propose a novel method for learning, in a supervised way, semantic representations for words and phrases. By embedding queries and documents in special matrices, our model disposes of an increased representational power with respect to existing approaches adopting a vector representation. We show that our model produces high-quality query expansion terms. Our expansion increase IR measures beyond expansion from current word-embeddings models and well-established traditional QE methods."},
{"Title": "On the challenges of physical implementations of RBMs", "URL": "https://dl.acm.org/doi/10.5555/2893873.2894060", "Full Abstract": "Restricted Boltzmann machines (RBMs) are powerful machine learning models, but learning and some kinds of inference in the model require sampling-based approximations, which, in classical digital computers, are implemented using expensive MCMC. Physical computation offers the opportunity to reduce the cost of sampling by building physical systems whose natural dynamics correspond to drawing samples from the desired RBM distribution. Such a system avoids the burn-in and mixing cost of a Markov chain. However, hardware implementations of this variety usually entail limitations such as low-precision and limited range of the parameters and restrictions on the size and topology of the RBM. We conduct software simulations to determine how harmful each of these restrictions is. Our simulations are based on the D-Wave Two computer, but the issues we investigate arise in most forms of physical computation. Our findings suggest that designers of new physical computing hardware and algorithms for physical computers should focus their efforts on overcoming the limitations imposed by the topology restrictions of currently existing physical computers."},
{"Title": "On the Equivalence between Deep NADE and Generative Stochastic Networks", "URL": "https://dl.acm.org/doi/10.1007/978-3-662-44845-8_21", "Full Abstract": "Neural Autoregressive Distribution Estimators (NADEs) have recently been shown as successful alternatives for modeling high dimensional multimodal distributions. One issue associated with NADEs is that they rely on a particular order of factorization for"},
{"Title": "Nonmonotonic logics", "URL": "https://dl.acm.org/doi/10.5555/1625015.1625094", "Full Abstract": "We propose a unifying framework for nonmonotonic logics, which subsumes previously published systems, and at the same time is very simple. We discuss some of the technicalities of the new general framework, illustrate briefly how some previous systems are special cases of it, and finish an informal discussion of the intuitive meaning of nonmonotonic inferences."},
{"Title": "Temporal logics in AI: semantical and ontological considerations", "URL": "https://dl.acm.org/doi/10.1016/0004-3702%2887%2990052-X", "Full Abstract": "No abstract available."},
{"Title": "Link Quality Aware Code Dissemination in Wireless Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2013.176", "Full Abstract": "Wireless reprogramming is a crucial technique for software deployment in wireless sensor networks (WSNs). Code dissemination is a basic building block to enable wireless reprogramming. We present ECD, an Efficient Code Dissemination protocol leveraging 1-hop link quality information based on the TinyOS platform. Compared to prior works, ECD has three salient features. First, it supports dynamically configurable packet sizes. By increasing the packet size for high PHY rate radios, it significantly improves the transmission efficiency. Second, it employs an accurate sender selection algorithm to mitigate transmission collisions and transmissions over poor links. Third, it employs a simple impact-based backoff timer design to shorten the time spent in coordinating multiple eligible senders so that the largest impact sender is most likely to transmit. We implement ECD based on TinyOS and evaluate its performance extensively via testbed experiments and simulations. Results show that ECD outperforms state-of-the-art protocols, Deluge and MNP, in terms of completion time and data traffic (e.g., about 20 percent less traffic and 20-30 percent shorter completion time compared to Deluge)."},
{"Title": "Reasoning about change: time and causation from the standpoint of artificial intelligence", "URL": "https://dl.acm.org/doi/book/10.5555/42333", "Full Abstract": "No abstract available."},
{"Title": "Robust Trajectory Estimation for Crowdsourcing-Based Mobile Applications", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2013.250", "Full Abstract": "Crowdsourcing-based mobile applications are becoming more and more prevalent in recent years, as smartphones equipped with various built-in sensors are proliferating rapidly. The large quantity of crowdsourced sensing data stimulates researchers to accomplish some tasks that used to be costly or impossible, yet the quality of the crowdsourced data, which is of great importance, has not received sufficient attention. In reality, the low-quality crowdsourced data are prone to containing outliers that may severely impair the crowdsourcing applications. Thus in this work, we conduct pioneer investigation considering crowdsourced data quality. Specifically, we focus on estimating user motion trajectory information, which plays an essential role in multiple crowdsourcing applications, such as indoor localization, context recognition, indoor navigation, etc. We resort to the family of robust statistics and design a robust trajectory estimation scheme, name TrMCD, which is capable of alleviating the negative influence of abnormal crowdsourced user trajectories, differentiating normal users from abnormal users, and overcoming the challenge brought by spatial unbalance of crowdsourced trajectories. Two real field experiments are conducted and the results show that TrMCD is robust and effective in estimating user motion trajectories and mapping fingerprints to physical locations."},
{"Title": "Omnidirectional Coverage for Device-Free Passive Human Detection", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2013.274", "Full Abstract": "Device-free Passive (DfP) human detection acts as a key enabler for emerging location-based services such as smart space, human-computer interaction, and asset security. A primary concern in devising scenario-tailored detecting systems is coverage of their monitoring units. While disk-like coverage facilitates topology control, simplifies deployment analysis, and is crucial for proximity-based applications, conventional monitoring units demonstrate directional coverage due to the underlying transmitter-receiver link architecture. To achieve omnidirectional coverage under such link-centric architecture, we propose the concept of omnidirectional passive human detection. The rationale is to exploit the rich multipath effect to blur the directional coverage. We harness PHY layer features to robustly capture the fine-grained multipath characteristics and virtually tune the shape of the coverage of the monitoring unit, which is previously prohibited with mere MAC layer RSSI. We design a fingerprinting scheme and a threshold-based scheme with off-the-shelf WiFi infrastructure and evaluate both schemes in typical clustered indoor scenarios. Experimental results demonstrate an average false positive of 8 percent and an average false negative of 7 percent for fingerprinting in detecting human presence in 4 directions. And both average false positive and false negative remain around 10 percent even with threshold-based methods."},
{"Title": "Wise counting", "URL": "https://dl.acm.org/doi/10.1145/2632951.2632963", "Full Abstract": "Radio Frequency Identification technology (RFID) is widely used in many applications, such as asset monitoring, e-passport and electronic payment, and is becoming one of the most effective solutions in cyber physical system. Since the identification alone does not provide any guarantee that tag corresponds to genuine identity, authentication of tag information is needed in most RFID systems. Meanwhile, as the number of tags is rapidly growing in recent years, per-tag based methods suffer from severely low efficiency and thus give way to probabilistic batch authentication. Most previous methods, however, share a common drawback from statistical perspective: they fail to explore correlation information, i.e., they do not comprehensively utilize all the information in authentication data structures. In addition, those schemes are not scalable well when multiple tag sets need to be verified simultaneously. In this paper, we propose a fast and efficient batch authentication scheme, Wise Counting (WIC), for large-scale RFID systems. We are the first to formally introduce the general batch authentication problem with multiple tag sets and give counterfeits estimation scheme with high efficiency. By employing a novel hierarchical authentication structure, we show that WIC is able to fast and efficiently authenticate both a single tag set and multiple tag sets in an easy, intuitive way. Through detailed theoretical analysis and extensive simulations, we validate the design of WIC and demonstrate its large superiority over state-of-the art approaches."},
{"Title": "Tagoram", "URL": "https://dl.acm.org/doi/10.1145/2639108.2639111", "Full Abstract": "In many applications, we have to identify an object and then locate the object to within high precision (centimeter- or millimeter-level). Legacy systems that can provide such accuracy are either expensive or suffering from performance degradation resulting from various impacts,"},
{"Title": "It starts with iGaze", "URL": "https://dl.acm.org/doi/10.1145/2639108.2639119", "Full Abstract": "In this work, we explore a new networking mechanism with smart glasses, through which users can express their interest and connect to a target simply by a gaze. Doing this, we attempt to let wearable devices understand human attention and intention, and pair devices carried by users according to such attention and intention. To achieve this ambitious goal, we propose a proof-of-concept system"},
{"Title": "Demo", "URL": "https://dl.acm.org/doi/10.1145/2639108.2641739", "Full Abstract": "In this demo, we propose a proof-of-concept networking system for smart glasses, through which users can express their interest and connect to a target simply by a gaze. Our system"},
{"Title": "Demo", "URL": "https://dl.acm.org/doi/10.1145/2639108.2641743", "Full Abstract": "In many applications, we have to identify an object and then locate the object to within high precision (centimeter- or millimeter-level). Tracking mobile RFID tags in real time has been a daunting task, especially challenging for achieving high precision. We achieve these three goals by leveraging the phase value of the backscattered signal, provided by the COTS RFID readers, to estimate the location of the object. To illustrate the basic idea of our system, we firstly focus on a simple scenario where the tag is moving along a fixed track known to the system. We propose Differential Augmented Hologram (DAH) which will facilitate the instant tracking of the mobile RFID tag to a high precision. We then devise a comprehensive solution to accurately recover the tag's moving trajectory and its locations, relaxing the assumption of knowing tag's track function in advance."},
{"Title": "NetMaster", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2014.39", "Full Abstract": "Smartphones nowadays are installed with diverse applications, each of which consumes energy and bandwidth. As more and more applications are crowded into a smart- phone, they cause serious problems with regard to battery life and bandwidth utilization. Existing proposals to tackle such challenges usually resort to two ways: avoiding energy- consuming network activities or improving communication efficiency in terms of power consumption. Those approaches either affect the smartphone users' experience, or offer little benefit in prolonging the battery life. Motivated by insightful understanding of users' habit, we in this paper propose a novel approach to orchestrate network activities of smartphone applications, based on user's habit. We implement our approach on smartphones as a middleware service called NetMaster. The performance evaluation with real traces shows that NetMaster reduces energy consumption of network activities by 77.8% in average and increases network bandwidth utilization by over 200%. The user experience is surprisingly well preserved. The chance of undesired interrupt during normal usage is less than 1%."},
{"Title": "CrossNavi", "URL": "https://dl.acm.org/doi/10.1145/2632048.2632083", "Full Abstract": "Crossroad is among the most dangerous parts outside for the visually impaired people. Numerous studies have exploited navigating systems for the visually impaired community, providing services ranging from block detection, route planning to realtime localization. However, none of them have addressed the safety issue in crossroad and integrated three key factors necessary for a practical crossroad navigation system: detecting the crossroad, locating zebra patterns, and guiding the user within zebra crossing when passing the road. Our"},
{"Title": "Intelligent sleep stage mining service with smartphones", "URL": "https://dl.acm.org/doi/10.1145/2632048.2632084", "Full Abstract": "Sleep quality plays a significant role in personal health. A great deal of effort has been paid to design sleep quality monitoring systems, providing services ranging from bedtime monitoring to sleep activity detection. However, as sleep quality is closely related to the distribution of sleep duration over different sleep stages, neither the bedtime nor the intensity of sleep activities is able to reflect sleep quality precisely. To this end, we present Sleep Hunter, a mobile service that provides a fine-grained detection of sleep stage transition for sleep quality monitoring and intelligent wake-up call. The rationale is that each sleep stage is accompanied by specific yet distinguishable body movements and acoustic signals. Leveraging the built-in sensors on smartphones, Sleep Hunter integrates these physical activities with sleep environment, inherent temporal relation and personal factors by a statistical model for a fine-grained sleep stage detection. Based on the duration of each sleep stage, Sleep Hunter further provides sleep quality report and smart call service for users. Experimental results from over 30 sets of nocturnal sleep data show that our system is superior to existing actigraphy-based sleep quality monitoring systems, and achieves satisfying detection accuracy compared with dedicated polysomnography-based devices."},
{"Title": "Scalable Data Access Control in RFID-Enabled Supply Chain", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2014.28", "Full Abstract": "By attaching RFID tags to products, supply chain participants can identify products and create product data to record the product particulars in transit. Participants along the supply chain share their product data to enable information exchange and support critical decisions in production operations. Such an information sharing essentially requires a data access control mechanism when the product data relates to sensitive business issues. However, existing access control solutions are ill suited to the RFID-enabled supply chain, as they are not scalable in handling a huge number of tags, introduce vulnerability to the product data, and performs poorly to support privilege revocation of product data. We present a new scalable data access control system that addresses these limitations. Our system provides an item-level data access control mechanism that defines and enforces access policies based on both the participants' role attribute and the products' RFID tag attribute. Our system further provides an item-level privilege revocation mechanism by allowing the participants to delegate encryption updates in revocation operation without disclosing the underlying data contents. We design a new updatable encryption scheme and integrate it with Cipher text Policy-Attribute Based Encryption (CP-ABE) to implement the key components of our system."},
{"Title": "ZiSense", "URL": "https://dl.acm.org/doi/10.1145/2668332.2668334", "Full Abstract": "To save energy, wireless sensor networks often run in a low duty cycle mode, where the radios of sensor nodes are scheduled between ON and OFF states. For nodes to communicate with each other, Low Power Listening (LPL) and Low Power Probing (LPP) are two types of rendezvous mechanisms. Nodes with LPL or LPP rely on signal strength or probe packets to detect potential transmissions, and then keep the radio-on for communications. Unfortunately, in many cases, signal strength and probe packets are susceptible to interference, resulting in undesirable radio on time when the signal strength of interference is above a threshold or a probe packet is interfered. To address the issue, we propose ZiSense, an energy efficient rendezvous mechanism which is resilient to interference. Instead of checking the signal strength or decoding the probe packets, ZiSense detects the existence of ZigBee transmissions and wakes up nodes accordingly. On sensor nodes with limited information and resource, we carefully study and extract short-term features purely from the time-domain RSSI sequence, and design a rule-based approach to efficiently identify the existence of ZigBee. We theoretically analyze the benefit of ZiSense in different environments and implement a prototype in TinyOS with TelosB motes. We examine ZiSense performance under controlled interference and office environments. The evaluation results show that, compared with state-of-the-art rendezvous mechanisms, ZiSense significantly reduces the energy consumption."},
{"Title": "Context-free Attacks Using Keyboard Acoustic Emanations", "URL": "https://dl.acm.org/doi/10.1145/2660267.2660296", "Full Abstract": "The emanations of electronic and mechanical devices have raised serious privacy concerns. It proves possible for an attacker to recover the keystrokes by acoustic signal emanations. Most existing malicious applications adopt context-based approaches, which assume that the typed texts are potentially correlated. Those approaches often incur a high cost during the context learning stage, and can be limited by randomly typed contents (e.g., passwords). Also, context correlations can increase the risk of successive false recognition. We present a context-free and geometry-based approach to recover keystrokes. Using off-the-shelf smartphones to record acoustic emanations from keystrokes, this design estimates keystrokes' physical positions based on the Time Difference of Arrival (TDoA) method. We conduct extensive experiments and the results show that more than 72.2\\% of keystrokes can be successfully recovered."},
{"Title": "Every Packet Counts", "URL": "https://dl.acm.org/doi/10.1109/ICNP.2014.30", "Full Abstract": "Delay is an important metric to understand and improve system performance. While existing approaches focus on aggregate delay statistics in pre-programmed granularity, providing only statistical results such as averages and deviations, those approaches fail to provide fine-grained delay measurement at a flexible level and thus may miss important delay characteristics. For example, delay anomalies, which are critical system performance indicators, may not be captured by existing coarse grained approaches. In this work, we propose a fine-grained delay measurement approach based on a new measurement structure design called order preserving aggregator (OPA). OPA can efficiently encode the ordering and loss information by exploiting inherent data characteristics. Based on OPA, we propose a two layer design to convey both ordering and time stamp information, and then derive per-packet delay/loss measurement with a small overhead. We evaluate our approach both analytically and experimentally with widely used real-world data sets. The results show that our approach can achieve accurate per-packet delay measurement with an average of per-packet relative error at 2%, and an average of aggregated relative error at 10 -- 5, while introducing less than 4 10 -- 4 additional overhead."},
{"Title": "On the learnability of infinitary regular sets", "URL": "https://dl.acm.org/doi/10.5555/114836.114848", "Full Abstract": "No abstract available."},
{"Title": "What is in a Step", "URL": "https://dl.acm.org/doi/10.5555/645867.670927", "Full Abstract": "No abstract available."},
{"Title": "Timed and Hybrid Statecharts and Their Textual Representation", "URL": "https://dl.acm.org/doi/10.5555/646842.706477", "Full Abstract": "No abstract available."},
{"Title": "How Vital is Liveness? Verifying Timing Properties of Reactive and Hybrid Systems (Extended Abstract)", "URL": "https://dl.acm.org/doi/10.5555/646727.703341", "Full Abstract": "No abstract available."},
{"Title": "System Specification and Refinement in Temporal Logic", "URL": "https://dl.acm.org/doi/10.5555/646830.759686", "Full Abstract": "No abstract available."},
{"Title": "Integration Graphs", "URL": "https://dl.acm.org/doi/10.5555/646874.709977", "Full Abstract": "No abstract available."},
{"Title": "Probabilistic verification", "URL": "https://dl.acm.org/doi/10.1006/inco.1993.1012", "Full Abstract": "No abstract available."},
{"Title": "Reachability Analysis of Planar Multi-limear Systems", "URL": "https://dl.acm.org/doi/10.5555/647762.735360", "Full Abstract": "No abstract available."},
{"Title": "BilBOWA", "URL": "https://dl.acm.org/doi/10.5555/3045118.3045199", "Full Abstract": "We introduce BilBOWA ("},
{"Title": "Show, attend and tell", "URL": "https://dl.acm.org/doi/10.5555/3045118.3045336", "Full Abstract": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO."},
{"Title": "Gated feedback recurrent neural networks", "URL": "https://dl.acm.org/doi/10.5555/3045118.3045338", "Full Abstract": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions."},
{"Title": "Difference target propagation", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-23528-8_31", "Full Abstract": "Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks."},
{"Title": "Artificial neural networks applied to taxi destination prediction", "URL": "https://dl.acm.org/doi/10.5555/3056172.3056178", "Full Abstract": "We describe our first-place solution to the ECML/PKDD discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of GPS points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence."},
{"Title": "A Hierarchical Recurrent Encoder-Decoder for Generative Context-Aware Query Suggestion", "URL": "https://dl.acm.org/doi/10.1145/2806416.2806493", "Full Abstract": "Users may strive to formulate an adequate textual query for their information need. Search engines assist the users by presenting query suggestions. To preserve the original search intent, suggestions should be context-aware and account for the previous queries issued by the user. Achieving context awareness is challenging due to data sparsity. We present a novel hierarchical recurrent encoder-decoder architecture that makes possible to account for sequences of previous queries of arbitrary lengths. As a result, our suggestions are sensitive to the order of queries in the context while avoiding data sparsity. Additionally, our model can suggest for rare, or long-tail, queries. The produced suggestions are synthetic and are sampled one word at a time, using computationally cheap decoding techniques. This is in contrast to current synthetic suggestion models relying upon machine learning pipelines and hand-engineered feature sets. Results show that our model outperforms existing context-aware approaches in a next query prediction setting. In addition to query suggestion, our architecture is general enough to be used in a variety of other applications."},
{"Title": "Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks", "URL": "https://dl.acm.org/doi/10.1109/TMM.2015.2477044", "Full Abstract": "Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. In this paper we focus on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description, and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism."},
{"Title": "Attention-based models for speech recognition", "URL": "https://dl.acm.org/doi/10.5555/2969239.2969304", "Full Abstract": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMET phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level."},
{"Title": "Equilibrated adaptive learning rates for non-convex optimization", "URL": "https://dl.acm.org/doi/10.5555/2969239.2969407", "Full Abstract": "Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent."},
{"Title": "A recurrent latent variable model for sequential data", "URL": "https://dl.acm.org/doi/10.5555/2969442.2969572", "Full Abstract": "In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the"},
{"Title": "BinaryConnect", "URL": "https://dl.acm.org/doi/10.5555/2969442.2969588", "Full Abstract": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN."},
{"Title": "Knowledge matters", "URL": "https://dl.acm.org/doi/10.5555/2946645.2946653", "Full Abstract": "We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-the-art machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two-tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is 1 if all of three shapes belong to the same category and otherwise the class is 0. In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization diculty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error."},
{"Title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "URL": "https://dl.acm.org/doi/10.5555/3016387.3016435", "Full Abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings."},
{"Title": "Problems in formal temporal reasoning", "URL": "https://dl.acm.org/doi/10.1016/0004-3702%2888%2990078-1", "Full Abstract": "No abstract available."},
{"Title": "Chronological ignorance: experiments in nonmonotonic temporal reasoning", "URL": "https://dl.acm.org/doi/10.1016/0004-3702%2888%2990085-9", "Full Abstract": "No abstract available."},
{"Title": "Temporal reasoning in artificial intelligence", "URL": "https://dl.acm.org/doi/10.5555/61371.61382", "Full Abstract": "No abstract available."},
{"Title": "Time for Action: On the Relation between Time, Knowledge, and Action", "URL": "https://dl.acm.org/doi/book/10.5555/892460", "Full Abstract": "We consider the role played by the concept of action in AI. We first briefly summarize the advantages and limitations of past approaches to taking the concept as primitive, as embodied in the situation calculus and dynamic logic. We also briefly summarize the alternative, namely adopting a temporal framework, and point out its complementary advantages and limitations. We then propose a framework that retains the advantages of both viewpoints, and that ties the notion of action closely to that of knowledge. Specifically, we propose starting with the notion of time lines, and defining the notion of action as the ability to make certain choices among sets of time lines. Our definitions shed new light on the connection between time, action, knowledge and ignorance, choice-making, feasibility, and simultaneous reasoning about the same events at different levels of detail."},
{"Title": "Belief as Defeasible Knowledge", "URL": "https://dl.acm.org/doi/book/10.5555/892461", "Full Abstract": "We investigate the relation between the notions of knowledge and belief. Contrary to the well-known slogan about knowledge being \"justified, true belief,\" we propose that belief be viewed as defeasible knowledge. Specifically, we offer a definition of belief as knowledge-relative-to-assumptions, and tie the definition to the notion of nonmonotonicity. Our definition has several advantages. First, it is short. Second, we do not need to add anything to the logic of knowledge: the right properties of belief fall out of the definition and the properties of knowledge. Third, the connection between knowledge and belief is derived from one fundamental principle, which is more enlightening than a collection of arbitrary-seeming axioms relating the two notions."},
{"Title": "New results on semantical non-monotonic reasoning", "URL": "https://dl.acm.org/doi/10.5555/73682.73684", "Full Abstract": "No abstract available."},
{"Title": "Time for action", "URL": "https://dl.acm.org/doi/10.5555/1623891.1623908", "Full Abstract": "We consider the role played by the concept of action in AI. We first briefly summarize the advantages and limitations of past approaches to taking the concept as primitive, as embodied in the situation calculus and dynamic logic. We also briefly summarize the alternative, namely adopting a temporal framework, and point out its complementary advantages and limitations. We then propose a framework that retains the advantages of both viewpoints, and that ties the notion of action closely to that of knowledge. Specifically, we propose starting with the notion of time lines, and defining the notion of action as the ability to make certain choices among sets of time lines. Our definitions shed new light on the connection between time, action, knowledge and ignorance, choice-making, feasibility, and simultaneous reasoning about the same events at different levels of detail."},
{"Title": "Belief as defeasible knowledge", "URL": "https://dl.acm.org/doi/10.5555/1623891.1623942", "Full Abstract": "We investigate the relation between the notions of knowledge and belief. Contrary to the well-known slogan about knowledge being \"justified, true belief,\" we propose that belief be viewed as defeasible knowledge. Specifically, we offer a definition of belief as knowledge-relative-to-assumptions, and tie the definition to the notion of nonmonotonicity. Our definition has several advantages. First, it is short. Second, we do not need to add anything to the logic of knowledge: the right properties of belief fall out of the definition and the properties of knowledge. Third, the connection between knowledge and belief is derived from one fundamental principle, which is more enlightening than a collection of arbitrary-seeming axioms relating the two notions."},
{"Title": "Argument systems", "URL": "https://dl.acm.org/doi/10.5555/112922.112947", "Full Abstract": "No abstract available."},
{"Title": "Epistemic Semantics for Fixed-Points Non-Monotonic Logics", "URL": "https://dl.acm.org/doi/10.5555/645874.671707", "Full Abstract": "No abstract available."},
{"Title": "Epistemic semantics for fixed-points non-monotonic logics", "URL": "https://dl.acm.org/doi/10.5555/1027014.1027030", "Full Abstract": "Default Logic and Autoepistemic Logic are the two best-known fixed-points non-monotonic logics. Despite the fact that they are known to be closely related and that the epistemic nature of Autoepistemic Logic is obvious, the only semantics that have been offered for Default Logic to date are complex and have little to do with epistemic notions [Etherington 1987]. In this paper we provide simple uniform epistemic semantics for the two logics. We do so by translating them both into a new logic, called GK, of Grounded Knowledge, which embodies a modification of preference semantics [Shoham 1987]. Beside their simplicity and uniformity, the semantics have two other advantages: They allow easy proofs of the connections between Default Logic and Autoepistemic Logic, and suggest a general class of logics of which the two logics are special cases."},
{"Title": "Protograms", "URL": "https://dl.acm.org/doi/book/10.5555/892494", "Full Abstract": "Motivated largely by tasks that require control of complex processes in a dynamic environment, we introduce a new computational construct called a protogram. A protogram is a program specifying an abstract course of action, a course that allows for a range of specific actions, from which a choice is made through interaction with other protograms. We discuss the intuition behind the notion, and then explore some of the details involved in implementing it. Specifically, we (a) describe a general scheme of protogram interaction, (b) describe a protogram interpreter that has been implemented, dealing with some special cases, (c) describe three applications of the protogram interpreter, one in data processing and two in robotics (both currently only implemented as simulations), (d) describe some more general possible implementations of a protogram interpreter, and (e) discuss how protograms can be useful for the Gofer project. We also briefly discuss the origins of protograms in psychology and linguistics, compare protograms to blackboard and subsumption architectures, and discuss directions for future research."},
{"Title": "On the complexity of monotonic inheritance with roles", "URL": "https://dl.acm.org/doi/book/10.5555/892495", "Full Abstract": "We investigate the complexity of reasoning with monotonic inheritance hierarchies that contain, beside ISA edges, also ROLE (or FUNCTION) edges. A ROLE edge is an edge labelled with a name such as spouse of or brother of. We call such networks ISAR networks. Given a network with n vertices and m edges, we consider two problems: ($P_1$) determining whether the network implies an isa relation between two particular nodes, and ($P_2$) determining all isa relations implied by the network. As is well known, without ROLE edges the time complexity of $P_1$, is O(m), and the time complexity of $P_2$ is O($n^3$). Unfortunately, the results do not extend naturally to ISAR networks, except in a very restricted case. For general ISAR network we first give an polynomial algorithm by an easy reduction to proposional Horn theory. As the degree of the polynomial is quite high (O(m$n^4$) for $P_1$, O(m$n^6$) for $P_2$), we then develop a more direct algorithm. For both $P_1$ and $P_2$ its complexity is O($n^3 + m^2$). Actually, a finer analysis of the algorithm reveals a complexity of O(nr(log r) + $n^2$r+ $n^3), where r is the number of different ROLE labels. One corolary is that if we fix the number of ROLE labels, the complexity of our algorithm drops back to O($n^3$)."},
{"Title": "On the complexity of monotonic inheritance with roles", "URL": "https://dl.acm.org/doi/10.5555/1865499.1865593", "Full Abstract": "We investigate the complexity of reasoning with monotonic inheritance hierarchies that contain, beside ISA edges, also ROLE (or FUNCTION) edges. A ROLE edge is an edge labelled with a name such as spouse-of or brother-of. We call such networks ISAR networks. Given a network with n vertices and m edges, we consider two problems: (P"},
{"Title": "Provably correct theories of action", "URL": "https://dl.acm.org/doi/10.5555/1865675.1865729", "Full Abstract": "Research on nonmonotonic temporal reasoning in general, and the Yale Shooting Problem in particular, has suffered from the absence of a criterion against which to evaluate solutions. Indeed, researchers in the area disagree not only on the solutions but also on the problems. We propose a formal yet intuitive criterion by which to evaluate theories of actions, define a monotonic class of theories that satisfy this criterion, and then provide their provably-correct nonmonotonic counterpart."},
{"Title": "AGENTO", "URL": "https://dl.acm.org/doi/10.5555/1865756.1865786", "Full Abstract": "In [9] we defined the concept of agent oriented programming (AOP), which can be viewed as a specialization of object oriented programming (OOP). AOP views objects as agents with mental state, and, in the spirit of speech act theory, identifies a number of message types - informing, requesting, offering, and so on. AOP is a general framework. In this paper we present a specific and simple language called AGENTO; we define its syntax, present its interpreter, and illustrate both through an example."},
{"Title": "A logic for perception and belief", "URL": "https://dl.acm.org/doi/book/10.5555/892517", "Full Abstract": "We present a modal logic for reasoning about perception and belief, captured respectively by the operators P and B. The B operator is the standard belief operator used in recent years, and the P operator is similarly defined. The contribution of the paper is twofold. First, in terms of P we provide a definition of perceptual indistinguishability, such as arises out of limited visual acuity. The definition is concise, intuitive (we find), and avoids traditional paradoxes. Second, we explore the bimodal B--P system. We argue that the relationship between the two modalities varies among settings: The agent may or may not have confidence in its perception, may or may not be accurate in it, and so on. We therefore define a number of agent types corresponding to these various assumptions, and for each such agent type we provide a sound and complete axiomatization of the B--P system."},
{"Title": "A propositional modal logic of time intervals", "URL": "https://dl.acm.org/doi/10.1145/115234.115351", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "A Logic of Relative Desire (Preliminary Report)", "URL": "https://dl.acm.org/doi/10.5555/646353.691198", "Full Abstract": "No abstract available."},
{"Title": "Relative localization of RFID tags using spatial-temporal phase profiling", "URL": "https://dl.acm.org/doi/10.5555/2789770.2789788", "Full Abstract": "Many object localization applications need the relative locations of a set of objects as oppose to their absolute locations. Although many schemes for object localization using Radio Frequency Identification (RFID) tags have been proposed, they mostly focus on absolute object localization and are not suitable for relative object localization because of large error margins and the special hardware that they require. In this paper, we propose an approach called Spatial-Temporal Phase Profiling (STPP) to RFID based relative object localization. The basic idea of STPP is that by moving a reader over a set of tags during which the reader continuously interrogating the tags, for each tag, the reader obtains a sequence of RF phase values, which we call a phase profile, from the tag's responses over time. By analyzing the spatial-temporal dynamics in the phase profiles, STPP can calculate the spatial ordering among the tags. In comparison with prior absolute object localization schemes, STPP requires neither dedicated infrastructure nor special hardware. We implemented STPP and evaluated its performance in two real-world applications: locating misplaced books in a library and determining baggage order in an airport. The experimental results show that STPP achieves about 84% ordering accuracy for misplaced books and 95% ordering accuracy for baggage handling."},
{"Title": "L", "URL": "https://dl.acm.org/doi/10.1109/TNET.2014.2310812", "Full Abstract": "In order to simultaneously achieve good energy efficiency and high packet delivery performance, a multihop forwarding scheme should generally involve three design elements: media access mechanism, link estimation scheme, and routing strategy. Disregarding the low-duty-cycle nature of media access often leads to overestimation of link quality. Neglecting the bursty loss characteristic of wireless links inevitably consumes much more energy than necessary and underutilizes wireless channels. The routing strategy, if not well tailored to the above two factors, results in poor packet delivery performance. In this paper, we propose"},
{"Title": "SmartGuide", "URL": "https://dl.acm.org/doi/10.1145/2746285.2746294", "Full Abstract": "We introduce SmartGuide, a light-weighted and efficient approach to localize and recognize a distant unknown building. Our approach relies on shooting only a single photo of a target building via a smartphone and a local 2D Google map. SmartGuide first extracts a partial top view contour of a building from its side-view photo by applying vanishing point and the Manhattan World Assumption, and then fetches a candidate building set from a local 2D Google map based on smartphone's GPS readings. Partial top view shape, orientation and distance relative to the camera are used as input parameters in a probability model, which adversely recognizes the best candidate building in the local map. Our model is developed based on kernel density estimation that helps reduce noise in the smartphone sensors, such as GPS readings and camera ray direction reported by noisy accelerometer and compass. Experimental results demonstrate that our approach recognizes buildings ranging from 20m to 520m and achieves 92.7% accuracy in downtown areas where the Manhattan World Assumption is applicable. In addition, the processing time is no more than 6 seconds for 87% of cases. Compared with existing building localization schemes, SmartGuide offers numerous advantages. Our method avoids taking multiple photos, intricate 3D reconstruction or any initial deployment cost of database construction, making it faster and less labor-intensive than existing solutions."},
{"Title": "LMDD", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2015.101", "Full Abstract": "Doors are important landmarks for indoor positioning systems. Hence an accurate and light-weight door detection approach is highly desired. The state-of-the-art solutions are either vision based or infrastructure based, which incur nontrivial device or management cost. This paper presents a novel approach, Light-weight Magnetic-based Door Detection (LMDD), which only relies on the information from built-in sensors of a smartphone. LMDD detects a door by analyzing the change of magnetic signal and extracting special features caused by doors. It is light-weight in both computation and infrastructure cost. We have implemented a prototype of LMDD that has been installed on various Android phones. Experimental results show that LMDD achieves door detection accuracy of 74% in average, ranging from 66% to 85% in various typical environments such as offices, classrooms, residential houses, and a hospital."},
{"Title": "PIC", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2015.104", "Full Abstract": "Many cloud platforms emerge to meet urgent requirements for large-volume personal image store, sharing and search. Though most would agree that images contain rich sensitive information (e.g., People, location and event) and people's privacy concerns hinder their participation into untrusted services, today's cloud platforms provide little support for image privacy protection. Facing large-scale images from multiple users, it is extremely challenging for the cloud to maintain the index structure and schedule parallel computation without learning anything about the image content and indices. In this work, we introduce a novel system PIC: a Privacy-preserving Image search system on Cloud, which is a step towards feasible cloud services which provide secure content-based large-scale image search with fine-grained access control. Users can search on others' images if they are authorized by the image owners. Majority of the computationally intensive jobs are handled by the cloud, and a querier can now simply send the query and receive the result. Specially, to deal with massive images, we design our system suitable for distributed and parallel computation and introduce several optimizations to further expedite the search process. Our security analysis and prototype system evaluation results show that PIC successfully protects the image privacy at a low cost of computation and communication."},
{"Title": "PLP", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2015.20", "Full Abstract": "Crowdsensing applications require individuals toshare local and personal sensing data with others to produce valuableknowledge and services. Meanwhile, it has raised concernsespecially for location privacy. Users may wish to prevent privacyleak and publish as many non-sensitive contexts as possible.Simply suppressing sensitive contexts is vulnerable to the adversariesexploiting spatio-temporal correlations in users' behavior.In this work, we present PLP, a crowdsensing scheme whichpreserves privacy while maximizes the amount of data collectionby filtering a user's context stream. PLP leverages a conditionalrandom field to model the spatio-temporal correlations amongthe contexts, and proposes a speed-up algorithm to learn theweaknesses in the correlations. Even if the adversaries are strongenough to know the filtering system and the weaknesses, PLPcan still provably preserves privacy, with little computationalcost for online operations. PLP is evaluated and validated overtwo real-world smartphone context traces of 34 users. Theexperimental results show that PLP efficiently protects privacywithout sacrificing much utility."},
{"Title": "Connecting the Dots", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2015.26", "Full Abstract": "In distributed networks such as wireless ad hoc networks, local and lossy logs are often available on individual nodes. We propose REFILL, which analyzes lossy and unsynchronized logs collected from individual nodes and reconstructs the network behaviors. We design an inference engine based on protocol semantics to abstract states on each node. Further we leverage inherent and implicit event correlations in and between nodes to connect interference engines and analyze logs from different nodes. Based on unsynchronized and incomplete logs, REFILL can reconstruct network behavior, recover the network scenario and understand what has happened in the network. We show that the result of REFILL can be used to guide protocol design, network management, diagnosis, etc. We implement REFILL and apply it to a large-scale wireless sensor network project. REFILL provides a detailed per-packet tracing information based on event flows. We show that REFILL can reveal and verify fundamental issues, like locating packet loss positions and root causes. Further, we present implications and demonstrate how to leverage REFILL to enhance network performance."},
{"Title": "Scan without a Glance", "URL": "https://dl.acm.org/doi/10.1109/ICPP.2015.34", "Full Abstract": "Mobile videos contain rich information which could be utilized for various applications, like criminal investigation and scene reconstruction. Today's crowd-sourced mobile video retrieval systems are built on video content comparison, and their wide adoption has been hindered by onerous computation of CV algorithms and redundant networking traffic of the video transmission. In this work, we propose to leverage Field of View(FoV) as a content-free descriptor to measure video similarity with little accuracy loss. Based on FoV, our system can filter out unmatched videos before any content analysis and video transmission, which dramatically cuts down the computation and communication cost for crowd-sourced mobile video retrieval. Moreover, we design a video segmentation algorithm and an R-Tree based indexing structure to further reduce the networking traffic for mobile clients and potentiate the efficiency for the cloud server. We implement a prototype system and evaluate it from different aspects. The results show that FoV descriptors are much smaller and significantly faster to extract and match compared to content descriptors, while the FoV based similarity measurement achieves comparable search accuracy with the content-based method. Our evaluation also shows that the proposed retrieval scheme is scalable with data size and can response in less than 100ms when the data set has tens of thousands of video segments, and the networking traffic between the client and the server is negligible."},
{"Title": "Enhancing wifi-based localization with visual clues", "URL": "https://dl.acm.org/doi/10.1145/2750858.2807516", "Full Abstract": "Indoor localization is of great importance to a wide range of applications in the era of mobile computing. Current mainstream solutions rely on Received Signal Strength (RSS) of wireless signals as fingerprints to distinguish and infer locations. However, those methods suffer from fingerprint ambiguity that roots in multipath fading and temporal dynamics of wireless signals. Though pioneer efforts have resorted to motion-assisted or peer-assisted localization, they neither work in real time nor work without the help of peer users, which introduces extra costs and constraints, and thus degrades their practicality. To get over these limitations, we propose Argus, an image-assisted localization system for mobile devices. The basic idea of Argus is to extract geometric constraints from crowdsourced photos, and to reduce fingerprint ambiguity by mapping the constraints jointly against the fingerprint space. We devise techniques for photo selection, geometric constraint extraction, joint location estimation, and build a prototype that runs on commodity phones. Extensive experiments show that Argus triples the localization accuracy of classic RSS-based method, in time no longer than normal WiFi scanning, with negligible energy consumption."},
{"Title": "See Through Walls with COTS RFID System!", "URL": "https://dl.acm.org/doi/10.1145/2789168.2790100", "Full Abstract": "Through-wall tracking has gained a lot of attentions in civilian applications recently. Many applications would benefit from such device-free tracking, e.g. elderly people surveillance, intruder detection, gaming, etc. In this work, we present a system, named Tadar, for tracking moving objects without instrumenting them us- ing COTS RFID readers and tags. It works even through walls and behind closed doors. It aims to enable a see-through-wall technology that is low-cost, compact, and accessible to civilian purpose. In traditional RFID systems, tags modulate their IDs on the backscatter signals, which is vulnerable to the interferences from the ambient reflections. Unlike past work, which considers such vulnerability as detrimental, our design exploits it to detect surrounding objects even through walls. Specifically, we attach a group of RFID tags on the outer wall and logically convert them into an antenna array, receiving the signals reflected off moving objects. This paper introduces two main innovations. First, it shows how to eliminate the flash (e.g. the stronger reflections off walls) and extract the reflections from the backscatter signals. Second, it shows how to track the moving object based on HMM (Hidden Markov Model) and its reflections. To the best of our knowledge, we are the first to implement a through-wall tracking using the COTS RFID systems. Empirical measurements with a prototype show that Tadar can detect objects behind 5\" hollow wall and 8\" concrete wall, and achieve median tracking errors of 7.8cm and 20cm in the X and Y dimensions."},
{"Title": "Kaleido", "URL": "https://dl.acm.org/doi/10.1145/2789168.2790106", "Full Abstract": "Recently a number of systems have been developed to implement and improve the visual communication over screen-camera links. In this paper we study an opposite problem: how to prevent unauthorized users from videotaping a video played on a screen, such as in a theater, while do not affect the viewing experience of legitimate audiences. We propose and develop a light-weight hardware-free system, called Kaleido, that ensures these properties by taking advantage of the limited disparities between the screen-eye channel and the screen-camera channel. Kaleido does not require any extra hardware and is purely based on re-encoding the original video frame into multiple frames used for displaying. We extensively test our system Kaleido using a variety of smartphone cameras. Our experiments confirm that Kaleido preserves the high-quality screen-eye channel while reducing the secondary screen-camera channel quality significantly."},
{"Title": "Quality-Aware Online Task Assignment in Mobile Crowdsourcing", "URL": "https://dl.acm.org/doi/10.1109/MASS.2015.40", "Full Abstract": "Mobile crowd sourcing (MCS) has grown to be a powerful computation paradigm to harness human power to solve real-world problems. Many commercial MCS platforms have arisen, enabling various novel applications. As crowd workers can be unreliable, a critical issue of these platforms is quality control. Many task assignment approaches have been proposed to increase the quality of crowd sourced tasks by matching workers and tasks in a bipartite graph. However, they fail to apply to MCS platforms where tasks are bound with locations. This paper considers the quality-aware online task assignment problem with location-based tasks. The goal is to optimize tasks' overall quality by assigning appropriate sets of tasks to workers in an online manner. To solve this problem, we propose a probabilistic quality measurement model and a hitchhiking model to characterize workers' behavior. Then we design a polynomial-time online assignment algorithm and prove that the proposed algorithm approximates the offline optimal solution with a competitive ratio of 10/7. Through extensive simulations, we demonstrate the efficiency and effectiveness of our solution."},
{"Title": "PerLoc", "URL": "https://dl.acm.org/doi/10.1109/MASS.2015.47", "Full Abstract": "With the rapid development of mobile applications, there is an urgent need for highly efficient indoor localization service. Dedicated systems achieve good accuracy at the cost of deploying special hardware. Fingerprint-based methods avoid maintaining the expensive infrastructure but suffer from intensive labor for site-survey and poor robustness. In this paper, we present Per Loc, an infrastructure-free localization system which leverages rich vision features in indoor environment with high efficiency and accuracy. Per Loc makes use of binocular ranging technique to calculate the depth of a feature point and then figures out its geographical coordinates to build up reference point database, for which we design a filtering scheme to keep the database efficient in storage and search delay. During the localization stage, users simply take a photo of surroundings and feature points are extracted automatically as input for search scheme. Then a fast two-stage search scheme is proposed to find the nearest neighbors of query feature points in reference point database. Based on the perspective projection model, we inversely calculate users' geographical location in real time. We implement the proposed localization system on commercial smartphones as well as laptops and conduct extensive experiments. Per Loc achieves 1.76m of average error in office environment, and 2.2m of average error in shopping mall."},
{"Title": "ShopMiner", "URL": "https://dl.acm.org/doi/10.1145/2809695.2809710", "Full Abstract": "Shopping behavior data are of great importance to understand the effectiveness of marketing and merchandising efforts. Online clothing stores are capable capturing customer shopping behavior by analyzing the click stream and customer shopping carts. Retailers with physical clothing stores, however, still lack effective methods to identify comprehensive shopping behaviors. In this paper, we show that backscatter signals of passive RFID tags can be exploited to detect and record how customers browse stores, which items of clothes they pay attention to, and which items of clothes they usually match with. The intuition is that the phase readings of tags attached on desired items will demonstrate distinct yet stable patterns in the time-series when customers look at, pick up or turn over desired items. We design ShopMiner, a framework that harnesses these unique spatial-temporal correlations of time-series phase readings to detect comprehensive shopping behaviors. We have implemented a prototype of ShopMiner, with a COTS RFID reader and four antennas, and tested its effectiveness in two typical indoor environments. Empirical studies from two-week shopping-like data show that ShopMiner, could achieve high accuracy and efficiency in customer shopping behavior identification."},
{"Title": "Q-Offload", "URL": "https://dl.acm.org/doi/10.1109/RTSS.2015.30", "Full Abstract": "Driven by the proliferation of mobile applications, the conflict between data communication requirement and limited battery capacity is becoming sharp on modern smartphones. Offloading mobile traffic from cellular to WiFi is widely recognized as a viable solution to improve the energy efficiency. However, through extensive field experiments, we find WiFi offloading is not always energy efficient and even consumes more energy than cellular network due to link quality variation. In addition, we also observe that practical data transmission deadline requirement and link utilization allows scheduling of data traffic to time periods with good link quality. Accordingly, we propose Q-offload, the first attempt towards energy efficient WiFi offloading with link dynamics. In Q-offload, we propose an iterative framework to achieve energy efficient WiFi offloading by exploiting good link quality while not affecting user experience. We evaluate the performance of Q-offload through both trace-driven analysis and real-world experiments. The results show that it can achieve 33.5%~55.7% energy efficiency improvement, compared with state-of-the-arts under different conditions."},
{"Title": "Proving partial order properties", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2894%2990009-4", "Full Abstract": "No abstract available."},
{"Title": "Development of Hybrid Systems", "URL": "https://dl.acm.org/doi/10.5555/646843.759703", "Full Abstract": "No abstract available."},
{"Title": "Symbolic Controller Synthesis for Discrete and Timed Systems", "URL": "https://dl.acm.org/doi/10.5555/646875.709990", "Full Abstract": "No abstract available."},
{"Title": "Reachability analysis of dynamical systems having piecewise-constant derivatives", "URL": "https://dl.acm.org/doi/10.1016/0304-3975%2894%2900228-B", "Full Abstract": "No abstract available."},
{"Title": "OBDD''s LTL MC Model Checking of Linear TL, Using OBDD''s", "URL": "https://dl.acm.org/doi/book/10.5555/903745", "Full Abstract": "In this straightforward paper we present algorithms for checking satisfiability and satisfiability over a finite-state fair transition system (model checking) of full (past included) propositional linear-time temporal logic, using OBDD's. In checking satisfiability over a fair transition system we take into account the full set of fairness requirements, including justice (weak fairness) and compassion (strong fairness). For the case that the formula is found to be satisfiable or satisfiable over a finite-state program, we describe algorithms that extract a satisfying model."},
{"Title": "On the learnability of infinitary regular sets", "URL": "https://dl.acm.org/doi/10.1006/inco.1995.1070", "Full Abstract": "No abstract available."},
{"Title": "Once and For All", "URL": "https://dl.acm.org/doi/10.5555/788017.788753", "Full Abstract": "It has long been known that past-time operators add no expressive power to linear temporal logics. In this paper, we consider the extension of branching temporal logics with past-time operators. Two possible views regarding the nature of past in a branching- time model induce two different such extensions. In the first view, past is branching and each moment in time may have several possible futures and several possible pasts. In the second view, past is linear and each moment in time may have several possible futures and a unique past. Both views assume that past is finite. We discuss the practice of these extensions as specification languages, characterize their expressive power, and examine the complexity of their model-checking and satisfiability problems."},
{"Title": "A Complete Proof Systems for QPTL", "URL": "https://dl.acm.org/doi/10.5555/788017.788779", "Full Abstract": "The paper presents an axiomatic system for \\emm{quantified propositional temporal logic (\\qptl), which is propositional temporal logic equipped with quantification over propositions (boolean variables). The advantages of this extended temporal logic is that its expressive power is strictly higher than that of the un-quantified version (\\ptl) and is equal to that of S1S, as well as that of \\omega-automata. Another important application of \\qptl\\ is its use for formulating and verifying refinement relations between reactive systems. In fact, the completeness proof is based on the reduction of a \\qptl\\ formula into a \\buchi\\ automaton, and performing equivalence transformations on this automata, formally justifying these transformations."},
{"Title": "Timing analysis of asynchronous circuits using timed automata", "URL": "https://dl.acm.org/doi/10.5555/646702.701843", "Full Abstract": "No abstract available."},
{"Title": "Using Ghost Variables to Prove Refinement", "URL": "https://dl.acm.org/doi/10.5555/646057.678341", "Full Abstract": "No abstract available."},
{"Title": "A Platform for Combining Deductive with Algorithmic Verification", "URL": "https://dl.acm.org/doi/10.5555/647765.735986", "Full Abstract": "No abstract available."},
{"Title": "Architectural complexity measures of recurrent neural networks", "URL": "https://dl.acm.org/doi/10.5555/3157096.3157301", "Full Abstract": "In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the"},
{"Title": "On multiplicative integration with recurrent neural networks", "URL": "https://dl.acm.org/doi/10.5555/3157382.3157418", "Full Abstract": "We introduce a general and simple structural design called \"Multiplicative Integration\" (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models."},
{"Title": "Binarized neural networks", "URL": "https://dl.acm.org/doi/10.5555/3157382.3157557", "Full Abstract": "We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At train-time the binary weights and activations are used for computing the parameter gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs, we conducted two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. We also report our preliminary results on the challenging ImageNet dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line."},
{"Title": "Professor forcing", "URL": "https://dl.acm.org/doi/10.5555/3157382.3157612", "Full Abstract": "The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar."},
{"Title": "Quantized neural networks", "URL": "https://dl.acm.org/doi/10.5555/3122009.3242044", "Full Abstract": "We introduce a method to train Quantized Neural Networks (QNNs) -- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At traintime the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves 51% top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online."},
{"Title": "Multiresolution recurrent neural networks", "URL": "https://dl.acm.org/doi/10.5555/3298023.3298046", "Full Abstract": "We introduce a new class of models called"},
{"Title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "URL": "https://dl.acm.org/doi/10.5555/3298023.3298047", "Full Abstract": "Sequential data often possesses hierarchical structures with complex dependencies between sub-sequences, such as found between the utterances in a dialogue. To model these dependencies in a generative framework, we propose a neural network-based generative architecture, with stochastic latent variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with other recent neural-network architectures. We evaluate the model performance through a human evaluation study. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate both the generation of meaningful, long and diverse responses and maintaining dialogue state."},
{"Title": "Denoising criterion for variational auto-encoding framework", "URL": "https://dl.acm.org/doi/10.5555/3298483.3298537", "Full Abstract": "Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tractable bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average log-likelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets."},
{"Title": "On random weights for texture generation in one layer CNNS", "URL": "https://dl.acm.org/doi/10.1109/ICASSP.2017.7952548", "Full Abstract": "Recent work in the literature has shown experimentally that one can use the lower layers of a trained convolutional neural network (CNN) to model natural textures. More interestingly, it has also been experimentally shown that only one layer with random filters can also model textures although with less variability. In this paper we ask the question as to why one layer CNNs with random filters are so effective in generating textures? We theoretically show that one layer convolutional architectures (without a non-linearity) paired with the an energy function used in previous literature, can in fact preserve and modulate frequency coefficients in a manner so that random weights and pretrained weights will generate the same type of images. Based on the results of this analysis we question whether similar properties hold in the case where one uses one convolution layer with a non-linearity. We show that in the case of ReLu non-linearity there are situations where only one input will give the minimum possible energy whereas in the case of no nonlinearity, there are always infinite solutions that will give the minimum possible energy. Thus we can show that in certain situations adding a ReLu non-linearity generates less variable images."},
{"Title": "A network of deep neural networks for Distant Speech Recognition", "URL": "https://dl.acm.org/doi/10.1109/ICASSP.2017.7953084", "Full Abstract": "Despite the remarkable progress recently made in distant speech recognition, state-of-the-art technology still suffers from a lack of robustness, especially when adverse acoustic conditions characterized by non-stationary noises and reverberation are met."},
{"Title": "Stdp-compatible approximation of backpropagation in an energy-based model", "URL": "https://dl.acm.org/doi/10.1162/NECO_a_00934", "Full Abstract": "We show that Langevin Markov chain Monte Carlo inference in an energy-based model with latent variables has the property that the early steps of inference, starting from a stationary point, correspond to propagating error gradients into internal layers, similar to backpropagation. The backpropagated error is with respect to output units that have received an outside driving force pushing them away from the stationary point. Backpropagated error gradients correspond to temporal derivatives with respect to the activation of hidden units. These lead to a weight update proportional to the product of the presynaptic firing rate and the temporal rate of change of the postsynaptic firing rate. Simulations and a theoretical argument suggest that this rate-based update rule is consistent with those associated with spike-timing-dependent plasticity. The ideas presented in this article could be an element of a theory for explaining how brains perform credit assignment in deep hierarchies as efficiently as backpropagation does, with neural computation corresponding to both approximate inference in continuous-valued latent variables and error backpropagation, at the same time."},
{"Title": "Multiple mental attitudes in agents", "URL": "https://dl.acm.org/doi/10.5555/137984.137993", "Full Abstract": "No abstract available."},
{"Title": "Multiple mental attitudes in agents", "URL": "https://dl.acm.org/doi/10.5555/1029762.1029771", "Full Abstract": "The TARK series of symposia has centered around the notions of knowledge and belief. The purpose of this panel is to ask whether additional categories should be brought into the picture, and if so how."},
{"Title": "The representational geometry of word meanings acquired by neural machine translation models", "URL": "https://dl.acm.org/doi/10.1007/s10590-017-9194-2", "Full Abstract": "This work is the first comprehensive analysis of the properties of word embeddings learned by neural machine translation (NMT) models trained on bilingual texts. We show the word representations of NMT models outperform those learned from monolingual text by established algorithms such as Skipgram and CBOW on tasks that require knowledge of semantic similarity and/or lexical---syntactic role. These effects hold when translating from English to French and English to German, and we argue that the desirable properties of NMT word embeddings should emerge largely independently of the source and target languages. Further, we apply a recently-proposed heuristic method for training NMT models with very large vocabularies, and show that this vocabulary expansion method results in minimal degradation of embedding quality. This allows us to make a large vocabulary of NMT embeddings available for future research and applications. Overall, our analyses indicate that NMT embeddings should be used in applications that require word concepts to be organised according to similarity and/or lexical function, while monolingual embeddings are better suited to modelling (nonspecific) inter-word relatedness."},
{"Title": "Protograms", "URL": "https://dl.acm.org/doi/10.5555/139492.139654", "Full Abstract": "No abstract available."},
{"Title": "On traffic laws for mobile robots", "URL": "https://dl.acm.org/doi/10.5555/139492.139664", "Full Abstract": "No abstract available."},
{"Title": "On the synthesis of useful social laws for artificial agent societies", "URL": "https://dl.acm.org/doi/10.5555/1867135.1867178", "Full Abstract": "We present a general model of social law in a computational system, and investigate some of its properties. The contribution of this paper is twofold. First, we argue that the notion of social law is not epiphenomenal, but rather should be built into the action representation; we then offer such a representation. Second, we investigate the complexity of automatically deriving useful social laws in this model, given descriptions of the agents' capabilities, and the goals they might encounter. We show that in general the problem is NP-complete, and identify precise conditions under which it becomes polynomial."},
{"Title": "Deriving properties of belief update from theories of action", "URL": "https://dl.acm.org/doi/10.5555/1867135.1867225", "Full Abstract": "Two areas that have attracted much interest in recent years, belief update and reasoning about action, have so far been largely disjoint. Indeed, at first glance there appears to be little connection between them. In this paper we argue that this first impression is wrong; specifically, we show that the postulates for belief update recently proposed in [Katsuno and Mendelzon, 1991], can in fact be analytically derived, using the formal theory of action proposed in [Lin and Shoham, 1991]."},
{"Title": "Concurrent actions in the situation calculus", "URL": "https://dl.acm.org/doi/10.5555/1867135.1867226", "Full Abstract": "We propose a representation of Concurrent actions; rather than invent a new formalism, we model them within the standard situation calculus by introducing the notions of global actions and primitive actions, whose relationship is analogous to' that between situations and fluents. The result is a framework in which situations and actions play quite symmetric roles. The rich structure of actions gives rise to' a new problem, which, due to' this symmetry between actions and situations, is analogous to' the traditional frame problem. In [Lin and Shoham 1991] we provided a solution to' the frame problem based on a formal adequacy criterion called \"epistemological completeness.\" Here we show how to' solve the new problem based on the same adequacy criterion."},
{"Title": "Multi-Agent Research in the Knobotics Group", "URL": "https://dl.acm.org/doi/10.5555/646907.710647", "Full Abstract": "No abstract available."},
{"Title": "A logic of knowledge and justified assumptions", "URL": "https://dl.acm.org/doi/10.1016/0004-3702%2892%2990019-T", "Full Abstract": "No abstract available."},
{"Title": "Emergent conventions in multi-agent systems", "URL": "https://dl.acm.org/doi/10.5555/3087223.3087246", "Full Abstract": "No abstract available."},
{"Title": "Agent-oriented programming", "URL": "https://dl.acm.org/doi/10.1016/0004-3702%2893%2990034-9", "Full Abstract": "No abstract available."},
{"Title": "Reasoning precisely with vague concepts", "URL": "https://dl.acm.org/doi/10.5555/1867270.1867334", "Full Abstract": "Many knowledge-based systems need to represent vague concepts. Although the practical approach of representing vague concepts as precise intervals over numbers is well-accepted in AI, there is no systematic method to delimit the boundaries of intervals, only ad hoc methods. We present a framework to reason precisely with vague concepts based on the observation that the vague concepts and their interval-boundaries are constrained by the underlying domain knowledge. The framework is comprised of a constraint language to represent logical constraints on vague concepts, as well as numerical constraints on the intervalboundaries; a query language to request information about the interval boundaries; and a computational mechanism to answer the queries. A key step in answering queries is preprocessing the constraints by extracting the numerical constraints from the logical constraints and combining them with the given numerical constraints."},
{"Title": "Towards knowledge-level analysis of motion planning", "URL": "https://dl.acm.org/doi/10.5555/1867270.1867370", "Full Abstract": "Inspired by the success of the distributed computing community in applying logics of knowledge and time to reasoning about distributed protocols, we aim for a similarly powerful and high-level abstraction when reasoning about control problems involving uncertainty. Here we concentrate on robot motion planning, with uncertainty in both control and sensing. This problem has already been well studied within the robotics community. Our contributions include the following: We define a new, natural problem in this domain: obtaining a sound and complete termination condition, given initial and goal locations. We consider a specific class of (simple) motion plans in R"},
{"Title": "Deriving properties of belief update from theories of action (II)", "URL": "https://dl.acm.org/doi/10.5555/1624025.1624128", "Full Abstract": "In [del Val and Shoham, 1992] we showed that the postulates for belief update recently proposed by Katsuno and Mendelzon [1991] can be analytically derived using the formal theory of action proposed by Lin and Shoham [1991]. The contribution of this paper is twofold; • Whereas in [del Val and Shoham, 1992] we only showed that our encoding of the update problem satisfied the KM postulates, here we use an independently motivated generalization of the theory of action used in that paper to provide a one-to-one correspondence between our construction and KM update semantics. • We show how the KM semantics can be generalized by relaxing our construction in a number of ways, each justified in certain intuitive circumstances and each corresponding to one specific postulate. It follows that there are reasonable update operators outside the KM family."},
{"Title": "Artificial Intelligence", "URL": "https://dl.acm.org/doi/book/10.5555/528987", "Full Abstract": "From the Publisher:"},
{"Title": "Belief as defeasible knowledge", "URL": "https://dl.acm.org/doi/10.1016/0004-3702%2893%2990107-M", "Full Abstract": "No abstract available."},
{"Title": "Logics of Mental Attitudes in AI", "URL": "https://dl.acm.org/doi/10.5555/645296.648467", "Full Abstract": "No abstract available."},
{"Title": "Deriving Properties of Belief Update from Theories of Action", "URL": "https://dl.acm.org/doi/book/10.5555/892535", "Full Abstract": "We present an approach to database update as a form of non monotonic temporal reasoning, the main idea of which is the (circumscriptive) minimization of changes with respect to a set of facts declared ``persistent by default.'' The focus of the paper is on the relation between this approach and the update semantics recently proposed by Katsuno and Mendelzon. Our contribution in this regard is twofold: - We prove a representation theorem for KM semantics in terms of a restricted subfamily of the operators defined by our construction. - We show how the KM semantics can be generalized by relaxing our construction in a number of ways, each justified in certain intuitive circumstances and each corresponding to one specific postulate. It follows that there are reasonable update operators outside the KM family. Our approach is not dependent for its plausibility on this connection with KM semantics. Rather, it provides a relatively rich and flexible framework in which the frame and ramification problems can be solved in a systematic way by reasoning about default persistence of facts."},
{"Title": "Knowledge as a tool in motion planning under uncertainty", "URL": "https://dl.acm.org/doi/10.5555/1028104.1028120", "Full Abstract": "Inspired by the success of the distributed computing community in applying logics of knowledge and time to reasoning about distributed protocols, we aim for a similarly powerful and high-level abstraction when reasoning about control problems involving uncertainty. Here we concentrate on robot motion planning, with uncertainty in both control and sensing. This problem has already been well studied within the robotics community. Our contributions include the following:"},
{"Title": "Co-Learning and the Evolution of Social Acitivity", "URL": "https://dl.acm.org/doi/book/10.5555/892545", "Full Abstract": "We introduce the notion of co-learning, which refers to a process in which several agents simultaneously try to adapt to one another's behavior so as to produce desirable global system properties. Of particular interest are two specific co-learning settings, which relate to the emergence of conventions and the evolution of cooperation in societies, respectively. We define a basic co-learning rule, called Highest Cumulative Reward (HCR), and show that it gives rise to quite nontrivial system dynamics. In general, we are interested in the eventual convergence of the co-learning system to desirable states, as well as in the efficiency with which this convergence is attained. Our results on eventual convergence are analytic; the results on efficiency properties include analytic lower bounds as well as empirical upper bounds derived from rigorous computer simulations."},
{"Title": "On parallel hashing and integer sorting", "URL": "https://dl.acm.org/doi/10.5555/90397.92373", "Full Abstract": "No abstract available."},
{"Title": "On Parallel Hashing and Integer Sorting (Extended Summary)", "URL": "https://dl.acm.org/doi/10.5555/646244.684360", "Full Abstract": "No abstract available."},
{"Title": "Randomized range-maxima in nearly-constant parallel time", "URL": "https://dl.acm.org/doi/book/10.5555/132473", "Full Abstract": "No abstract available."},
{"Title": "Converting high probability into nearly-constant time—with applications to parallel hashing", "URL": "https://dl.acm.org/doi/10.1145/103418.103453", "Full Abstract": "Copyright © 1991 ACM."},
{"Title": "Fast hashing on a PRAM—designing by expectation", "URL": "https://dl.acm.org/doi/10.5555/127787.127837", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Converting high probability into nearly-constant time—with applications to parallel hashing", "URL": "https://dl.acm.org/doi/book/10.5555/116336", "Full Abstract": "No abstract available."},
{"Title": "Fast hashing on a PRAM—designing by expectation", "URL": "https://dl.acm.org/doi/book/10.5555/116337", "Full Abstract": "No abstract available."},
{"Title": "A neural network model for a randomized frequency-spatial transformation", "URL": "https://dl.acm.org/doi/book/10.5555/116508", "Full Abstract": "No abstract available."},
{"Title": "Elections in anonymous networks", "URL": "https://dl.acm.org/doi/book/10.5555/121828", "Full Abstract": "No abstract available."},
{"Title": "A simple randomized sieve algorithm for the closet-pair problem", "URL": "https://dl.acm.org/doi/book/10.5555/121811", "Full Abstract": "No abstract available."},
{"Title": "Towards a theory of nearly constant time parallel algorithms", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1991.185438", "Full Abstract": "It is demonstrated that randomization is an extremely powerful tool for designing very fast and efficient parallel algorithms. Specifically, a running time of O(lg* n) (nearly-constant), with high probability, is achieved using n/lg* n (optimal speedup) processors for a wide range of fundamental problems. Also given is a constant time algorithm which, using n processors, approximates the sum of n positive numbers to within an error which is smaller than the sum by an order of magnitude. A variety of known and new techniques are used. New techniques, which are of independent interest, include estimation of the size of a set in constant time for several settings, and ways for deriving superfast optimal algorithms from superfast nonoptimal ones."},
{"Title": "Towards a theory of nearly constant time parallel algorithms", "URL": "https://dl.acm.org/doi/book/10.5555/130191", "Full Abstract": "No abstract available."},
{"Title": "Leaders election without a conflict resolution rule—fast and efficient randomized simulations among CRCW PRAMs", "URL": "https://dl.acm.org/doi/book/10.5555/130192", "Full Abstract": "No abstract available."},
{"Title": "Towards Energy Efficient Duty-Cycled Networks: Analysis, Implications and Improvement", "URL": "https://dl.acm.org/doi/10.1109/TC.2015.2417558", "Full Abstract": "Duty cycling mode is widely adopted in wireless sensor networks to save energy. Existing duty-cycling protocols cannot well adapt to different data rates and dynamics, resulting in a high energy consumption in real networks. Improving those protocols may require global information or heavy computation and thus may not be practical, leading to many empirical parameters in real protocols. To fill the gap between the application requirement and protocol performance, in this paper, we analyze the energy consumption for duty cycled sensor networks with different data rates. Our analysis shows that existing protocols cannot lead to an efficient energy consumption in various scenarios. Based on the analysis, we design a light-weight adaptive duty-cycling protocol (LAD), which reduces the energy consumption under different data rates and protocol dynamics. LAD can adaptively adjust the protocol parameters according to network conditions such as data rate and achieve an optimal energy efficiency. To make LAD practical in real network, we further pre-calculate optimal parameters offline and store them on sensor nodes, which significantly reduces the computation time. We theoretically validate the performance improvement of the protocol. We implement the protocol in TinyOS and extensively evaluate it on 40 TelosB nodes. The evaluation results show the energy consumption can be reduced by 28.2-40.1 percent compared with state-of-the-art protocols. Results based on data from a 1,200-node operational network further show the effectiveness and scalability of the design."},
{"Title": "Continuous Answering Holistic Queries over Sensor Networks", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2015.2407892", "Full Abstract": "Sensor networks are widely used in various domains like the intelligent transportation systems. Users issue queries to sensors and collect sensing data. Due to the low quality sensing devices or random link failures, sensor data are often noisy. In order to increase the reliability of the query results, continuous queries are often employed. In this work we focus on continuous holistic queries like Median. Existing approaches are mainly designed for non-holistic queries like Average. However, it is not trivial to answer holistic ones due to their non-decomposable property. We first propose two schemes based on the data correlation between different rounds, with one for getting the exact answers and the other one for deriving the approximate results. We then combine the two proposed schemes into a hybrid approach, which is adaptive to the data changing speed. We evaluate this design through extensive simulations. The results show that our approach significantly reduces the traffic cost compared with previous works while maintaining the same accuracy."},
{"Title": "Hitchhike", "URL": "https://dl.acm.org/doi/10.1109/TWC.2015.2487961", "Full Abstract": "Recently, carrying control signals on passing data packets has emerged as a promising direction for efficient control information transmission. With control messages carried on data payload, the extra air time needed for control packets like RTS/CTS is eliminated and thus channel utilization is improved. However, carrying control signals on the data payload of a packet requires the data packet to have a sufficiently large SNR, otherwise both the data packet and the control messages are lost. In this paper, we propose"},
{"Title": "Privacy-Aware High-Quality Map Generation with Participatory Sensing", "URL": "https://dl.acm.org/doi/10.1109/TMC.2015.2421946", "Full Abstract": "Accurate maps are increasingly important with the growth of smart phones and the development of location-based services. Several crowdsourcing based map generation protocols that rely on users to provide their traces have been proposed. Being creative, however, those methods pose a significant threat to user privacy as the traces can easily imply user behavior patterns. On the flip side, crowdsourcing-based map generation method does need individual locations. To address the issue, we present a systematic participatory-sensing-based high-quality map generation scheme, PMG, that meets the privacy demand of individual users. To be specific, the individual users merely need to upload unorganized sparse location points to reduce the risk of exposing users’ traces and utilize the"},
{"Title": "BackPos", "URL": "https://dl.acm.org/doi/10.1109/TMC.2015.2424437", "Full Abstract": "Radio frequency identification (RFID) technology has been widely adopted in a variety of applications from logistics to access control. Many applications gain benefits from knowing the exact position of an RFID-tagged object. Existing localization algorithms in wireless network, however, can hardly be directly employed due to tag’s limited capabilities in terms of energy and memory. In this paper, we propose BackPos, a fine-grained backscatter positioning technique using the commercial off-the-shelf (COTS) RFID products with detected phases. Our studies show that the phase is a stable indicator highly related to tag’s position and preserved over frequency or tag orientation, but challenged by its periodicity and tag’s diversity. We attempt to infer the distance differences from phases detected by antennas under triangle constraint. Further, hyperbolic positioning using the distance differences is employed to shrink the tag’s candidate positions until finding out the real one. In combination with interrogation zone, we finally relax the triangle constraint and allow arbitrary deployment of antennas by sacrificing the feasible region. We implement a prototype of BackPos with COTS RFID products and evaluate this design in various scenarios. The results show that BackPos achieves the mean accuracy of"},
{"Title": "Exploring cross-application cellular traffic optimization with Baidu TrafficGuard", "URL": "https://dl.acm.org/doi/10.5555/2930611.2930616", "Full Abstract": "As mobile cellular devices and traffic continue their rapid growth, providers are taking larger steps to optimize traffic, with the hopes of improving user experiences while reducing congestion and bandwidth costs. This paper presents the design, deployment, and experiences with Baidu TrafficGuard, a cloud-based mobile proxy that reduces cellular traffic using a network-layer VPN. The VPN connects a client-side proxy to a centralized traffic processing cloud. TrafficGuard works transparently across heterogeneous applications, and effectively reduces cellular traffic by 36% and overage instances by 10.7 times for roughly 10 million Android users in China. We discuss a large-scale cellular traffic analysis effort, how the resulting insights guided the design of TrafficGuard, and our experiences with a variety of traffic optimization techniques over one year of deployment."},
{"Title": "Duplicate detectable opportunistic forwarding in duty-cycled wireless sensor networks", "URL": "https://dl.acm.org/doi/10.1109/TNET.2014.2387440", "Full Abstract": "Opportunistic routing, offering relatively efficient and adaptive forwarding in low-duty-cycled sensor networks, generally allows multiple nodes to forward the same packet simultaneously, especially in networks with intensive traffic. Uncoordinated transmissions often incur a number of duplicate packets, which are further forwarded in the network, occupy the limited network resource, and hinder the packet delivery performance. Existing solutions to this issue, e.g., overhearing or coordination based approaches, either cannot scale up with the system size, or suffer high control overhead. We present Duplicate-Detectable Opportunistic Forwarding (DOF), a duplicate-free opportunistic forwarding protocol for low-duty-cycled wireless sensor networks. DOF enables senders to obtain the information of all potential forwarders via a slotted acknowledgment scheme, so the data packets can be sent to the deterministic next-hop forwarder. Based on light-weight coordination, DOF explores the opportunities as many as possible and removes duplicate packets from the forwarding process. We implement DOF and evaluate its performance on an indoor testbed with 20 TelosB nodes. The experimental results show that DOF reduces the average duplicate ratio by 90%, compared to state-of-the-art opportunistic protocols, and achieves 61.5% enhancement in network yield and 51.4% saving in energy consumption."},
{"Title": "Privacy-friendly photo capturing and sharing system", "URL": "https://dl.acm.org/doi/10.1145/2971648.2971662", "Full Abstract": "The wide adoption of smart devices with onboard cameras facilitates photo capturing and sharing, but greatly increases people's concern on privacy infringement. Here we seek a solution to respect the privacy of persons being photographed in a smarter way that they can be automatically erased from photos captured by smart devices according to their requirements. To make this work, we need to address three challenges: 1) how to enable users explicitly express their privacy protection intentions without wearing any visible specialized tag, and 2) how to associate the intentions with persons in captured photos accurately and efficiently. Furthermore, 3) the association process itself should not cause portrait information leakage and should be accomplished in a privacy-preserving way. In this work, we design, develop, and evaluate a system, called"},
{"Title": "Indoor localization via multi-modal sensing on smartphones", "URL": "https://dl.acm.org/doi/10.1145/2971648.2971668", "Full Abstract": "Indoor localization is of great importance to a wide range of applications in shopping malls, office buildings and public places. The maturity of computer vision (CV) techniques and the ubiquity of smartphone cameras hold promise for offering sub-meter accuracy localization services. However, pure CV-based solutions usually involve hundreds of photos and pre-calibration to construct image database, a labor-intensive overhead for practical deployment. We present"},
{"Title": "Fast Composite Counting in RFID Systems", "URL": "https://dl.acm.org/doi/10.1109/TNET.2015.2483681", "Full Abstract": "Counting the number of tags is a fundamental issue and has a wide range of applications in RFID systems. Most existing protocols, however, only apply to the scenario where a single reader counts the number of tags covered by its radio, or at most the union of tags covered by multiple readers. They are unable to achieve more complex counting objectives, i.e., counting the number of tags in a composite set expression such as $S_{1 \\bigcup S_{2 - S_{3 \\bigcap S_{4$. This type of counting has realistic significance as it provides more diversity than existing counting scenario, and can be applied in various applications. We formally introduce the RFID composite counting problem, which aims at counting the tags in an arbitrary set expression and obtain its strong lower bounds on the communication cost. We then propose a generic Composite Counting Framework CCF that provides estimates for any set expression with desired accuracy. The communication cost of CCF is proved to be within a small factor from the optimal. We build a prototype system for CCF using USRP software defined radio and Intel WISP computational tags. Also, extensive simulations are conducted to evaluate the performance of CCF. The experimental results show that CCF is generic, accurate and time-efficient."},
{"Title": "Lasagna", "URL": "https://dl.acm.org/doi/10.1145/2973750.2973752", "Full Abstract": "The proliferation of mobile devices has enabled extensive mobile-data supported applications,"},
{"Title": "Making sense of mechanical vibration period with sub-millisecond accuracy using backscatter signals", "URL": "https://dl.acm.org/doi/10.1145/2973750.2973759", "Full Abstract": "Traditional vibration inspection systems, equipped with separated sensing and communication modules, are either very expensive ("},
{"Title": "Decimeter level passive tracking with wifi", "URL": "https://dl.acm.org/doi/10.1145/2980115.2980131", "Full Abstract": "Pioneer approaches for WiFi-based sensing usually employ learning-based techniques to seek appropriate statistical features, but do not support precise tracking without prior training. Thus to advance passive sensing, the ability to track fine-grained human mobility information acts as a key enabler. In this paper, we proposed"},
{"Title": "Secure and Private RFID-Enabled Third-Party Supply Chain Systems", "URL": "https://dl.acm.org/doi/10.1109/TC.2016.2538260", "Full Abstract": "Radio Frequency Identification (RFID) is a key emerging technology for supply chain systems. By attaching RFID tags to various products, product-related data can be efficiently indexed, retrieved and shared among multiple participants involved in an RFID-enabled supply chain. The flexible data access property, however, raises security and privacy concerns. In this paper, we target at security and privacy issues in RFID-enabled supply chain systems. We investigate RFID-enabled Third-party Supply chain (RTS) systems and identify several inherent security and efficiency requirements. We further design a Secure RTS system called SRTS, which leverages RFID tags to deliver computation-lightweight crypto-IDs in the RTS system to meet both the security and efficiency requirements. SRTS introduces a Private Verifiable Signature (PVS) scheme to generate computation-lightweight crypto-IDs for product batches, and couples the primitive in RTS system through careful design. We conduct theoretical analysis and experiments to demonstrate the security and efficiency of SRTS."},
{"Title": "CARM", "URL": "https://dl.acm.org/doi/10.1109/TMC.2015.2508814", "Full Abstract": "Received Signal Strength (RSS) maps provide fundamental information for mobile users, aiding the development of conflict graph and improving communication quality to cope with the complex and unstable wireless channels. In this paper, we present CARM: a scheme that exploits crowd-sensing to construct outdoor RSS maps using smartphone measurements. An alternative yet impractical approach in literature is to appeal to professionals with customized devices. Our work distinguishes itself from previous studies by supporting off-the-shelf smartphone devices, and more importantly, by mitigating the error-prone nature and inaccuracies of these devices to build RSS maps through crowd-sensing. The main challenges are that, we need to calibrate error-prone smartphone measurements with “inaccurate” and “incomplete” data. To address these challenges, we build the measurement error model of smartphone based on the experimental observations and analyses. Moreover, we propose an iterative method based on Davidon-Fletcher-Powell (DFP) algorithm, to estimate the parameters for the error models of each smartphone and the signal propagation models of each AP simultaneously. The key intuition is that, the calibrated measurements based on the error model are constrained by the physics of the signal propagation model. Finally, a model-driven RSS map construction scheme is built upon these two models with these estimated parameters. The theoretical analyses prove the optimality and convergence of this iterative method. Also, the crowd-sensing experiments show that, CARM can achieve an accurate RSS map, decreasing the average error from 19.8 to 8.5 dBm."},
{"Title": "Content Distribution for Mobile Internet", "URL": "https://dl.acm.org/doi/book/10.5555/2978148", "Full Abstract": "This book investigates the cloud-based techniques of content distribution mainly for mobile Internet. It starts with hot topics such as cellular traffic optimization and video content delivery. By integrating the cloud scheme, it further tackles issues of traffic-saving, energy-efficient, high-speed, and delay-tolerant content delivery with regard to mobile Internet. It covers both theoretical algorithms and their real-world system implementations. In particular, various well-known cloud platforms such as Baidu Traffic Guard, Tencent QQXuanfeng, Google Drive, Microsoft OneDrive, and Dropbox are elaborated respectively in the book. Lastly, it includes an educational and experimental cloud computing platform allowing public access, which benefits researchers, practitioners, and developers in the field of cloud computing/storage and mobile Internet. Throughout the book there are helpful and practical tips on setting up cloud systems that readers can easily follow."},
{"Title": "Fast and Adaptive Continuous Scanning in Large-Scale RFID Systems", "URL": "https://dl.acm.org/doi/10.1109/TNET.2016.2521333", "Full Abstract": "Radio Frequency Identification RFID technology plays an important role in supply chain logistics and inventory control. In these applications, a series of scanning operations at different locations are often needed to cover the entire inventory tags. In such continuous scanning scenario, adjacent scans inevitably read overlapping tags multiple times. Most existing methods suffer from low scanning efficiency when the overlap is small, since they do not distinguish the size of overlap which is an important factor of scanning performance. In this paper, we analytically unveil the fundamental relationship between the performance of continuous scanning and the size of overlap, deriving a critical threshold for the selection of scanning strategy. Further, we design an accurate estimator to approximate the overlap. Combining the estimate and a compact data structure, an adaptive scanning scheme is introduced to achieve low communication time. Through detailed analysis and extensive simulations, we demonstrate that the proposed scheme significantly outperforms previous approach in total scanning time."},
{"Title": "Every Packet Counts", "URL": "https://dl.acm.org/doi/10.1109/TNET.2016.2523127", "Full Abstract": "Delay is an important metric to understand and improve system performance. While existing approaches focus on aggregated delay statistics in pre-programmed granularity and provide results such as average and deviation, those approaches may not provide fine-grained delay measurement and thus may miss important delay characteristics. For example, delay anomaly, which is a critical system performance indicator, may not be captured by coarse-grained approaches. We propose a new measurement structure design called order preserving aggregator OPA. Based on OPA, we can efficiently encode and recover the ordering and loss information by exploiting inherent data characteristics. We then propose a two-layer design to convey both ordering and time stamp, and efficiently derive per-packet delay/loss measurement. We evaluate our approach both analytically and experimentally. The results show that our approach can achieve per-packet delay measurement with an average of per-packet relative error at 2%, and an average of aggregated relative error at $10^{-5$, while introducing additional communication overhead in the order of $10^{-4$ in terms of number of packets. While at a low data rate, the computation overhead of OPA is acceptable. Reducing the computation and communication overhead under high data rate, to make OPA more practical in real applications, will be our future direction."},
{"Title": "Data-Structures for the Verification of Timed Automata", "URL": "https://dl.acm.org/doi/10.5555/646883.756990", "Full Abstract": "No abstract available."},
{"Title": "Symbolic Model Checking with Rich ssertional Languages", "URL": "https://dl.acm.org/doi/10.5555/647766.733608", "Full Abstract": "No abstract available."},
{"Title": "Some Progress in the Symbolic Verification of Timed Automata", "URL": "https://dl.acm.org/doi/10.5555/647766.736016", "Full Abstract": "No abstract available."},
{"Title": "Verification engineering", "URL": "https://dl.acm.org/doi/10.1145/1283920.259407", "Full Abstract": "No abstract available."},
{"Title": "Verification engineering", "URL": "https://dl.acm.org/doi/10.1145/259380.259407", "Full Abstract": "No abstract available."},
{"Title": "A Compositional Real-Time Semantics of STATEMATE Designs", "URL": "https://dl.acm.org/doi/10.5555/646738.702095", "Full Abstract": "No abstract available."},
{"Title": "Verifying out-of-order executions", "URL": "https://dl.acm.org/doi/10.5555/646703.701870", "Full Abstract": "No abstract available."},
{"Title": "Two Decades of Temporal Logic", "URL": "https://dl.acm.org/doi/10.5555/795663.796320", "Full Abstract": "Abstract This year (1997) marks the 20th anniversary of the introduction of Temporal Logic (TL) into Computer Science as a language proposed for the specification and verification of reactive systems. The talk will present a personal view of the main contributions and achievements of TL over the last two decades emphasizing the recent, increase of acceptance and adoption of the temporal methodology by industry. We will then proceed to identify future challenges and directions that may extend the applicability of TL to wider classes of applications and the treatment of ever larger systems. We start by identifying the class of reactive systems for whose specification and verification TL has proved to be a most natural and effective language. These are systems whose role is to maintain an ongoing interaction with their environment, rather than produce a final result on termination. Most, embedded systems which control an external physical environment be-long to this important class. Such systems must be specified and analyzed in terms of their behavior."},
{"Title": "Translation Validation", "URL": "https://dl.acm.org/doi/10.5555/646482.691453", "Full Abstract": "No abstract available."},
{"Title": "Deductive vs. Model-Theoretic Approaches to Formal Verification (Abstract of Invited Talk)", "URL": "https://dl.acm.org/doi/10.5555/648234.753475", "Full Abstract": "No abstract available."},
{"Title": "Translation Validation for Synchronous Languages", "URL": "https://dl.acm.org/doi/10.5555/646252.686146", "Full Abstract": "No abstract available."},
{"Title": "Algorithmic Verification of Linear Temporal Logic Specifications", "URL": "https://dl.acm.org/doi/10.5555/646252.756784", "Full Abstract": "No abstract available."},
{"Title": "Modularization and Abstraction", "URL": "https://dl.acm.org/doi/10.5555/645727.667507", "Full Abstract": "No abstract available."},
{"Title": "On Discretization of Delays in Timed Automata and Digital Circuits", "URL": "https://dl.acm.org/doi/10.5555/646733.701304", "Full Abstract": "No abstract available."},
{"Title": "Herbrand Automata for Hardware Verification", "URL": "https://dl.acm.org/doi/10.5555/646733.701323", "Full Abstract": "No abstract available."},
{"Title": "Fair Synchronous Transition Systems and Their Liveness Proofs", "URL": "https://dl.acm.org/doi/10.5555/646845.706935", "Full Abstract": "No abstract available."},
{"Title": "Translation Validation", "URL": "https://dl.acm.org/doi/10.5555/647539.729871", "Full Abstract": "Translation validation is an alternative to the verification of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verification), each individual translation (i.e. a run of the compiler) is followed by a validation phase which verifies that the target code produced on this run correctly implements the submitted source program. In order to be a practical alternative to compiler verification, a key feature of this validation is its full automation."},
{"Title": "A Fast Algorithm for Scheduling Time-Constrained Instructions on Processors with ILP", "URL": "https://dl.acm.org/doi/10.5555/522344.825707", "Full Abstract": "Instruction scheduling is central to achieving performance in modern processors with instruction level parallelism (ILP). Classical work in this area has spanned the theoretical foundations of algorithms for instruction scheduling with provable optimality, as well as heuristic approaches with experimentally validated performance improvements. Typically, the theoretical foundations are developed in the context of basic-blocks of code. In this paper, we provide the theoretical foundations for scheduling basic-blocks of instructions with time-constraints, which can play an important role in compile-time ILP optimizations in embedded applications. We present an algorithm for scheduling unit-execution-time instructions on machines with multiple pipelines, in the presence of precedence constraints, release-times, deadlines, and latencies $l_{ij$ between any pairs of instructions $i$ and $j$. Our algorithm runs in time $O(n^3\\alpha(n))$, where $\\alpha(n)$ is the functional inverse of the Ackermann function. It can be used construct feasible schedules for two classes of instances:one pipeline and the latencies between instructions are restricted to the values of 0 and 1, and arbitrary number of pipelines and monotone-interval order precedences. %The algorithm can also be used to construct minimal tardiness %schedules in polynomial time. Our result can be seen as a natural extension of previous work on instruction scheduling for pipelined machines in the presence of deadlines."},
{"Title": "Verification of Data-Insensitive CIrcuits", "URL": "https://dl.acm.org/doi/10.5555/646185.683079", "Full Abstract": "There is a large class of circuits (including pipeline and out-of-order execution components) which can be formally verified while completely ignoring the precise characteristics (e.g. word-size) of the data manipulated by the circuits. In the literature, this is often described as the use of uninterpreted functions, implying that the concrete operations applied to the data are abstracted into unknown and featureless functions. In this paper, we briefly introduce an abstract unifying model for such data-insensitive circuits, and claim that the development of such models, perhaps even a theory of circuit schemas, can significantly contribute to the development of efficient and comprehensive verification algorithms combining deductive as well as enumerative methods.As a case study, we present in this paper an algorithm for out-of-order execution with in-order retirement and show it to be a refinement of the sequential instruction execution algorithm. Refinement is established by deductively proving (using pvs) that the register files of the out-of-order algorithm and the sequential algorithm agree at all times if the two systems are synchronized at instruction retirement time."},
{"Title": "Plan, attend, generate", "URL": "https://dl.acm.org/doi/10.5555/3295222.3295299", "Full Abstract": "We investigate the integrationofaplanning mechanism into sequence-to-sequence models using attention. Wedevelopamodel which can plan ahead inthe future when it computes its alignments between input and output sequences, constructingamatrix of proposed future alignments andacommitment vector that governs whetherto follow or recompute the plan. This mechanismisinspiredbythe recently proposed strategic attentive reader and writer (STRAW) model for Reinforcement Learning. Our proposed modelisend-to-end trainable using primarily differentiable operations. Weshow that it outperformsastrong baselineoncharacter-level translation tasks from WMT'15, the algorithmic taskoffinding Eulerian circuitsofgraphs, and question generation from the text. Our analysis demonstrates that the model computes qualitatively intuitive alignments, converges faster than the baselines, and achieves superior performance with fewer parameters."},
{"Title": "Z-forcing", "URL": "https://dl.acm.org/doi/10.5555/3295222.3295416", "Full Abstract": "Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortised variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables."},
{"Title": "Dynamic neural turing machine with continuous and discrete addressing schemes", "URL": "https://dl.acm.org/doi/10.1162/neco_a_01060", "Full Abstract": "We extend the neural Turing machine NTM model into a dynamic neural Turing machine D-NTM by introducing trainable address vectors. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies, including both linear and nonlinear ones. We implement the D-NTM with both continuous and discrete read and write mechanisms. We investigate the mechanisms and effects of learning to read and write into a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU controller. We provide extensive analysis of our model and compare different variations of neural Turing machines on this task. We show that our model outperforms long short-term memory and NTM variants. We provide further experimental results on the sequential <inline-formula><inline-graphic href=\"neco_a_01060inline1.gif\"/></inline-formula>MNIST, Stanford Natural Language Inference, associative recall, and copy tasks."},
{"Title": "Towards End-to-end Spoken Language Understanding", "URL": "https://dl.acm.org/doi/10.1109/ICASSP.2018.8461785", "Full Abstract": "Spoken language understanding system is traditionally designed as a pipeline of a number of components. First, the audio signal is processed by an automatic speech recognizer for transcription or n-best hypotheses. With the recognition results, a natural language understanding system classifies the text to structured data as domain, intent and slots for down-streaming consumers, such as dialog system, hands-free applications. These components are usually developed and optimized independently. In this paper, we present our study on an end-to-end learning system for spoken language understanding. With this unified approach, we can infer the semantic meaning directly from audio features without the intermediate text representation. This study showed that the trained model can achieve reasonable good result and demonstrated that the model can capture the semantic attention directly from the audio features."},
{"Title": "Monaural Singing Voice Separation with Skip-Filtering Connections and Recurrent Inference of Time-Frequency Mask", "URL": "https://dl.acm.org/doi/10.1109/ICASSP.2018.8461822", "Full Abstract": "Singing voice separation based on deep learning relies on the usage of time-frequency masking. In many cases the masking process is not a learnable function or is not encapsulated into the deep learning optimization. Consequently, most of the existing methods rely on a post processing step using the generalized Wiener filtering. This work proposes a method that learns and optimizes (during training) a source-dependent mask and does not need the aforementioned post processing step. We introduce a recurrent inference algorithm, a sparse transformation step to improve the mask generation process, and a learned denoising filter. Obtained results show an increase of 0.49 dB for the signal to distortion ratio and 0.30 dB for the signal to interference ratio, compared to previous state-of-the-art approaches for monaural singing voice separation."},
{"Title": "Dynamic Frame Skipping for Fast Speech Recognition in Recurrent Neural Network Based Acoustic Models", "URL": "https://dl.acm.org/doi/10.1109/ICASSP.2018.8462615", "Full Abstract": "A recurrent neural network is a powerful tool for modeling sequential data such as text and speech. While recurrent neural networks have achieved record-breaking results in speech recognition, one remaining challenge is their slow processing speed. The main cause comes from the nature of recurrent neural networks that read only one frame at each time step. Therefore, reducing the number of reads is an effective approach to reducing processing time. In this paper, we propose a novel recurrent neural network architecture called Skip-RNN, which dynamically skips speech frames that are less important. The Skip-RNN consists of an acoustic model network and skip-policy network that are jointly trained to classify speech frames and determine how many frames to skip. We evaluate our proposed approach on the Wall Street Journal corpus and show that it can accelerate acoustic model computation by up to 2.4 times without any noticeable degradation in transcription accuracy."},
{"Title": "Image-to-image translation for cross-domain disentanglement", "URL": "https://dl.acm.org/doi/10.5555/3326943.3327062", "Full Abstract": "Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets."},
{"Title": "MetaGAN", "URL": "https://dl.acm.org/doi/10.5555/3327144.3327163", "Full Abstract": "In this paper, we propose a conceptually simple and general framework called MetaGAN for few-shot learning problems. Most state-of-the-art few-shot classification models can be integrated with MetaGAN in a principled and straightforward way. By introducing an adversarial generator conditioned on tasks, we augment vanilla few-shot classification models with the ability to discriminate between real and fake data. We argue that this GAN-based approach can help few-shot classifiers to learn sharper decision boundary, which could generalize better. We show that with our MetaGAN framework, we can extend supervised few-shot learning models to naturally cope with unlabeled data. Different from previous work in semi-supervised few-shot learning, our algorithms can deal with semi-supervision at both sample-level and task-level. We give theoretical justifications of the strength of MetaGAN, and validate the effectiveness of MetaGAN on challenging few-shot image classification benchmarks."},
{"Title": "Dendritic cortical microcircuits approximate the backpropagation algorithm", "URL": "https://dl.acm.org/doi/10.5555/3327546.3327550", "Full Abstract": "Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances – error backpropagation – appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem."},
{"Title": "Bayesian model-agnostic meta-learning", "URL": "https://dl.acm.org/doi/10.5555/3327757.3327835", "Full Abstract": "Due to the inherent model uncertainty, learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines efficient gradient-based meta-learning with nonparametric varia-tional inference in a principled probabilistic framework. Unlike previous methods, during fast adaptation, the method is capable of learning complex uncertainty structure beyond a simple Gaussian approximation, and during meta-update, a novel Bayesian mechanism prevents meta-level overfitting. Remaining a gradient-based method, it is also the first Bayesian model-agnostic meta-learning method applicable to various tasks including reinforcement learning. Experiment results show the accuracy and robustness of the proposed method in sinusoidal regression, image classification, active learning, and reinforcement learning."},
{"Title": "Sparse attentive backtracking", "URL": "https://dl.acm.org/doi/10.5555/3327757.3327863", "Full Abstract": "Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly long-term dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention."},
{"Title": "Towards non-saturating recurrent units for modelling long-term dependencies", "URL": "https://dl.acm.org/doi/10.1609/aaai.v33i01.33013280", "Full Abstract": "Modelling long-term dependencies is a challenge for recurrent neural networks. This is primarily due to the fact that gradients vanish during training, as the sequence length increases. Gradients can be attenuated by transition operators and are attenuated or dropped by activation functions. Canonical architectures like LSTM alleviate this issue by skipping information through a memory mechanism. We propose a new recurrent architecture (Non-saturating Recurrent Unit; NRU) that relies on a memory mechanism but forgoes both saturating activation functions and saturating gates, in order to further alleviate vanishing gradients. In a series of synthetic and real world tasks, we demonstrate that the proposed model is the only model that performs among the top 2 models across all tasks with and without long-term dependencies, when compared against a range of other architectures."},
{"Title": "Combined reinforcement learning via abstract representations", "URL": "https://dl.acm.org/doi/10.1609/aaai.v33i01.33013582", "Full Abstract": "In the quest for efficient and robust reinforcement learning methods, both model-free and model-based approaches offer advantages. In this paper we propose a new way of explicitly bridging both approaches via a shared low-dimensional learned encoding of the environment, meant to capture summarizing abstractions. We show that the modularity brought by this approach leads to good generalization while being computationally efficient, with planning happening in a smaller latent state space. In addition, this approach recovers a sufficient low-dimensional representation of the environment, which opens up new strategies for interpretable AI, exploration and transfer learning."},
{"Title": "On social laws for artificial agent societies", "URL": "https://dl.acm.org/doi/10.1016/0004-3702%2894%2900007-N", "Full Abstract": "No abstract available."},
{"Title": "Provably correct theories of action", "URL": "https://dl.acm.org/doi/10.1145/201019.201021", "Full Abstract": "We investigate logical formalization of the effects of actions in the situation calculus. We propose a formal criterion against which to evaluate theories of deterministic actions. We show how the criterion provides us a formal foundation upon which to tackle the frame problem, as well as its variant in the context of concurrent actions. Our main technical contributions are in formulating a wide class of monotonic causal theories that satisfy the criterion, and showing that each such theory can be reformulated succinctly in circumscription."},
{"Title": "Adaptive load balancing", "URL": "https://dl.acm.org/doi/10.5555/1622826.1622841", "Full Abstract": "We study the process of multi-agent reinforcement learning in the context of load balancing in a distributed system, without use of either central coordination or explicit communication. We first define a precise framework in which to study adaptive load balancing, important features of which are its stochastic nature and the purely local information available to individual agents. Given this framework, we show illuminating results on the interplay between basic adaptive behavior parameters and their effect on system efficiency. We then investigate the properties of adaptive load balancing in heterogeneous populations, and address the issue of exploration vs. exploitation in that context. Finally, we show that naive use of communication may not improve, and might even harm system efficiency."},
{"Title": "Non-monotonic temporal reasoning", "URL": "https://dl.acm.org/doi/10.5555/216136.216143", "Full Abstract": "No abstract available."},
{"Title": "Knowledge considerations in robotics and distribution of robotic tasks", "URL": "https://dl.acm.org/doi/10.5555/1625855.1625868", "Full Abstract": "We develop a formal tool for representing and analyzing informational aspects of robotic tasks, based on the formal concept of 'knowledge.' Specifically, we adopt the notion of knowledge-based protocols from distributed systems, and define the notions of knowledge complexity of a robotic task and knowledge capability of a robot. The resulting formalism naturally captures previous work in the areas of robot information management, but is sufficiently rigorous and natural to allow many extensions. In this paper we show one novel application - the automated distribution of robotic tasks."},
{"Title": "Knowledge Considerations in Robotics", "URL": "https://dl.acm.org/doi/10.5555/647202.718888", "Full Abstract": "No abstract available."},
{"Title": "The open scientific borders of AI, and the case of economics", "URL": "https://dl.acm.org/doi/10.1145/242224.242238", "Full Abstract": "No abstract available."},
{"Title": "An Adaptive Agent for Automated Web Browsing", "URL": "https://dl.acm.org/doi/book/10.5555/892109", "Full Abstract": "The current exponential growth of the Internet precipitates a need for new tools to help people cope with the volume of information. To complement recent work on creating searchable indexes of the World-Wide Web and systems for filtering incoming e-mail and Usenet news articles, we describe a system which learns to browse the Internet on behalf of a user. Every day it presents a selection of interesting Web pages. The user evaluates each page, and given this feedback the system adapts and attempts to produce better pages the following day. After demonstrating that our system is able to learn a model of a user with a single well-defined interest, we present an initial experiment where over the course of 24 days the output of our system was compared to both randomly-selected and human-selected pages. It consistently performed better than the random pages, and was better than the human-selected pages half of the time."},
{"Title": "Fab", "URL": "https://dl.acm.org/doi/10.1145/245108.245124", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "On the emergence of social conventions", "URL": "https://dl.acm.org/doi/10.1016/S0004-3702%2897%2900028-3", "Full Abstract": "No abstract available."},
{"Title": "Economic principles of multi-agent systems", "URL": "https://dl.acm.org/doi/10.1016/S0004-3702%2897%2900029-5", "Full Abstract": "No abstract available."},
{"Title": "Conditional utility, utility independence, and utility networks", "URL": "https://dl.acm.org/doi/10.5555/2074226.2074276", "Full Abstract": "We introduce a new interpretation of two related notions - conditional utility and utility independence. Unlike the traditional interpretation, the new interpretation render the notions the direct analogues of their probabilistic counterparts. To capture these notions formally, we appeal to the notion of utility distribution, introduced in previous paper. We show that utility distributions, which have a structure that is identical to that of probability distributions, can be viewed as a special case of an additive multiattribute utility functions, and show how this special case permits us to capture the novel senses of conditional utility and utility independence. Finally, we present the notion of utility networks, which do for utilities what Bayesian networks do for probabilities. Specifically, utility networks exploit the new interpretation of conditional utility and utility independence to compactly represent a utility distribution."},
{"Title": "A symmetric view of utilities and probabilities", "URL": "https://dl.acm.org/doi/10.5555/1622270.1622347", "Full Abstract": "Motivated by the need to reason about utilities, and inspired by the success of bayesian networks in representing and reasoning about probabilities, we introduce the notion of utility distributions, in which utilities have the structure of probabilities. We furthermore define the notion of a bi-distribution, a structure that includes in a symmetric fashion both a probability distribution and autility distribution. We give several examples of bi-distributions. We also show that every state space with standard probability distribution and utility function can be embedded in a bi-distribution, and provide bounds on the size requirements of this bi-distribution. Finally, we suggest a reinterpretation of the von-Neumann and Morgenstern theorem in light of this new model."},
{"Title": "Applications of a logic of knowledge to motion planning under uncertainty", "URL": "https://dl.acm.org/doi/10.1145/265910.265912", "Full Abstract": "Inspired by the success of the distributed computing community in apply logics of knowledge and time to reasoning about distributed protocols, we aim for a similarly powerful and high-level abstraction when reasoning about control problems involving uncertainty. This paper concentrates on robot motion planning with uncertainty in both control and sensing, a problem that has already been well studied within the robotics community. First, a new and natural problem in this domain is defined: does there exists a sound and complete termination condition for a motion, given initial and goal locations? If yes, how to construct it? Then we define a high-level language, a logic of time and knowledge, which we use to reason about termination conditions and to state general conditions for the existence of sound and complete termination conditions in a broad domain. Finally, we show that sound termination conditions that are optimal in a precise sense provide a natural example of knowledge-based programs with multiple implementations."},
{"Title": "On the knowledge requirements of tasks", "URL": "https://dl.acm.org/doi/10.1016/S0004-3702%2897%2900061-1", "Full Abstract": "No abstract available."},
{"Title": "Conditional, hierarchical, multi-agent preferences", "URL": "https://dl.acm.org/doi/10.5555/645876.671893", "Full Abstract": "We develop a revealed-preference theory for multiple agents. Some features of our construction, which draws heavily on Jeffrey's utility theory and on formal constructions by Domotor and Fishburn, are as follows. First, our system enjoys the \"small-worlds\" property. Second, it represents hierarchical preferences. As a result our expected utility representation is reminscent of type constructions in game theory, except that our construction features higher order utilities as well as higher order probabilities. Finally, our construction includes the representation of conditional preferences, including counterfactual preferences."},
{"Title": "Conditional utility, utility independence and utility networks", "URL": "https://dl.acm.org/doi/10.5555/306795.306801", "Full Abstract": "No abstract available."},
{"Title": "Truth Revelation in Rapid, Approximately Efficient Combinatorial Auctions", "URL": "https://dl.acm.org/doi/book/10.5555/892447", "Full Abstract": "Some important classical mechanisms considered in Microeconomics and Game Theory require the solution of a difficult optimization problem. This is true of mechanisms for combinatorial auctions, which have in recent years assumed practical importance, and in particular of the gold standard for combinatorial auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these mechanisms - in particular, their truth revelation properties - assumes that the optimization problems are solved precisely. In reality, these optimization problems can usually be solved only in an approximate fashion. We investigate the impact on such mechanisms of replacing exact solutions by approximate ones. Specifically, we look at a particular greedy optimization method, which has empirically been shown to perform well. We show that the GVA payment scheme does not provide for a truth revealing mechanism. We introduce another scheme that does guarantee truthfulness for a restricted class of players. We demonstrate the latter property by identifying sufficient conditions for a combinatorial auction to be truth-revealing, conditions which have applicability beyond the specific auction studied here."},
{"Title": "Expected utility networks", "URL": "https://dl.acm.org/doi/10.5555/2073796.2073838", "Full Abstract": "We introduce a new class of graphical representations, expected utility networks (EUNs), and discuss some of its properties and potential applications to artificial intelligence and economic theory."},
{"Title": "Parallel hashing and integer sorting", "URL": "https://dl.acm.org/doi/10.1016/0196-6774%2891%2990034-V", "Full Abstract": "No abstract available."},
{"Title": "Leaders Election Without Conflict Resolution Rule - Fast and Efficient Randomized Simulations among CRCW PRAMs", "URL": "https://dl.acm.org/doi/10.5555/646385.689874", "Full Abstract": "No abstract available."},
{"Title": "Efficient Randomized Dictionary Matching Algorithms (Extended Abstract)", "URL": "https://dl.acm.org/doi/10.5555/647812.738140", "Full Abstract": "No abstract available."},
{"Title": "Polynomial Hash Functions Are Reliable (Extended Abstract)", "URL": "https://dl.acm.org/doi/10.5555/646246.756769", "Full Abstract": "No abstract available."},
{"Title": "Randomized Range-Maxima inNearly-Constant Parallel Time", "URL": "https://dl.acm.org/doi/10.5555/646335.688108", "Full Abstract": "No abstract available."},
{"Title": "Dynamic generation of discrete random variates", "URL": "https://dl.acm.org/doi/10.5555/313559.313807", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Triply-Logarithmic Upper and Lower Bounds for Minimum, Range Minima, and Related Problems with Integer Inputs", "URL": "https://dl.acm.org/doi/10.5555/645929.672846", "Full Abstract": "No abstract available."},
{"Title": "Approximate data structures with applications", "URL": "https://dl.acm.org/doi/10.5555/314464.314493", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Optimal parallel approximation for prefix sums and integer sorting", "URL": "https://dl.acm.org/doi/10.5555/314464.314499", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "The QRQW PRAM", "URL": "https://dl.acm.org/doi/10.5555/314464.314666", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Simple Fast Parallel Hashing", "URL": "https://dl.acm.org/doi/10.5555/646248.685175", "Full Abstract": "No abstract available."},
{"Title": "Designing algorithms by expectations", "URL": "https://dl.acm.org/doi/10.1016/0020-0190%2894%2900052-2", "Full Abstract": "No abstract available."},
{"Title": "Efficient low-contention parallel algorithms", "URL": "https://dl.acm.org/doi/10.1145/181014.181382", "Full Abstract": "The queue-read, queue-write (QRQW) PRAM model [GMR94] permits concurrent reading and writing, but at a cost proportional to the number of readers/writers to a memory location in a given step. The QRQW model reflects the contention properties of most parallel machines more accurately than either the well-studied CRCW or EREW models: the CRCW model does not adequately penalize algorithms with high contention to shared memory locations, while the EREW model is too strict in its insistence on zero contention at each step. Of primary practical and theoretical interest, then, is the design of fast and efficient QRQW algorithms for problems for which all previous algorithms either suffer from high contention, fail to be fast, or fail to be work-optimal."},
{"Title": "An optical simulation of shared memory", "URL": "https://dl.acm.org/doi/10.1145/181014.181406", "Full Abstract": "We present a work-optimal randomized algorithm for simulating a shared memory machine (PRAM) on an optical communication parallel computer (OCPC). The OCPC model is motivated by the potential of optical communication for parallel computation. The memory of an OCPC is divided into modules, one module per processor. Each memory module only services a request on a timestep if it receives exactly one memory request."},
{"Title": "Fast and Efficient Simulations among CRCW PRAMs", "URL": "https://dl.acm.org/doi/10.1006/jpdc.1994.1127", "Full Abstract": "We study the problem of fast leaders election on TOLERANT, a CRCW PRAM model which tolerates concurrent write but does not support symmetry breaking. The leaders election problem is related to the problem of simulating stronger CRCW models, which support leaders election by predefined conflict resolution rules. We give a randomized simulation of MAXIMUM-a very strong CRCW PRAM-on TOLERANT. The simulation is optimal, is reliable, and runs in nearly doubly logarithmic time and linear space. This is the first simulation which is fast, optimal, and space-efficient, and therefore grants true comparison of algorithms running on different CRCW PRAMs. Moreover, it implies that the memory to which concurrent read or concurrent write is allowed should never be more than linear-the rest of the memory can always be addressed under the EREW convention. The techniques presented in this paper tackle fundamental difficulties in the design of fast parallel algorithms."},
{"Title": "A Simple Randomized Sieve Algorithm for the Closest-Pair Problem", "URL": "https://dl.acm.org/doi/10.1006/inco.1995.1049", "Full Abstract": "We present a linear time randomized sieve algorithm for the closest-pair problem. The algorithm and its analysis are simple. The algorithm is extended to obtain a randomized linear time approximation algorithm for the closest bichromatic pair problem."},
{"Title": "A note on reducing parallel model simulations to integer sorting", "URL": "https://dl.acm.org/doi/10.5555/645605.662933", "Full Abstract": "We show that simulating a step of a FETCH&ADD PRAM model on an EREW PRAM model can be made as efficient as integer sorting. In particular, we present several efficient reductions of the simulation problem to various integer sorting problems. By using some recent algorithms for integer sorting, we get simulation algorithms on CREW and EREW that take o(n lg n) operations where n is the number of processors in the simulated CROW machine. Previous simulations were using /spl Theta/(n lg n) operations. Some of the more interesting simulation results are obtained by using a bootstrapping technique with a CRCW PRAM algorithm for hashing."},
{"Title": "Fast parallel algorithms for minimum and related problems with small integer inputs", "URL": "https://dl.acm.org/doi/10.5555/645605.663241", "Full Abstract": "The fundamental problem of minimum computation has several well studied generalizations such as prefix minima, range minima, and all nearest smaller values computations. Recent papers introduced parallel algorithms for these problems when the n input elements are given from an integer domain [1.s], obtaining O(lg lg lg s) running time and linear work for s/spl ges/n. However, most of these algorithms have the running time of O(lg lg lg n) (rather than O(lg lg lg s)) for all values of s/spl les/n, except for the case s=O(1) in which case the running time is O(/spl alpha/(n)). In this paper we focus on the range s/spl les/n and provide linear-work algorithms whose running time is O(lg lg lg s+f(n)) for all s/spl ges/0, where f(n) is either one of the slow growing functions lg* n or /spl alpha/(n). We show how to generalize our algorithms to the case that the domain size s is unknown, with the same complexities. (All previous algorithms work only under the assumption that the domain sire s is known.) Moreover, we make our algorithms output-sensitive with the lg lg lg s term replaced by lg lg lg M, where M is the maximum input value. In fact, for the minimum computation problem the running time is O(lg lg lg m) for all s/spl ges/0, where m is the minimum input value."},
{"Title": "Provably efficient scheduling for languages with fine-grained parallelism", "URL": "https://dl.acm.org/doi/10.1145/215399.215403", "Full Abstract": "Copyright © 1995 ACM."},
{"Title": "Montage", "URL": "https://dl.acm.org/doi/10.1109/TMC.2016.2577586", "Full Abstract": "In this work, we design and develop"},
{"Title": "Pervasive Floorplan Generation Based on Only Inertial Sensing: Feasibility, Design, and Implementation", "URL": "https://dl.acm.org/doi/10.1109/JSAC.2017.2679659", "Full Abstract": "Mobile crowdsourcing is deemed as a powerful technique to solve traditional problems. But the crowdsourced data from smartphones are generally low quality, which can induce crucial challenges and hurt the applicability of crowdsourcing applications. This paper presents our study to address such challenges in a concrete application, namely, floorplan generation. Existing proposals mostly rely on infrastructural references or accurate data sources, which are restricted in terms of applicability and pervasiveness. Our proposal called SenseWit is motivated by the observation that people&#x2019;s behavior offers meaningful clues for location inference. The noise, ambiguity, and behavior diversity contained in the crowdsourced data, however, mean non-trivial challenges in generating high-quality floorplans. We propose: 1) a novel concept called Nail to identify featured locations in indoor space and 2) a heuristic pathlet bundling algorithm to progressively discover the internal layouts of a floorplan. We implement SenseWit and conduct real-world experiments in different spaces to demonstrate its efficacy. This paper offers an efficient technique to obtain high-quality structures (either logical or physical) from low-quality data. We believe it can be generalized to other crowdsourcing applications."},
{"Title": "Peer-to-Peer Indoor Navigation Using Smartphones", "URL": "https://dl.acm.org/doi/10.1109/JSAC.2017.2680844", "Full Abstract": "Most of existing indoor navigation systems work in a client/server manner, which needs to deploy comprehensive localization services together with precise indoor maps <italic>a prior</italic>. In this paper, we design and realize a peer-to-peer navigation system (<italic>ppNav</italic>), on smartphones, which enables the fast-to-deploy navigation services, avoiding the requirements of pre-deployed location services and detailed floorplans. <italic>ppNav</italic> navigates a user to the destination by tracking user mobility, promoting timely walking tips and alerting potential deviations, according to a previous traveller&#x2019;s trace experience. Specifically, we utilize the ubiquitous WiFi fingerprints in a novel diagrammed form and extract both radio and visual features of the diagram to track relative locations and exploit fingerprint similarity trend for deviation detection. We further devise techniques to lock on a user to the nearest reference path in case he/she arrives at an uncharted place. Consolidating these techniques, we implement <italic>ppNav</italic> on commercial mobile devices and validate its performance in real environments. Our results show that <italic>ppNav</italic> achieves delightful performance, with an average relative error of 0.9 m in trace tracking and a maximum delay of nine samples (about 4.5 s) in deviation detection."},
{"Title": "Inferring Motion Direction using Commodity Wi-Fi for Interactive Exergames", "URL": "https://dl.acm.org/doi/10.1145/3025453.3025678", "Full Abstract": "In-air interaction acts as a key enabler for ambient intelligence and augmented reality. As an increasing popular example, exergames, and the alike gesture recognition applications, have attracted extensive research in designing accurate, pervasive and low-cost user interfaces. Recent advances in wireless sensing show promise for a ubiquitous gesture-based interaction interface with Wi-Fi. In this work, we extract complete information of motion-induced Doppler shifts with only commodity Wi-Fi. The key insight is to harness antenna diversity to carefully eliminate random phase shifts while retaining relevant Doppler shifts. We further correlate Doppler shifts with motion directions, and propose a light-weight pipeline to detect, segment, and recognize motions without training. On this basis, we present"},
{"Title": "Toward More Rigorous and Practical Cardinality Estimation for Large-Scale RFID Systems", "URL": "https://dl.acm.org/doi/10.1109/TNET.2016.2634551", "Full Abstract": "Cardinality estimation is one of the fundamental problems in large-scale radio frequency identification systems. While many efforts have been made to achieve faster approximate counting, the accuracy of estimates itself has not received enough attention. Specifically, most state-of-the-art schemes share a two-phase paradigm implicitly or explicitly, which needs a rough estimate first and then refines it to a final estimate meeting the desired accuracy; we observe that the final estimate can largely deviate from the expectation due to the skewed rough estimate, i.e., the accuracy of final estimates is not rigorously bounded. This negative impact is hidden because former solutions either assume perfect rough estimates or rough estimates that can be produced by uniform random data or perfect hash functions that can turn any data into uniform random data. Unfortunately, both of them are hard to meet in practice. To address the above issues, we propose a novel scheme, namely, “rigorous and practical cardinality RPC” estimation. RPC adopts the two-phase paradigm, in which the rough estimate is derived in the first phase using pairwise-independent hashing. In the second phase, we employ $t$ -wise-independent hashing to reinforce the rough estimate to meet arbitrary accuracy requirements. We validate the effectiveness and performance of RPC through theoretical analysis and extensive simulations. The results show that the RPC can meet the desired accuracy all the time with diverse practical settings while previous designs fail with non-uniform data."},
{"Title": "Widar", "URL": "https://dl.acm.org/doi/10.1145/3084041.3084067", "Full Abstract": "Various pioneering approaches have been proposed for Wi-Fi-based sensing, which usually employ learning-based techniques to seek appropriate statistical features, yet do not support precise tracking without prior training. Thus to advance passive sensing, the ability to track fine-grained human mobility information acts as a key enabler. In this paper, we propose Widar, a Wi-Fi-based tracking system that simultaneously estimates a human's moving velocity (both speed and direction) and location at a decimeter level. Instead of applying statistical learning techniques, Widar builds a theoretical model that geometrically quantifies the relationships between CSI dynamics and the user's location and velocity. On this basis, we propose novel techniques to identify frequency components related to human motion from noisy CSI readings and then derive a user's location in addition to velocity. We implement Widar on commercial Wi-Fi devices and validate its performance in real environments. Our results show that Widar achieves decimeter-level accuracy, with a median location error of 25 cm given initial positions and 38 cm without them and a median relative velocity error of 13%."},
{"Title": "Design and Implementation of an RFID-Based Customer Shopping Behavior Mining System", "URL": "https://dl.acm.org/doi/10.1109/TNET.2017.2689063", "Full Abstract": "Shopping behavior data is of great importance in understanding the effectiveness of marketing and merchandising campaigns. Online clothing stores are capable of capturing customer shopping behavior by analyzing the click streams and customer shopping carts. Retailers with physical clothing stores, however, still lack effective methods to comprehensively identify shopping behaviors. In this paper, we show that backscatter signals of passive RFID tags can be exploited to detect and record how customers browse stores, which garments they pay attention to, and which garments they usually pair up. The intuition is that the phase readings of tags attached to items will demonstrate distinct yet stable patterns in a time-series when customers look at, pick out, or turn over desired items. We design ShopMiner, a framework that harnesses these unique spatial-temporal correlations of time-series phase readings to detect comprehensive shopping behaviors. We have implemented a prototype of ShopMiner with a COTS RFID reader and four antennas, and tested its effectiveness in two typical indoor environments. Empirical studies from two-week shopping-like data show that ShopMiner is able to identify customer shopping behaviors with high accuracy and low overhead, and is robust to interference."},
{"Title": "iSelf", "URL": "https://dl.acm.org/doi/10.1145/3121049", "Full Abstract": "It has been a consensus that a certain relationship exists between personal emotions and usage pattern of the smartphone. Based on users’ emotions and personalities, more and more applications are developed to provide intelligent automation services on the smartphone, such as music recommendations or stranger introductions on social networking sites. Most existing work studies this relationship by learning large amounts of samples, which are manually labeled and collected from smartphone users. The manual labeling process, however, is very time-consuming and labor-intensive. To address this issue, we propose iSelf, a system that provides a general service of automatic detection of a user’s emotions in cold-start conditions with a smartphone. With the technology of transfer learning, iSelf achieves high accuracy given only a few labeled samples. We also embed a hybrid public/personal inference engine and validation system into iSelf, to make it maintain updates continuously. Through extensive experiments in real traces, the inferring accuracy is tested above 74% and can be improved increasingly through validation and updates. The application program interface has been open online for other developers."},
{"Title": "PIC: Enable Large-Scale Privacy Preserving Content-Based Image Search on Cloud", "URL": "https://dl.acm.org/doi/10.1109/TPDS.2017.2712148", "Full Abstract": "Many cloud platforms emerge to meet urgent requirements for large-volume personal image store, sharing and search. Though most would agree that images contain rich sensitive information (e.g., people, location and event) and people&#x2019;s privacy concerns hinder their participation into untrusted services, today&#x2019;s cloud platforms provide little support for image privacy protection. Facing large-scale images from multiple users, it is extremely challenging for the cloud to maintain the index structure and schedule parallel computation without learning anything about the image content and indices. In this work, we introduce a novel system PIC: A Privacy-preserving Image search system on Cloud, which is a step towards feasible cloud services which provide secure content-based large-scale image search with fine-grained access control. Users can search on others&#x2019; images if they are authorized by the image owners. Majority of the computationally intensive jobs are handled by the cloud, and a querier can now simply send the query and receive the result. Specially, to deal with massive images, we design our system suitable for distributed and parallel computation and introduce several optimizations to further expedite the search process. Our security analysis and prototype system evaluation results show that PIC successfully protects the image privacy at a low cost of computation and communication."},
{"Title": "Revisiting Reading Rate with Mobility", "URL": "https://dl.acm.org/doi/10.1145/3143361.3143387", "Full Abstract": "Radio-frequency identification (RFID) systems, as major enablers of automatic identification, are currently supplemented with various interesting sensing functions, e.g., motion tracking. All these sensing applications forcedly require much higher reading rate (i.e., sampling rate) such that any fast movement of tagged objects can be accurately captured in a timely manner through tag readings. However, COTS RFID systems suffer from an extremely low individual reading rate when multiple tags are present, due to their intense channel contention in the link layer. In this work, we present a holistic system, called Tagwatch, a rate-adaptive reading system for COTS RFID devices. This work revisits the reading rate from a distinctive perspective: mobility. We observe that the reading demands of mobile tags are considerably more urgent than those of stationary tags because the states of the latter nearly remain unchanged; meanwhile, only a few tags (e.g., < 20%) are actually in motion despite the existence of a massive amount of tags in practice. Thus, Tagwatch adaptively improves the reading rates for mobile tags by cutting down the readings of stationary tags. Our main contribution is a two-phase reading design, wherein the mobile tags are discriminated in the Phase I and exclusively read in the Phase II. We built a prototype of Tagwatch with COTS RFID readers and tags. Results from our microbenchmark analysis demonstrate that the new design outperforms the reading rate by 3.2x when 5% of tags are moving."},
{"Title": "Design and Implementation of a CSI-Based Ubiquitous Smoking Detection System", "URL": "https://dl.acm.org/doi/10.1109/TNET.2017.2752367", "Full Abstract": "Even though indoor smoking ban is being put into practice in civilized countries, existing vision or sensor-based smoking detection methods cannot provide ubiquitous detection service. In this paper, we take the first attempt to build a ubiquitous passive smoking detection system, Smokey, which leverages the patterns smoking leaves on WiFi signal to identify the smoking activity even in the non-line-of-sight and through-wall environments. We study the behaviors of smokers and leverage the common features to recognize the series of motions during smoking, avoiding the target-dependent training set to achieve the high accuracy. We design a foreground detection-based motion acquisition method to extract the meaningful information from multiple noisy subcarriers even influenced by posture changes. Without the requirement of target’s compliance, we leverage the rhythmical patterns of smoking to detect the smoking activities. We also leverage the diversity of multiple antennas to enhance the robustness of Smokey. Due to the convenience of integrating new antennas, Smokey is scalable in practice for ubiquitous smoking detection. We prototype Smokey with the commodity WiFi infrastructure and evaluate its performance in real environments. Experimental results show Smokey is accurate and robust in various scenarios."},
{"Title": "Tagbeat", "URL": "https://dl.acm.org/doi/10.1109/TNET.2017.2769138", "Full Abstract": "Traditional vibration inspection systems, equipped with separated sensing and communication modules, are either very expensive e.g., hundreds of dollars and/or suffer from occlusion and narrow field of view e.g., laser. In this paper, we present an RFID-based solution, Tagbeat, to inspect mechanical vibration using COTS RFID tags and readers. Making sense of micro and high-frequency vibration using random and low-frequency readings of tag has been a daunting task, especially challenging for achieving sub-millisecond period accuracy. Our system achieves these three goals by discerning the change pattern of backscatter signal replied from the tag, which is attached on the vibrating surface and displaced by the vibration within a small range. This paper introduces three main innovations. First, it shows how one can utilize COTS RFID to sense mechanical vibration and accurately discover its period with a few periods of short and noisy samples. Second, a new digital microscope is designed to amplify the micro-vibration-induced weak signals. Third, Tagbeat introduces compressive reading to inspect high-frequency vibration with relatively low RFID read rate. We implement Tagbeat using a COTS RFID device and evaluate it with a commercial centrifugal machine. Empirical benchmarks with a prototype show that Tagbeat can inspect the vibration period with a mean accuracy of $0.36ms$ and a relative error rate of 0.03%. We also study three cases to demonstrate how to associate our inspection solution with the specific domain requirements."},
{"Title": "Enabling Contactless Detection of Moving Humans with Dynamic Speeds Using CSI", "URL": "https://dl.acm.org/doi/10.1145/3157677", "Full Abstract": "Device-free passive detection is an emerging technology to detect whether there exist any moving entities in the areas of interest without attaching any device to them. It is an essential primitive for a broad range of applications including intrusion detection for safety precautions, patient monitoring in hospitals, child and elder care at home, and so forth. Despite the prevalent signal feature Received Signal Strength (RSS), most robust and reliable solutions resort to a finer-grained channel descriptor at the physical layer, e.g., the Channel State Information (CSI) in the 802.11n standard. Among a large body of emerging techniques, however, few of them have explored the full potential of CSI for human detection. Moreover, space diversity supported by nowadays popular multiantenna systems are not investigated to a comparable extent as frequency diversity. In this article, we propose a novel scheme for device-free PAssive Detection of moving humans with dynamic Speed (PADS). Both full information (amplitude and phase) of CSI and space diversity across multiantennas in MIMO systems are exploited to extract and shape sensitive metrics for accuracy and robust target detection. We prototype PADS on commercial WiFi devices, and experiment results in different scenarios demonstrate that PADS achieves great performance improvement in spite of dynamic human movements."},
{"Title": "Towards web-based delta synchronization for cloud storage services", "URL": "https://dl.acm.org/doi/10.5555/3189759.3189774", "Full Abstract": "Delta synchronization (sync) is crucial for network-level efficiency of cloud storage services. Practical delta sync techniques are, however, only available for PC clients and mobile apps, but not web browsers--the most pervasive and OS-independent access method. To understand the obstacles of web-based delta sync, we implement a delta sync solution, WebRsync, using state-of-the-art web techniques based on rsync, the de facto delta sync protocol for PC clients. Our measurements show that WebRsync severely suffers from the inefficiency of JavaScript execution inside web browsers, thus leading to frequent stagnation and even hanging. Given that the computation burden on the web browser mainly stems from data chunk search and comparison, we reverse the traditional delta sync approach by lifting all chunk search and comparison operations from the client side to the server side. Inevitably, this brings considerable computation overhead to the servers. Hence, we further leverage locality-aware chunk matching and lightweight checksum algorithms to reduce the overhead. The resulting solution, WebR2sync+, outpaces WebRsync by an order of magnitude, and is able to simultaneously support 6800- 8500 web clients' delta sync using a standard VM server instance based on a Dropbox-like system architecture."},
{"Title": "Orientation-aware RFID tracking with centimeter-level accuracy", "URL": "https://dl.acm.org/doi/10.1109/IPSN.2018.00057", "Full Abstract": "RFID tracking attracts a lot of research efforts in recent years. Most of the existing approaches, however, adopt an orientation-oblivious model. When tracking a target whose orientation changes, those approaches suffer from serious accuracy degradation. In order to achieve target tracking with pervasive applicability in various scenarios, we in this paper propose OmniTrack, an orientation-aware RFID tracking approach. Our study discovers the linear relationship between the tag orientation and the phase change of the backscattered signals. Based on this finding, we propose an orientation-aware phase model to explicitly quantify the respective impact of the read-tag distance and the tag's orientation. OmniTrack addresses practical challenges in tracking the location and orientation of a mobile tag. Our experimental results demonstrate that OmniTrack achieves centimeter-level location accuracy and has significant advantages in tracking targets with varing orientations, compared to the state-of-the-art approaches."},
{"Title": "RED: RFID-based Eccentricity Detection for High-speed Rotating Machinery", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2018.8485873", "Full Abstract": "Eccentricity detection is a crucial issue for highspeed rotating machinery, which concerns the stability and safety of the machinery. Conventional techniques in industry for eccentricity detection are mainly based on measuring certain physical indicators, which are costly and hard to deploy. In this paper, we propose RED, a non-intrusive, low-cost, and realtime RFID-based eccentricity detection approach. Differing from the existing RFID-based sensing approaches, RED utilizes the temporal and phase distributions of tag readings as effective features for eccentricity detection. RED includes a Markov chain based model called RUM, which only needs a few sample readings from the tag to make a highly accurate and precise judgement. We implement RED with commercial-of-the-shelf RFID reader and tags, and evaluate its performance across various scenarios. The overall accuracy is 93.59&#x0025; and the detection latency is 0.68 seconds in average."},
{"Title": "Acousticcardiogram: Monitoring Heartbeats using Acoustic Signals on Smart Devices", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2018.8485978", "Full Abstract": "Vital signs such as heart rate and heartbeat interval are currently measured by electrocardiograms (ECG) or wearable physiological monitors. These techniques either require contact with the patient&#x0027;s skin or are usually uncomfortable to wear, rendering them too expensive and user-unfriendly for daily monitoring. In this paper, we propose a new noninvasive technology to generate an Acousticcardiogram (ACG) that precisely monitors heartbeats using inaudible acoustic signals. ACG uses only commodity microphones and speakers commonly equipped on ubiquitous off-the-shelf devices, such as smartphones and laptops. By transmitting an acoustic signal and analyzing its reflections off human body, ACG is capable of recognizing the heart rate as well as heartbeat rhythm. We employ frequency-modulated sound signals to separate reflection of heart from that of background motions and breath, and continuously track the phase changes of the acoustic data. To translate these acoustic data into heart and breath rates, we leverage the dual microphone design on COTS mobile devices to suppress direct echo from speaker to microphones, identify heart rate in frequency domain, and adopt an advanced algorithm to extract individual heartbeats. We implement ACG on commercial devices and validate its performance in real environments. Experimental results demonstrate ACG monitors user&#x0027;s heartbeat accurately, with median heart rate estimation error of 0.6 beat per minute (bpm), and median heartbeat interval estimation error of 19 ms."},
{"Title": "Robust Spinning Sensing with Dual-RFID-Tags in Noisy Settings", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2018.8486312", "Full Abstract": "Conventional spinning inspection systems, equipped with separated sensors (e.g., accelerometer, laser, etc.) and communication modules, are either very expensive and/or suffering from occlusion and narrow field of view. The recently proposed RFID-based sensing solution draws much attention due to its intriguing features, such as being cost-effective, applicable to occluded objects and auto-identification, etc. However, this solution only works in quiet settings where the reader and spinning object remain absolutely stationary, as their shaking would ruin the periodicity and sparsity of the spinning signal, making it impossible to be recovered. This work introduces Tagtwins, a robust spinning sensing system that can work in noisy settings. It addresses the challenge by attaching dual RFID tags on the spinning surface and developing a new formulation of spinning signal that is shaking-resilient, even if the shaking involves unknown trajectories. Our main contribution lies in two newly developed techniques, relative spinning signal and dual compressive reading. We analytically demonstrate that our solution can work in various settings. We have implemented Tagtwins with COTS RFID devices and evaluated it extensively. Experimental results show that Tagtwins can inspect the rotation frequency with high accuracy and robustness."},
{"Title": "Verifying Tomasulo's Algoithm by Refinement", "URL": "https://dl.acm.org/doi/10.5555/520550.835050", "Full Abstract": "In this paper Tomasulo's algorithm for out-of-order execution is shown to be a refinement of the sequential instruction execution algorithm. Correctness of Tomasulo's algorithm is established by proving that the register files of Tomasulo's algorithm and the sequential algorithm agree once all instructions have been completed."},
{"Title": "Orthogonal Polyhedra", "URL": "https://dl.acm.org/doi/10.5555/646879.710439", "Full Abstract": "In this paper we investigate orthogonal polyhedra, i.e. polyhedra which are finite unions of full-dimensional hyper-rectangles. We define representation schemes for these polyhedra based on their vertices, and show that these compact representation schemes are canonical for all (convex and non-convex) polyhedra in any dimension. We then develop efficient algorithms for membership, face-detection and Boolean operations for these representations."},
{"Title": "Decidable integration graphs", "URL": "https://dl.acm.org/doi/10.1006/inco.1998.2774", "Full Abstract": "No abstract available."},
{"Title": "Proving refinement using transduction", "URL": "https://dl.acm.org/doi/10.1007/s004460050062", "Full Abstract": "When designing distributed systems, one is faced with the problem of verifying a refinement between two specifications, given at different levels of abstraction. Suggested verification techniques in the literature include refinement mappings and various forms of simulation. We present a verification method, in which refinement between two systems is proven by constructing a transducer that inputs a computation of a concrete system and outputs a matching computation of the abstract system. The transducer uses a FIFO queue that holds segments of the concrete computation that have not been matched yet. This allows a finite delay between the occurrence of a concrete event and the determination of the corresponding abstract event. This delay often makes the use of prophecy variables or backward simulation unnecessary.An important generalization of the method is to prove refinement modulo some transformation on the observed sequences of events. The method is adapted by replacing the FIFO queue by a component that allows the appropriate transformation on sequences of events. A particular case is partial-order refinement, i.e., refinement that preserves only a subset of the orderings between events of a system. Examples are sequential consistency and serializability. The case of sequential consistency is illustrated on a proof of sequential consistency of a cache protocol."},
{"Title": "Verifying Liveness by Augmented Abstraction", "URL": "https://dl.acm.org/doi/10.5555/647849.737064", "Full Abstract": "The paper deals with the proof method of verification by augmented finitary abstraction (VAA), which presents an effective approach to the verification of the temporal properties of (potentially infinite-state) reactive systems. The method consists of a two-step process by which, in a first step, the system and its temporal specification are combined an then abstracted into a finite-state Büchi automaton. The second step uses model checking to establish emptiness of the abstracted automaton."},
{"Title": "Deciding Equality Formulas by Small Domains Instantiations", "URL": "https://dl.acm.org/doi/10.5555/647768.733921", "Full Abstract": "We introduce an efficient decision procedure for the theory of equality based on finite instantiations. When using the finite instantiations method, it is a common practice to take a range of [1."},
{"Title": "A Perfect Verification", "URL": "https://dl.acm.org/doi/10.5555/647544.730605", "Full Abstract": "The paper presents an approach to the formal verification of a complete software system intended to support the flagship product of Perfecto Technologies which enforces application security over an open communication net."},
{"Title": "A Framework for Scheduler Synthesis", "URL": "https://dl.acm.org/doi/10.5555/827271.829109", "Full Abstract": "In this paper we present a framework integrating specification and scheduler generation for real-time systems. In a first step, the system, which can include arbitrarily designed tasks (cyclic or sporadic, with or without precedence constraints, any number of resources and CPUs) is specified as a timed Petri-net. In a second step, our tool generates the most general non-preemptive online scheduler for the specification, using a controller synthesis technique."},
{"Title": "A Comparison of Two Verification Methods for Speculative Instruction Execution", "URL": "https://dl.acm.org/doi/10.5555/646484.691761", "Full Abstract": "In this paper we describe and compare two methodologies for verifying the correctness of a speculative out-of-order execution system with interrupts. Both methods are deductive (we use PVS) and are based on refinement. The first proof is by direct refinement to a sequential system; the second proof combines refinement with induction over the number of retirement buffer slots."},
{"Title": "Liveness and Acceleration in Parameterized Verification", "URL": "https://dl.acm.org/doi/10.5555/647769.734100", "Full Abstract": "No abstract available."},
{"Title": "Rigorous development of embedded systems", "URL": "https://dl.acm.org/doi/10.1145/354880.354881", "Full Abstract": "No abstract available."},
{"Title": "Verification by Augmented Finitary Abstraction", "URL": "https://dl.acm.org/doi/10.1006/inco.2000.3000", "Full Abstract": "The paper deals with the proof method of verification by finitary abstraction (VFA), which presents a feasible approach to the verification of the temporal properties of (potentially infinite-state) reactive systems. The method consists of a two-step process by which, in a first step, the system and its temporal specification are jointly abstracted into a finite-state system and a finite-state specification. The second step uses model checking to establish the validity of the abstracted property over the abstracted system. The VFA method can be considered a viable alternative to verification by temporal deduction which, up to now, has been the main method generally applicable for verification of infinite-state systems. The paper presents a general recipe for the joint abstraction, which is shown to be sound, where soundness means that validity over the abstract system implies validity over the concrete (original) system. To make the method applicable for the verification of liveness properties, pure abstraction is sometimes no longer adequate. We show that by augmenting the system by an appropriate (and standardly constructible) progress monitor, we obtain an augmented system, whose computations are essentially the same as the original system, and which may now be abstracted while preserving the desired liveness properties. We refer to the extended method as verification by augmented abstraction (VAA). We then proceed to show that the VAA method is sound and complete for proving all properties expressible by temporal logic (including both safety and liveness). Completeness establishes that whenever the property is valid, there exists a finitary abstraction which abstracts the system, augmented by an appropriate progress monitor, into a finite-state system which validated the abstracted property."},
{"Title": "Formal Verification of the Ricart-Agrawala Algorithm", "URL": "https://dl.acm.org/doi/10.5555/646838.708648", "Full Abstract": "This paper presents the first formal verification of the Ricart-Agrawala algorithm RA81. for distributed mutual exclusion of an arbitrary number of nodes. It uses the Temporal Methodology of [MP95a]. We establish both the safety property of mutual exclusion and the liveness property of accessibility. To establish these properties for an arbitrary number of nodes, parameterized proof rules are used as presented in [MP95a] (for safety) and [MP94] (for liveness). A new and efficient notation is introduced to facilitate the presentation of liveness proofs by verification diagrams."},
{"Title": "Scheduling time-constrained instructions on pipelined processors", "URL": "https://dl.acm.org/doi/10.1145/383721.383733", "Full Abstract": "In this work we investigate the problem of scheduling instructions on idealized microprocessors with multiple pipelines, in the presence of precedence constraints, release-times, deadlines, and latency constraints. A latency of"},
{"Title": "Model Checking with Strong Fairness,", "URL": "https://dl.acm.org/doi/book/10.5555/903616", "Full Abstract": "In this paper we present a coherent framework for symbolic model checking of linear-time temporal logic (LTL) properties over finite state reactive systems, taking full fairness constraints into consideration. We use the computational model of a fair discrete system (FDS) which takes into account both justice (weak fairness) and compassion (strong fairness). The approach presented here reduces the model checking problem into the question of whether a given FDS is feasible (i.e. has at least one computation). The contribution of the paper is twofold: On the methodological level, it presents a direct self-contained exposition of full LTL symbolic model checking without resorting to reductions to either CTL or automata. On the technical level, it extends previous methods by dealing with compassion at the algorithmic level instead of adding it to the specification, and providing the first symbolic method for checking feasibility of FDS's (equivalently, symbolically checking for the emptiness of Streett automata), based on the Emerson-Lei fixpoint characterization of both weak and strong fairness. Finally, we extend CTL* with past operators, and show that the basic symbolic feasibility algorithm presented here, can be used to model check an arbitrary CTL* formula over an FDS with full fairness constraints."},
{"Title": "Automatic Deductive Verification with Invisible Invariants", "URL": "https://dl.acm.org/doi/10.5555/646485.694452", "Full Abstract": "The paper presents a method for the automatic verification of a certain class of parameterized systems. These are bounded-data systems consisting of"},
{"Title": "Symbolic model checking with rich assertional languages", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2800%2900103-1", "Full Abstract": "No abstract available."},
{"Title": "Verification by Augmented Abstraction", "URL": "https://dl.acm.org/doi/10.1006/jcss.2000.1744", "Full Abstract": "This paper deals with the proof method of verification by finitary abstraction (vfa), which presents an alternative approach to the verification of (potentially infinite-state) reactive systems. We assume that the negation of the property to be verified is given by the user in the form of an infinite-state nondeterministic B chi discrete system (bds). The method consists of a two-step process by which, in a first step, the system and its (negated) specification are combined into a single infinite-state fair discrete system (fds, which is similar to a bds but with Streett acceptance conditions), which is abstracted into a finite-state automaton. The second step uses model checking to establish that the abstracted automaton is infeasible, i.e., has no computations. The vfa method can be considered as a viable alternative to verification by temporal deduction, which, up to now, has been the main method generally applicable for verification of infinite-state systems. The paper presents a general recipe for an fds abstraction, which is shown to be sound, where soundness means that infeasibility of the abstracted fds implies infeasibility of the unabstracted one, implying in turn the validity of the property over the concrete (infinite-state) system. To make the method applicable for the verification of liveness properties, pure abstraction is sometimes no longer adequate. We show that by augmenting the system with an appropriate (and standardly constructible) progress monitor, we obtain an augmented system, whose computations are essentially the same as those of the original system and which may now be abstracted while preserving the desired liveness properties. We refer to the extended method as verification by augmented abstraction (vaa). We then proceed to show that the vaa method is sound and complete for proving all properties whose negations are expressible by a bds. Given that every linear temporal logic (ltl) property can be translated to a bds, this establishes that the vaa method is sound and complete for proving the validity of all ltl properties, including both safety and liveness."},
{"Title": "DATA FILTERING AND DISTRIBUTION MODELING ALGORITHMS FOR MACHING LEARNING (Ph.D. Thesis)", "URL": "https://dl.acm.org/doi/book/10.5555/902863", "Full Abstract": "This thesis is concerned with the analysis of algorithms for machine learning. The main focus is on the role of the distribution of the examples used for learning. Chapters 2 and 3 are concerned with algorithms for learning concepts from random examples. Briefly, the goal of the learner is to observe a set of labeled instances and generate a hypothesis that approximates the rule that maps the instances to their labels. Chapter 2 describes and analyses an algorithm for improving the performance of a general concept learning algorithm by selecting those labeled instances that are most informative. This work is an improvement over previous work by Schapire. The analysis provides upper bounds on the time, space and number of examples that are required for concept learning. Chapter 3 is concerned with situations in which the learner can select, out of a stream of random instances, those for which it wants to know the label. We analyze an algorithm of Seung et.al. for selecting such instances, and prove that it is effective for the Perceptron concept class. Both Chapters 2 and 3 show situations in which a carefully selected exponentially small fraction of the random training examples are sufficient for learning. Chapter 4 is concerned with learning distributions of binary vectors. Here we present a new distribution model that can represent combinations of correlation patterns. We describe two different algorithms for learning this distribution model from random examples, and provide experimental evidence that they are effective. We conclude, in Chapter 5, with a brief discussion of the possible use of our algorithms in real world problems and compare them with classical approaches from pattern recognition."},
{"Title": "UNSUPERVISED LEARNING OF DISTRIBUTIONS ON BINARY VECTORS USING TWO LAYER NETWORKS", "URL": "https://dl.acm.org/doi/book/10.5555/902676", "Full Abstract": "We present a distribution model for binary vectors, called the influence combination model and show how this model can be used as the basis for unsupervised learning algorithms for feature selection. The model can be represented by a particular type of Boltzmann machine with a bipartite graph structure that we call the combination machine. This machine is closely related to the Harmonium model defined by Smolensky. In the first part of the paper we analyze properties of this distribution representation scheme. We show that arbitrary distributions of binary vectors can be approximated by the combination model. We show how the weight vectors in the model can be interpreted as high order correlation patterns among the input bits, and how the combination machine can be used as a mechanism for detecting these patterns. We compare the combination model with the mixture model and with principle component analysis. In the second part of the paper we present two algorithms for learning the combination model from examples. The first learning algorithm is the standard gradient ascent heuristic for computing maximum likelihood estimates for the parameters of the model. Here we give a closed form for this gradient that is significantly easier to compute than the corresponding gradient for the general Boltzmann machine. The second learning algorithm is a greedy method that creates the hidden units and computes their weights one at a time. This method is a variant of projection pursuit density estimation. In the third part of the paper we give experimental results for these learning methods on synthetic data and on natural data of handwritten digit images."},
{"Title": "ON-LINE PREDICTION AND CONVERSION STRATEGIES", "URL": "https://dl.acm.org/doi/book/10.5555/902846", "Full Abstract": "We study the problem of deterministically predicting boolean values by combining the boolean predictions of several experts. Previous on-line algorithms for this problem predict with the weighted majority of the experts' predictions. These algorithms give each expert an exponential weight beta^m where beta is a constant in [0,1) and m is the number of mistakes made by the expert in the past. We show that it is better to use sums of binomials as weights. In particular, we present a deterministic algorithm using binomial weights that has a better worst case mistake bound than the best deterministic algorithm using exponential weights. The binomial weights naturally arise from a version space argument. We also show how both exponential and binomial weighting schemes can be used to make prediction algorithms robust against noise."},
{"Title": "On-line prediction and conversion strategies", "URL": "https://dl.acm.org/doi/10.5555/214169.214210", "Full Abstract": "No abstract available."},
{"Title": "Equivalence of equilibrium propagation and recurrent backpropagation", "URL": "https://dl.acm.org/doi/10.1162/neco_a_01160", "Full Abstract": "Recurrent backpropagation and equilibrium propagation are supervised learning algorithms for fixed-point recurrent neural networks, which differ in their second phase. In the first phase, both algorithms converge to a fixed point that corresponds to the configuration where the prediction is made. In the second phase, equilibrium propagation relaxes to another nearby fixed point corresponding to smaller prediction error, whereas recurrent backpropagation uses a side network to compute error derivatives iteratively. In this work, we establish a close connection between these two algorithms. We show that at every moment in the second phase, the temporal derivatives of the neural activities in equilibrium propagation are equal to the error derivatives computed iteratively by recurrent backpropagation in the side network. This work shows that it is not required to have a side network for the computation of error derivatives and supports the hypothesis that in biological neural networks, temporal derivatives of neural activities may code for error signals."},
{"Title": "Gated orthogonal recurrent units", "URL": "https://dl.acm.org/doi/10.1162/neco_a_01174", "Full Abstract": "We present a novel recurrent neural network RNN-based model that combines the remembering ability of unitary evolution RNNs with the ability of gated RNNs to effectively forget redundant or irrelevant information in its memory. We achieve this by extending restricted orthogonal evolution RNNs with a gating mechanism similar to gated recurrent unit RNNs with a reset gate and an update gate. Our model is able to outperform long short-term memory, gated recurrent units, and vanilla unitary or orthogonal RNNs on several long-term-dependency benchmark tasks. We empirically show that both orthogonal and unitary RNNs lack the ability to forget. This ability plays an important role in RNNs. We provide competitive results along with an analysis of our model on many natural sequential tasks, including question answering, speech spectrum prediction, character-level language modeling, and synthetic tasks that involve long-term dependencies such as algorithmic, denoising, and copying tasks."},
{"Title": "A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies", "URL": "https://dl.acm.org/doi/10.1109/ICRA.2019.8794310", "Full Abstract": "Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage simulation and off-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is first trained through a meta-learning strategy in simulation. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with only a few real world expert demonstrations, we show successful planning performances in different navigation tasks."},
{"Title": "Interpolation consistency training for semi-supervised learning", "URL": "https://dl.acm.org/doi/10.5555/3367471.3367546", "Full Abstract": "We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets."},
{"Title": "Interpolated Adversarial Training", "URL": "https://dl.acm.org/doi/10.1145/3338501.3357369", "Full Abstract": "Adversarial robustness has become a central goal in deep learning, both in theory and in practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how achieving adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error (when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%."},
{"Title": "InfoMask: Masked Variational Latent Representation to Localize Chest Disease", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-32226-7_82", "Full Abstract": "The scarcity of richly annotated medical images is limiting supervised deep learning based solutions to medical image analysis tasks, such as localizing discriminatory radiomic disease signatures. Therefore, it is desirable to leverage unsupervised and weakly supervised models. Most recent weakly supervised localization methods apply attention maps or region proposals in a multiple instance learning formulation. While attention maps can be noisy, leading to erroneously highlighted regions, it is not simple to decide on an optimal window/bag size for multiple instance learning approaches. In this paper, we propose a learned spatial masking mechanism to filter out irrelevant background signals from attention maps. The proposed method minimizes mutual information between a masked variational representation and the input while maximizing the information between the masked representation and class labels. This results in more accurate localization of discriminatory regions. We tested the proposed model on the ChestX-ray8 dataset to localize pneumonia from chest X-ray images without using any pixel-level or bounding-box annotations."},
{"Title": "Depth with nonlinearity creates no bad local minima in ResNets", "URL": "https://dl.acm.org/doi/10.1016/j.neunet.2019.06.009", "Full Abstract": "In this paper, we prove that depth with nonlinearity creates no bad local minima in a type of arbitrarily deep ResNets with arbitrary nonlinear activation functions, in the sense that the values of all local minima are no worse than the"},
{"Title": "On adversarial mixup resynthesis", "URL": "https://dl.acm.org/doi/10.5555/3454287.3454678", "Full Abstract": "In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research."},
{"Title": "Updates of equilibrium prop match gradients of backprop through time in an RNN with static input", "URL": "https://dl.acm.org/doi/10.5555/3454287.3454923", "Full Abstract": "Equilibrium Propagation (EP) is a biologically inspired learning algorithm for convergent recurrent neural networks, i.e. RNNs that are fed by a static input"},
{"Title": "Unsupervised state representation learning in atari", "URL": "https://dl.acm.org/doi/10.5555/3454287.3455074", "Full Abstract": "State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods. The code associated with this work is available at https://github.com/mila-iqia/atari-representation-learning"},
{"Title": "How to initialize your network? robust initialization for WeightNorm & ResNets", "URL": "https://dl.acm.org/doi/10.5555/3454287.3455265", "Full Abstract": "Residual networks (ResNet) and weight normalization play an important role in various deep learning applications. However, parameter initialization strategies have not been studied previously for weight normalized networks and, in practice, initialization methods designed for un-normalized networks are used as a proxy. Similarly, initialization for ResNets have also been studied for un-normalized networks and often under simplified settings ignoring the shortcut connection. To address these issues, we propose a novel parameter initialization strategy that avoids explosion/vanishment of information across layers for weight normalized networks with and without residual connections. The proposed strategy is based on a theoretical analysis using mean field approximation. We run over 2,500 experiments and evaluate our proposal on image datasets showing that the proposed initialization outperforms existing initialization methods in terms of generalization performance, robustness to hyper-parameter values and variance between seeds, especially when networks get deeper in which case existing methods fail to even start training. Finally, we show that using our initialization in conjunction with learning rate warmup is able to reduce the gap between the performance of weight normalized and batch normalized networks."},
{"Title": "Gradient based sample selection for online continual learning", "URL": "https://dl.acm.org/doi/10.5555/3454287.3455345", "Full Abstract": "A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a fixed subset of constraints that best approximate the feasible region defined by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efficient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method."},
{"Title": "Non-normal recurrent neural network (nnRNN)", "URL": "https://dl.acm.org/doi/10.5555/3454287.3455506", "Full Abstract": "A recent strategy to circumvent the exploding and vanishing gradient problem in RNNs, and to allow the stable propagation of signals over long time scales, is to constrain recurrent connectivity matrices to be orthogonal or unitary. This ensures eigenvalues with unit norm and thus stable dynamics and training. However this comes at the cost of reduced expressivity due to the limited variety of orthogonal transformations. We propose a novel connectivity structure based on the Schur decomposition, though we avoid computing it explicitly, and a splitting of the Schur form into normal and non-normal parts. This allows to parametrize matrices with unit-norm eigenspectra without orthogonality constraints on eigenbases. The resulting architecture ensures access to a larger space of spectrally constrained matrices, of which orthogonal matrices are a subset. This crucial difference retains the stability advantages and training speed of orthogonal RNNs while enhancing expressivity, especially on tasks that require computations over ongoing input sequences."},
{"Title": "MelGAN", "URL": "https://dl.acm.org/doi/10.5555/3454287.3455622", "Full Abstract": "Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks."},
{"Title": "Wasserstein dependency measure for representation learning", "URL": "https://dl.acm.org/doi/10.5555/3454287.3455685", "Full Abstract": "Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound on mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge."},
{"Title": "Toward Training Recurrent Neural Networks for Lifelong Learning", "URL": "https://dl.acm.org/doi/10.1162/neco_a_01246", "Full Abstract": "Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting."},
{"Title": "A Large-Scale, Open-Domain, Mixed-Interface Dialogue-Based ITS for STEM", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-52240-7_70", "Full Abstract": "We present Korbit, a large-scale, open-domain, mixed-interface, dialogue-based intelligent tutoring system (ITS). Korbit uses machine learning, natural language processing and reinforcement learning to provide interactive, personalized learning online. Korbit has been designed to easily scale to thousands of subjects, by automating, standardizing and simplifying the content creation process. Unlike other ITS, a teacher can develop new learning modules for Korbit in a matter of hours. To facilitate learning across a wide range of STEM subjects, Korbit uses a mixed-interface, which includes videos, interactive dialogue-based exercises, question-answering, conceptual diagrams, mathematical exercises and gamification elements. Korbit has been built to scale to millions of students, by utilizing a state-of-the-art cloud-based micro-service architecture. Korbit launched its first course in 2019 and has over 7, 000 students have enrolled. Although Korbit was designed to be open-domain and highly scalable, A/B testing experiments with real-world students demonstrate that both student learning outcomes and student motivation are substantially improved compared to typical online courses."},
{"Title": "Revisiting fundamentals of experience replay", "URL": "https://dl.acm.org/doi/10.5555/3524938.3525225", "Full Abstract": "Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected ("},
{"Title": "Learning to navigate the synthetically accessible chemical space using reinforcement learning", "URL": "https://dl.acm.org/doi/10.5555/3524938.3525282", "Full Abstract": "Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in generative modeling of novel chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel reinforcement learning (RL) setup for de novo drug design: Policy Gradient for Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and clogP. Moreover, we validate PGFS in an in-silico proof-of-concept associated with three HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process."},
{"Title": "Taming the computational complexity of combinatorial auctions", "URL": "https://dl.acm.org/doi/10.5555/1624218.1624297", "Full Abstract": "In combinatorial auctions, multiple goods are sold simultaneously and bidders may bid for arbitrary combinations of goods. Determining the outcome of such an auction is an optimization problem that is NP-complete in the general case. We propose two methods of overcoming this apparent intractability. The first method, which is guaranteed to be optimal, reduces running time by structuring the search space so that a modified depth-first search usually avoids even considering allocations that contain conflicting bids. Caching and pruning are also used to speed searching. Our second method is a heuristic, market-based approach. It sets up a virtual multi-round auction in which a virtual agent represents each original bid bundle and places bids, according to a fixed strategy, for each good in that bundle. We show through experiments on synthetic data that (a) our first method finds optimal allocations quickly and offers good anytime performance, and (b) in many cases our second method, despite lacking guarantees regarding optimality or running time, quickly reaches solutions that are nearly optimal."},
{"Title": "Speeding up ascending-bid auctions", "URL": "https://dl.acm.org/doi/10.5555/1624218.1624298", "Full Abstract": "In recent years auctions have grown in interest within the AI community as innovative mechanisms for resource allocation. The primary contribution of this paper is to identify a family of hybrid auctions, called survival auctions, which combine the benefits of both sealed-bid auctions (namely, quick and predictable termination time) and ascending-bid auctions (namely, more information revelation often leading, among other things, to better allocations and greater expected revenue). Survival auctions are multi-round sealed-bid auctions with an information-revelation component, in which some bidders are eliminated from the auction from one round to the next. These auctions are intuitive, easy to implement, and most importantly provably optimal. More precisely, we show that (a) the survival auction in which all but the lowest bidder make it into the next round (the auction lasts for ("},
{"Title": "Taming the Computational Complexity of Combinatorial Auctions", "URL": "https://dl.acm.org/doi/10.5555/646307.687748", "Full Abstract": "No abstract available."},
{"Title": "Speeding Up Ascending-Bid Auctions", "URL": "https://dl.acm.org/doi/10.5555/646307.687749", "Full Abstract": "No abstract available."},
{"Title": "Truth revelation in approximately efficient combinatorial auctions", "URL": "https://dl.acm.org/doi/10.1145/336992.337016", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "An Algorithm for Multi-Unit Combinatorial Auctions", "URL": "https://dl.acm.org/doi/10.5555/647288.721424", "Full Abstract": "No abstract available."},
{"Title": "Towards a universal test suite for combinatorial auction algorithms", "URL": "https://dl.acm.org/doi/10.1145/352871.352879", "Full Abstract": "Copyright © 2000 ACM."},
{"Title": "Bidding clubs", "URL": "https://dl.acm.org/doi/10.1145/352871.352899", "Full Abstract": "Copyright © 2000 ACM."},
{"Title": "Fair imposition", "URL": "https://dl.acm.org/doi/10.5555/1642194.1642239", "Full Abstract": "We introduce a new notion, related to auctions and mechanism design, called fair imposition. In this setting a center wishes to fairly and efficiently allocate tasks among a set of agents, not knowing their cost structure. As in the study of auctions, the main abstacle to overcome is the self-interest of the agents, which will in general cause them to hide their true costs. Unlike the auction setting, however, here the center has the power to impose arbitrary behavior on the agents, and furthermore wishes to distribute the cost as fairly as possible among them. We define the problem precisely, present solution criteria for these problems (the central of which is called"},
{"Title": "Belief Fusion", "URL": "https://dl.acm.org/doi/10.1023/A%3A1008375000210", "Full Abstract": "We introduce a new operator –"},
{"Title": "Polynomial-time reinforcement learning of near-optimal policies", "URL": "https://dl.acm.org/doi/10.5555/777092.777127", "Full Abstract": "Inspired by recent results on polynomial time reinforcement algorithms that accumulate near-optimal rewards, we look at the related problem of quickly learning near-optimal policies. The new problem is obviously related to the previous one, but different in important ways. We provide simple algorithms for MDPs, zero-sum and common-payoff Stochastic Games, and a uniform framework for proving their polynomial complexity. Unlike the previously studied problem, these bounds use the minimum between the mixing time and a new quantity - the spectral radius. Unlike the previous results, our results apply uniformly to the average and discounted cases."},
{"Title": "Bidding clubs in first-price auctions", "URL": "https://dl.acm.org/doi/10.5555/777092.777152", "Full Abstract": "We introduce a class of mechanisms, called <i>bidding clubs</i>, that allow agents to coordinate their bidding in auctions. Bidding clubs invite a set of agents to join, and each invited agent freely chooses whether to accept the invitation or whether to participate independently in the auction. Clubs first conduct a \"pre-auction\"; depending on the outcome of the pre-auction some subset of the members of the club bid in the primary auction in a prescribed way. We model this setting as a Bayesian game, including agents' choices of whether or not to accept a bidding club's invitation. We examine the specific case of bidding clubs for first-price auctions, showing the existence of a Bayes-Nash equilibrium where agents choose to participate in bidding clubs when invited and truthfully declare their valuations to the coordinator. Furthermore, we show that the existence of bidding clubs benefits all agents, including those who do not belong to a bidding club."},
{"Title": "Dispersion games", "URL": "https://dl.acm.org/doi/10.5555/777092.777156", "Full Abstract": "Dispersion games are the generalization of the <i>anticoordination game</i> to arbitrary numbers of agents and actions. In these games agents prefer outcomes in which the agents are maximally dispersed over the set of possible actions. This class of games models a large number of natural problems, including load balancing in computer science, niche selection in economics, and division of roles within a team in robotics. Our work consists of two main contributions. First, we formally define and characterize some interesting classes of dispersion games. Second, we present several learning strategies that agents can use in these games, including traditional learning rules from game theory and artificial intelligence, as well as some special purpose strategies. We then evaluate analytically and empirically the performance of each of these strategies."},
{"Title": "Mechanism design with execution uncertainty", "URL": "https://dl.acm.org/doi/10.5555/2073876.2073925", "Full Abstract": "We introduce the notion of fault tolerant mechanism design, which extends the standard game theoretic framework of mechanism design to allow for uncertainty about execution. Specifically, we define the problem of task allocation in which the private information of the agents is not only their costs to attempt the tasks, but also their probabilities of failure. For several different instances of this setting we present technical results, including positive ones in the form of mechanisms that are incentive compatible, individually rational and efficient, and negative ones in the form of impossibility theorems."},
{"Title": "Truth revelation in approximately efficient combinatorial auctions", "URL": "https://dl.acm.org/doi/10.1145/585265.585266", "Full Abstract": "Some important classical mechanisms considered in Microeconomics and Game Theory require the solution of a difficult optimization problem. This is true of mechanisms for combinatorial auctions, which have in recent years assumed practical importance, and in particular of the gold standard for combinatorial auctions, the Generalized Vickrey Auction (GVA). Traditional analysis of these mechanisms---in particular, their truth revelation properties---assumes that the optimization problems are solved precisely. In reality, these optimization problems can usually be solved only in an approximate fashion. We investigate the impact on such mechanisms of replacing exact solutions by approximate ones. Specifically, we look at a particular greedy optimization method. We show that the GVA payment scheme does not provide for a truth revealing mechanism. We introduce another scheme that does guarantee truthfulness for a restricted class of players. We demonstrate the latter property by identifying natural properties for combinatorial auctions and showing that, for our restricted class of players, they imply that truthful strategies are dominant. Those properties have applicability beyond the specific auction studied."},
{"Title": "Learning the Empirical Hardness of Optimization Problems", "URL": "https://dl.acm.org/doi/10.5555/647489.727158", "Full Abstract": "We propose a new approach for understanding the algorithm-specific empiricalh ardness of NP-Hard problems. In this work we focus on the empirical hardness of the winner determination problem--an optimization problem arising in combinatorial auctions--when solved by ILOG's CPLEX software. We consider nine widely-used problem distributions and sample randomly from a continuum of parameter settings for each distribution. We identify a large number of distribution-nonspecific features of data instances and use statisticalregression techniques to learn, evaluate and interpret a function from these features to the predicted hardness of an instance."},
{"Title": "Incentive mechanisms for smoothing out a focused demand for network resources", "URL": "https://dl.acm.org/doi/10.1016/S0140-3664%2802%2900139-1", "Full Abstract": "We explore the problem of sharing network resources when users' preferences lead to temporally concentrated loads, resulting in an inefficient use of the network. In such cases external incentives can be supplied to smooth out demand, obviating the need for expensive technological mechanisms. Taking a game-theoretic approach, we consider a setting in which bandwidth or access to service is available during different time slots at a fixed cost, but all agents have a natural preference for choosing the same time slot. We present four mechanisms that motivate users to distribute the load by probabilistically waiving the cost for each time slot, and analyze the equilibria that arise under these mechanisms."},
{"Title": "On cheating in sealed-bid auctions", "URL": "https://dl.acm.org/doi/10.1145/779928.779938", "Full Abstract": "Motivated by the rise of online auctions and their relative lack of security, this paper analyzes two forms of cheating in sealed-bid auctions. The first type of cheating we consider occurs when the seller spies on the bids of a second-price auction and then inserts a fake bid in order to increase the payment of the winning bidder. In the second type, a bidder cheats in a first-price auction by examining the competing bids before deciding on his own bid. In both cases, we derive equilibrium strategies when bidders are aware of the possibility of cheating. These results provide insights into sealed-bid auctions even in the absence of cheating, including some counterintuitive results on the effects of overbidding in a first-price auction.footnotetext[1]This work was supported in part by DARPA grant F30602-00-2-0598."},
{"Title": "Towards a general theory of non-cooperative computation", "URL": "https://dl.acm.org/doi/10.1145/846241.846249", "Full Abstract": "We generalize the framework of non-cooperative computation (NCC), recently introduced by Shoham and Tennenholtz, to apply to cryptographic situations. We consider functions whose inputs are held by separate, self-interested agents. We consider four components of each agent's utility function: (a) the wish to know the correct value of the function, (b) the wish to prevent others from knowing it, (c) the wish to prevent others from knowing one's own private input, and (d) the wish to know other agents' private inputs. We provide an exhaustive game theoretic analysis of all 24 possible lexicographic orderings among these four considerations, for the case of Boolean functions (mercifully, these 24 cases collapse to four). In each case we identify the class of functions for which there exists an incentive-compatible mechanism for computing the function. In this article we only consider the situation in which the inputs of different agents are probabilistically independent."},
{"Title": "A portfolio approach to algorithm select", "URL": "https://dl.acm.org/doi/10.5555/1630659.1630927", "Full Abstract": "No abstract available."},
{"Title": "An improved algorithm for CIOQ switches", "URL": "https://dl.acm.org/doi/10.1145/1150334.1150342", "Full Abstract": "The problem of maximizing the weighted throughput in various switching settings has been intensively studied recently through competitive analysis. To date, the most general model that has been investigated is the standard CIOQ (Combined Input and Output Queued) switch architecture with internal fabric speedup"},
{"Title": "Maximizing throughput in multi-queue switches", "URL": "https://dl.acm.org/doi/10.1007/s00453-005-1190-x", "Full Abstract": "We study a basic problem in Multi-Queue switches. A switch connectsm input ports to a single output port. Each input port is equipped with an incoming FIFO queue with bounded capacityB. A switch serves its input queues by transmitting packets arriving at these queues, one packet per time unit. Since the arrival rate can be higher than the transmission rate and each queue has limited capacity, packet loss may occur as a result of insufficient queue space. The goal is to maximize the number of transmitted packets. This general scenario models most current networks (e.g. IP networks) which only support a \"best effort\" service in which all packet streams are treated equally. A 2-competitive algorithm for this problem was designed in [5] for arbitraryB. Recently, a (17/9 ź 1.89)-competitive algorithm was presented forB>1 in [3]. Our main result in this paper shows that forB which is not too small our algorithm can do better than 1.89, and approach a competitive ratio ofe/(e ź 1) ź 1.58."},
{"Title": "Multiplexing packets with arbitrary deadlines in bounded buffers", "URL": "https://dl.acm.org/doi/10.1007/11785293_4", "Full Abstract": "We study the online problem of multiplexing packets with arbitrary deadlines in bounded multi-buffer switch. In this model, a switch consists of"},
{"Title": "Tell me who I am", "URL": "https://dl.acm.org/doi/10.1145/1148109.1148111", "Full Abstract": "We consider a model of recommendation systems, where each member from a given set of"},
{"Title": "Tradeoffs in worst-case equilibria", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2006.05.010", "Full Abstract": "We investigate the problem of routing traffic through a congested network in an environment of non-cooperative users. We use the worst-case coordination ratio suggested by Koutsoupias and Papadimitriou to measure the performance degradation due to the lack of a centralized traffic regulating authority. We provide a full characterization of the worst-case coordination ratio in the restricted assignment and unrelated parallel links model. In particular, we quantify the tradeoff between the \"negligibility\" of the traffic controlled by each user and the worst-case coordination ratio. We analyze both pure and mixed strategies systems and identify the range where their performance is similar."},
{"Title": "Load balancing of temporary tasks in the ", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2006.05.016", "Full Abstract": "We consider the on-line load balancing problem where there are"},
{"Title": "A general approach to online network optimization problems", "URL": "https://dl.acm.org/doi/10.1145/1198513.1198522", "Full Abstract": "We study a wide range of online graph and network optimization problems, focusing on problems that arise in the study of connectivity and cuts in graphs. In a general online network design problem, we have a communication network known to the algorithm in advance. What is not known in advance are the connectivity (bandwidth) or cut demands between vertices in the network which arrive online.We develop a unified framework for designing online algorithms for problems involving connectivity and cuts. We first present a general"},
{"Title": "An improved algorithm for online coloring of intervals with bandwidth", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2006.06.014", "Full Abstract": "We present an improved online algorithm for coloring interval graphs with bandwidth. This problem has recently been studied by Adamy and Erlebach and a 195-competitive online strategy has been presented. We improve this by presenting a 10-competitive strategy. To achieve this result, we use variants of an optimal online coloring algorithm due to Kierstead and Trotter."},
{"Title": "Minimizing Total Flow Time and Total Completion Time with Immediate Dispatching", "URL": "https://dl.acm.org/doi/10.1007/s00453-006-0193-6", "Full Abstract": "We consider the problem of scheduling jobs arriving over time in a multiprocessor setting, with immediate dispatching, disallowing job migration. The goal is to minimize both the total flow time (total time in the system) and the total completion time. Previous studies have shown that while preemption (interrupt a job and later continue its execution) is inherent to make a scheduling algorithm efficient, migration (continue the execution on a different machine) is not. Still, the current non-migratory online algorithms suffer from a need for a central queue of unassigned jobs which is a \"no option\" in large computing systems, such as the Web. We introduce a simple online non-migratory algorithm IMD, which employs immediate dispatching, i.e., it immediately assigns released jobs to one of the machines. We show that the performance of this algorithm is within a logarithmic factor of the optimal migratory offline algorithm, with respect to the total flow time, and within a small constant factor of the optimal migratory offline algorithm, with respect to the total completion time. This solves an open problem suggested by Awerbuch et al. (STOC 99)."},
{"Title": "Truthful Approximation Mechanisms for Scheduling Selfish Related Machines", "URL": "https://dl.acm.org/doi/10.1007/s00224-006-1316-9", "Full Abstract": "We consider the problem of scheduling jobs on related machines owned by selfish agents. We provide a 5-approximation deterministic truthful mechanism, the first deterministic truthful result for the problem. Previously, Archer and Tardos showed a 2-approximation randomized mechanism which is truthful in expectation only (a weaker notion of truthfulness). In case the number of machines is constant, we provide a deterministic Fully Polynomial-Time Approximation Scheme (FPTAS) and a suitable payment scheme that yields a truthful mechanism for the problem. This result, which is based on converting FPTAS to monotone FPTAS, improves a previous result of Auletta et al., who showed a (4 + ε)-approximation truthful mechanism."},
{"Title": "Truthful unsplittable flow for large capacity networks", "URL": "https://dl.acm.org/doi/10.1145/1248377.1248432", "Full Abstract": "The"},
{"Title": "Collaborate with Strangers to Find Own Preferences", "URL": "https://dl.acm.org/doi/10.5555/1340393.1340398", "Full Abstract": "We consider a model with"},
{"Title": "Fast load balancing via bounded best response", "URL": "https://dl.acm.org/doi/10.5555/1347082.1347117", "Full Abstract": "It is known that the dynamics of best response in an environment of non-cooperative users may converge to a good solution when users play sequentially, but may cycle far away from the global optimum solution when users play concurrently. We introduce the notion of bounded best response where users react with best response subject to rules that are forced locally by the system. We investigate the problem of load balancing tasks on machines in a bipartite graph model and show that the dynamics of concurrent bounded best response converges to a near-optimum solution quickly, i.e., with poly-logarithmic number of rounds. This is in contrast to the concurrent best response dynamics which cycles far away from the optimum and to any sequential dynamics which requires at least a linear number of rounds to get to a reasonable solution."},
{"Title": "(Almost) optimal coordination mechanisms for unrelated machine scheduling", "URL": "https://dl.acm.org/doi/10.5555/1347082.1347118", "Full Abstract": "We investigate the influence of different algorithmic choices on the approximation ratio in selfish scheduling. Our goal is to design local policies that minimize the inefficiency of resulting equilibria. In particular, we design optimal coordination mechanisms for unrelated machine scheduling, and improve the known approximation ratio from Θ"},
{"Title": "A Preemptive Algorithm for Maximizing Disjoint Paths on Trees", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-69903-3_29", "Full Abstract": "We consider the online version of the maximum vertex disjoint path problem when the underlying network is a tree. In this problem, a sequence of requests arrives in an online fashion, where every request is a path in the tree. The online algorithm may accept a request only if it does not share a vertex with a previously accepted request. The goal is to maximize the number of accepted requests. It is known that no online algorithm can have a competitive ratio better than"},
{"Title": "Truthful Unification Framework for Packing Integer Programs with Choices", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-70575-8_68", "Full Abstract": "One of the most interesting research directions within the field of algorithmic mechanism design revolves the study of hard combinatorial optimization problems. In this setting, many common algorithmic techniques cannot be utilized as they violate certain monotonicity properties that are imperative for truthfulness. Consequently, it seems of the essence to develop alternative methods, which can underlie truthful mechanisms. In particular, since many problems can be formulated as instances of integer linear programs, it seems that devising techniques that apply to integer linear programs is significantly important."},
{"Title": "Fast convergence to nearly optimal solutions in potential games", "URL": "https://dl.acm.org/doi/10.1145/1386790.1386832", "Full Abstract": "We study the speed of convergence of decentralized dynamics to approximately optimal solutions in potential games. We consider α-Nash dynamics in which a player makes a move if the improvement in his payoff is more than an α factor of his own payoff. Despite the known polynomial convergence of α-Nash dynamics to approximate Nash equilibria in symmetric congestion games [7], it has been shown that the convergence time to approximate Nash equilibria in asymmetric congestion games is exponential [25]. In contrast to this negative result, and as the main result of this paper, we show that for asymmetric congestion games with linear and polynomial delay functions, the convergence time of α-Nash dynamics to an approximate optimal solution is polynomial in the number of players, with approximation ratio that is arbitrarily close to the price of anarchy of the game. In particular, we show this polynomial convergence under the minimal liveness assumption that each player gets at least one chance to move in every"},
{"Title": "Optimal oblivious routing in polynomial time", "URL": "https://dl.acm.org/doi/10.1145/780542.780599", "Full Abstract": "A recent seminal result of Racke is that for any network there is an oblivious routing algorithm with a polylog competitive ratio with respect to congestion. Unfortunately, Racke's construction is not polynomial time. We give a polynomial time construction that guarantee's Racke's bounds, and more generally gives the true optimal ratio for any network."},
{"Title": "Reducing truth-telling online mechanisms to online optimization", "URL": "https://dl.acm.org/doi/10.1145/780542.780616", "Full Abstract": "We describe a general technique for converting an online algorithm"},
{"Title": "Distributed error confinement", "URL": "https://dl.acm.org/doi/10.1145/872035.872041", "Full Abstract": "We initiate the study of error confinement in distributed applications, where the goal is that only nodes that were directly hit by a fault may deviate from their correct external behavior, and only temporarily. The external behavior of all other nodes must remain impeccable, even though their internal state may be affected. Error confinement is impossible if an adversary is allowed to inflict arbitrary transient faults on the system, since the faults might completely wipe out input values. We introduce a new fault tolerance measure we call"},
{"Title": "A general approach to online network optimization problems", "URL": "https://dl.acm.org/doi/10.5555/982792.982879", "Full Abstract": "We study a wide range of online graph and network optimization problems, focusing on problems that arise in the study of connectivity and cuts in graphs. In a general online network design problem, we have a communication network known to the algorithm in advance. What is not known in advance are the bandwidth or cut demands between nodes in the network. Our results include an"},
{"Title": "The zero-one principle for switching networks", "URL": "https://dl.acm.org/doi/10.1145/1007352.1007369", "Full Abstract": "Recently, approximation analysis has been extensively used to study algorithms for routing weighted packets in various network settings. Although different techniques were applied in the analysis of diverse models, one common property was evident: the analysis of input sequences composed solely of two different values is always substantially easier, and many results are known only for restricted value sequences. Motivated by this, we introduce our zero-one principle for switching networks which characterizes a wide range of algorithms for which achieving c-approximation (as well as c-competitiveness) with respect to sequences composed of 0's and 1's implies achieving c-approximation. The zero-one principle proves to be very efficient in the design of switching algorithms, and substantially facilitates their analysis. We present three applications. First, we consider the Multi-Queue QoS Switching model and design a 3-competitive algorithm, improving the result from [6]. Second, we study the Weighted Dynamic Routing problem on a line topology of length k and present a (k+1)-competitive algorithm, which improves and generalizes the results from [1,12]. As a third application, we consider the work of [11], that compares the performance of local algorithms to the global optimum in various network topologies, and generalize their results from 2-value sequences to arbitrary value sequences."},
{"Title": "All-norm approximation algorithms", "URL": "https://dl.acm.org/doi/10.1016/j.jalgor.2004.02.003", "Full Abstract": "A major drawback in optimization problems and in particular in scheduling problems is that for every measure there may be a different optimal solution. In many cases the various measures are different"},
{"Title": "Online packet switching", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-31833-0_1", "Full Abstract": "We discuss packet switching for single-queue, multi-queue buffers and CIOQ buffers. We evaluate the algorithms by competitive analysis. We also mention the zero-one principle that applies to general switching networks."},
{"Title": "On-line generalized Steiner problem", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2004.05.021", "Full Abstract": "The generalized Steiner problem (GSP) is defined as follows. We are given a graph with non-negative edge weights and a set of pairs of vertices. The algorithm has to construct minimum weight subgraph such that the two nodes of each pair are connected by a path.Off-line GSP approximation algorithms were given in Agarwal et al. (SIAM J. Comput. 24(3) (1995) 440) and Goemans and Williamson (SIAM J. Comput. 24(2) (1995) 296). We consider the on-line GSP, in which pairs of vertices arrive on-line and are needed to be connected immediately.We show that the online Min-Cost (i.e. greedy) strategy for this problem has O(log"},
{"Title": "Optimal oblivious routing in polynomial time", "URL": "https://dl.acm.org/doi/10.1016/j.jcss.2004.04.010", "Full Abstract": "A recent seminal result of Räcke is that for any undirected network there is an oblivious routing algorithm with a polylogarithmic competitive ratio with respect to congestion. Unfortunately, Räcke's construction is not polynomial time. We give a polynomial time construction that guarantees Räcke's bounds, and more generally gives the true optimal ratio for any (undirected or directed) network."},
{"Title": "On-Line Load Balancing of Temporary Tasks on Identical Machines", "URL": "https://dl.acm.org/doi/10.1137/S0895480197329296", "Full Abstract": "We prove an exact lower bound of 2-\\frac{1{"},
{"Title": "Truthful approximation mechanisms for scheduling selfish related machines", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-31856-9_6", "Full Abstract": "We consider the problem of scheduling jobs on related machines owned by selfish agents. Previously, Archer and Tardos showed a 2-approximation randomized mechanism which is truthful in expectation only (a weaker notion of truthfulness). We provide a 5-approximation deterministic truthful mechanism, the first deterministic truthful result for the problem."},
{"Title": "The Price of Routing Unsplittable Flow", "URL": "https://dl.acm.org/doi/10.1145/1060590.1060599", "Full Abstract": "The essence of the routing problem in real networks is that the traffic demand from a source to destination must be satisfied by choosing a single path between source and destination. The splittable version of this problem is when demand can be satisfied by many paths, namely a flow from source to destination. The unsplittable, or discrete version of the problem is more realistic yet is more complex from the algorithmic point of view; in some settings optimizing such unsplittable traffic flow is computationally intractable.In this paper, we assume this more realistic unsplittable model, and investigate the \"price of anarchy\", or deterioration of network performance measured in total traffic latency under the selfish user behavior. We show that for linear edge latency functions the price of anarchy is exactly $2.618 for weighted demand and exactly $2.5 for unweighted demand. These results are easily extended to (weighted or unweighted) atomic \"congestion games\", where paths are replaced by general subsets. We also show that for polynomials of degree d edge latency functions the price of anarchy is d"},
{"Title": "Convex programming for scheduling unrelated parallel machines", "URL": "https://dl.acm.org/doi/10.1145/1060590.1060639", "Full Abstract": "We consider the classical problem of scheduling parallel unrelated machines. Each job is to be processed by exactly one machine. Processing job j on machine i requires time p"},
{"Title": "Admission control to minimize rejections and online set cover with repetitions", "URL": "https://dl.acm.org/doi/10.1145/1073970.1074010", "Full Abstract": "We study the admission control problem in general networks. Communication requests arrive over time, and the online algorithm accepts or rejects each request while maintaining the capacity limitations of the network. The admission control problem has been usually analyzed as a benefit problem, where the goal is to devise an online algorithm that accepts the maximum number of requests possible. The problem with this objective function is that even algorithms with optimal competitive ratios may reject almost all of the requests, when it would have been possible to reject only a few. This could be inappropriate for settings in which rejections are intended to be rare events."},
{"Title": "Collaborate with strangers to find own preferences", "URL": "https://dl.acm.org/doi/10.1145/1073970.1074014", "Full Abstract": "We consider a model with"},
{"Title": "Management of Multi-Queue Switches in QoS Networks", "URL": "https://dl.acm.org/doi/10.1007/s00453-005-1159-9", "Full Abstract": "The concept of Quality of Service (QoS) networks has gained growing attention recently, as the traffic volume in the Internet constantly increases, and QoS guarantees are essential to ensure proper operation of most communication-based applications. A QoS switch serves m incoming queues by transmitting packets arriving to these queues through one output port, one packet per time step. Each packet is marked with a value indicating its priority in the network. Since the queues have bounded capacities and the rate of arriving packets can be much higher than the transmission rate, packets can be lost due to insufficient queue space. The goal is to maximize the total value of transmitted packets. This problem encapsulates two dependent questions: buffer management, namely which packets to admit into the queues, and scheduling, i.e. which queue to use for transmission in each time step. We use competitive analysis to study online switch performance in QoS-based networks. Specifically, we provide a novel generic technique that decouples the buffer management and scheduling problems. Our technique transforms any single-queue buffer management policy (preemptive or non-preemptive) to a scheduling and buffer management algorithm for our general m queues model, whose competitive ratio is at most twice the competitive ratio of the given buffer management policy. We use our technique to derive concrete algorithms for the general preemptive and non-preemptive cases, as well as for the interesting special cases of the 2-value model and the unit-value model. We also provide a 1.58-competitive randomized algorithm for the unit-value case. This case is interesting by itself since most current networks (e.g. IP networks) do not yet incorporate full QoS capabilities, and treat all packets equally."},
{"Title": "Fair versus Unrestricted Bin Packing", "URL": "https://dl.acm.org/doi/10.5555/645900.672599", "Full Abstract": "We consider the Unrestricted Bin Packing problem where we have bins of equal size and a sequence of items. The goal is to maximize the number of items that are packed in the bins by an on-line algorithm. We investigate the power of performing admission control on the items, i.e., rejecting items while there is enough space to pack them, versus behaving fairly, i.e., rejecting an item only when there is not enough space to pack it. We show that by performing admission control on the items, we get better performance for various measures compared with the performance achieved on the fair version of the problem. Our main result shows that we can pack 2/3 of the items for sequences in which the optimal can pack all the items."},
{"Title": "Maximizing job benefits on-line", "URL": "https://dl.acm.org/doi/10.5555/646688.703098", "Full Abstract": "We consider a benefit model for on-line preemptive scheduling. In this model jobs arrive to the on-line scheduler at their release time. Each job arrives with its own execution time and its benefit function. The flow time of a job is the time that passes from its release to its completion. The benefit function specifies the benefit gained for any given flow time. A scheduler's goal is to maximize the total gained benefit. We present a constant competitive ratio algorithm for that model in the uniprocessor case for benefit functions that do not decrease too fast.We also extend the algorithm to the multiprocessor case while maintaining constant competitiveness. The multiprocessor algorithm does not use migration, i.e., preempted jobs continue their execution on the same processor on which they were originally processed."},
{"Title": "Ancient and New Algorithms for Load Balancing in the lp Norm", "URL": "https://dl.acm.org/doi/10.1007/s004530010051", "Full Abstract": "We consider the on-line load balancing problem where there are m identical machines (servers) and a sequence of jobs. The jobs arrive one by one and should be assigned to one of the machines in an on-line fashion. The goal is to minimize the sum (over all machines) of the squares of the loads, instead of the traditional maximum load."},
{"Title": "Competitive Routing of Virtual Circuits with Unknown Duration", "URL": "https://dl.acm.org/doi/10.1006/jcss.1999.1662", "Full Abstract": "In this paper we present a strategy to route unknown duration virtual circuits in a high-speed communication network. Previous work on virtual circuit routing concentrated on the case where the call duration is known in advance. We show that by allowing O(logn) reroutes per call, we can achieve O(logn) competitive ratio with respect to the maximum load (congestion) for the unknown duration case, where n is the number of nodes in the network. This is in contrast to the (n) lower bound on the competitive ratio for this case if no rerouting is allowed (Azar et al., 1992, Proc. 33rd IEEE Annual Symposium of Foundations of Computer Science, pp. 218 225). Our routing algorithm can be also applied in the context of machine load balancing of tasks with unknown duration. We present an algorithm that makes O(logn) reassignments per task and achieves O(logn) competitive ratio with respect to the load, where n is the number of parallel machines. For a special case of unit load tasks we design a constant competitive algorithm. The previously known algorithms that achieve up to polylogarithmic competitive ratio for load balancing of tasks with unknown duration dealt only with special cases of related machines case and unit-load tasks with restricted assignment (Azar et al., 1993, Proc. Workshop on Algorithms and Data Structures, pp. 119 130; Azar et al., 1992, Proc. 3rd ACM-SIAM Symposium on Discrete Algorithms, pp. 203 210)."},
{"Title": "Strongly Polynomial Algorithms for the Unsplittable Flow Problem", "URL": "https://dl.acm.org/doi/10.5555/645590.659925", "Full Abstract": "We provide the first strongly polynomial algorithms with the best approximation ratio for all three variants of the unsplittable flow problem (UFP). In this problem we are given a (possibly directed) capacitated graph with n vertices and m edges, and a set of terminal pairs each with its own demand and profit. The objective is to connect a subset of the terminal pairs each by a single flow path as to maximize the total profit of the satisfied terminal pairs subject to the capacity constraints. Classical UFP, in which demands must be lower than edge capacities, is known to have an"},
{"Title": "On-line bin-stretching", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2800%2900258-9", "Full Abstract": "We are given a sequence of items that can be packed into"},
{"Title": "The Multicast Bandwidth Advantage in Serving a Web Site", "URL": "https://dl.acm.org/doi/10.5555/648089.747494", "Full Abstract": "Delivering popular web pages to the clients results in high bandwidth and high load on the web servers. A method to overcome this problem is to send these pages, requested by many users, via multicast. In this paper, we provide an analytic criterion to determine which pages to multicast, and analyze the overall saving factor as compared with a unicast delivery. The analysis is based on the well known observation that page popularity follows a Zipf-like distribution. Interestingly, we can obtain closed-form analytical expressions for the saving factor, that show the multicast advantage as a function of the site hit-rate, the allowed latency and the Zipf parameter."},
{"Title": "Temporary tasks assignment resolved", "URL": "https://dl.acm.org/doi/10.5555/545381.545396", "Full Abstract": "Among all basic on-line load balancing problems, the only unresolved problem was load balancing of temporary tasks on unrelated machines. This open problem exists for almost a decade, see [Borodin El-Yaniv]. We resolve this problem by providing an unapproximability result. In addition, a newer open question is to identify the dependency of the competitive ratio on the durations of jobs in the case where durations are known. We resolve this problem by characterizing this dependency. Finally, we provide a"},
{"Title": "Minimizing the Flow Time Without Migration", "URL": "https://dl.acm.org/doi/10.1137/S009753970037446X", "Full Abstract": "We consider the classical problem of scheduling jobs in a multiprocessor setting in order to minimize the flow time (total time in the system). The performance of the algorithm, both in offline and online settings, can be significantly improved if we allow preemption, i.e., interrupt a job and later continue its execution, perhaps migrating it to a different machine. Preemption is inherent to make a scheduling algorithm efficient. While in the case of a single processor most operating systems can easily handle preemptions, migrating a job to a different machine results in a huge overhead. Thus, it is not commonly used in most multiprocessor operating systems. The natural question is whether migration is an inherent component for an efficient scheduling algorithm in either the online or offline setting."},
{"Title": "On-line scheduling with precedence constraints", "URL": "https://dl.acm.org/doi/10.1016/S0166-218X%2801%2900272-4", "Full Abstract": "No abstract available."},
{"Title": "All-Norm Approximation Algorithms", "URL": "https://dl.acm.org/doi/10.5555/645901.672767", "Full Abstract": "A major drawback in optimization problems and in particular in scheduling problems is that for every measure there may be a different optimal solution. In many cases the various measures are different"},
{"Title": "Off-line temporary tasks assignment", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2801%2900254-7", "Full Abstract": "In this paper we consider the temporary tasks assignment problem. In this problem, there are"},
{"Title": "On-line restricted assignment of temporary tasks with unknown durations", "URL": "https://dl.acm.org/doi/10.1016/S0020-0190%2802%2900350-2", "Full Abstract": "We consider load balancing of temporary tasks on"},
{"Title": "Beating the logarithmic lower bound", "URL": "https://dl.acm.org/doi/10.1023/A%3A1022933824889", "Full Abstract": "We consider the maximum disjoint paths problem and its generalization, the call control problem, in the on-line setting. In the maximum disjoint paths problem, we are given a sequence of connection requests for some communication network. Each request consists of a pair of nodes, that wish to communicate over a path in the network. The request has to be immediately connected or rejected, and the goal is to maximize the number of connected pairs, such that no two paths share an edge. In the call control problem, each request has an additional bandwidth specification, and the goal is to maximize the total bandwidth of the connected pairs (throughput), while satisfying the bandwidth constraints (assuming each edge has unit capacity). These classical problems are central in routing and admission control in high speed networks and in optical networks.We present the first known constant-competitive algorithms for both problems on the line. This settles an open problem of Garay et al. and of Leonardi. Moreover, to the best of our knowledge, all previous algorithms for any of these problems, are Ω(log"},
{"Title": "A Generic Scheme for Building Overlay Networks in Adversarial Scenarios", "URL": "https://dl.acm.org/doi/10.5555/838237.838599", "Full Abstract": "This paper presents a generic scheme for a central, yet untackled issue in overlay dynamic networks: maintaining stability over long life and against malicious adversaries. The generic scheme maintains desirable properties of the underlying structure including low diameter, and efficient routing mechanism, as well as balanced node dispersal. These desired properties are maintained in a decentralized manner without resorting to global updates or periodic stabilization protocols even against an adaptive adversary that controls the arrival and departure of nodes."},
{"Title": "Minimizing total flow time and total completion time with immediate dispatching", "URL": "https://dl.acm.org/doi/10.1145/777412.777415", "Full Abstract": "We consider the problem of scheduling jobs arriving over time in a multiprocessor setting, with"},
{"Title": "Combining online algorithms for rejection and acceptance", "URL": "https://dl.acm.org/doi/10.1145/777412.777438", "Full Abstract": "Resource allocation and admission control are critical tasks in a communication network, that often must be performed online. Algorithms for these types of problems have been considered both under benefit models (e.g., with a goal of approximately maximizing the number of calls accepted) and under cost models (e.g., with a goal of approximately minimizing the number of calls rejected). Unfortunately, algorithms designed for these two measures can often be quite different, even polar opposites (e.g., [1, 8]). In this work we consider the problem of combining algorithms designed for each of these objectives in a way that simultaneously is good under both measures. More formally, we are given an algorithm"},
{"Title": "On-Line Load Balancing of Temporary Tasks", "URL": "https://dl.acm.org/doi/10.1006/jagm.1995.0799", "Full Abstract": "This paper considers the nonpreemptive on-line load balancing problem where tasks havelimited durationin time. Upon arrival, each task has to be immediately assigned to one of the machines, increasing the load on this machine for the duration of the task by an amount that depends on both the machine and the task. The goal is to minimize the maximum load. Azar, Broder, and Karlin studied theunknown durationcase where the duration of a task is not known upon its arrival (On-line load balancingin“Proc. 33rd IEEE Annual Symposium on Foundations of Computer Science, 1992,” pp. 218 225). They focused on the special case in which for each task there is a subset of machines capable of executing it, and the increase in load due to assigning the task to one of these machines depends only on the task and not on the machine. For this case, they showed anO(n2/3)- competitive algorithm, and an (n)lower bound on the competitive ratio, wherenis the number of the machines. This paper closes the gap by giving anO(n)-competitive algorithm. In addition, trying to overcome the (n)lower bound for the case of unknown task duration, this paper initiates a study of the load balancing problem for tasks withknown duration(i.e., the duration of a task becomes known upon its arrival). For this case we show anO(lognT)-competitive algorithm, whereTis the ratio of the maximum possible duration of a task to the minimum possible duration of a task. The paper explores an alternative way to overcome the (n)bound; it considers therelated machinescase with unknown task duration. In the related machines case, a task can be executed by any machine and the increase in load depends on the speed of the machine and the weight of the task. For this case the paper gives a 20-competitive algorithm and shows a lower bound of 3 o(1) on the competitive ratio."},
{"Title": "Approximation schemes for scheduling", "URL": "https://dl.acm.org/doi/10.5555/314161.314371", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "On-line routing of virtual circuits with applications to load balancing and machine scheduling", "URL": "https://dl.acm.org/doi/10.1145/258128.258201", "Full Abstract": "In this paper we study the problem of on-line allocation of routes to virtual circuits (both"},
{"Title": "On-Line Load Balancing of Temporary Tasks on Identical Machines", "URL": "https://dl.acm.org/doi/10.5555/523986.857994", "Full Abstract": "We prove an exact lower bound of 2-1/m on the competitive ratio of any deterministic algorithm for load balancing of temporary tasks on m identical machines. We also show a lower bound of 2-1/m for randomized algorithms for small m and 2-2/(m+1) for general m. If in addition, we restrict the sequence to polynomial length, then the lower bound for randomized algorithms becomes 2-O((log log m)/(log m)) for general m."},
{"Title": "On-Line Machine Covering", "URL": "https://dl.acm.org/doi/10.5555/647907.739828", "Full Abstract": "No abstract available."},
{"Title": "On Two Dimensional Packing", "URL": "https://dl.acm.org/doi/10.1006/jagm.1997.0876", "Full Abstract": "The paper considerspacking of rectanglesinto an infinite bin. Similar to theTetris game, the rectangles arrive from the top and, once placed, cannot be moved again. The rectangles are moved inside the bin to reach their place. For the case in which rotations are allowed, we design an algorithm whose performance ratio is constant. In contrast, if rotations are not allowed, we show that no algorithm of constant ratio exists. For this case we design an algorithm with performance ratio ofO(log(1/ )), where is the minimum width of any rectangle. We also show that no algorithm can achieve a better ratio than (log(1/ ))for this case."},
{"Title": "Ancient and new algorithms for load balancing in the L", "URL": "https://dl.acm.org/doi/10.5555/314613.314774", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "On-line Load Balancing", "URL": "https://dl.acm.org/doi/10.5555/647371.723909", "Full Abstract": "No abstract available."},
{"Title": "Approximation Schemes for Covering and Scheduling on Related Machines", "URL": "https://dl.acm.org/doi/10.5555/646687.702950", "Full Abstract": "No abstract available."},
{"Title": "On-Line Bin-Stretching", "URL": "https://dl.acm.org/doi/10.5555/646975.711406", "Full Abstract": "We are given a sequence of items that can be packed into m unit size bins. In the classical bin packing problem we fix the size of the bins and try to pack the items in the minimum number of such bins. In contrast, in the bin-stretching problem we fix the number of bins and try to pack the items while stretching the size of the bins as least as possible. We present two on-line algorithms for the bin-stretching problem that guarantee a stretching factor of 5/3 for any number"},
{"Title": "Beating the logarithmic lower bound", "URL": "https://dl.acm.org/doi/10.5555/314500.314512", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "New Approximation Guarantees for Minimum-Weight  ", "URL": "https://dl.acm.org/doi/10.1137/S009753979528826X", "Full Abstract": "We consider a formalization of the following problem. A salesperson must sell some quota of brushes in order to win a trip to Hawaii. This salesperson has a map (a weighted graph) in which each city has an attached demand specifying the number of brushes that can be sold in that city. What is the best route to take to sell the quota while traveling the least distance possible? Notice that unlike the standard traveling salesman problem, not only do we need to figure out the order in which to visit the cities, but we must decide the more fundamental question: which cities do we want to visit?In this paper we give the first approximation algorithm having a polylogarithmic performance guarantee for this problem, as well as for the slightly more general \"prize-collecting traveling salesman problem\" (PCTSP) of Balas, and a variation we call the \"bank robber problem\" (also called the \"orienteering problem\" by Golden, Levi, and Vohra). We do this by providing an"},
{"Title": "Minimizing the flow time without migration", "URL": "https://dl.acm.org/doi/10.1145/301250.301304", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Off-Line Temporary Tasks Assignment", "URL": "https://dl.acm.org/doi/10.5555/647909.740156", "Full Abstract": "In this paper we consider the temporary tasks assignment problem. In this problem, there are"},
{"Title": "Independent Sets in Hypergraphs with Applications to Routing via Fixed Paths", "URL": "https://dl.acm.org/doi/10.5555/646976.711546", "Full Abstract": "No abstract available."},
{"Title": "Resource augmentation in load balancing", "URL": "https://dl.acm.org/doi/book/10.5555/869293", "Full Abstract": "We consider load-balancing in the following setting. The on-line algorithm is allowed to use $n$ machines, whereas the optimal off-line algorithm is limited to $m$ machines, for some fixed $m > n$. We show that while the greedy algorithm has a competitive ratio which decays linearly in the inverse of $n/m$, the best on-line algorithm has a ratio which decays exponentially in $n/m$. Specifically, we give an algorithm with competitive ratio of $1+2^{- \\frac{n{m (1- o (1))$, and a lower bound of $1+ e^{ - \\frac{n{m (1+ o(1))$ on the competitive ratio of any randomized algorithm. We also consider the preemptive case. We show an on-line algorithm with a competitive ratio of $1+ e^{ - \\frac{n{m (1+ o(1))$. We show that the algorithm is optimal by proving a matching lower bound. We also consider the non-preemptive model with temporary tasks. We prove that for $n=m+1$, the greedy algorithm is optimal. (It is not optimal for permanent tasks.)"},
{"Title": "On-line load balancing with applications to machine scheduling and virtual circuit routing", "URL": "https://dl.acm.org/doi/10.1145/167088.167248", "Full Abstract": "Copyright © 1993 ACM."},
{"Title": "Online Load Balancing of Temporary Tasks", "URL": "https://dl.acm.org/doi/10.5555/645929.672712", "Full Abstract": "No abstract available."},
{"Title": "Throughput-competitive on-line routing", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1993.366884", "Full Abstract": "We develop a framework that allows us to address the issues of admission control and routing in high-speed networks under the restriction that once a call is admitted and routed, it has to proceed to completion and no reroutings are allowed. The \"no rerouting\" restriction appears in all the proposals for future high-speed networks and stems from current hardware limitations, in particular the fact that the bandwidth-delay product of the newly developed optical communication links far exceeds the buffer capacity of the network. In case the goal is to maximize the throughput, our framework yields an on-line O(log nT)-competitive strategy, where n is the number of nodes in the network and T is the maximum call duration. In other words, our strategy results in throughput that is within O(log nT) factor of the highest possible throughput achievable by an omniscient algorithm that knows all of the requests in advance. Moreover, we show that no on-line strategy can achieve a better competitive ratio. Our framework leads to competitive strategies applicable in several more general settings. Extensions include assigning each connection an associated \"profit\" that represents the importance of this connection, and addressing the issue of call-establishment costs."},
{"Title": "On-line steiner trees in the Euclidean plane", "URL": "https://dl.acm.org/doi/10.1007/BF02573969", "Full Abstract": "Suppose we are given a sequence ofn points in the Euclidean plane, and our objective is to construct, on-line, a connected graph that connects all of them, trying to minimize the total sum of lengths of its edges. The points appear one at a time, and at each step the on-line algorithm must construct a connected graph that contains all current points by connecting the new point to the previously constructed graph. This can be done by joining the new point (not necessarily by a straight line) to any point of the previous graph (not necessarily one of the given points). The performance of our algorithm is measured by its competitive ratio: the supremum, over all sequences of points, of the ratio between the total length of the graph constructed by our algorithm and the total length of the best Steiner tree that connects all the points. There are known on-line algorithms whose competitive ratio isO(logn) even for all metric spaces, but the only lower bound known is of [IW] for some contrived discrete metric space. Moreover, for the plane, on-line algorithms could have been more powerful and achieve a better competitive ratio, and no nontrivial lower bounds for the best possible competitive ratio were known. Here we prove an almost tight lower bound of Ω(logn/log logn) for the competitive ratio of any on-line algorithm. The lower bound holds for deterministic algorithms as well as for randomized ones, and obviously holds in any Euclidean space of dimension greater than 2 as well."},
{"Title": "Competitive routing of virtual circuits with unknown duration", "URL": "https://dl.acm.org/doi/10.5555/314464.314508", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Improved Approximation Guarantees for Minimum-Weight k-trees and Prize-collecting Salesmen", "URL": "https://dl.acm.org/doi/book/10.5555/865092", "Full Abstract": "Consider a salesperson that must sell some quota of brushes in order to win a trip to Hawaii. This salesperson has a map (a weighted graph) in which each city has an attached demand specifying the number of brushes that can be sold in that city. What is the best route to take to sell the quota while traveling the least distance possible? Notice that unlike the standard traveling salesman problem, not only do we need to figure out the order in which to visit the cities, but we must decide the more fundamental question: which cities do we want to visit? In this paper we give the first approximation algorithms with poly-logarithmic performance guarantees for this problem, as well as for the slightly more general PCTSP problem of Balas, and a variation we call the \"bank-robber problem\" (also called the \"orienteering problem\" by Golden, Levi, and Vohra). We do this by providing an O(log^2 k) approximation to the k-MST problem which is defined as follows. Given an undirected graph on n nodes with non-negative edge weights and an integer k > n, find the tree of least weight that spans k vertices. (If desired, one may specify in the problem a \"root vertex\" that must be in the tree as well.) Our result improves on the previous best bound of O(k^0.5) of Ravi et al. and comes quite close to the bound of O(log k) of Garg and Hochbaum for the special case of points in 2-dimensional Euclidean space."},
{"Title": "Competitive multicast routing", "URL": "https://dl.acm.org/doi/10.1007/BF01196262", "Full Abstract": "In this paper, we introduce and solve the multicast routing problem for virtual circuit environment without making any assumptions about the communication patterns, or about the network topology. By multicast we refer to the case were one source transmits to several destination the same information. Also, we allow arbitrary interleaving of subscription patterns for different multicast groups, i.e. the destinations for each group arrive at an arbitrary order and may be interleaved with destinations of other groups. Our goal is to make route selection so as to minimize congestion of the bottleneck link. This is the first analytical treatment for this problem in its full generality. The main contribution of this paper is an online competitive routing strategy that has an"},
{"Title": "The competitiveness of on-line assignments", "URL": "https://dl.acm.org/doi/10.1006/jagm.1995.1008", "Full Abstract": "No abstract available."},
{"Title": "Improved approximation guarantees for minimum-weight ", "URL": "https://dl.acm.org/doi/10.1145/225058.225139", "Full Abstract": "Copyright © 1995 ACM."},
{"Title": "Load balancing in the L/sub p/ norm", "URL": "https://dl.acm.org/doi/10.5555/795662.796265", "Full Abstract": "In the load balancing problem, there is a set of servers, and jobs arrive sequentially. Each job can be run on some subset of the servers, and must be assigned to one of them in an online fashion. Traditionally, the assignment of jobs to servers is measured by the L/sub /spl infin// norm; in other words, an assignment of jobs to servers is quantified by the maximum load assigned to any server. In this measure the performance of the greedy load balancing algorithm may be a logarithmic factor higher than the offline optimal. In many applications, the L/sub /spl infin// norm is not a suitable way to measure how well the jobs are balanced, If each job sees a delay that is proportional to the number of jobs on its server, then the average delay among all jobs is proportional to the sum of the squares of the numbers of jobs assigned to the servers. Minimizing the average delay is equivalent to minimizing the Euclidean (or L/sub 2/) norm. For any fixed p, 1/spl les/p>/spl infin/, we show that the greedy algorithm performs within a constant factor of the offline optimal with respect to the L/sub p/ norm. The constant grows linearly with p, which is best possible, but does not depend on the number of servers and jobs."},
{"Title": "On-line generalized Steiner problem", "URL": "https://dl.acm.org/doi/10.5555/313852.313888", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Making commitments in the face of uncertainty", "URL": "https://dl.acm.org/doi/10.1145/237814.238000", "Full Abstract": "Copyright © 1996 ACM."},
{"Title": "On Two Dimensional Packing", "URL": "https://dl.acm.org/doi/10.5555/645898.672268", "Full Abstract": "No abstract available."},
{"Title": "Routing Strategies for Fast Networks", "URL": "https://dl.acm.org/doi/10.1109/12.485380", "Full Abstract": "Modern fast packet switching networks are being forced to rethink the routing schemes that are used in more traditional networks. The reexamination is necessitated because in these fast networks switches on the message's route can afford to make only minimal and simple operations. For example, examining a table of a size proportional to the network size is out of the question. In this paper we examine routing strategies for such networks based on flooding and predefined routes. Our concern is to get both efficient routing and an even (balanced) use of network resources. We present efficient algorithms for assigning weights to edges in a controlled flooding scheme but show that the flooding scheme is not likely to yield a balanced use of the resources. We then present efficient algorithms for choosing routes along: 1) bfs trees and 2) shortest paths. We show that in both cases a balanced use of network resources can be guaranteed."},
{"Title": "On Capital Investment", "URL": "https://dl.acm.org/doi/10.5555/646250.685654", "Full Abstract": "No abstract available."},
{"Title": "On-line Competive Algorithms for Call Admission in Optical Networks", "URL": "https://dl.acm.org/doi/10.5555/647906.739793", "Full Abstract": "No abstract available."},
{"Title": "Tight complexity bounds for parallel comparison sorting", "URL": "https://dl.acm.org/doi/10.1109/SFCS.1986.57", "Full Abstract": "The time complexity of sorting n elements using p ≥ n processors on Valiant's parallel comparison tree model is considered. The following results are obtained. 1. We show that this time complexity is Θ(logn/log(1+p/n)). This complements the AKS sorting network in settling the wider problem of comparison sort of n elements by p processors, where the problem for p ≤ n was resolved. To prove the lower bound, we show that to achieve time k ≤ logn, we need Ω(kn1+1/k) comparisons. Häggkvist and Hell proved a similar result only for fixed k. 2. For every fixed time k, we show that: (a) Ω(n1+1/k lognl/k) comparisons are required, (O(n1+1/k logn) are known to be sufficient in this case), and (b) there exists a randomized algorithm for comparison sort in time k with an expected number of O(n1+1/k) comparisons. This implies that for every fixed k, any deterministic comparison sort algorithm must be asymptotically worse than this randomized algorithm. The lower bound improves on Häggkvist-Hell's lower bound. 3. We show that \"approximate sorting\" in time 1 requires asymptotically more than nlogn processors. This settles a problem raised by M. Rabin."},
{"Title": "Tight comparison bounds on the complexity of parallel sorting", "URL": "https://dl.acm.org/doi/10.1137/0216032", "Full Abstract": "No abstract available."},
{"Title": "Sorting, approximate sorting, and searching in rounds", "URL": "https://dl.acm.org/doi/10.1137/0401028", "Full Abstract": "No abstract available."},
{"Title": "Finding an approximate maximum", "URL": "https://dl.acm.org/doi/10.1137/0218017", "Full Abstract": "No abstract available."},
{"Title": "Parallel selection", "URL": "https://dl.acm.org/doi/10.1016/0166-218X%2890%2990128-Y", "Full Abstract": "No abstract available."},
{"Title": "Routing strategies for fast networks", "URL": "https://dl.acm.org/doi/10.5555/131408.131436", "Full Abstract": "No abstract available."},
{"Title": "On-line Steiner trees in the Euclidean plane", "URL": "https://dl.acm.org/doi/10.1145/142675.142744", "Full Abstract": "Suppose we are given a sequence of"},
{"Title": "The competitiveness of on-line assignments", "URL": "https://dl.acm.org/doi/10.5555/139404.139450", "Full Abstract": "Consider the on-line problem where a number of servers are ready to provide service to a set of customers. Each customer's job can be handled by any of a"},
{"Title": "Comparison-sorting and selecting in totally monotone matrices", "URL": "https://dl.acm.org/doi/10.5555/139404.139484", "Full Abstract": "An"},
{"Title": "Fair versus Unrestricted Bin Packing", "URL": "https://dl.acm.org/doi/book/10.5555/870707", "Full Abstract": "We consider the Unrestricted Bin Packing problem where we have bins of equal size and a sequence of items. The goal is to maximize the number of items that are packed in the bins by an on-line algorithm. We investigate the power of performing admission control on the items, i.e., rejecting items while there is enough space to pack them, versus behaving fairly, i.e., rejecting an item only when there is not enough space to pack it. We show that by performing admission control on the items, we get better performance for various measures compared with the performance achieved on the fair version of the problem. Our main result shows that we can pack $2/3$ of the items for sequences in which the optimal can pack all the items."},
{"Title": "On-Line Scheduling with Precedence Constraints", "URL": "https://dl.acm.org/doi/10.5555/645900.672457", "Full Abstract": "We consider the on-line problem of scheduling jobs with precedence constraints on"},
{"Title": "Resource Augmentation in Load Balancing", "URL": "https://dl.acm.org/doi/10.5555/645900.672469", "Full Abstract": "We consider load balancing in the following setting. The online algorithm is allowed to use"},
{"Title": "Management of multi-queue switches in QoS networks", "URL": "https://dl.acm.org/doi/10.1145/780542.780556", "Full Abstract": "The concept of Quality of Service (QoS) networks has gained growing attention recently, as the traffic volume in the Internet constantly increases, and QoS guarantees are essential to ensure proper operation of most communication based applications. A QoS switch serves"},
{"Title": "The online set cover problem", "URL": "https://dl.acm.org/doi/10.1145/780542.780558", "Full Abstract": "Let"},
{"Title": "Packet routing and information gathering in lines, rings and trees", "URL": "https://dl.acm.org/doi/10.1007/11561071_44", "Full Abstract": "We study the problem of online packet routing and information gathering in lines, rings and trees. A network consist of"},
{"Title": "The hardness of network design for unsplittable flow with selfish users", "URL": "https://dl.acm.org/doi/10.1007/11671411_4", "Full Abstract": "In this paper we consider the network design for selfish users problem, where we assume the more realistic unsplittable model in which the users can have general demands and each user must choose a single path between its source and its destination. This model is also called atomic (weighted) network congestion game. The problem can be presented as follows : given a network, which edges should be removed to minimize the cost of the worst Nash equilibrium?"},
{"Title": "Combinatorial Algorithms for the Unsplittable Flow\nProblem", "URL": "https://dl.acm.org/doi/10.1007/s00453-005-1172-z", "Full Abstract": "We provide combinatorial algorithms for the unsplittable flow problem (UFP) that either match or improve the previously best results. In the UFP we are given a (possibly directed) capacitated graph with n vertices and m edges, and a set of terminal pairs each with its own demand and profit. The objective is to connect a subset of the terminal pairs each by a single flow path subject to the capacity constraints such that the total profit of the connected pairs is maximized.We consider three variants of the problem. First is the classical UFP in which the maximum demand is at most the minimum edge capacity. It was previously known to have an O(√m) approximation algorithm; the algorithm is based on the randomized rounding technique and its analysis makes use of the Chernoff bound and the FKG inequality.We provide a combinatorial algorithm that achieves the same approximation ratio and whose analysis is considerably simpler. Second is the extended UFP in which some demands might be higher than edge capacities. Our algorithm for this case improves the best known approximation ratio. We also give a lower bound that shows that the extended UFP is provably harder than the classical UFP. Finally, we consider the bounded UFP in which the maximum demand is at most 1/K times the minimum edge capacity for some K > 1. Here we provide combinatorial algorithms that match the currently best known algorithms. All of our algorithms are strongly polynomial and some can even be used in the online setting."},
{"Title": "Optimal node routing", "URL": "https://dl.acm.org/doi/10.1007/11672142_49", "Full Abstract": "We study route selection for packet switching in the competitive throughput model. In contrast to previous papers which considered competitive algorithms for packet scheduling, we consider the packet routing problem (output port selection in a node). We model the node routing problem as follows: a node has an arbitrary number of input ports and an arbitrary number of output queues. At each time unit, an arbitrary number of new packets may arrive, each packet is associated with a subset of the output ports (which correspond to the next edges on the allowed paths for the packet). Each output queue transmits packets in some arbitrary manner. Arrival and transmission are arbitrary and controlled by an adversary. The node routing algorithm has to route each packet to one of the allowed output ports, without exceeding the size of the queues. The goal is to maximize the number of the transmitted packets. In this paper, we show that all non-refusal algorithms are 2-competitive. Our main result is an almost optimal $\\frac{e{e-1 \\approx 1.58$-competitive algorithm, for a large enough queue size. For packets with arbitrary values (allowing preemption) we present a 2-competitive algorithm for any queue size."},
{"Title": "Foreword", "URL": "https://dl.acm.org/doi/10.5555/3118767.3119114", "Full Abstract": "No abstract available."},
{"Title": "Multiple intents re-ranking", "URL": "https://dl.acm.org/doi/10.1145/1536414.1536505", "Full Abstract": "One of the most fundamental problems in web search is how to re-rank result web pages based on user logs. Most traditional models for re-ranking assume each query has a single intent. That is, they assume all users formulating the same query have similar preferences over the result web pages. It is clear that this is not true for a large portion of queries as different users may have different preferences over the result web pages. Accordingly, a more accurate model should assume that queries have multiple intents. In this paper, we introduce the"},
{"Title": "Efficient Low-Contention Parallel Algorithms", "URL": "https://dl.acm.org/doi/book/10.5555/899290", "Full Abstract": "The queue-read, queue-write (QRQW) parallel random access machine (PRAM) model permits concurrent reading and writing to shared memory locations, but at a cost proportional to the number of readers/writers to any one memory location in a given step. The QRQW PRAM model reflects the contention properties of most commercially available parallel machines more accurately than either the well-studied CRCW PRAM or EREW PRAM models, and can be efficiently emulated with only logarithmic slowdown on hypercubetype non-combining networks. This paper describes fast, low-contention, work-optimal, randomized QRQW PRAM algorithms for the fundamental problems of load balancing, multiple compaction, generating a random permutation, parallel hashing, and distributive sorting. These logarithmic or sublogarithmic time algorithms considerably improve upon the best known EREW PRAM algorithms for these problems, while avoiding the high-contention steps typical of CRCW PRAM algorithms. An illustrative experiment demonstrates the performance advantage of a new QRQW random permutation algorithm when compared with the popular EREW algorithm. Finally, this paper presents new randomized algorithms for integer sorting and general sorting."},
{"Title": "The Queue-Read Queue-Write PRAM Model: Accounting for Contention in ParallelAlgorithms", "URL": "https://dl.acm.org/doi/book/10.5555/899291", "Full Abstract": "This paper introduces the queue-read, queue-write (QRQW) parallel random access machine (PRAM) model, which permits concurrent reading and writing to shared memory locations, but at a cost proportional to the number of readers/writers to any one memory location in a given step. Prior to this work there were no formal complexity models that accounted for the contention to memory locations, despite its large impact on the performance of parallel programs. The QRQW PRAM model reflects the contention properties of most commercially available parallel machines more accurately than either the well-studied CRCW PRAM or EREW PRAM models: the CRCW model does not adequately penalize algorithms with high contention to shared memory locations, while the EREW model is too strict in its insistence on zero contention at each step. The QRQW PRAM is strictly more powerful than the EREW PRAM. This paper shows a separation of the square root of lg n between the two models, and presents faster and more efficient QRQW algorithms for several basic problems, such as linear compaction, leader election, and processor allocation. Furthermore, we present a work-preserving emulation of the QRQW PRAM with only logarithmic slowdown on Valiant's BSP model, and hence on hypercube-type non-combining networks, even when latency, synchronization, and memory granularity overheads are taken into account. This matches the best known emulation result for the EREW PRAM, and considerably improves upon the best known efficient emulation for the CRCW PRAM on such networks. Finally, the paper presents several lower bound results for this model, including lower bounds on the time required for broadcasting and for leader election."},
{"Title": "Bifocal sampling for skew-resistant join size estimation", "URL": "https://dl.acm.org/doi/10.1145/233269.233340", "Full Abstract": "This paper introduces"},
{"Title": "The Queue-Read Queue-Write Asynchronous PRAM Model", "URL": "https://dl.acm.org/doi/book/10.5555/899296", "Full Abstract": "This paper presents results for the queue-read, queue-write asynchronous parallel random access machine (QRQW ASYNCHRONOUS PRAM) model, which is the asynchronous variant of the QRQW PRAM model. The QRQW PRAM family of models, which was introduced earlier by the authors, permit concurrent reading and writing to shared memory locations, but each memory location is viewed as having a queue which can service at most one request at a time. In the basic QRQW PRAM model each processor executes a series of reads to shared memory locations, a series of local computation steps, and a series of writes to shared memory locations, and then synchronizes with all other processors; thus this can be viewed as a bulk-synchronous model. In contrast, in the QRQW ASYNCHRONOUS PRAM model discussed in this paper, there is no synchronization between processors, and each processor proceeds at its own pace. Thus, the QRQW ASYNCHRONOUS PRAM serves as a better model for truly asynchronous parallel machines than the original QRQW PRAM. In this paper we elaborate on the QRQW ASYNCHRONOUS PRAM model, and we demonstrate the power of full asynchrony over bulk-asynchrony by presenting a work and time optimal deterministic algorithm on the QRQW ASYNCHRONOUS PRAM for the leader election problem and a simple randomized work and time optimal algorithm on the QRQW ASYNCHRONOUS PRAM for sorting. In contrast, no tight bounds are known on the QRQW PRAM for either deterministic or randomized parallel algorithms for leader election and the only work and time optimal algorithms for sorting known on the QRQW PRAM are those inherited from the EREW PRAM, which are considerably more complicated. Our sorting algorithm is an asynchronous version of an earlier sorting algorithm we developed for the QRQW PRAM, for which we use an interesting analysis to bound the running time to be O(lg n). We also present a randomized algorithm to simulate one step of a CRCW PRAM on a QRQW ASYNCHRONOUS PRAM in sublogarithmic time if the maximum concurrency in the step is relatively small."},
{"Title": "The space complexity of approximating the frequency moments", "URL": "https://dl.acm.org/doi/10.1145/237814.237823", "Full Abstract": "Copyright © 1996 ACM."},
{"Title": "An Effective Load Balancing Policy for Geometric-Decaying Algorithms", "URL": "https://dl.acm.org/doi/10.1006/jpdc.1996.0098", "Full Abstract": "Parallel algorithms are often first designed as a sequence of rounds, where each round includes any number of independent constant time operations. This so-called work time presentation is then followed by a processor scheduling implementation on a more concrete computational model. Many parallel algorithms are geometric-decaying in the sense that the sequence of work loads is upper bounded by a decreasing geometric series. A standard scheduling implementation of such algorithms consists of a repeated application of load balancing. We present a more effective, yet as simple, policy for the utilization of load balancing in geometric-decaying algorithms. By making a more careful choice of when and how often load balancing should be employed, and by using a simple amortization argument, we show that the number of required applications of load balancing should be nearly constant. The policy is not restricted to any particular model of parallel computation, and, up to a constant factor, it is the best possible."},
{"Title": "The Queue-Read Queue-Write Asynchronous PRAM Model", "URL": "https://dl.acm.org/doi/10.5555/646669.701092", "Full Abstract": "No abstract available."},
{"Title": "Modeling Skewed Distribution Using Multifractals and the `80-20' Law", "URL": "https://dl.acm.org/doi/10.5555/645922.673328", "Full Abstract": "No abstract available."},
{"Title": "Efficient Low-Contention Parallel Algorithms", "URL": "https://dl.acm.org/doi/10.1006/jcss.1996.0079", "Full Abstract": "The queue-read, queue-write (qrqw) parallel random access machine (pram) model permits concurrent reading and writing to shared memory locations, but at a cost proportional to the number of readers/writers to any one memory location in a given step. Theqrqw prammodel reflects the contention properties of most commercially available parallel machines more accurately than either the well-studiedcrcw pramorerew prammodels, and can be efficiently emulated with only logarithmic slowdown on hypercube-type noncombining networks. This paper describes fast, low-contention, work-optimal, randomizedqrqw pramalgorithms for the fundamental problems of load balancing, multiple compaction, generating a random permutation, parallel hashing, and distributive sorting. These logarithmic or sublogarithmic time algorithms considerably improve upon the best knownerew pramalgorithms for these problems, while avoiding the high-contention steps typical ofcrcw pramalgorithms. An illustrative experiment demonstrates the performance advantage of a newqrqwrandom permutation algorithm when compared with the popularerewalgorithm. Finally, this paper presents new randomized algorithms for integer sorting and general sorting."},
{"Title": "How to Make Personalized Web Browising Simple, Secure, and Anonymous", "URL": "https://dl.acm.org/doi/10.5555/647501.728165", "Full Abstract": "No abstract available."},
{"Title": "Space-efficient scheduling of parallelism with synchronization variables", "URL": "https://dl.acm.org/doi/10.1145/258492.258494", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "Can shared-memory model serve as a bridging model for parallel computation?", "URL": "https://dl.acm.org/doi/10.1145/258492.258500", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "Modeling parallel bandwidth", "URL": "https://dl.acm.org/doi/10.1145/258492.258502", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "Fast Incremental Maintenance of Approximate Histograms", "URL": "https://dl.acm.org/doi/10.5555/645923.673669", "Full Abstract": "No abstract available."},
{"Title": "Accounting for Memory Bank Contention and Delay in High-Bandwidth Multiprocessors", "URL": "https://dl.acm.org/doi/10.1109/71.615440", "Full Abstract": "For years, the computation rate of processors has been much faster than the access rate of memory banks, and this divergence in speeds has been constantly increasing in recent years. As a result, several shared-memory multiprocessors consist of more memory banks than processors. The object of this paper is to provide a simple model (with only a few parameters) for the design and analysis of irregular parallel algorithms that will give a reasonable characterization of performance on such machines. For this purpose, we extend Valiant's bulk-synchronous parallel (BSP) model with two parameters: a parameter for memory bank delay, the minimum time for servicing requests at a bank, and a parameter for memory bank expansion, the ratio of the number of banks to the number of processors. We call this model the (d, x)-BSP. We show experimentally that the (d, x)-BSP captures the impact of bank contention and delay on the CRAY C90 and J90 for irregular access patterns, without modeling machine-specific details of these machines. The model has clarified the performance characteristics of several unstructured algorithms on the CRAY C90 and J90, and allowed us to explore tradeoffs and optimizations for these algorithms. In addition to modeling individual algorithms directly, we also consider the use of the (d, x)-BSP as a bridging model for emulating a very high-level abstract model, the Parallel Random Access Machine (PRAM). We provide matching upper and lower bounds for emulating the EREW and QRQW PRAMs on the (d, x)-BSP."},
{"Title": "Parallel algorithms column", "URL": "https://dl.acm.org/doi/10.1145/262301.262305", "Full Abstract": "Copyright © 1997 Author."},
{"Title": "Can a Shared-Memory Model Serve as a Bridging Model for Parallel Computation", "URL": "https://dl.acm.org/doi/book/10.5555/899329", "Full Abstract": "There has been a great deal of interest recently in the development of general-purpose bridging models for parallel computation. Models such as the BSP and LogP have been proposed as more realistic alternatives to the widely-used PRAM model. The BSP and LogP models imply a rather different style for designing algorithms when compared to the PRAM model. Indeed, while many consider data parallelism as a convenient style, and the shared-memory abstraction as an easy-to-use platform, the bandwidth limitations of current machines have diverted much attention to message-passing and distributed-memory models (such as the BSP and LogP) that account more properly for these limitations. In this paper we consider the question of whether a shared-memory model can serve as an effective bridging model for parallel computation. In particular, can a shared-memory model be as effective as, say, the BSP? As a candidate for a bridging model, we introduce the Queuing Shared Memory (QSM) model, which accounts for limited communication bandwidth while still providing a simple shared-memory abstraction. We substantiate the ability of the QSM to serve as a bridging model by providing a simple work-preserving emulation of the QSM on both the BSP, and on a related model, the (d,x)-BSP. We present evidence that the features of the QSM are essential to its effectiveness as a bridging model. In addition, we describe scenarios in which the high-level QSM more accurately models certain machines than the more detailed BSP and LogP models. Finally, we present algorithmic results for the QSM, as well as general strategies for mapping algorithms designed for the BSP or PRAM models onto the QSM model. Our main conclusion is that shared-memory models can potentially serve as viable alternatives to existing message-passing, distributed-memory bridging models."},
{"Title": "Lightweight security primitives for E-Commerce", "URL": "https://dl.acm.org/doi/10.5555/1267279.1267288", "Full Abstract": "Emerging applications in electronic commerce often involve very low-cost transactions, which execute in the context of ongoing, extended client-server relationships. For example, consider a website (server) which offers repeated authenticated personalized stock quotes to each of its subscribers (clients). The value of a single transaction (e.g., delivery of a web-page with a customized set of quotes) does not warrant the cost of executing a handshake and key distribution protocol. Also, a client might not always use the same machine during such an extended relationship (e.g., a PC at home, a laptop on a trip). Typical transport/session-layer security mechanisms such as SSL and S-HTTP either require handshake/key distribution for each transaction or do not support client mobility."},
{"Title": "FBSleuth", "URL": "https://dl.acm.org/doi/10.1145/3196494.3196521", "Full Abstract": "Fake base station (FBS) crime is a type of wireless communication crime that has appeared recently. The key to enforcing the laws on regulating FBS based crime is not only to arrest but also to convict criminals effectively. Much work on FBS discovering, localization, and tracking can assist the arresting, but the problem of collecting evidence accurately to support a proper conviction has not been addressed yet."},
{"Title": "Content Distribution for Mobile Internet", "URL": "https://dl.acm.org/doi/book/10.5555/3279194", "Full Abstract": "This book investigates the cloud-based techniques of content distribution mainly for mobile Internet. It starts with hot topics such as cellular traffic optimization and video content delivery. By integrating the cloud scheme, it further tackles issues of traffic-saving, energy-efficient, high-speed, and delay-tolerant content delivery with regard to mobile Internet. It covers both theoretical algorithms and their real-world system implementations. In particular, various well-known cloud platforms such as Baidu Traffic Guard, Tencent QQXuanfeng, Google Drive, Microsoft One Drive, and Dropbox are elaborated respectively in the book. Lastly, it includes an educational and experimental cloud computing platform allowing public access, which benefits researchers, practitioners, and developers in the field of cloud computing/storage and mobile Internet. Throughout the book there are helpful and practical tips on setting up cloud systems that readers can easily follow."},
{"Title": "Widar2.0", "URL": "https://dl.acm.org/doi/10.1145/3210240.3210314", "Full Abstract": "This paper presents Widar2.0, the first WiFi-based system that enables passive human localization and tracking using a single link on commodity off-the-shelf devices. Previous works based on either specialized or commercial hardware all require multiple links, preventing their wide adoption in scenarios like homes where typically only one single AP is installed. The key insight underlying Widar2.0 to circumvent the use of multiple links is to leverage multi-dimensional signal parameters from one single link. To this end, we build a unified model accounting for Angle-of-Arrival, Time-of-Flight, and Doppler shifts together and devise an efficient algorithm for their joint estimation. We then design a pipeline to translate the erroneous raw parameters into precise locations, which first finds parameters corresponding to the reflections of interests, then refines range estimates, and ultimately outputs target locations. Our implementation and evaluation on commodity WiFi devices demonstrate that Widar2.0 achieves better or comparable performance to state-of-the-art localization systems, which either use specialized hardwares or require 2 to 40 Wi-Fi links."},
{"Title": "Wireless Indoor Localization", "URL": "https://dl.acm.org/doi/book/10.5555/3294754", "Full Abstract": "This book provides a comprehensive and in-depth understanding of wireless indoor localization for ubiquitous applications. The past decade has witnessed a flourishing of WiFi-based indoor localization, which has become one of the most popular localization solutions and has attracted considerable attention from both the academic and industrial communities. Specifically focusing on WiFi fingerprint based localization via crowdsourcing, the book follows a top-down approach and explores the three most important aspects of wireless indoor localization: deployment, maintenance, and service accuracy. After extensively reviewing the state-of-the-art literature, it highlights the latest advances in crowdsourcing-enabled WiFi localization. It elaborated the ideas, methods and systems for implementing the crowdsourcing approach for fingerprint-based localization. By tackling the problems such as: deployment costs of fingerprint database construction, maintenance overhead of fingerprint database updating, floor plan generation, and location errors, the book offers a valuable reference guide for technicians and practitioners in the field of location-based services. As the first of its kind, introducing readers to WiFi-based localization from a crowdsourcing perspective, it will greatly benefit and appeal to scientists and researchers in mobile and ubiquitous computing and related areas."},
{"Title": "MindID", "URL": "https://dl.acm.org/doi/10.1145/3264959", "Full Abstract": "Person identification technology recognizes individuals by exploiting their unique, measurable physiological and behavioral characteristics. However, the state-of-the-art person identification systems have been shown to be vulnerable, e.g., anti-surveillance prosthetic masks can thwart face recognition, contact lenses can trick iris recognition, vocoder can compromise voice identification and fingerprint films can deceive fingerprint sensors. EEG (Electroencephalography)-based identification, which utilizes the user's brainwave signals for identification and offers a more resilient solution, has recently drawn a lot of attention. However, the state-of-the-art systems cannot achieve similar accuracy as the aforementioned methods. We propose MindID, an EEG-based biometric identification approach, with the aim of achieving high accuracy and robust performance. At first, the EEG data patterns are analyzed and the results show that the Delta pattern contains the most distinctive information for user identification. Next, the decomposed Delta signals are fed into an attention-based Encoder-Decoder RNNs (Recurrent Neural Networks) structure which assigns varying attention weights to different EEG channels based on their importance. The discriminative representations learned from the attention-based RNN are used to identify the user through a boosting classifier. The proposed approach is evaluated over 3 datasets (two local and one public). One local dataset (EID-M) is used for performance assessment and the results illustrate that our model achieves an accuracy of 0.982 and significantly outperforms the state-of-the-art and relevant baselines. The second local dataset (EID-S) and a public dataset (EEG-S) are utilized to demonstrate the robustness and adaptability, respectively. The results indicate that the proposed approach has the potential to be widely deployed in practical settings."},
{"Title": "Vehicle-Based Bi-Objective Crowdsourcing", "URL": "https://dl.acm.org/doi/10.1109/TITS.2017.2766769", "Full Abstract": "Mobile crowdsourcing is an emerging complex problem solving paradigm that makes use of pervasive mobile devices equipped with multi-functional sensors. Recently, vehicles have also been increasingly adopted for mobile crowdsourcing, as the vehicles, as well as drivers, can provide diverse sensing capability and predictable mobility. Existing mobile crowdsourcing algorithms mostly recruit workers to complete one kind of sensing tasks, i.e., location-based query tasks or automatic sensing tasks. In this paper, we investigate the possibility of recruiting a set of vehicles to simultaneously complete these two categories of tasks, so as to maximize the sensing utility of each participant. We first model the worker recruitment for vehicle-based crowdsourcing as a bi-objective optimization problem with respect to the sensing capability and predictable mobility of vehicles. The recruitment problem is proven to be NP-hard, and we design two heuristic algorithms based on the bi-objective greedy strategy and the multi-objective genetic algorithm to find the solutions. The experimental results with a real-world traffic trace data set show that the proposed algorithms outperform some existing algorithms in finding solutions that maximize both objectives."},
{"Title": "ChromaCode", "URL": "https://dl.acm.org/doi/10.1145/3241539.3241543", "Full Abstract": "Hidden screen-camera communication techniques emerge as a new paradigm that embeds data imperceptibly into regular videos while remaining unobtrusive to human viewers. Three key goals on imperceptible, high rate, and reliable communication are desirable but conflicting, and existing solutions usually made a trade-off among them. In this paper, we present the design and implementation of ChromaCode, a screen-camera communication system that achieves all three goals simultaneously. In our design, we consider for the first time color space for perceptually uniform lightness modifications. On this basis, we design an outcome-based adaptive embedding scheme, which adapts to both pixel lightness and regional texture. Last, we propose a concatenated code scheme for robust coding and devise multiple techniques to overcome various screen-camera channel errors. Our prototype and experiments demonstrate that ChromaCode achieves remarkable raw throughputs of >700 kbps, data goodputs of 120 kbps with BER of 0.05, and with fully imperceptible flicker for viewing proved by user study, which significantly outperforms previous works."},
{"Title": "A Quantitative and Comparative Study of Network-Level Efficiency for Cloud Storage Services", "URL": "https://dl.acm.org/doi/10.1145/3274526", "Full Abstract": "Cloud storage services such as Dropbox and OneDrive provide users with a convenient and reliable way to store and share data from anywhere, on any device, and at any time. Their cornerstone is the"},
{"Title": "3D-OmniTrack", "URL": "https://dl.acm.org/doi/10.1145/3302506.3310386", "Full Abstract": "RFID tracking has attracted significant interest from both academia and industry due to its low cost and ease of deployment. Previous works focus more on tracking in 2D space or separately consider tracking of the location and the orientation. They especially struggle in 3D situations due to the increase in the degree of freedom and the limited information conveyed by the RFID tags. In this paper, we propose 3D-OmniTrack, an approach that can accurately track the 3D location and orientation of an object. We introduce a polarization-sensitive phase model in an RFID system, which takes into consideration both the distance and the 3D posture of an object. Based on this model, we design an algorithm to accurately track the object in 3D space. We conduct real-world experiments and present results that show 3D-OmniTrack can achieve centimeter-level location accuracy with the average orientation error of 5°. 3D-OmniTrack has significant advantages in both the accuracy and the efficiency, compared with state-of-the-art approaches."},
{"Title": "CellTradeMap: Delineating Trade Areas for Urban Commercial Districts with Cellular Networks", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2019.8737564", "Full Abstract": "Understanding customer mobility patterns to commercial districts is crucial for urban planning, facility management, and business strategies. Trade areas are a widely applied measure to quantity where the visitors are from. Traditional trade area analysis is limited to small-scale or store-level studies because information such as visits to competitor commercial entities and place of residence is collected by labour-intensive questionnaires or heavily biased location-based social media data. In this paper, we propose CellTradeMap, a novel district-level trade area analysis framework using mobile flow records (MFRs), a type of fine-grained cellular network data. CellTradeMap extracts robust location information from the irregularly sampled, noisy MFRs, adapts the generic trade area analysis framework to incorporate cellular data, and enhances the original trade area model with cellular-based features. We evaluate CellTradeMap on a large-scale cellular network dataset covering 3.5 million mobile phone users in a metropolis in China. Experimental results show that the trade areas extracted by CellTradeMap are aligned with domain knowledge and CellTradeMap can model trade areas with a high predictive accuracy."},
{"Title": "TwinLeak: RFID-based Liquid Leakage Detection in Industrial Environments", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2019.8737621", "Full Abstract": "Liquid leakage detection is a crucial issue in modern industry, which concerns industrial safety. Traditional solutions, which generally rely on specialized sensors, suffer from intrusive deployment, high cost, and high power consumption. Such problems prohibit applying those solutions for large-scale and continuously industrial monitoring. In this work, we present a RFID-based solution, TwinLeak, to detect liquid leakage using COTS RFID devices. Detecting the leakage accurately with coarse-grained RSSI and phase readings of tags has been a daunting task, which is especially challenging when low detection delay is required. Our system achieves these goals based on the fact that the inductive coupling between two adjacent tags is highly sensitive to the liquid leaked between them. Therefore, instead of judging according to the signals of each individual tag, TwinLeak utilizes the relationship between the signals of two tags as an effective feature for leakage detection. Specifically, Twin-Leak extracts discriminative signal features from short segments of signals and instantly identifies leakage using a light-weight classifier. A model-guided method for leakage progress tracking is further devised to simultaneously estimate the leakage volume and rate. We implement TwinLeak, evaluate its performance across various scenarios, and deploy it in a real-world industrial IoT system. In average, TwinLeak achieves a TPR higher than 97.2&#x0025;, a FPR lower than 0.5&#x0025;, and a relative property estimation error around 10&#x0025;, while triggering early alarms after only about $4.6mL$ liquid leaks."},
{"Title": "Pair-Navi: Peer-to-Peer Indoor Navigation with Mobile Visual SLAM", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2019.8737640", "Full Abstract": "Existing indoor navigation solutions usually require pre-deployed comprehensive location services with precise indoor maps and, more importantly, all rely on dedicatedly installed or existed infrastructure. In this paper, we present Pair-Navi, an infrastructure-free indoor navigation system that circumvents all these requirements by reusing a previous traveler&#x2019;s (i.e. leader) trace experience to navigate future users (i.e. followers) in a Peer-to-Peer (P2P) mode. Our system leverages the advances of visual SLAM on commercial smartphones. Visual SLAM systems, however, are vulnerable to environmental dynamics in the precision and robustness and involve intensive computation that prohibits real-time applications. To combat environmental changes, we propose to cull non-rigid contexts and keep only the static and rigid contents in use. To enable real-time navigation on mobiles, we decouple and reorganize the highly coupled SLAM modules for leaders and followers. We implement Pair-Navi on commodity smartphones and validate its performance in three diverse buildings. Our results show that Pair-Navi achieves an immediate navigation success rate of 98.6&#x0025;, which maintains as 83.4&#x0025; even after two weeks since the leaders&#x2019; traces were collected, outperforming the state-of-the-art solutions by &#x003E;50&#x0025;. Being truly infrastructure-free, Pair-Navi sheds lights on practical indoor navigations for mobile users."},
{"Title": "LEGO-Fi: Transmitter-Transparent CTC with Cross-Demapping", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM.2019.8737659", "Full Abstract": "Cross-Technology Communication (CTC) is an e-merging technique that enables direct communication across different wireless technologies. The state-of-the-art works in this area propose physical-level CTC, in which the transmitters emulate signals that follow the receiver&#x2019;s standard. Physical-level CTC means considerable processing complexity at the transmitter, which doesn&#x2019;t apply to the communication from a low-end transmitter to a high-end receiver, e.g. from ZigBee to WiFi. This paper presents transmitter-transparent cross-technology communication, which leaves the processing complexity solely at the receiver side and therefore makes a critical advance toward bidirectional high-throughput CTC. We implement our proposal as LEGO-Fi, the communication from ZigBee to WiFi. The key technique inside is cross-demapping, which stems from two key technical insights: (1) A ZigBee packet leaves distinguishable features when passing the WiFi modules. (2) Compared to ZigBee&#x2019;s simple encoding and modulation schemes, the rich processing capacity of WiFi offers extra flexibility to process a ZigBee packet. The evaluation results show that LEGO-Fi achieves a throughput of 213. 6Kbps, which is respectively $ 13000\\times$ and $ 1200\\times$ faster than FreeBee and ZigFi, the two existing ZigBee-to-WiFi CTC approaches."},
{"Title": "An In-depth Study of Commercial MVNO", "URL": "https://dl.acm.org/doi/10.1145/3307334.3326070", "Full Abstract": "Recent years have witnessed the rapid growth of mobile virtual network operators (MVNOs), which operate on top of the existing cellular infrastructures of base carriers while offering cheaper or more flexible data plans compared to those of the base carriers. In this paper, we present a nearly two-year measurement study towards understanding various key aspects of today's MVNO ecosystem, including its architecture, performance, economics, customers, and the complex interplay with the base carrier. Our study focuses on a large commercial MVNO with \\reviseabout 1 million customers, operating atop a nation-wide base carrier. Our measurements clarify several key concerns raised by MVNO customers, such as inaccurate billing and potential performance discrimination with the base carrier. We also leverage big data analytics and machine learning to optimize an MVNO's key businesses such as data plan reselling and customer churn mitigation. Our proposed techniques can help achieve %will lead to higher revenues and improved services for commercial MVNOs."},
{"Title": "Zero-Effort Cross-Domain Gesture Recognition with Wi-Fi", "URL": "https://dl.acm.org/doi/10.1145/3307334.3326081", "Full Abstract": "Wi-Fi based sensing systems, although sound as being deployed almost everywhere there is Wi-Fi, are still practically difficult to be used without explicit adaptation efforts to new data domains. Various pioneering approaches have been proposed to resolve this contradiction by either translating features between domains or generating domain-independent features at a higher learning level. Still, extra training efforts are necessary in either data collection or model re-training when new data domains appear, limiting their practical usability. To advance cross-domain sensing and achieve fully zero-effort sensing, a domain-independent feature at the lower signal level acts as a key enabler. In this paper, we propose Widar3.0, a Wi-Fi based zero-effort cross-domain gesture recognition system. The key insight of Widar3.0 is to derive and estimate velocity profiles of gestures at the lower signal level, which represent unique kinetic characteristics of gestures and are irrespective of domains. On this basis, we develop a one-fits-all model that requires only one-time training but can adapt to different data domains. We implement this design and conduct comprehensive experiments. The evaluation results show that without re-training and across various domain factors (i.e. environments, locations and orientations of persons), Widar3.0 achieves 92.7% in-domain recognition accuracy and 82.6%-92.4% cross-domain recognition accuracy, outperforming the state-of-the-art solutions. To the best of our knowledge, Widar3.0 is the first zero-effort cross-domain gesture recognition work via Wi-Fi, a fundamental step towards ubiquitous sensing."},
{"Title": "Understanding Fileless Attacks on Linux-based IoT Devices with HoneyCloud", "URL": "https://dl.acm.org/doi/10.1145/3307334.3326083", "Full Abstract": "With the wide adoption, Linux-based IoT devices have emerged as one primary target of today's cyber attacks. Traditional malware-based attacks can quickly spread across these devices, but they are well-understood threats with effective defense techniques such as malware fingerprinting and community-based fingerprint sharing. Recently, fileless attacks---attacks that do not rely on malware files---have been increasing on Linux-based IoT devices, and posing significant threats to the security and privacy of IoT systems. Little has been known in terms of their characteristics and attack vectors, which hinders research and development efforts to defend against them. In this paper, we present our endeavor in understanding fileless attacks on Linux-based IoT devices in the wild. Over a span of twelve months, we deploy 4 hardware IoT honeypots and 108 specially designed software IoT honeypots, and successfully attract a wide variety of real-world IoT attacks. We present our measurement study on these attacks, with a focus on fileless attacks, including the prevalence, exploits, environments, and impacts. Our study further leads to multi-fold insights towards actionable defense strategies that can be adopted by IoT vendors and end users."},
{"Title": "Understanding and Detecting Overlay-based Android Malware at Market Scales", "URL": "https://dl.acm.org/doi/10.1145/3307334.3326094", "Full Abstract": "As a key UI feature of Android, overlay enables one app to draw over other apps by creating an extra View layer on top of the host View. While greatly facilitating user interactions with multiple apps at the same time, it is often exploited by malicious apps (malware) to attack users. To combat this threat, prior countermeasures concentrate on restricting the capabilities of overlays at the OS level, while barely seeing adoption by Android due to the concern of sacrificing overlays' usability. To address this dilemma, a more pragmatic approach is to enable the early detection of overlay-based malware at the app market level during the app review process, so that all the capabilities of overlays can stay unchanged. Unfortunately, little has been known about the feasibility and effectiveness of this approach for lack of understanding of malicious overlays in the wild. To fill this gap, in this paper we perform the first large-scale comparative study of overlay characteristics in benign and malicious apps using static and dynamic analyses. Our results reveal a set of suspicious overlay properties strongly correlated with the malice of apps, including several novel features. Guided by the study insights, we build OverlayChecker, a system that is able to automatically detect overlay-based malware at market scales. OverlayChecker has been adopted by one of the world's largest Android app stores to check around 10K newly submitted apps per day. It can efficiently (within 2 minutes per app) detect nearly all (96%) overlay-based malware using a single commodity server."},
{"Title": "NEIVA", "URL": "https://dl.acm.org/doi/10.1145/3326285.3329038", "Full Abstract": "With the popularization of advanced cellular networks, mobile video occupies nearly three quarters of cellular network traffic. While previous adaptive bitrate (ABR) algorithms perform well under broadband network, their performance degrades in cellular networks due to throughput fluctuation. Through real world 4G/LTE network measurement, we find that throughput in cellular networks exhibits high fluctuation. It follows Markov behaviors with different states and different transition probability among states. We further find that the transition probability is stable along time but varies significantly under different environments. This inspires us to design ABR algorithms by improving throughput prediction in cellular networks. We propose NEIVA, a network environment identification based video bitrate adaption method in cellular networks. NEIVA trains a network environment identifier based on throughput data and trains a hidden Markov model (HMM) based throughput predictor for different environments. In online video bitrate selection, NEIVA utilizes the environment identifier to select the model for corresponding environment. Then NEIVA predicts future network performance by combining offline model and online throughput data. We implement NEIVA with MPC and evaluate it in real environment. The evaluation results show that with manually identifying environment, NEIVA improves 20% -- 25% bandwidth prediction accuracy and 11% -- 20% QoE improvement over the baseline predictors. With online environment identification, online NEIVA achieves 3.8% and 11.1% average QoE improvement over MPC and HMM, respectively."},
{"Title": "Mobile Gaming on Personal Computers with Direct Android Emulation", "URL": "https://dl.acm.org/doi/10.1145/3300061.3300122", "Full Abstract": "Playing Android games on Windows x86 PCs has gained enormous popularity in recent years, and the de facto solution is to use mobile emulators built with the AOVB (Android-x86 On VirtualBox) architecture. When playing heavy 3D Android games with AOVB, however, users often suffer unsatisfactory smoothness due to the considerable overhead of full virtualization. This paper presents DAOW, a game-oriented Android emulator implementing the idea of direct Android emulation, which eliminates the overhead of full virtualization by directly executing Android app binaries on top of x86-based Windows. Based on pragmatic, efficient instruction rewriting and syscall emulation, DAOW offers foreign Android binaries direct access to the domestic PC hardware through Windows kernel interfaces, achieving nearly native hardware performance. Moreover, it leverages graphics and security techniques to enhance user experiences and prevent cheating in gaming. As of late 2018, DAOW has been adopted by over 50 million PC users to run thousands of heavy 3D Android games. Compared with AOVB, DAOW improves the smoothness by 21% on average, decreases the game startup time by 48%, and reduces the memory usage by 22%."},
{"Title": "Validating Software Pipelining Optimizations,", "URL": "https://dl.acm.org/doi/book/10.5555/903622", "Full Abstract": "There is a growing awareness, both in industry and academia, of the crucial role of formally proving the correctness of safety-critical components of systems. Most formal verification methods verify the correctness of a high-level representation of the system against a given specification. However, if one wishes to infer from such a verification the correctness of the code which runs on the actual target architecture, it is essential to prove that the high-level representation is correctly implemented at the lower level. That is, it is essential to verify the the correctness of the translation from the high-level source-code representation to the object code, a translation which is typically performed by a compiler (or a code generator in case the source is a specification rather than a programming language). Formally verifying a full-fledged optimizing compiler, as one would verify any other large program, is not feasible due to its size, ongoing evolution and modification, and, possibly, proprietary considerations. The translation validation method used in this paper is a novel approach that offers an alternative to the verification of translators in general and compilers in particular. According to the translation validation approach, rather than verifying the compiler itself, one constructs a validation tool which, after every run of the compiler, formally confirms that the target code produced on that run is a correct translation of the source program. The paper presents a method for translation validation of a specific optimization used to increase the instruction level parallelism in EPIC type of architectures. Based on our general methodology to establish simulation relation between source and target based on computational induction, we describe an algorithm that automatically produces assertions that help i this process."},
{"Title": "Validation of Optimizing Compilers,", "URL": "https://dl.acm.org/doi/book/10.5555/903623", "Full Abstract": "There is a growing awareness, both in industry and academia, of the crucial role of formally proving the correctness of safety-critical components of systems. Most formal verification methods verify the correctness of a high-level representation of the system against a given specification. However, if one wishes to infer from such a verification the correctness of the code which runs on the actual target architecture, it is essential to prove that the high-level representation is correctly implemented at the lower level. That is, it is essential to verify the the correctness of the translation from the high-level source-code representation to the object code, a translation which is typically performed by a compiler (or a code generator in case the source is a specification rather than a programming language). Formally verifying a full-fledged optimizing compiler, as one would verify any other large program, is not feasible due to its size, ongoing evolution and modification, and, possibly, proprietary considerations. The translation validation method used in this paper is a novel approach that offers an alternative to the verification of translators in general and compilers in particular. According to the translation validation approach, rather than verifying the compiler itself, one constructs a validation tool which, after every run of the compiler, formally confirms that the target code produced on that run is a correct translation of the source program. The paper presents a methodology for translation validation of optimizing compilers. We distinguish between structure preserving optimizations, for which we establish simulation relation between source and target based on computational induction, and structure modifying optimizations, for which we develop specialized ``meta-rules\". We present some examples that illustrate the use of the methodology, including a ``real-life\" validation of an EPIC compiler which uncovered a bug in the compiler."},
{"Title": "Sticks and stones", "URL": "https://dl.acm.org/doi/10.1145/383962.383971", "Full Abstract": "We consider the problem of Uniform Algorithmic Verification of Parameterized Systems, which requires establishing in a single verification effort the correctness of a parameterized family of systems for any value of the parameter. As has been observed by several researchers, using regular expressions or equivalent formalisms (e.g. WS1S) as assertional language, we can perform symbolic model checking of systems of unbounded number of states."},
{"Title": "From Falsification to Verification", "URL": "https://dl.acm.org/doi/10.5555/646839.708656", "Full Abstract": "This paper enhances the linear temporal logic model checking process with the ability to automatically generate a deductive proof that the system meets its temporal specification. Thus, we emphasize the point of view that model checking can also be used to justify why the system actually works. We show that, by exploiting the information in the graph that is generated during a failed search for counterexamples, we can generate a fully deductive proof that the system meets its specification."},
{"Title": "Beyond Regular Model Checking", "URL": "https://dl.acm.org/doi/10.5555/646839.708790", "Full Abstract": "In recent years, it has been established that regular model checking can be successfully applied to several parameterized verification problems. However, there are many parameterized verification problems that cannot be described by regular languages, and thus cannot be verified using regular model checking. In this study we try to practice symbolic model checking using classes of languages more expressive than the regular languages. We provide three methods for the uniform verification of non-regular parameterized systems."},
{"Title": "Range Allocation for Equivalence Logic", "URL": "https://dl.acm.org/doi/10.5555/646839.759698", "Full Abstract": "The range allocation problem was recently introduced as part of an efficient decision procedure for deciding satisfiability of equivalence logic formulas with or without uninterpreted functions. These type of formulas are mainly used when proving equivalence or refinement between systems (hardware designs, compiler's translation, etc). The problem is to find in polynomial time a small finite domain for each of the variables in an equality formula ϕ, such that ϕ is valid if and only if it is valid over this small domain. The heuristic that was presented for finding small domains was static, i.e. it finds a small set of integer constants for each variable. In this paper we show new, more flexible range allocation methods. We also show the limitations of these and other related approaches by proving a lower bound on the size of the state space generated by such procedures. To prove this lower bound we reduce the question to a graph theoretic counting question, which we believe to be of independent interest."},
{"Title": "Model-Checking and Abstraction to the Aid of Parameterized Systems", "URL": "https://dl.acm.org/doi/10.5555/646542.696212", "Full Abstract": "Parameterized systems are systems that involve numerous instantiations of the same finite-state module. Examples of parameterized systems include tele-communication protocols, bus protocols, cache coherence protocols, and many other protocols that underly current state-of-the-art systems. Formal verification of parameterized systems is known to be undecidable [AK86] and thus cannot be automated. Recent research has shown that in many cases it is possible to use abstraction methods to generate a finite-state systems from a parameterized systems. The finite-state system can then be model-checked. If successful, it is possible to conclude that the original parameterized system satisfies its requirements. Otherwise, it is often the case that the counterexample produced by the model checker can indicate an error in the original parameterized system. This combined technique allows for automatic verification of parameterized systems.This presentation describes our recent approaches that combine abstraction and model-checking to verify safety as well we liveness properties of parameterized systems. We start with the method of invisible invariants [APR+01] that combines a small-model theorem with an heuristics to generate proofs of correctness of parameterized systems. We also describe the method of network invariants [ZPK02, KPSZ02] which allows to explicitly describe a finite-system that, in a precise sense, has the same external behavior as an infinite-state one, and can be used for model-checking properties."},
{"Title": "Automatic Verification of Probabilistic Free Choice", "URL": "https://dl.acm.org/doi/10.5555/646541.696191", "Full Abstract": "We study automatic methods for establishing P-validity (validity with probability 1) of simple temporal properties over finite-state probabilistic systems. The proposed approach replaces P-validity with validity over a non-probabilistic version of the system, in which probabilistic choices are replaced by non-deterministic choices constrained by compassion (strong fairness) requirements. \"Simple\" properties are temporal properties whose only temporal operators are l (eventually) and its dual (always). In general, the appropriate compassion requirements are \"global,\" since they involve global states of the system. Yet, in many cases they can be transformed into \"local\" requirements, which enables their verification by model checkers. We demonstrate our methodology of translating the problem of P-validity into that of verification of a system with local compassion requirement on the \"courteous philosophers\" algorithm of [LR81], a parameterized probabilistic system that is notoriously difficult to verify, and outline a verification of the algorithm that was obtained by the tlv model checker."},
{"Title": "Erratum to \"Verification by augmented finitary abstraction,\" vol. 163, no. 1 (2000) pp. 203-243, doi:10.1006/inco.2000.3000", "URL": "https://dl.acm.org/doi/10.5555/512520.512524", "Full Abstract": "The paper deals with the proof method of verification by finitary abstraction (VFA), which presents a feasible approach to the verification of the temporal properties of (potentially infinite-state) reactive systems. The method consists of a two-step process by which, in a first step, the system and its temporal specification are jointly abstracted into a finite-state system and a finite-state specification. The second step uses model checking to establish the validity of the abstracted property over the abstracted system. The VFA method can be considered a viable alternative to verification by temporal deduction which, up to now, has been the main method generally applicable for verification of infinite-state systems. The paper presents a general recipe for the joint abstraction, which is shown to be sound, where soundness means that validity over the abstract system implies validity over the concrete (original) system. To make the method applicable for the verification of liveness properties, pure abstraction is sometimes no longer adequate. We show that by augmenting the system by an appropriate (and standardly constructible) progress monitor, we obtain an augmented system, whose computations are essentially the same as the original system, and which may now be abstracted while preserving the desired liveness properties. We refer to the extended method as verification by augmented abstraction (VAA). We then proceed to show that the VAA method is sound and complete for proving all properties expressible by temporal logic (including both safety and liveness). Completeness establishes that whenever the property is valid, there exists a finitary abstraction which abstracts the system, augmented by an appropriate progress monitor, into a finite-state system which validated the abstracted property. Copyright 2000 Academic Press."},
{"Title": "TimeC", "URL": "https://dl.acm.org/doi/10.1023/A%3A1015131814255", "Full Abstract": "Enabled by RISC technologies, low-cost commodity microprocessors are performing at ever increasing levels, significantly via instruction level parallelism (ILP). This in turn increases the opportunities for their use in a variety of day-to-day applications ranging from the simple control of appliances such as microwave ovens, to sophisticated systems for cabin control in modern aircraft. Indeed, embedded applications such as these represent segments in the computer industry with great potential for growth. However, this growth is currently impeded by the lack of robust optimizing compiler technologies that support the assured, rapid and inexpensive prototyping of real-time software in the context of microprocessors with ILP. In this paper we describe a novel notation, TimeC, for specifying timing constraints in programs, i&gt;independent of the base language being used to develop the embedded application; TimeC specifications are language independent and can be instrumented into imperative and object-oriented languages non-intrusively. As we will show, the program synthesis problem that arise out of Time_tract specifications, a subset of TimeC, are always tractable. In contrast, a range of specification mechanisms proposed earlier yield substantially intractable synthesis questions, thereby limiting their potential utility. We will compare the tractability and related expressive power issues between TimeC and some of the extant mechanisms for specifying properties of timed programs."},
{"Title": "Liveness with (0, 1, infty)-Counter Abstraction", "URL": "https://dl.acm.org/doi/10.5555/647771.734286", "Full Abstract": "We introduce the (0, 1, )-counter abstraction method by which a parameterized system of unbounded size is abstracted into a finite-state system. Assuming that each process in the parameterized system is finite-state, the abstract variables are limited counters which count, for each local state s of a process, the number of processes which currently are in local state s. The counters are saturated at 2, which means that ("},
{"Title": "Network Invariants in Action", "URL": "https://dl.acm.org/doi/10.5555/646737.701938", "Full Abstract": "The paper presents the method of network invariants for verifying a wide spectrum of LTL properties, including liveness, of parameterized systems. This method can be applied to establish the validity of the property over a system"},
{"Title": "A Deductive Proof System for CTL", "URL": "https://dl.acm.org/doi/10.5555/646737.756915", "Full Abstract": "The paper presents a sound and (relatively) complete deductive proof system for the verification of CTL* properties over possibly infinite-state reactive systems.T he proof system is based on a set of proof rules for the verification of basic CTL* formulas, namely CTL* formulas with no embedded path quantifiers.W e first show how to decompose the proof of a general (non-basic) CTL* formula into proofs of basic CTL* formulas.W e then present proof rules for some of the most useful basic CTL* formulas, then present a methodology for transforming an arbitrary basic formula into one of these special cases."},
{"Title": "Applications of Formal Methods in Biology", "URL": "https://dl.acm.org/doi/10.5555/646847.707119", "Full Abstract": "From the first introduction of the notion of \"Reactive Systems\" and development of specification languages (such as Temporal Logic and Statecharts) and verification methods for this class of systems, it has been stated that this notion encompasses a wider class of systems than just programs or hardware designs, and should be applicable to other complex systems unrelated to computers. In a similar vein, the acronym UML talks about \"modeling language\" rather than \"programming language\", implying that the approach should be applicable to a more general class of systems than just computer-related.While this claim of wider applicability has been always implied, it was never before seriously substantiated. In this talk, I will describe some recent attempts to apply the discipline of formal methods to the modeling, analysis, and prediction of biological systems. This corresponds to an emerging trend in Biology, according to which Biology in the 21st century will have to direct its attention towards understanding how component parts collaborate to create a whole system or organism. The transition from identifying building blocks (analysis) to integrating the parts into a whole (synthesis) will have to use mathematics and algorithmics. We need a language that is legible both to biologists and computers, and that is faithful to the logic of the biological system of interest.In search for an appropriate rigorous approach to modeling biological systems, we examined formal modeling methods in computer science that were originally developed for specification, design, and analysis of reactive systems. We found that the visual formalism of statecharts can address this challenge, within the general framework of object-oriented modeling. This conclusion followed an initial study we carried out, in which we constructed a detailed executable model for T cell activation, and were able, using verification techniques to find and correct a flaw in the original model.Following this preliminary study, we have now undertaken the more challenging project of applying and extending this methodology for constructing a detailed model of the developmental processes that lead to the formation of the egg-laying system in the nematode C. elegans. The model is built to capture in a natural yet rigorous and analyzable way the aspects of concurrency, multi scalar data, and hierarchical organization. This project involves a close collaboration with Naaman Kam, David Harel, and Irun Cohen from the department of Immunology at the Weizmann Institute."},
{"Title": "Embedded Systems", "URL": "https://dl.acm.org/doi/10.5555/646788.704045", "Full Abstract": "In this position paper, we mention some of the challenges in specification and verification which are raised by the emerging discipline of embedded systems. The main proposition of the paper is that a feasible solution to the problem of effective, reliable, and dependable construction of embedded systems can be provided by a seamless development process based on a formal specification of the required system, which proceeds by the activities of verification and analysis of the specification at very early stages of the design, and then followed by automatic code generation, preceded if necessary by code distribution and allocation.As a prototype example of such a development process, we quote some experiences from the Sacres project and its follow-up Safeair. Necessary extensions to these preliminary experiments are discussed and evaluated."},
{"Title": "Validating software pipelining optimizations", "URL": "https://dl.acm.org/doi/10.1145/581630.581676", "Full Abstract": "The paper presents a method for translation validation of a specific optimization, software pipelining optimization, used to increase the instruction level parallelism in EPIC type of architectures. Using a methodology as in [15] to establish simulation relation between source and target based on computational induction, we describe an algorithm that automatically produces a set of decidable proof obligations. The paper also describes"},
{"Title": "The small model property", "URL": "https://dl.acm.org/doi/10.1016/S0890-5401%2802%2993175-5", "Full Abstract": "Efficient decision procedures for equality logic (quantifier-free predicate calculus + the equality sign) are of major importance when proving logical equivalence between systems. We introduce an efficient decision procedure for the theory of equality based on finite instantiations. The main idea is to analyze the structure of the formula and compute accordingly a small domain to each variable such that the formula is satisfiable iff it can be satisfied over these domains. We show how the problem of finding these small domains can be reduced to an interesting graph theoretic problem. This method enabled us to verify formulas containing hundreds of integer and floating point variables that could not be efficiently handled with previously known techniques."},
{"Title": "Smart Play-out of Behavioral Requirements", "URL": "https://dl.acm.org/doi/10.5555/646187.683385", "Full Abstract": "We describe a methodology for executing scenario-based requirements of reactive systems, focusing on \"playing-out\" the behavior using formal verification techniques for driving the execution. The methodology is implemented in full in our play-engine tool. The approach appears to be useful in many stages in the development of reactive systems, and might also pave the way to systems that are constructed directly from their requirements, without the need for intra-object or intra-component modeling or coding."},
{"Title": "Formal Modeling of C. elegans Development", "URL": "https://dl.acm.org/doi/10.5555/648295.754699", "Full Abstract": "We present preliminary results of a new approach to the formal modeling of biological phenomena. The approach stems from the conceptual compatibility of the methods and logic of data collection and analysis in the field of developmental genetics with the languages, methods and tools of scenario-based reactive system design. In particular, we use the recently developed methodology consisting of the language of live sequence charts with the play-in/play-out process, to model the well-characterized process of cell fate acquisition during C. elegans vulval development."},
{"Title": "Predicting a binary sequence almost as well as the optimal biased coin", "URL": "https://dl.acm.org/doi/10.1145/238061.238072", "Full Abstract": "Copyright © 1996 ACM."},
{"Title": "Game theory, on-line prediction and boosting", "URL": "https://dl.acm.org/doi/10.1145/238061.238163", "Full Abstract": "Copyright © 1996 ACM."},
{"Title": "Experiments with a new boosting algorithm", "URL": "https://dl.acm.org/doi/10.5555/3091696.3091715", "Full Abstract": "No abstract available."},
{"Title": "On-line prediction and conversion strategies", "URL": "https://dl.acm.org/doi/10.1023/A%3A1018348209754", "Full Abstract": "No abstract available."},
{"Title": "Learning Under Persistent Drift", "URL": "https://dl.acm.org/doi/10.5555/646944.712239", "Full Abstract": "No abstract available."},
{"Title": "Using and combining predictors that specialize", "URL": "https://dl.acm.org/doi/10.1145/258533.258616", "Full Abstract": "Copyright © 1997 ACM."},
{"Title": "How to use expert advice", "URL": "https://dl.acm.org/doi/10.1145/258128.258179", "Full Abstract": "We analyze algorithms that predict a binary value by combining the predictions of several prediction strategies, called"},
{"Title": "Boosting the margin", "URL": "https://dl.acm.org/doi/10.5555/645526.657129", "Full Abstract": "No abstract available."},
{"Title": "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting", "URL": "https://dl.acm.org/doi/10.1006/jcss.1997.1504", "Full Abstract": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line."},
{"Title": "Selective Sampling Using the Query by Committee Algorithm", "URL": "https://dl.acm.org/doi/10.1023/A%3A1007330508534", "Full Abstract": "We analyze the “query by committee” algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons."},
{"Title": "Efficient learning of typical finite automata from random walks", "URL": "https://dl.acm.org/doi/10.1006/inco.1997.2648", "Full Abstract": "No abstract available."},
{"Title": "Large margin classification using the perceptron algorithm", "URL": "https://dl.acm.org/doi/10.1145/279943.279985", "Full Abstract": "Copyright © 1998 ACM."},
{"Title": "Self bounding learning algorithms", "URL": "https://dl.acm.org/doi/10.1145/279943.279993", "Full Abstract": "Copyright © 1998 ACM."},
{"Title": "Behavorial mechanism design as an online marketing tool", "URL": "https://dl.acm.org/doi/10.1145/988772.988816", "Full Abstract": "No abstract available."},
{"Title": "An Efficient Boosting Algorithm for Combining Preferences", "URL": "https://dl.acm.org/doi/10.5555/645527.756505", "Full Abstract": "No abstract available."},
{"Title": "Designing efficient online trading systems", "URL": "https://dl.acm.org/doi/10.1145/988772.988822", "Full Abstract": "No abstract available."},
{"Title": "Run the GAMUT", "URL": "https://dl.acm.org/doi/10.5555/1018410.1018840", "Full Abstract": "We present GAMUT^1, a suite of game generators designed for testing game-theoretic algorithms. We explain why such a generator is necessary, offer a way of visualizing relationships between the sets of games supported by GAMUT, and give an overview of GAMUTýs architecture. We highlight the importance of using comprehensive test data by benchmarking existing algorithms. We show surprisingly large variation in algorithm performance across different sets of games for two widely-studied problems: computing Nash equilibria and multiagent learning in repeated games."},
{"Title": "Using contracts to influence the outcome of a game", "URL": "https://dl.acm.org/doi/10.5555/1597148.1597188", "Full Abstract": "We consider how much influence a center can exert on a game if its only power is to propose contracts to the agents before the original game, and enforce the contracts after the game if all agents sign it. Modelling the situation as an extensive-form game, we note that the outcomes that are enforceable are precisely those in which the payoff to each agent is higher than its payoff In at least one of the Nash equilibria of the original game. We then show that these outcomes can still be achieved without any effort actually expended by the center: We propose a mechanism in which the center does not monitor the game, and the contracts are written so that in equilibrium all agents sign and obey the contract, with no need for center intervention."},
{"Title": "Simple search methods for finding a Nash equilibrium", "URL": "https://dl.acm.org/doi/10.5555/1597148.1597255", "Full Abstract": "We present two simple search methods for computing a sample Nash equilibrium in a normal-form game: one for 2- player games and one for"},
{"Title": "Understanding random SAT", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-30201-8_33", "Full Abstract": "It is well known that the ratio of the number of clauses to the number of variables in a random"},
{"Title": "New criteria and a new algorithm for learning in multi-agent systems", "URL": "https://dl.acm.org/doi/10.5555/2976040.2976177", "Full Abstract": "We propose a new set of criteria for learning algorithms in multi-agent systems, one that is more stringent and (we argue) better justified than previous proposed criteria. Our criteria, which apply most straightforwardly in repeated games with average rewards, consist of three requirements: (a) against a specified class of opponents (this class is a parameter of the criterion) the algorithm yield a payoff that approaches the payoff of the best response, (b) against other opponents the algorithm's payoff at least approach (and possibly exceed) the security level payoff (or max-imin value), and (c) subject to these requirements, the algorithm achieve a close to optimal payoff in self-play. We furthermore require that these average payoffs be achieved quickly. We then present a novel algorithm, and show that it meets these new criteria for a particular parameter class, the class of stationary opponents. Finally, we show that the algorithm is effective not only in theory, but also empirically. Using a recently introduced comprehensive game theoretic test suite, we show that the algorithm almost universally outperforms previous learning algorithms."},
{"Title": "On cheating in sealed-bid auctions", "URL": "https://dl.acm.org/doi/10.5555/1062104.1700884", "Full Abstract": "Motivated by the rise of online auctions and their relative lack of security, this paper analyzes two forms of cheating in sealed-bid auctions. The first type of cheating we consider occurs when the seller examines the bids of a second-price auction before the auction clears and then submits a shill bid in order to increase the payment of the winning bidder. In the second type, a bidder cheats in a first-price auction by examining the competing bids before submitting his own bid. In both cases, we derive equilibrium strategies when bidders are aware of the possibility of cheating. These results provide insights into sealed-bid auctions even in the absence of cheating, including some counterintuitive results on the effects of overbidding in a first-price auction."},
{"Title": "Marginal contribution nets", "URL": "https://dl.acm.org/doi/10.1145/1064009.1064030", "Full Abstract": "We present a new approach to representing coalitional games based on rules that describe the marginal contributions of the agents. This representation scheme captures characteristics of the interactions among the agents in a natural and concise manner. We also develop efficient algorithms for two of the most important solution concepts, the Shapley value and the core, under this representation. The Shapley value can be computed in time linear in the size of the input. The emptiness of the core can be determined in time exponential only in the treewidth of a graphical interpretation of our representation."},
{"Title": "Fast and compact", "URL": "https://dl.acm.org/doi/10.5555/1619410.1619412", "Full Abstract": "We study a simple, yet rich subclass of congestion games that we call singleton games. These games are exponentially more compact than general congestion games. In contrast with some other compact subclasses. we show tractability of many natural game-theoretic questions. such as finding a sample or optimal Nash equilibrium. For best- and better-response dynamics, we establish polynomial upper and lower bounds on the rate of convergence and present experimental results. We also consider a natural generalization of singleton games and show that many tractability results carry over."},
{"Title": "Learning against opponents with bounded memory", "URL": "https://dl.acm.org/doi/10.5555/1642293.1642424", "Full Abstract": "Recently, a number of authors have proposed criteria for evaluating learning algorithms in multiagent systems. While well-justified, each of these has generally given little attention to one of the main challenges of a multi-agent setting: the capability of the other agents to adapt and learn as well. We propose extending existing criteria to apply to a class of adaptive opponents with bounded memory. We then show an algorithm that provably achieves an o-best response against this richer class of opponents while simultaneously guaranteeing a minimum payoff against any opponent and performing well in self-play. This new algorithm also demonstrates strong performance in empirical tests against a variety of opponents in a wide range of environments."},
{"Title": "Non-cooperative computation", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2005.05.009", "Full Abstract": "We introduce the concept of non-cooperative computation (NCC), which is the joint computation of a function by self-motivated agents, where each of the agents possesses one of the inputs to the function. In NCC the agents communicate their input (truthfully or not) to a trusted center, which performs a commonly-known computation and distributes the results to the agents. The question is whether the agents can be incented to communicate their true input to the center, allowing all agents to compute the function correctly. NCC is a game theoretic concept and specifically is couched in terms of mechanism design. NCC is a very broad framework and is specialized by imposing specific structure on the agents' utility functions. The technical results we present are specific to the setting in which each agent has a primary interest in computing the function and a secondary interest in preventing the others from computing it (properties called correctness and exclusivity). For this setting we provide a complete characterization of the Boolean functions that are non-cooperatively computable. We do this for three versions of NCC: a basic deterministic version, a probabilistic version and a version in which the computation can be subsidized by the center. The analysis turns out to depend on whether the inputs of the agents are probabilistically correlated or not and we analyze both cases."},
{"Title": "Combinatorial Auctions", "URL": "https://dl.acm.org/doi/book/10.5555/1076465", "Full Abstract": "No abstract available."},
{"Title": "Learning against multiple opponents", "URL": "https://dl.acm.org/doi/10.1145/1160633.1160766", "Full Abstract": "We address the problem of learning in repeated n-player (as opposed to 2-player) general-sum games, paying particular attention to the rarely addressed situation in which there are a mixture of agents of different types. We propose new criteria requiring that the agents employing a particular learning algorithm work together to achieve a joint best-response against a target class of opponents, while guaranteeing they each achieve at least their individual security-level payoff against any possible set of opponents. We then provide algorithms that provably meet these criteria for two target classes: stationary strategies and adaptive strategies with a bounded memory. We also demonstrate that the algorithm for stationary strategies outperforms existing algorithms in tests spanning a wide variety of repeated games with more than two players."},
{"Title": "Multi-attribute coalitional games", "URL": "https://dl.acm.org/doi/10.1145/1134707.1134726", "Full Abstract": "We study coalitional games where the value of cooperation among the agents are solely determined by the attributes the agents possess, with no assumption as to how these attributes jointly determine this value. This framework allows us to model diverse economic interactions by picking the right attributes. We study the computational complexity of two coalitional solution concepts for these games -- the Shapley value and the core. We show how the positive results obtained in this paper imply comparable results for other games studied in the literature."},
{"Title": "On strictly competitive multi-player games", "URL": "https://dl.acm.org/doi/10.5555/1597538.1597636", "Full Abstract": "We embark on an initial study of a new class of strategic (normal-form) games, so-called ranking games, in which the payoff to each agent solely depends on his position in a ranking of the agents induced by their actions. This definition is motivated by the observation that in many strategic situations such as parlor games, competitive economic scenarios, and some social choice settings, players are merely interested in performing optimal relative to their opponents rather than in absolute measures. A simple but important subclass of ranking games are single-winner games where in any outcome one agent wins and all others lose. We investigate the computational complexity of a variety of common game-theoretic solution concepts in ranking games and deliver hardness results for iterated weak dominance and mixed Nash equilibria when there are more than two players and pure Nash equilibria when the number of players is unbounded. This dashes hope that multi-player ranking games can be solved efficiently, despite the structural restrictions of these games."},
{"Title": "A game-theoretic analysis of strictly competitive multiagent scenarios", "URL": "https://dl.acm.org/doi/10.5555/1625275.1625470", "Full Abstract": "This paper is a comparative study of game-theoretic solution concepts in strictly competitive multiagent scenarios, as commonly encountered in the context of parlor games, competitive economic situations, and some social choice settings. We model these scenarios as ranking games in which every outcome is a ranking of the players, with higher ranks being preferred over lower ones. Rather than confining our attention to one particular solution concept, we give matching upper and lower bounds for various comparative ratios of solution concepts within ranking games. The solution concepts we consider in this context are security level strategies (maximin), Nash equilibrium, and correlated equilibrium. Additionally, we also examine quasistrict equilibrium, an equilibrium refinement proposed by Harsanyi, which remedies some apparent shortcomings of Nash equilibrium when applied to ranking games. In particular, we compute the price of cautiousness, i.e., the worst-possible loss an agent may incur by playing maximin instead of the worst (quasi-strict) Nash equilibrium, the mediation value, i.e., the ratio between the social welfare obtained in the best correlated equilibrium and the best Nash equilibrium, and the enforcement value, i.e., the ratio between the highest obtainable social welfare and that of the best correlated equilibrium."},
{"Title": "Spiteful bidding in sealed-bid auctions", "URL": "https://dl.acm.org/doi/10.5555/1625275.1625471", "Full Abstract": "We study the bidding behavior of spiteful agents who, contrary to the common assumption of self-interest, maximize a convex combination of their own profit and their competitors' losses. The motivation for this assumption stems from inherent spitefulness or, for example, from competitive scenarios such as in closed markets where the loss of a competitor will likely result in future gains for oneself. We derive symmetric Bayes Nash equilibria for spiteful agents in 1st-price and 2nd-price sealedbid auctions. In 1st-price auctions, bidders become \"more truthful\" the more spiteful they are. Surprisingly, the equilibrium strategy in 2nd-price auctions does not depend on the number of bidders. Based on these equilibria, we compare the revenue in both auction types. It turns out that expected revenue in 2nd-price auctions is higher than expected revenue in 1st-price auctions in the case of even the most modestly spiteful agents, provided they still care at least at little for their own profit. In other words, revenue equivalence only holds for auctions in which all agents are either self-interested or completely malicious. We furthermore investigate the impact of common knowledge on spiteful bidding. Divulging the bidders' valuations reduces revenue in 2nd-price auctions, whereas it has the opposite effect in 1st-price auctions."},
{"Title": "A general criterion and an algorithmic framework for learning in multi-agent systems", "URL": "https://dl.acm.org/doi/10.1007/s10994-006-9643-2", "Full Abstract": "We offer a new formal criterion for agent-centric learning in multi-agent systems, that is, learning that maximizes one's rewards in the presence of other agents who might also be learning (using the same or other learning algorithms). This new criterion takes in as a parameter the class of opponents. We then provide a modular approach for achieving effective agent-centric learning; the approach consists of a number of basic algorithmic building blocks, which can be instantiated and composed differently depending on the environment setting (for example, 2- versus"},
{"Title": "Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules", "URL": "https://dl.acm.org/doi/10.5555/3524938.3525585", "Full Abstract": "Robust perception relies on both bottom-up and top-down signals. Bottom-up signals consist of what's directly observed through sensation. Topdown signals consist of beliefs and expectations based on past experience and short-term memory, such as how the phrase 'peanut butter and ...' will be completed. The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow. We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention. Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data. We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the"},
{"Title": "Small-GAN", "URL": "https://dl.acm.org/doi/10.5555/3524938.3525773", "Full Abstract": "Recent work by Brock et al. (2018) suggests that Generative Adversarial Networks (GANs) benefit disproportionately from large mini-batch sizes. Unfortunately, using large batches is slow and expensive on conventional hardware. Thus, it would be nice if we could generate batches that were"},
{"Title": "Perceptual generative autoencoders", "URL": "https://dl.acm.org/doi/10.5555/3524938.3525986", "Full Abstract": "Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimension of data can be much lower than the ambient dimension. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to a latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. Specifically, we enforce the consistency in both the data space and the latent space with theoretically justified data and latent reconstruction losses. The resulting generative model, which we call a"},
{"Title": "DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-58598-3_35", "Full Abstract": "Visual similarity plays an important role in many computer vision applications. Deep metric learning (DML) is a powerful framework for learning such similarities which not only generalize from training data to identically distributed test distributions, but in particular also translate to"},
{"Title": "A Learning-Based Algorithm to Quickly Compute Good Primal Solutions for Stochastic Integer Programs", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-58942-4_7", "Full Abstract": "We propose a novel approach using supervised learning to obtain near-optimal primal solutions for two-stage stochastic integer programming (2SIP) problems with constraints in the first and second stages. The goal of the algorithm is to predict a"},
{"Title": "Generative adversarial networks", "URL": "https://dl.acm.org/doi/10.1145/3422622", "Full Abstract": "Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the"},
{"Title": "Your GAN is secretly an energy-based model and you should use discriminator driven latent sampling", "URL": "https://dl.acm.org/doi/10.5555/3495724.3496753", "Full Abstract": "The sum of the implicit generator log-density log"},
{"Title": "Hybrid models for learning to branch", "URL": "https://dl.acm.org/doi/10.5555/3495724.3497242", "Full Abstract": "A recent Graph Neural Network (GNN) approach for learning to branch has been shown to successfully reduce the running time of branch-and-bound (B & B) algorithms for Mixed Integer Linear Programming (MILP). While the GNN relies on a GPU for inference, MILP solvers are purely CPU-based. This severely limits its application as many practitioners may not have access to high-end GPUs. In this work, we ask two key questions. First, in a more realistic setting where only a CPU is available, is the GNN model still competitive? Second, can we devise an alternate computationally inexpensive model that retains the predictive power of the GNN architecture? We answer the first question in the negative, and address the second question by proposing a new hybrid architecture for efficient branching on CPU machines. The proposed architecture combines the expressive power of GNNs with computationally inexpensive multi-layer perceptrons (MLP) for branching. We evaluate our methods on four classes of MILP problems, and show that they lead to up to 26% reduction in solver running time compared to state-of-the-art methods without a GPU, while extrapolating to harder problems than it was trained on."},
{"Title": "Untangling tradeoffs between recurrence and self-attention in neural networks", "URL": "https://dl.acm.org/doi/10.5555/3495724.3497355", "Full Abstract": "Attention and self-attention mechanisms, are now central to state-of-the-art deep learning on sequential tasks. However, most recent progress hinges on heuristic approaches with limited understanding of attention's role in model optimization and computation, and rely on considerable memory and computational resources that scale poorly. In this work, we present a formal analysis of how self-attention affects gradient propagation in recurrent networks, and prove that it mitigates the problem of vanishing gradients when trying to capture long-term dependencies by establishing concrete bounds for gradient norms. Building on these results, we propose a relevancy screening mechanism, inspired by the cognitive process of memory consolidation, that allows for a scalable use of sparse self-attention with recurrence. While providing guarantees to avoid vanishing gradients, we use simple numerical experiments to demonstrate the tradeoffs in performance and computational resources by efficiently balancing attention and recurrence. Based on our results, we propose a concrete direction of research to improve scalability of attentive networks."},
{"Title": "A Comparative Study of Learning Outcomes for Online Learning Platforms", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-78270-2_59", "Full Abstract": "Personalization and active learning help educational systems to close the gap between students with varying abilities. We run a comparative head-to-head study of learning outcomes for two popular online platforms: Platform A, which delivers content over lecture videos and multiple-choice quizzes, and Platform B, which provides interactive problem-solving exercises and personalized feedback. We observe a statistically significant increase in the learning outcomes on Platform B. Further, the results of the self-assessment questionnaire suggest that participants using Platform B improve their metacognition."},
{"Title": "Deep learning for AI", "URL": "https://dl.acm.org/doi/10.1145/3448250", "Full Abstract": "How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?"},
{"Title": "Interpolation consistency training for semi-supervised learning", "URL": "https://dl.acm.org/doi/10.1016/j.neunet.2021.10.008", "Full Abstract": "We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets. Our theoretical analysis shows that ICT corresponds to a certain type of data-adaptive regularization with unlabeled points which reduces overfitting to labeled points under high confidence values."},
{"Title": "Predicting Tactical Solutions to Operational Planning Problems Under Imperfect Information", "URL": "https://dl.acm.org/doi/10.1287/ijoc.2021.1091", "Full Abstract": "This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions of operational solutions (TDOSs). The problem we address occurs in the context of two-stage stochastic programming, where the second stage is demanding computationally. We aim to predict at a high speed the expected TDOS associated with the second-stage problem, conditionally on the first-stage variables. This may be used in support of the solution to the overall two-stage problem by avoiding the online generation of multiple second-stage scenarios and solutions. We formulate the tactical prediction problem as a stochastic optimal prediction program, whose solution we approximate with supervised machine learning. The training data set consists of a large number of deterministic operational problems generated by controlled probabilistic sampling. The labels are computed based on solutions to these problems (solved independently and offline), employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application on load planning for rail transportation show that deep learning models produce accurate predictions in very short computing time (milliseconds or less). The predictive accuracy is close to the lower bounds calculated based on sample average approximation of the stochastic prediction programs."},
{"Title": "Tackling Climate Change with Machine Learning", "URL": "https://dl.acm.org/doi/10.1145/3485128", "Full Abstract": "The authors have requested minor, non-substantive changes to the VoR and, in accordance with ACM policies, a Corrected Version of Record was published on February 25, 2022. For reference purposes, the VoR may still be accessed via the Supplemental Material section on this citation page."},
{"Title": "Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks", "URL": "https://dl.acm.org/doi/10.1007/978-3-662-44848-9_34", "Full Abstract": "In this paper we propose and investigate a novel nonlinear unit, called"},
{"Title": "Interpolated Adversarial Training", "URL": "https://dl.acm.org/doi/10.1016/j.neunet.2022.07.012", "Full Abstract": "Adversarial robustness has become a central goal in deep learning, both in the theory and the practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how the adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error ( when there is no adversary) from 4.43% to 12.32%, whereas with our Interpolated adversarial training we retain the adversarial robustness while achieving a standard test error of only 6.45%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1% to just 45.5%. Moreover, we provide mathematical analysis of Interpolated Adversarial Training to confirm its efficiencies and demonstrate its advantages in terms of robustness and generalization."},
{"Title": "The effect of diversity in meta-learning", "URL": "https://dl.acm.org/doi/10.1609/aaai.v37i7.26012", "Full Abstract": "Recent studies show that task distribution plays a vital role in the meta-learner's performance. Conventional wisdom is that task diversity should improve the performance of meta-learning. In this work, we find evidence to the contrary; (i) our experiments draw into question the efficacy of our learned models: similar manifolds can be learned with a subset of the data (lower task diversity). This finding questions the advantage of providing more data to the model, and (ii) adding diversity to the task distribution (higher task diversity) sometimes hinders the model and does not lead to a significant improvement in performance as previously believed. To strengthen our findings, we provide both empirical and theoretical evidence."},
{"Title": "Adaptive discrete communication bottlenecks with dynamic vector quantization for heterogeneous representational coarseness", "URL": "https://dl.acm.org/doi/10.1609/aaai.v37i7.26061", "Full Abstract": "Vector Quantization (VQ) is a method for discretizing latent representations and has become a major part of the deep learning toolkit. It has been theoretically and empirically shown that discretization of representations leads to improved generalization, including in reinforcement learning where discretization can be used to bottleneck multi-agent communication to promote agent specialization and robustness. The discretization tightness of most VQ-based methods is defined by the number of discrete codes in the representation vector and the codebook size, which are fixed as hyperparameters. In this work, we propose learning to dynamically select discretization tightness conditioned on inputs, based on the hypothesis that data naturally contains variations in complexity that call for different levels of representational coarseness which is observed in many heterogeneous data sets. We show that dynamically varying tightness in communication bottlenecks can improve model performance on visual reasoning and reinforcement learning tasks with heterogeneity in representations."},
{"Title": "Interventional causal representation learning", "URL": "https://dl.acm.org/doi/10.5555/3618408.3618426", "Full Abstract": "Causal representation learning seeks to extract high-level latent factors from low-level sensory data. Most existing methods rely on observational data and structural assumptions (e.g., conditional independence) to identify the latent factors. However, interventional data is prevalent across applications. Can interventional data facilitate causal representation learning? We explore this question in this paper. The key observation is that interventional data often carries geometric signatures of the latent factors' support (i.e. what values each latent can possibly take). For example, when the latent factors are causally connected, interventions can break the dependency between the intervened latents' support and their ancestors'. Leveraging this fact, we prove that the latent causal factors can be identified up to permutation and scaling given data from perfect do interventions. Moreover, we can achieve block affine identification, namely the estimated latent factors are only entangled with a few other latents if we have access to data from imperfect interventions. These results highlight the unique power of interventional data in causal representation learning; they can enable provable identification of latent factors without any assumptions about their distributions or dependency structure."},
{"Title": "FAENet", "URL": "https://dl.acm.org/doi/10.5555/3618408.3618769", "Full Abstract": "Applications of machine learning techniques for materials modeling typically involve functions known to be equivariant or invariant to specific symmetries. While graph neural networks (GNNs) have proven successful in such tasks, they enforce symmetries via the model architecture, which often reduces their expressivity, scalability and comprehensibility. In this paper, we introduce (1) a flexible framework relying on stochastic frame-averaging (SFA) to make any model E(3)-equivariant or invariant through data transformations. (2) FAENet: a simple, fast and expressive GNN, optimized for SFA, that processes geometric information without any symmetry-preserving design constraints. We prove the validity of our method theoretically and empirically demonstrate its superior accuracy and computational scalability in materials modeling on the OC20 dataset (S2EF, IS2RE) as well as common molecular modeling tasks (QM9, QM7-X). A package implementation is available at https://faenet.readthedocs.io."},
{"Title": "Tell Me Who I Am: An Interactive Recommendation System", "URL": "https://dl.acm.org/doi/10.1007/s00224-008-9100-7", "Full Abstract": "We consider a model of recommendation systems, where each member from a given set of"},
{"Title": "Maximum Bipartite Flow in Networks with Adaptive Channel Width", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02930-1_29", "Full Abstract": "Traditionally, combinatorial optimization problems (such as maximum flow, maximum matching, etc.) have been studied for networks where each link has a fixed capacity. Recent research in wireless networking has shown that it is possible to design networks where the capacity of the links can be changed <em>adaptively</em> to suit the needs of specific applications. In particular, one gets a choice of having <em>few</em> high capacity outgoing links or <em>many</em> low capacity ones at any node of the network. This motivates us to have a re-look at the traditional combinatorial optimization problems and design algorithms to solve them in this new framework. In particular, we consider the problem of <em>maximum bipartite flow</em>, which has been studied extensively in the traditional network model. One of the motivations for studying this problem arises from the need to maximize the throughput of an infrastructure wireless network comprising base-stations (one set of vertices in the bipartition) and clients (the other set of vertices in the bipartition). We show that this problem has a significantly different combinatorial structure in this new network model from the classical one. While there are several polynomial time algorithms solving the maximum bipartite flow problem in traditional networks, we show that the problem is NP-hard in the new model. In fact, our proof extends to showing that the problem is APX-hard. We complement our lower bound by giving two algorithms for solving the problem approximately. The first algorithm is deterministic and achieves an approximation factor of <em>O</em> (log<em>N</em> ), where there are <em>N</em> nodes in the network, while the second algorithm (which is our main contribution) is randomized and achieves an approximation factor of $\\frac{e{e-1$."},
{"Title": "Buffer management for colored packets with deadlines", "URL": "https://dl.acm.org/doi/10.1145/1583991.1584068", "Full Abstract": "We consider buffer management of unit packets with deadlines for a multi-port device with reconfiguration overhead. The goal is to maximize the throughput of the device, i.e., the number of packets delivered by their deadline. For a single port or with free reconfiguration, the problem reduces to the well-known packets scheduling problem, where the celebrated earliest-deadline-first (EDF) strategy is optimal 1-competitive. However, EDF is not 1-competitive when there is a reconfiguration overhead. We design an online algorithm that achieves a competitive ratio of 1 -"},
{"Title": "Admission control to minimize rejections and online set cover with repetitions", "URL": "https://dl.acm.org/doi/10.1145/1644015.1644026", "Full Abstract": "We study the admission control problem in general networks. Communication requests arrive over time, and the online algorithm accepts or rejects each request while maintaining the capacity limitations of the network. The admission control problem has been usually analyzed as a benefit problem, where the goal is to devise an online algorithm that accepts the maximum number of requests possible. The problem with this objective function is that even algorithms with optimal competitive ratios may reject almost all of the requests, when it would have been possible to reject only a few. This could be inappropriate for settings in which rejections are intended to be rare events."},
{"Title": "Monotonicity in bargaining networks", "URL": "https://dl.acm.org/doi/10.5555/1873601.1873668", "Full Abstract": "We study bargaining networks, discussed in a recent paper of Kleinberg and Tardos [KT08], from the perspective of cooperative game theory. In particular we examine three solution concepts, the nucleolus, the core center and the core median. All solution concepts define unique solutions, so they provide testable predictions. We define a new monotonicity property that is a natural axiom of any bargaining game solution, and we prove that all three of them satisfy this monotonicity property. This is actually in contrast to the conventional wisdom for general cooperative games that monotonicity and the core condition (which is a basic property that all three of them satisfy) are incompatible with each other. Our proofs are based on a primal-dual argument (for the nucleolus) and on the FKG inequality (for the core center and the core median). We further observe some qualitative differences between the solution concepts. In particular, there are cases where a strict version of our monotonicity property is a natural axiom, but only the core center and the core median satisfy it. On the other hand, the nucleolus is easy to compute, whereas computing the core center or the core median is #P-hard (yet it can be approximated in polynomial time)."},
{"Title": "Truthful unsplittable flow for large capacity networks", "URL": "https://dl.acm.org/doi/10.1145/1721837.1721852", "Full Abstract": "The"},
{"Title": "A Preemptive Algorithm for Maximizing Disjoint Paths on Trees", "URL": "https://dl.acm.org/doi/10.5555/3118227.3118482", "Full Abstract": "We consider the on-line version of the maximum vertex disjoint path problem when the underlying network is a tree. In this problem, a sequence of requests arrives in an on-line fashion, where every request is a path in the tree. The on-line algorithm may accept a request only if it does not share a vertex with a previously accepted request. The goal is to maximize the number of accepted requests. It is known that no on-line algorithm can have a competitive ratio better than Ω(logźn) for this problem, even if the algorithm is randomized and the tree is simply a line. Obviously, it is desirable to beat the logarithmic lower bound. Adler and Azar (Proc. of the 10th ACM-SIAM Symposium on Discrete Algorithm, pp. 1---10, 1999) showed that if preemption is allowed (namely, previously accepted requests may be discarded, but once a request is discarded it can no longer be accepted), then there is a randomized on-line algorithm that achieves constant competitive ratio on the line. In the current work we present a randomized on-line algorithm with preemption that has constant competitive ratio on any tree. Our results carry over to the related problem of maximizing the number of accepted paths subject to a capacity constraint on vertices (in the disjoint path problem this capacity is 1). Moreover, if the available capacity is at least 4, randomization is not needed and our on-line algorithm becomes deterministic."},
{"Title": "Distributed error confinement", "URL": "https://dl.acm.org/doi/10.1145/1798596.1798601", "Full Abstract": "We study error confinement in distributed applications, which can be viewed as an extreme case of various fault locality notions studied in the past. Error confinement means that to the external observer, only nodes that were directly hit by a fault may deviate from their specified correct behavior, and only temporarily. The externally observable behavior of all other nodes must remain impeccable, even though their internal state may be affected. Error confinement is impossible if an adversary is allowed to inflict arbitrary transient faults on the system, since the faults might completely wipe out input values. We introduce a new fault-tolerance measure we call"},
{"Title": "How to allocate goods in an online market?", "URL": "https://dl.acm.org/doi/10.5555/1882123.1882130", "Full Abstract": "We study an online version of Fisher's linear case market. In this market there arembuyers and a set of"},
{"Title": "Ranking with submodular valuations", "URL": "https://dl.acm.org/doi/10.5555/2133036.2133117", "Full Abstract": "We study the problem of ranking with submodular valuations. An instance of this problem consists of a ground set ["},
{"Title": "Maximum bipartite flow in networks with adaptive channel width", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2010.10.023", "Full Abstract": "Traditionally, network optimization problems assume that each link in the network has a fixed capacity. Recent research in wireless networking has shown that it is possible to design networks where the capacity of the links can be changed adaptively to suit the needs of specific applications. In particular, one gets a choice of having a few high capacity outgoing links or many low capacity ones at any node of the network. This motivates us to have a re-look at classical network optimization problems and design algorithms to solve them in this new framework. In particular, we consider the problem of maximum bipartite flow, which has been studied extensively in the fixed-capacity network model. One of the motivations for studying this problem arises from the need to maximize the throughput of an infrastructure wireless network comprising base-stations (one set of vertices in the bipartition) and clients (the other set of vertices in the bipartition). We show that this problem has a significantly different combinatorial structure in this new network model from the fixed-capacity one. While there are several polynomial time algorithms for the maximum bipartite flow problem in traditional networks, we show that the problem is NP-hard in the new model. In fact, our proof extends to showing that the problem is APX-hard. We complement our lower bound by giving two algorithms for solving the problem approximately. The first algorithm is deterministic and achieves an approximation factor of O(logN), where N is the number of nodes in the network, while the second algorithm is randomized and achieves an approximation factor of ee-1."},
{"Title": "Recommender systems with non-binary grades", "URL": "https://dl.acm.org/doi/10.1145/1989493.1989528", "Full Abstract": "We consider the interactive model of recommender systems, in which users are asked about just a few of their preferences, and in return the system outputs an approximation of all their preferences. The measure of performance is the"},
{"Title": "Submodular max-SAT", "URL": "https://dl.acm.org/doi/10.5555/2040572.2040609", "Full Abstract": "We introduce the submodular Max-SAT problem. This problem is a natural generalization of the classical Max-SAT problem in which the additive objective function is replaced by a submodular one. We develop a randomized linear-time 2/3-approximation algorithm for the problem. Our algorithm is applicable even for the online variant of the problem. We also establish hardness results for both the online and offline settings. Notably, for the online setting, the hardness result proves that our algorithm is best possible, while for the offline setting, the hardness result establishes a computational separation between the classical Max-SAT and the submodular Max-SAT."},
{"Title": "Optimal discovery strategies in white space networks", "URL": "https://dl.acm.org/doi/10.5555/2040572.2040650", "Full Abstract": "The whitespace-discovery problem describes two parties, Alice and Bob, trying to discovery one another and establish communication over one of a given large segment of communication channels. Subsets of the channels are occupied in each of the local environments surrounding Alice and Bob, as well as in the global environment (Eve). In the absence of a common clock for the two parties, the goal is to devise time-invariant (stationary) strategies minimizing the discovery time."},
{"Title": "Prompt mechanism for ad placement over time", "URL": "https://dl.acm.org/doi/10.5555/2050805.2050811", "Full Abstract": "Consider video ad placement into commercial breaks in a television channel. The ads arrive online over time and each has an expiration date. The commercial breaks are typically of some uniform duration; however, the video ads may have an arbitrary size. Each ad has a private value and should be posted into some break at most once by its expiration date. The player who own the ad gets her value if her ad had been broadcasted by the ad's expiration date (obviously, after ad's arrival date), and zero value otherwise. Arranging the ads into the commercial breaks while maximizing the players' profit is a classical problem of ad placement subject to the capacity constraint that should be solved truthfully. However, we are interested not only in truthfulness but also in a prompt mechanism where the payment is determined for an agent at the very moment of the broadcast. The promptness of the mechanism is a crucial requirement for our algorithm, since it allows a payment process without any redundant relation between an auctioneer and players. An inability to resolve this problem could even prevent the application of such mechanisms in a real marketing process. We design a 6-approximation prompt mechanism for the problem. Previously Cole et al considered a special case where all ads have the same size which is equal to the break duration. For this particular case they achieved a 2-approximation prompt mechanism. The general case of ads with arbitrary size is considerably more involved and requires designing a new algorithm, which we call the Gravity Algorithm."},
{"Title": "Buffer Management for Colored Packets with Deadlines", "URL": "https://dl.acm.org/doi/10.1007/s00224-011-9346-3", "Full Abstract": "We consider buffer management of unit packets with deadlines for a multi-port device with reconfiguration overhead. The goal is to maximize the throughput of the device, i.e., the number of packets delivered by their deadline. For a single port or with free reconfiguration, the problem reduces to the well-known packets scheduling problem, where the celebrated earliest-deadline-first (EDF) strategy is optimal 1-competitive. However, EDF is not 1-competitive when there is a reconfiguration overhead. We design an online algorithm that achieves a competitive ratio of 1−"},
{"Title": "Asymptotically optimal algorithm for stochastic adwords", "URL": "https://dl.acm.org/doi/10.1145/2229012.2229043", "Full Abstract": "In this paper we consider the adwords problem in the"},
{"Title": "Mastering multi-player games", "URL": "https://dl.acm.org/doi/10.5555/2343776.2343825", "Full Abstract": "We consider multi-player games, and the guarantees that a master player that plays on behalf of a set of players can offer them, without making any assumptions on the rationality of the other players. Our model consists of an ("},
{"Title": "Efficient submodular function maximization under linear packing constraints", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-31594-7_4", "Full Abstract": "We study the problem of maximizing a monotone submodular set function subject to linear packing constraints. An instance of this problem consists of a matrix"},
{"Title": "The Queue-Read Queue-Write PRAM Model", "URL": "https://dl.acm.org/doi/10.1137/S009753979427491", "Full Abstract": "This paper introduces the queue-read queue-write ({\\sc qrqw) parallel random access machine ({\\sc pram) model, which permits concurrent reading and writing to shared-memory locations, but at a cost proportional to the number of readers/writers to any one memory location in a given step. Prior to this work there were no formal complexity models that accounted for the contention to memory locations, despite its large impact on the performance of parallel programs. The {\\sc qrqw pram model reflects the contention properties of most commercially available parallel machines more accurately than either the well-studied {\\sc crcw pram or {\\sc erew pram models: the {\\sc crcw model does not adequately penalize algorithms with high contention to shared-memory locations, while the {\\sc erew model is too strict in its insistence on zero contention at each step."},
{"Title": "Curbing Junk E-Mail via Secure Classification", "URL": "https://dl.acm.org/doi/10.5555/647502.728317", "Full Abstract": "No abstract available."},
{"Title": "The queue-read queue-write asynchronous PRAM model", "URL": "https://dl.acm.org/doi/10.1016/S0304-3975%2897%2900193-X", "Full Abstract": "No abstract available."},
{"Title": "New sampling-based summary statistics for improving approximate query answers", "URL": "https://dl.acm.org/doi/10.1145/276304.276334", "Full Abstract": "In large data recording and warehousing environments, it is often advantageous to provide fast, approximate answers to queries, whenever possible. Before DBMSs providing highly-accurate approximate answers can become a reality, many new techniques for summarizing data and for estimating answers from summarized data must be developed. This paper introduces two new sampling-based summary statistics, concise samples and counting samples, and presents new techniques for their fast incremental maintenance regardless of the data distribution. We quantify their advantages over standard sample views in terms of the number of additional sample points for the same view size, and hence in providing more accurate query answers. Finally, we consider their application to providing fast approximate answers to hot list queries. Our algorithms maintain their accuracy in the presence of ongoing insertions to the data warehouse."},
{"Title": "Wavelet-based histograms for selectivity estimation", "URL": "https://dl.acm.org/doi/10.1145/276304.276344", "Full Abstract": "Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query"},
{"Title": "Modeling and optimizing I/O throughput of multiple disks on a bus (summary)", "URL": "https://dl.acm.org/doi/10.1145/277851.277936", "Full Abstract": "For a wide variety of computational tasks, disk I/O continues to be a serious obstacle to high performance. The focus of the present paper is on systems that use multiple disks per SCSI bus. We measured the performance of concurrent random I/Os, and observed bus-related phenomena that impair performance. We describe these phenomena, and present a new I/O performance model that accurately predicts the average bandwidth achieved by a heavy workload of random reads from disks on a SCSI bus. This model, although relatively simple, predicts performance on several platforms to within 12% for I/O sizes in the range 16-128 KB. We describe a technique to improve the I/O bandwidth by 10-20% for random-access workloads that have large I/Os and high concurrency. This technique increases the percentage of disk head positioning time that is overlapped with data transfers, and increases the percentage of transfers that occur at bus bandwidth, rather than at disk-head bandwidth."},
{"Title": "Triply-Logarithmic Parallel Upper and Lower Bounds for Minimum and Range Minima over Small Domains", "URL": "https://dl.acm.org/doi/10.1006/jagm.1997.0905", "Full Abstract": "We consider the problem of computing the minimum ofnvalues, and several well-known generalizations prefix minima, range minima, and all nearest smaller values (ANSV) for input elements drawn from the integer domain 1 s, wheres n. In this article we give simple and efficient algorithms for all of the preceding problems. These algorithms all takeO(logloglogs) time using an optimal number of processors andO(ns ) space (for constant <1) on the COMMON CRCW PRAM. The best known upper bounds for the range minima and ANSV problems were previouslyO(loglogn) (using algorithms for unbounded domains). For the prefix minima and for the minimum problems, the improvement is with regard to the model of computation. We also prove a lower bound of (loglogn) for domain sizes=2 (lognloglogn). Since, forsat the lower end of this range, loglogn= (logloglogs), this demonstrates that any algorithm running ino(logloglogs) time must restrict the range ofson which it works."},
{"Title": "Augmenting Suffix Trees, with Applications", "URL": "https://dl.acm.org/doi/10.5555/647908.740124", "Full Abstract": "Information retrieval and data compression are the two main application areas where the rich theory of string algorithmics plays a fundamental role. In this paper, we consider one algorithmic problem from each of these areas and present highly efficient (linear or near linear time) algorithms for both problems. Our algorithms rely on augmenting the suffix tree, a fundamental data structure in string algorithmics. The augmentations are nontrivial and they form the technical crux of this paper. In particular, they consist of adding extra edges to suffix trees, resulting in Directed Acyclic Graphs (DAGs). Our algorithms construct these \"suffix DAGs\" and manipulate them to solve the two problems efficiently."},
{"Title": "On secure and pseudonymous client-relationships with multiple servers", "URL": "https://dl.acm.org/doi/10.5555/1267147.1267156", "Full Abstract": "This paper introduces a cryptographic engine, Janus, that assists clients in establishing and maintaining secure and pseudonymous relationships with multiple servers. The setting is such that clients reside on a particular subnet (e.g., corporate intranet, ISP) and the servers reside anywhere on the Internet. The Janus engine allows for each client-server relationship to use either weak or strong authentication on each interaction. At the same time, each interaction preserves privacy by neither revealing a client's true identity (\"modulo\" the subnet) nor the set of servers with which a particular client interacts. Furthermore, clients do not need any secure long-term memory, enabling scalability and mobility. The interaction model extends to allow servers to send data back to clients via e-mail at a later date. Hence, our results complement the functionality of current network anonymity tools and remailers"},
{"Title": "Simple Fast Parallel Hashing by Oblivious Execution", "URL": "https://dl.acm.org/doi/10.1137/S0097539794291580", "Full Abstract": "A hash table is a representation of a set in a linear size data structure that supports constant-time membership queries. We show how to construct a hash table for any given set of n keys in O(lg lg n) parallel time with high probability, using n processors on a weak version of a concurrent-read concurrent-write parallel random access machine ( crcw pram ). Our algorithm uses a novel approach of hashing by \"oblivious execution\" based on probabilistic analysis. The algorithm is simple and has the following structure: Partition the input set into buckets by a random polynomial of constant degree. For t := 1 to O (lg lg n ) do Allocate M t memory blocks, each of size K t . Let each bucket select a block at random, and try to injectively map its keys into the block using a random linear function. Buckets that fail carry on to the next iteration. The crux of the algorithm is a careful a priori selection of the parameters M t and K t . The algorithm uses only O (lg lg n ) random words and can be implemented in a work-efficient manner."},
{"Title": "On the optimality of parsing in dynamic dictionary based data compression", "URL": "https://dl.acm.org/doi/10.5555/314500.314935", "Full Abstract": "No abstract available."},
{"Title": "Synopsis data structures for massive data sets", "URL": "https://dl.acm.org/doi/10.5555/314500.315083", "Full Abstract": "No abstract available."},
{"Title": "The Space Complexity of Approximating the Frequency Moments", "URL": "https://dl.acm.org/doi/10.1006/jcss.1997.1545", "Full Abstract": "The frequency moments of a sequence containingmielements of typei, 1 i n, are the numbersFk= ni=1mki. We consider the space complexity of randomized algorithms that approximate the numbersFk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbersF0,F1, andF2can be approximated in logarithmic space, whereas the approximation ofFkfork 6 requiresn (1)space. Applications to data bases are mentioned as well."},
{"Title": "Consistent, yet anonymous, Web access with LPWA", "URL": "https://dl.acm.org/doi/10.1145/293411.293447", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Provably efficient scheduling for languages with fine-grained parallelism", "URL": "https://dl.acm.org/doi/10.1145/301970.301974", "Full Abstract": "Many high-level parallel programming languages allow for fine-grained parallelism. As in the popular work-time framework for parallel algorithm design, programs written in such languages can express the full parallelism in the program without specifying the mapping of program tasks to processors. A common concern in executing such programs is to schedule tasks to processors dynamically so as to minimize not only the execution time, but also the amount of space (memory) needed. Without careful scheduling, the parallel execution on"},
{"Title": "The Effect of Flexible Parsing for Dynamic Dictionary Based Data Compression", "URL": "https://dl.acm.org/doi/10.5555/789086.789624", "Full Abstract": "We report on the performance evaluation of greedy parsing with a single step lookahead, denoted as flexible parsing. We also introduce a new finger-print based data structure which enables efficient, linear time implementation."},
{"Title": "An Optical Simulation of Shared Memory", "URL": "https://dl.acm.org/doi/10.1137/S0097539795290507", "Full Abstract": "We present a work-optimal randomized algorithm for simulating a shared memory machine (PRAM) on an optical communication parallel computer (OCPC). The OCPC model is motivated by the potential of optical communication for parallel computation. The memory of an OCPC is divided into modules, one module per processor. Each memory module only services a request on a timestep if it receives exactly one memory request.Our algorithm simulates each step of an"},
{"Title": "Modeling and optimizing I/O throughput of multiple disks on a bus", "URL": "https://dl.acm.org/doi/10.1145/301453.301482", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Round-like behavior in multiple disks on a bus", "URL": "https://dl.acm.org/doi/10.1145/301816.301821", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Tracking join and self-join sizes in limited storage", "URL": "https://dl.acm.org/doi/10.1145/303976.303978", "Full Abstract": "Copyright © 1999 ACM."},
{"Title": "Dynamic microprogramming", "URL": "https://dl.acm.org/doi/10.1145/362575.362580", "Full Abstract": "Copyright © 1971 ACM."},
{"Title": "Programming Languages", "URL": "https://dl.acm.org/doi/book/10.5555/542880", "Full Abstract": "No abstract available."},
{"Title": "Implementation consideration for machine translation", "URL": "https://dl.acm.org/doi/10.1145/800178.810158", "Full Abstract": "This paper describes the implementation and operational features of a machine translation (MT) system for Spanish and English text. Sample translations from Spanish to English and English to Spanish are illustrated. The system's computer hardware and software requirements are also presented, along with an assessment of the ongoing machine dictionary management requirements."},
{"Title": "Text Processing", "URL": "https://dl.acm.org/doi/book/10.5555/539893", "Full Abstract": "No abstract available."},
{"Title": "Apple Pascal", "URL": "https://dl.acm.org/doi/book/10.5555/542789", "Full Abstract": "No abstract available."},
{"Title": "Introduction to Programming with ESP and Pascal", "URL": "https://dl.acm.org/doi/book/10.5555/542718", "Full Abstract": "No abstract available."},
{"Title": "Basic Apple II", "URL": "https://dl.acm.org/doi/book/10.5555/542723", "Full Abstract": "No abstract available."},
{"Title": "A perspective on machine translation", "URL": "https://dl.acm.org/doi/10.1145/358027.358035", "Full Abstract": "Copyright © 1984 ACM."},
{"Title": "A computer science curriculum for liberal arts colleges (panel session)", "URL": "https://dl.acm.org/doi/10.1145/323287.323301", "Full Abstract": "No abstract available."},
{"Title": "Advanced Placement computer science exam (panel session)", "URL": "https://dl.acm.org/doi/10.1145/323287.323384", "Full Abstract": "No abstract available."},
{"Title": "Programming languages (2nd ed.)", "URL": "https://dl.acm.org/doi/book/10.5555/4690", "Full Abstract": "No abstract available."},
{"Title": "Liberal arts curriculum and computer science education (panel presentation)", "URL": "https://dl.acm.org/doi/10.1145/320435.320474", "Full Abstract": "No abstract available."},
{"Title": "A model curriculum for a liberal arts degree in computer science", "URL": "https://dl.acm.org/doi/10.1145/5666.5667", "Full Abstract": "This report proposes developing a rigorous undergraduate curriculum for a B.A.-degree program in computer science. The curriculum is intended as a model not only for high-quality undergraduate colleges and universities, but also for larger universities with strong computer science programs in a liberal arts setting."},
{"Title": "Discourse and cohension in expository text", "URL": "https://dl.acm.org/doi/10.3115/991365.991419", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "On knowledge-based machine translation", "URL": "https://dl.acm.org/doi/10.3115/991365.991549", "Full Abstract": "This paper describes the design of the knowledge representation medium used for representing concepts and assertions, respectively, in a subworld chosen for a knowledge-based machine translation system. This design is used in the TRANSLATOR machine translation project. The knowledge representation language, or interlingua, has two components, DIL and TIL. DIL stands for 'dictionary of interlingua' and descibes the semantics of a subworld. TIL stands for 'text of interlingua' and is responsible for producing an interlingua text, which represents the meaning of an input text in the terms of the interlingua. We maintain that involved analysis of various types of linguistic and encyclopaedic meaning is necessary for the task of automatic translation. The mechanisms for extracting and manipulating and reproducing the meaning of texts will be reported in detail elsewhere. The linguistic (including the syntactic) knowledge about source and target languages is used by the mechanisms that translate texts into and from the interlingua. Since interlingua is an artificial language, we can (and do, through TIL) control the syntax and semantics of the allowed interlingua elements. The interlingua suggested for TRANSLATOR has a broader coverage than other knowledge representation schemata for natural language. It involves the knowledge about discourse, speech acts, focus, time, space and other facets of the overall meaning of texts."},
{"Title": "Programming languages; (2nd ed.)", "URL": "https://dl.acm.org/doi/book/10.5555/22964", "Full Abstract": "No abstract available."},
{"Title": "Computing as a discipline: preliminary report of the ACM task force on the core of computer science", "URL": "https://dl.acm.org/doi/10.1145/52964.52975", "Full Abstract": "It is ACM's 40th year and an old debate continues. Is computer science a science? An engineering discipline? Or merely a technology, an inventor and purveyor of computing commodities? What is the intellectual substance of the discipline? Is it lasting, or will it fade within a generation? Do core curricula in computer science and engineering accurately reflect the field? How can theory and lab work be integrated in a computing curriculum?"},
{"Title": "Report of the ACM Task Force on The Core of Computer Science", "URL": "https://dl.acm.org/doi/book/10.1145/2594497", "Full Abstract": "No abstract available."},
{"Title": "Computing as a discipline", "URL": "https://dl.acm.org/doi/10.1145/63238.63239", "Full Abstract": "The final report of the Task Force on the Core of Computer Science presents a new intellectual framework for the discipline of computing and a new basis for computing curricula. This report has been endorsed and approved for release by the ACM Education Board."},
{"Title": "Computing as a Discipline", "URL": "https://dl.acm.org/doi/10.1109/2.19833", "Full Abstract": "A summary is given of a report that had the following goals: to describe computer science in a way that emphasizes fundamental questions and significant accomplishments; to propose a teaching paradigm for computer science that conforms to traditional scientific standards, emphasizes the development of competence in the field, and harmoniously integrates theory, experimentation, and design; and to give a detailed example of an introductory course sequence in computer science that is based on the curriculum model and the disciplinary description. This task was extended to encompass both computer science and computer engineering. This summary encompasses: paradigms; the role of programming; a description of computing; a curriculum model; an introductory sequence; laboratories; and accreditation."},
{"Title": "Lock-free collaboration support for cloud storage services with operation inference and transformation", "URL": "https://dl.acm.org/doi/10.5555/3386691.3386694", "Full Abstract": "This paper studies how today's cloud storage services support collaborative file editing. As a tradeoff for transparency/user-friendliness, they do not ask collaborators to use version control systems but instead implement their own heuristics for handling conflicts, which however often lead to unexpected and undesired experiences. With measurements and reverse engineering, we unravel a number of their design and implementation issues as the root causes of poor experiences. Driven by the findings, we propose to reconsider the collaboration support of cloud storage services from a novel perspective of"},
{"Title": "Experiences of landing machine learning onto market-scale mobile malware detection", "URL": "https://dl.acm.org/doi/10.1145/3342195.3387530", "Full Abstract": "App markets, being crucial and critical for today's mobile ecosystem, have also become a natural malware delivery channel since they actually \"lend credibility\" to malicious apps. In the past decade, machine learning (ML) techniques have been explored for automated, robust malware detection. Unfortunately, to date, we have yet to see an ML-based malware detection solution deployed at market scales. To better understand the real-world challenges, we conduct a collaborative study with a major Android app market (T-Market) offering us large-scale ground-truth data. Our study shows that the key to successfully developing such systems is manifold, including"},
{"Title": "Experience", "URL": "https://dl.acm.org/doi/10.1145/3372224.3380897", "Full Abstract": "Almost every Android user has unsatisfying experiences regarding responsiveness, in particular Application Not Responding (ANR) and System Not Responding (SNR) that directly disrupt user experience. Unfortunately, the community have limited understanding of the prevalence, characteristics, and root causes of unresponsiveness. In this paper, we make an in-depth study of ANR and SNR at scale based on fine-grained system-level traces crowdsourced from 30,000 Android systems. We find that ANR and SNR occur prevalently on all the studied 15 hardware models, and better hardware does not seem to relieve the problem. Moreover, as Android evolves from version 7.0 to 9.0, there are fewer ANR events but more SNR events. Most importantly, we uncover multifold root causes of ANR and SNR and pinpoint the largest inefficiency which roots in Android's flawed implementation of Write Amplification Mitigation (WAM). We design a practical approach to eliminating this largest root cause; after large-scale deployment, it reduces almost all (>99%) ANR and SNR caused by WAM while only decreasing 3% of the data write speed. In addition, we document important lessons we have learned from this study, and have also released our measurement code/data to the research community."},
{"Title": "Combating packet collisions using non-stationary signal scaling in LPWANs", "URL": "https://dl.acm.org/doi/10.1145/3386901.3388913", "Full Abstract": "LoRa, a representative Low-Power Wide Area Network (LPWAN) technology, has been shown as a promising platform to connect Internet of Things. Practical LoRa deployments, however, suffer from collisions, especially in dense networks and wide coverage areas expected by LoRa applications. Existing collision resolution approaches do not exploit the coding properties of LoRa and thus cannot work well for low SNR LoRa signals. We propose"},
{"Title": "BlueDoor", "URL": "https://dl.acm.org/doi/10.1145/3386901.3389025", "Full Abstract": "Today's smart devices like fitness tracker, smartwatch, etc., often employ Bluetooth Low Energy (BLE) for data transmission. Such devices thus become our information portal, e.g., SMS message and notifications are delivered to those devices through BLE. In this study, we present BlueDoor, which can obtain unauthorized information from smart devices via BLE vulnerability. We thoroughly examine the BLE protocol, and leverage its intrinsic properties designed for low-cost embedded and wearable devices to bypass the encryption and authentication in BLE. By mimicking a low capacity device to downgrade the process of encryption key negotiation and authentication, BlueDoor can enforce a new key with the peripheral BLE device and pass the authentication without user participation. As a result, BlueDoor can extract BLE packets as well as read/write stored data on BLE devices. We show that BlueDoor works well on the fundamental design tradeoff of using BLE on diverse embedded and wearable devices, and thus can be generalized to various BLE devices. We implement the BlueDoor design and examine its performance on 15 COTS BLE enabled smart devices, including fitness trackers, smartwatch, smart bulb, etc. The results show that BlueDoor can break the information flow and obtain different types of information (e.g., SMS message, notifications) delivered to BLE devices. In addition to privacy threats, this further means traditional operations such as using SMS for verification in widely adopted authentication, are insecure."},
{"Title": "mmVib", "URL": "https://dl.acm.org/doi/10.1145/3372224.3419202", "Full Abstract": "Vibration measurement is a crucial task in industrial systems, where vibration characteristics reflect the health and indicate anomalies of the objects. Previous approaches either work in an intrusive manner or fail to capture the micrometer-level vibrations. In this work, we propose mmVib, a practical approach to measure micrometer-level vibrations with mmWave radar. By introducing a"},
{"Title": "Patronus", "URL": "https://dl.acm.org/doi/10.1145/3384419.3430713", "Full Abstract": "The widespread adoption and ubiquity of smart devices equipped with microphones ("},
{"Title": "Aloba", "URL": "https://dl.acm.org/doi/10.1145/3384419.3430719", "Full Abstract": "Backscatter communication holds potential for ubiquitous and low-cost connectivity among low-power IoT devices. To avoid interference between the carrier signal and the backscatter signal, recent works propose a frequency-shifting technique to separate these two signals in the frequency domain. Such proposals, however, have to occupy the precious wireless spectrum that is already overcrowded, and increase the power, cost, and complexity of the backscatter tag. In this paper, we revisit the classic ON-OFF Keying (OOK) modulation and propose Aloba, a backscatter system that takes the ambient LoRa transmissions as the excitation and piggybacks the in-band OOK modulated signals over the LoRa transmissions. Our design enables the backsactter signal to work in the same frequency band of the carrier signal, meanwhile achieving good tradeoff between transmission range and link throughput. The key contributions of Aloba include: i) the design of a low-power backscatter tag that can pick up the ambient LoRa signals from other signals; ii) a novel decoding algorithm to demodulate both the carrier signal and the backscatter signal from their superposition. The design of Aloba completely unleashes the backscatter tag's ability in OOK modulation and achieves flexible data rate at different transmission range. We implement Aloba and conduct head-to-head comparison with the state-of-the-art LoRa backscatter system PLoRa in various settings. The experiment results show Aloba can achieve 39.5--199.4 Kbps data rate at various distances, 10.4--52.4X higher than PLoRa."},
{"Title": "Symphony", "URL": "https://dl.acm.org/doi/10.1145/3384419.3430724", "Full Abstract": "Sound recognition is an important and popular function of smart devices. The location of sound is basic information associated with the acoustic source. Apart from sound recognition, whether the acoustic sources can be localized largely affects the capability and quality of the smart device's interactive functions. In this work, we study the problem of concurrently localizing multiple acoustic sources with a smart device (e.g., a smart speaker like Amazon Alexa). The existing approaches either can only localize a single source, or require deploying a distributed network of microphone arrays to function. Our proposal called"},
{"Title": "Wi-fi see it all", "URL": "https://dl.acm.org/doi/10.1145/3384419.3430725", "Full Abstract": "Wi-Fi imaging has attracted significant interests due to the ubiquitous availability of Wi-Fi devices today. In this paper, we present"},
{"Title": "LiTag", "URL": "https://dl.acm.org/doi/10.1145/3384419.3430777", "Full Abstract": "The development of Internet of Things calls for ubiquitous and low-cost localization and posture estimation. We present LiTag, a visible light based localization and posture estimation solution with COTS cameras. The core of LiTag is based on the design of a chip-less and battery-less optical tag which can show different color patterns from different observation directions. After capturing a photo containing the tag, LiTag can calculate the tag position and posture by combining the color pattern and the geometry relation between the camera image plane and the real world. Unlike existing marker-based visible localization and posture estimation approaches, LiTag can work with a single camera without calibration, which significantly reduces the calibration overhead and deployment costs. We implement LiTag and evaluate its performance extensively. Results show that LiTag can provide the tag position with a median error of 1.6"},
{"Title": "Vernier: Accurate and Fast Acoustic Motion Tracking Using Mobile Devices", "URL": "https://dl.acm.org/doi/10.1109/TMC.2019.2945955", "Full Abstract": "Acoustic motion tracking has been viewed as a promising user interaction technique in many scenarios such as Virtual Reality (VR), Smart Appliance, video gaming, etc. Existing acoustic motion tracking approaches, however, suffer from long window of accumulated signal and time-consuming signal processing. They are inherently difficult to achieve both high accuracy and low delay. In this paper, we present Vernier, an efficient and accurate acoustic tracking method based on commodity mobile devices. We design a new approach to efficiently and accurately derive phase change and thus moving distance. Vernier significantly reduces the tracking delay/overhead by removing the complicated frequency analysis and long window of signal accumulation, while keeping a high tracking accuracy. We implement Vernier on Android, and evaluate its performance with COTS mobile devices including Samsung Galaxy S7 and Sony L50t. Experimental results show that Vernier outperforms previous approaches with a tracking error less than 4 mm. The tracking speed achieves 3&#x00D7; improvement to the previous phase based approaches and 10&#x00D7; to Doppler Effect based approaches. Vernier is also validated in applications like controlling and drawing, and we believe it is generally applicable in many real applications."},
{"Title": "Beyond QoE: Diversity Adaptation in Video Streaming at the Edge", "URL": "https://dl.acm.org/doi/10.1109/TNET.2020.3032416", "Full Abstract": "Adaptive bitrate (ABR) algorithms are critical techniques for high quality-of-experience (QoE) Internet video delivery. Early ABR algorithms conducting the overall QoE function of fixed parameters are limited by the fact that the QoE of end-users are diverse such that the video bitrate is often chosen in a misleading way. State-of-the-art ABR algorithms like MPC and Pensieve utilize offline modeling techniques and result in performance degradation for online QoE diversity adaptation. To address this issue, we propose Elephanta, an online ABR algorithm for edge users, which incorporates user QoE perception interface and adaptation algorithm with flexible parameters. In order to avoid overhead from updating parameters online, we model video streaming as a renewal system and formulate the specific QoE function into flexible formats by setting constraints on corresponding QoE metrics. To validate parameter settings, we emulate Elephanta under 1500 throughput traces, including FCC broadband, $3G$ HSDPA data set from the Internet, as well as the $4G$ /LTE data set we collect. Evaluation results show that Elephanta achieves QoE improvement of 7% over MPC and 3% over Pensieve under QoE diversity in part because of its superior adaptability to QoE diversity. We implemented Elephanta in dash.js at the client side for subjective experiments. We observed the diverse QoE preferences across users and 19/21 users (strongly) agree that Elephanta is responsive to parameter changes while watching videos."},
{"Title": "Deep AI Enabled Ubiquitous Wireless Sensing", "URL": "https://dl.acm.org/doi/10.1145/3436729", "Full Abstract": "With the development of the Internet of Things (IoT), many kinds of wireless signals (e.g., Wi-Fi, LoRa, RFID) are filling our living and working spaces nowadays. Beyond communication, wireless signals can sense the status of surrounding objects, known as"},
{"Title": "Full-Dimension Relative Positioning for RFID-Enabled Self-Checkout Services", "URL": "https://dl.acm.org/doi/10.1145/3448094", "Full Abstract": "Self-checkout services in today's retail stores are well received as they set free the labor force of cashiers and shorten conventional checkout lines. However, existing self-checkout options either require customers to scan items one by one, which is troublesome and inefficient, or rely on deployments of massive sensors and cameras together with complex tracking algorithms. On the other hand, RFID-based item-level tagging in retail offers an extraordinary opportunity to enhance current checkout experiences. In this work, we propose Taggo, a lightweight and efficient self-checkout schema utilizing well-deployed RFIDs. Taggo attaches a few anchor tags on the four upper edges of each shopping cart, so as to figure out which cart each item belongs to, through relative positioning among the tagged items and anchor tags without knowing their absolute positions. Specifically, a full-dimension ordering technique is devised to accurately determine the order of tags in each dimension, as well as to address the negative impacts from imperfect measurements in indoor surroundings. Besides, we design a holistic classifying solution based on probabilistic modeling to map each item to the correct cart that carries it. We have implemented Taggo with commercial RFID devices and evaluated it extensively in our lab environment. On average, Taggo achieves 90% ordering accuracy in real-time, eventually producing 95% classifying accuracy."},
{"Title": "Privacy-Preserving Outlier Detection with High Efficiency over Distributed Datasets", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM42981.2021.9488710", "Full Abstract": "The ability to detect outliers is crucial in data mining, with widespread usage in many fields, including fraud detection, malicious behavior monitoring, health diagnosis, etc. With the tremendous volume of data becoming more distributed than ever, global outlier detection for a group of distributed datasets is particularly desirable. In this work, we propose PIF (Privacy-preserving Isolation Forest), which can detect outliers for multiple distributed data providers with high efficiency and accuracy while giving certain security guarantees. To achieve the goal, PIF makes an innovative improvement to the traditional iForest algorithm, enabling it in distributed environments. With a series of carefully-designed algorithms, each participating party collaborates to build an ensemble of isolation trees efficiently without disclosing sensitive information of data. Besides, to deal with complicated real-world scenarios where different kinds of partitioned data are involved, we propose a comprehensive schema that can work for both horizontally and vertically partitioned data models. We have implemented our method and evaluated it with extensive experiments. It is demonstrated that PIF can achieve comparable AUC to existing iForest on average and maintains a linear time complexity without privacy violation."},
{"Title": "Dancing Waltz with Ghosts", "URL": "https://dl.acm.org/doi/10.1145/3412382.3458258", "Full Abstract": "Recently, mmWave has been widely used in fine-grained sensing applications due to its short wavelength and large bandwidth. One mmWave device usually can measure the target's 1D micro-displacement along the line-of-sight (LOS) direction. In this work, we try to empower mmWave with the capability of measuring 2D micro-displacements. Our insight is that although the mmWave reflection from one path contains only 1D observation, the spatial separability of mmWave offers an opportunity to separate multipath reflections from the received signal. Combining the coherent observations from multipath reflections can restore the 2D orbit of the target. Based on this insight, we present GWaltz, a mmWave sensing system that manages to measure sub-mm-level 2D orbits of rotating machinery. In GWaltz, we first reveal the relationship between the rotor's movement and the observed ghost multipath reflections (GMRs) and then design a set of novel signal processing techniques to restore the rotor orbit from the poor-quality GMR signals. We implement GWaltz with a commercial mmWave radar, and our evaluation results show that it achieves an absolute error of about 8.42um when measuring 100um-diameter rotor orbits."},
{"Title": "A nationwide study on cellular reliability", "URL": "https://dl.acm.org/doi/10.1145/3452296.3472908", "Full Abstract": "With recent advances on cellular technologies (such as 5G) that push the boundary of cellular performance, cellular reliability has become a key concern of cellular technology adoption and deployment. However, this fundamental concern has never been addressed due to the challenges of measuring cellular reliability on mobile devices and the cost of conducting large-scale measurements. This paper closes the knowledge gap by presenting the first large-scale, in-depth study on cellular reliability with more than 70 million Android phones across 34 different hardware models. Our study identifies the critical factors that affect cellular reliability and clears up misleading intuitions indicated by common wisdom. In particular, our study pinpoints that software reliability defects are among the main root causes of cellular data connection failures. Our work provides actionable insights for improving cellular reliability at scale. More importantly, we have built on our insights to develop enhancements that effectively address cellular reliability issues with remarkable real-world impact---our optimizations on Android's cellular implementations have effectively reduced 40% cellular connection failures for 5G phones and 36% failure duration across all phones."},
{"Title": "A nationwide census on wifi security threats", "URL": "https://dl.acm.org/doi/10.1145/3447993.3448620", "Full Abstract": "Carrying over 75% of the last-mile mobile Internet traffic, WiFi has inevitably become an enticing target for various security threats. In this work, we characterize a wide variety of real-world WiFi threats at an unprecedented scale, involving 19 million WiFi access points (APs) mostly located in China, by deploying a crowdsourced security checking system on 14 million mobile devices in the wild. Leveraging the collected data, we reveal the landscape of nationwide WiFi threats for the first time. We find that the prevalence, riskiness, and breakdown of WiFi threats deviate significantly from common understandings and prior studies. In particular, we detect attacks at around 4% of all WiFi APs, uncover that most WiFi attacks are driven by an underground economy, and provide strong evidence of web analytics platforms being the bottleneck of its monetization chain. Further, we provide insightful guidance for defending against WiFi attacks at scale, and some of our efforts have already yielded real-world impact---effectively disrupted the WiFi attack ecosystem."},
{"Title": "Hybrid Systems", "URL": "https://dl.acm.org/doi/book/10.5555/861940", "Full Abstract": "No abstract available."},
{"Title": "Smart play-out", "URL": "https://dl.acm.org/doi/10.1145/949344.949353", "Full Abstract": "We describe \"smart play-out\", a new method for executing and analyzing scenario based behavior, which is part of the Play-In/Play-Out methodology and the Play-Engine tool. Behavior is \"played in\" directly from the system's GUI, and as this is being done the Play-Engine continuously constructs Live Sequence Charts (LSCs), a powerful extension of sequence diagrams. Later, behavior can be \"played out\" freely from the GUI, and the tool executes the LSCs directly, thus driving the system's behavior. An inherent difficulty in constructing a ``play-out\" mechanism is how to resolve the nondeterminism allowed by the LSC specification in order to obtain an executable model. Smart play-out, is a recent strengthening of the play-out mechanism, which addresses this problem by using powerful verification methods, mainly model-checking, to execute and analyze the LSCs, helping the execution to avoid deadlocks and violations. Thus, smart play-out utilizes verification techniques to run programs, rather than to verify a program with respect to given requirements, as in traditional verification approaches. The ideas appear to be relevant in various stages of system development, including requirements specification and analysis, implementation and testing."},
{"Title": "erratum to \"The small model property: How small can it be?\" [Inform. comput. 178(2002)279-293]", "URL": "https://dl.acm.org/doi/10.1016/S0890-5401%2803%2900117-2", "Full Abstract": "No abstract available."},
{"Title": "Smart Play-Out Extended", "URL": "https://dl.acm.org/doi/10.5555/1018442.1022049", "Full Abstract": "Smart play-out is a powerful technique for executing live sequence charts (LSCs). It uses verification techniques to help run a program, rather than to prove properties thereof. In this paper we extend smart play-out to cover a larger set of the LSC language features and to deal more efficiently with larger models. The extensions cover two key features of the rich version of LSCs, namely, time and forbidden elements. The former is crucial for systems with time constraints and/or time-driven behavior, and the latter allows specifying invariants and contracts on behavior. Forbidden elements can also help reduce the state space considered, thus enabling smart play-out to handle larger models."},
{"Title": "Model checking and abstraction to the aid of parameterized systems (a survey)", "URL": "https://dl.acm.org/doi/10.1016/j.cl.2004.02.006", "Full Abstract": "Parameterized systems are systems that involve numerous instantiations of the same finite-state module, and depend on a parameter which defines their size. Examples of parameterized systems include sensor systems, telecommunication protocols, bus protocols, cache coherence protocols, and many other protocols that underly current state-of-the-art systems. Formal verification of parameterized systems is known to be undecidable (Inform. Process. Lett. 22 (6)) and thus cannot be automated. Recent research has shown that it is often the case that a combination of methodologies allows to reduce the problem of verification of a parameterized system into the problem of verification of a finite-state system, that can be automatically verified. This paper describes several recent methodologies, based on model checking and abstraction. We start with the method of invisible auxiliary assertions that combines a small-model theorem with heuristics to automatically generate auxiliary constructs used in proofs of correctness of parameterized systems. We also describe the method of counter abstraction that offers simple liveness proofs for many parameterized systems, and discuss novel methodologies of using counter abstraction to automatically verify that probabilistic parameterized system satisfy their temporal specifications with probability 1."},
{"Title": "Synthesis revisited", "URL": "https://dl.acm.org/doi/10.5555/2137662.2137683", "Full Abstract": "Constructing a program from a specification is a long-known general and fundamental problem. Besides its theoretical interest, this question also has practical implications, since finding good synthesis algorithms could bring about a major improvement in the reliable development of complex systems. In this paper we describe a methodology for synthesizing statechart models from scenario-based requirements. The requirements are given in the language of live sequence charts (LSCs), and may be played in directly from the GUI, and the resulting statecharts are of the object-oriented variant, as adopted in the UML. We have implemented our algorithms as part of the Play-Engine tool and the generated statechart model can then be executed using existing UML case tools."},
{"Title": "Abstraction for liveness", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-30579-8_10", "Full Abstract": "Unlike model checking which is restricted to finite-state systems, there are two methods which can be applied for the verification of arbitrary infinite-state systems. These are the methods of"},
{"Title": "Shape analysis by predicate abstraction", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-30579-8_12", "Full Abstract": "The paper presents an approach for shape analysis based on predicate abstraction. Using a predicate base that involves reachability relations between program variables pointing into the heap, we are able to analyze functional properties of programs with destructive heap updates, such as list reversal and various in-place list sorts. The approach allows verification of both safety and liveness properties. The abstraction we use does not require any abstract representation of the heap nodes (e.g. abstract shapes), only reachability relations between the program variables."},
{"Title": "A compositional approach to CTL", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2004.09.023", "Full Abstract": "The paper presents a compositional approach to the verification of CTL"},
{"Title": "A discrete-time UML semantics for concurrency and communication in safety-critical applications", "URL": "https://dl.acm.org/doi/10.1016/j.scico.2004.05.012", "Full Abstract": "We define a subset krtUML of UML which is rich enough to express such modelling entities of UML, used in real-time applications, as active objects, dynamic object creation and destruction, dynamically changing communication topologies, combinations of synchronous and asynchronous communication, and shared memory usage through object attributes. We define a formal interleaving semantics for this kernel language by associating with each model"},
{"Title": "Temporal logic for scenario-based specifications", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-31980-1_29", "Full Abstract": "We provide semantics for the powerful scenario-based language of live sequence charts (LSCs). We show how the semantics of live sequence charts can be captured using temporal logic. This is done by studying various subsets of the LSC language and providing an explicit translation into temporal logic. We show how a kernel subset of the LSC language (which omits variables, for example) can be embedded within the temporal logic CTL"},
{"Title": "Separating fairness and well-foundedness for the analysis of fair discrete systems", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-31980-1_9", "Full Abstract": "Fair discrete systems (FDSs) are a computational model of concurrent programs where fairness assumptions are specified in terms of sets of states. The analysis of fair discrete systems involves a non-trivial interplay between fairness and well-foundedness (ranking functions). This interplay has been an obstacle for automation. The contribution of this paper is a new analysis of temporal properties of FDSs. The analysis uses a domain of binary relations over states labeled by sets of indices of fairness requirements. The use of labeled relations separates the reasoning on well-foundedness and fairness."},
{"Title": "Bridging the gap between fair simulation and trace inclusion", "URL": "https://dl.acm.org/doi/10.1016/j.ic.2005.01.006", "Full Abstract": "The paper considers the problem of checking abstraction between two finite-state fair discrete systems. In automata-theoretic terms this is trace inclusion between two nondeterministic Streett automata. We propose to reduce this problem to an algorithm for checking fair simulation between two generalized Buchi automata. For solving this question we present a new triply nested @m-calculus formula which can be implemented by symbolic methods. We then show that every trace inclusion of this type can be solved by fair simulation, provided we augment the concrete system (the contained automaton) by an appropriate 'non-constraining' automaton. This establishes that fair simulation offers a complete method for checking trace inclusion for finite-state systems. We illustrate the feasibility of the approach by algorithmically checking abstraction between finite state systems whose abstraction could only be verified by deductive methods up to now."},
{"Title": "TVOC", "URL": "https://dl.acm.org/doi/10.1007/11513988_29", "Full Abstract": "We describe a tool called TVOC, that uses the"},
{"Title": "IIV", "URL": "https://dl.acm.org/doi/10.1007/11513988_39", "Full Abstract": "This paper describes the"},
{"Title": "Real time temporal logic", "URL": "https://dl.acm.org/doi/10.1007/11603009_2", "Full Abstract": "This paper attempts to improve our understanding of timed languages and their relation to timed automata. We start by giving a constructive proof of the folk theorem stating that timed languages specified by the past fragment of mitl, can be accepted by deterministic timed automata. On the other hand we provide a proof that certain languages expressed in the future fragment of mitl are not deterministic, and analyze the reason for this asymmetry."},
{"Title": "Ranking abstraction as companion to predicate abstraction", "URL": "https://dl.acm.org/doi/10.1007/11562436_1", "Full Abstract": "Predicate abstraction has become one of the most successful methodologies for proving safety properties of programs. Recently, several abstraction methodologies have been proposed for proving liveness properties. This paper studies “ranking abstraction” where a program is augmented by a nonconstraining progress monitor, and further abstracted by predicate-abstraction, to allow for automatic verification of progress properties. Unlike most liveness methodologies, the augmentation does not require a complete ranking function that is expected to decrease with each step. Rather, the inputs are component rankings from which a complete ranking function may be formed."},
{"Title": "Ranking abstraction as a companion to predicate abstraction", "URL": "https://dl.acm.org/doi/10.1007/11562948_1", "Full Abstract": "Predicate abstraction has become one of the most successful methodologies for proving safety properties of programs. Unfortunately, it cannot be used for verifying all liveness properties. In order to handle liveness properties, we introduce the method of"},
{"Title": "Translation and Run-Time Validation of Loop Transformations", "URL": "https://dl.acm.org/doi/10.1007/s10703-005-3402-z", "Full Abstract": "This paper presents new approaches to the validation of loop optimizations that compilers use to obtain the highest performance from modern architectures. Rather than verify the compiler, the approach of"},
{"Title": "An Adaptive Version of the Boost by Majority Algorithm", "URL": "https://dl.acm.org/doi/10.1023/A%3A1010852229904", "Full Abstract": "We propose a new boosting algorithm. This boosting algorithm is an adaptive version of the boost by majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity of AdaBoost."},
{"Title": "Drifting Games and Brownian Motion", "URL": "https://dl.acm.org/doi/10.1006/jcss.2001.1802", "Full Abstract": "We combine the results of 13 and 8 and derive a continuous variant of a large class of drifting games. Our analysis furthers the understanding of the relationship between boosting, drifting games, and Brownian motion and yields a differential equation that describes the core of the problem."},
{"Title": "The Nonstochastic Multiarmed Bandit Problem", "URL": "https://dl.acm.org/doi/10.1137/S0097539701398375", "Full Abstract": "In the multiarmed bandit problem, a gambler must decide which arm of"},
{"Title": "Predicting a binary sequence almost as well as the optimal biased coin", "URL": "https://dl.acm.org/doi/10.1016/S0890-5401%2802%2900033-0", "Full Abstract": "We apply the exponential weight algorithm, introduced and Littlestone and Warmuth [26] and by Vovk [35] to the problem of predicting a binary sequence almost as well as the best biased coin. We first show that for the case of the logarithmic loss, the derived algorithm is equivalent to the Bayes algorithm with Jeffrey's prior, that was studied by Xie and Barron [38] under probabilistic assumptions. We derive a uniform bound on the regret which holds for any sequence. We also show that if the empirical distribution of the sequence is bounded away from 0 and from 1, then, as the length of the sequence increases to infinity, the difference between this bound and a corresponding bound on the average case regret of the same algorithm (which is asymptotically optimal in that case) is only 1/2. We show that this gap of 1/2 is necessary by calculating the regret of the min-max optimal algorithm for this problem and showing that the asymptotic upper bound is tight. We also study the application of this algorithm to the square loss and show that the algorithm that is derived in this case is different from the Bayes algorithm and is better than it for prediction in the worst-case."},
{"Title": "Unsupervised Improvement of Visual Detectors using Co-Training", "URL": "https://dl.acm.org/doi/10.5555/946247.946615", "Full Abstract": "One significant challenge in the construction of visualdetection systems is the acquisition of sufficient labeleddata. This paper describes a new technique for trainingvisual detectors which requires only a small quantity of labeleddata, and then uses unlabeled data to improve performanceover time. Unsupervised improvement is based onthe co-training framework of Blum and Mitchell, in whichtwo disparate classifiers are trained simultaneously. Unlabeledexamples which are confidently labeled by one classifierare added, with labels, to the training set of the otherclassifier. Experiments are presented on the realistic task ofautomobile detection in roadway surveillance video. In thisapplication, co-training reduces the false positive rate by afactor of 2 to 11 from the classifier trained with labeled dataalone."},
{"Title": "An efficient boosting algorithm for combining preferences", "URL": "https://dl.acm.org/doi/10.5555/945365.964285", "Full Abstract": "We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the \"collaborative-filtering\" problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations."},
{"Title": "Predicting genetic regulatory response using classification", "URL": "https://dl.acm.org/doi/10.1093/bioinformatics/bth923", "Full Abstract": "Studying gene regulatory mechanisms in simple model organisms through analysis of high-throughput genomic data has emerged as a central problem in computational biology. Most approaches in the literature have focused either on finding a few strong regulatory patterns or on learning descriptive models from training data. However, these approaches are not yet adequate for making accurate predictions about which genes will be up- or down-regulated in new or held-out experiments. By introducing a predictive methodology for this problem, we can use powerful tools from machine learning and assess the statistical significance of our predictions."},
{"Title": "Predicting genetic regulatory response using classification", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-32280-1_1", "Full Abstract": "We present a novel classification-based algorithm called GeneClass for learning to predict gene regulatory response. Our approach is motivated by the hypothesis that in simple organisms such as"},
{"Title": "Profile-Based String Kernels for Remote Homology Detection and Motif Extraction", "URL": "https://dl.acm.org/doi/10.5555/1018417.1019251", "Full Abstract": "We introduce novel profile-based string kernels for use with support vector machines (SVMs) for the problems of protein classification and remote homology detection. These kernels use probabilistic profiles, such as those produced by the PSI-BLAST algorithm, to define position-dependent mutation neighborhoods along protein sequences for inexact matching of k-length subsequences (\"k-mers\") in the data. By use of an efficient data structure, the kernels are fast to compute once the profiles have been obtained. For example, the time needed to run PSI-BLAST in order to build the pro- files is significantly longer than both the kernel computation time and the SVM training time. We present remote homology detection experiments based on the SCOP database where we show that profile-based string kernels used with SVM classifiers strongly outperform all recently presented supervised SVM methods. We also show how we can use the learned SVM classifier to extract \"discriminative sequence motifs\" short regions of the original profile that contribute almost all the weight of the SVM classification score and show that these discriminative motifs correspond to meaningful structural features in the protein data. The use of PSI-BLAST profiles can be seen as a semi-supervised learning technique, since PSI-BLAST leverages unlabeled data from a large sequence database to build more informative profiles. Recently presented \"cluster kernels\" give general semi-supervised methods for improving SVM protein classification performance. We show that our profile kernel results are comparable to cluster kernels while providing much better scalability to large datasets."},
{"Title": "Motif discovery through predictive modeling of gene regulation", "URL": "https://dl.acm.org/doi/10.1007/11415770_41", "Full Abstract": "We present MEDUSA, an integrative method for learning motif models of transcription factor binding sites PSSMs by incorporating promoter sequence and transcriptome gene expression data. We use a modern large-margin machine learning approach, based on boosting, to enable feature selection from the high-dimensional search space of candidate binding sequences while avoiding overfitting. At each iteration of the algorithm, MEDUSA builds a motif model whose presence in the promoter region of a gene, coupled with activity of a regulator in an experiment, is predictive of differential expression. In this way, we learn motifs that are functional and predictive of regulatory response rather than motifs that are simply overrepresented in promoter sequences. Moreover, MEDUSA produces a model of the transcriptional control logic that can predict the expression of any gene in the organism, given the sequence of the promoter region of the target gene and the expression state of a set of known or putative transcription factors and signaling molecules. Each motif model is either a"},
{"Title": "Learning the structure of manifolds using random projections", "URL": "https://dl.acm.org/doi/10.5555/2981562.2981622", "Full Abstract": "We present a simple variant of the"},
{"Title": "Random projection trees and low dimensional manifolds", "URL": "https://dl.acm.org/doi/10.1145/1374376.1374452", "Full Abstract": "We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data without having to explicitly learn this structure."},
{"Title": "From microscopy images to models of cellular processes", "URL": "https://dl.acm.org/doi/10.5555/3120828.3120831", "Full Abstract": "The advance of fluorescent tagging and of confocal microscopy is allowing biologists to image biochemical processes at a level of detail that was unimaginable just a few years ago. However, as the analysis of these images is done mostly by hand, there is a severe bottleneck in transforming these images into useful quantitative data that can be used to evaluate mathematical models."},
{"Title": "Invited talk: Drifting games, boosting and online learning", "URL": "https://dl.acm.org/doi/10.1145/1553374.1553539", "Full Abstract": "No abstract available."},
{"Title": "Random projection trees for vector quantization", "URL": "https://dl.acm.org/doi/10.1109/TIT.2009.2021326", "Full Abstract": "A simple and computationally efficient scheme for tree-structured vector quantization is presented. Unlike previous methods, its quantization error depends only on the intrinsic dimension of the data distribution, rather than the apparent dimension of the space in which the data happen to lie."},
{"Title": "Near-optimal search in continuous domains", "URL": "https://dl.acm.org/doi/10.5555/1619797.1619832", "Full Abstract": "We investigate search problems in continuous state and action spaces with no uncertainty. Actions have costs and can only be taken at discrete time steps (unlike the case with continuous control). Given an admissible heuristic function and a starting state, the objective is to find a minimum-cost plan that reaches a goal state. As the continuous domain does not allow the tight optimality results that are possible in the discrete case (for example by A*), we instead propose and analyze an approximate forward-search algorithm that has the following provable properties. Given a desired accuracy Ε, and a bound"},
{"Title": "Asymptotically optimal repeated auctions for sponsored search", "URL": "https://dl.acm.org/doi/10.1145/1282100.1282112", "Full Abstract": "We investigate asymptotically optimal keyword auctions, that is, auctions which maximize revenue as the number of bidders grows. We do so under two alternative behavioral assumptions. The first explicitly models the repeated nature of keyword auctions. It introduces a novel assumption on individual bidding, namely that bidders never overbid their value, and bid their actual value if shut out for long enough. Under these conditions we present a broad class of repeated auctions that are asymptotically optimal among all sequential auctions (a superset of repeated auctions). Those auctions have varying payment schemes but share the ranking method. The Google auction belongs to this class, but not the Yahoo auction, and indeed we show that the latter is not asymptotically optimal. (Nonetheless, with some additional distributional assumptions, the Yahoo auction can be shown to belong to a broad category of auctions that are asymptotically optimal among all auction mechanisms that do not rely on ad relevance.) We then look at the one-shot keyword auction, which can be taken to model repeated auctions in which relatively myopic bidders converge on the equilibrium of the full-information stage game. In this case we show that the Google auction remains asymptotically optimal and the Yahoo auction suboptimal. The distributional assumptions under which our theorems hold are quite general. We do however show that the Google auction is not asymptotically revenue-maximizing for general distributions."},
{"Title": "An overview of combinatorial auctions", "URL": "https://dl.acm.org/doi/10.1145/1345037.1345039", "Full Abstract": "Copyright © 2007 Authors."},
{"Title": "Eliciting properties of probability distributions", "URL": "https://dl.acm.org/doi/10.1145/1386790.1386813", "Full Abstract": "We investigate the problem of truthfully eliciting an expert's assessment of a property of a probability distribution, where a property is any real-valued function of the distribution such as mean or variance. We show that not all properties are elicitable; for example, the mean is elicitable and the variance is not. For those that are elicitable, we provide a representation theorem characterizing all payment (or \"score\") functions that induce truthful revelation. We also consider the elicitation of sets of properties. We then observe that properties can always be inferred from sets of elicitable properties. This naturally suggests the concept of elicitation complexity; the elicitation complexity of property is the minimal size of such a set implying the property. Finally we discuss applications to prediction markets."},
{"Title": "Self-financed wagering mechanisms for forecasting", "URL": "https://dl.acm.org/doi/10.1145/1386790.1386820", "Full Abstract": "We examine a class of wagering mechanisms designed to elicit truthful predictions from a group of people without requiring any outside subsidy. We propose a number of desirable properties for wagering mechanisms, identifying one mechanism - weighted-score wagering - that satisfies all of the properties. Moreover, we show that a single-parameter generalization of weighted-score wagering is the only mechanism that satisfies these properties. We explore some variants of the core mechanism based on practical considerations."},
{"Title": "Essentials of Game Theory", "URL": "https://dl.acm.org/doi/book/10.5555/1481632", "Full Abstract": "No abstract available."},
{"Title": "Bayesian coalitional games", "URL": "https://dl.acm.org/doi/10.5555/1619995.1620012", "Full Abstract": "We introduce Bayesian Coalitional Games (BCGs), a generalization of classical coalitional games to settings with uncertainties. We define the semantics of BCG using the partition model, and generalize the notion of payoffs to contracts among agents. To analyze these games, we extend the solution concept of the core under three natural interpretations-- ex ante, ex interim, and ex post--which coincide with the classical definition of the core when there is no uncertainty. In the special case where agents are risk-neutral, we show that checking for core emptiness under all three interpretations can be simplified to linear feasibility problems similar to that of their classical counterpart."},
{"Title": "Optimal testing of structured knowledge", "URL": "https://dl.acm.org/doi/10.5555/1620163.1620238", "Full Abstract": "Adopting a decision-theoretic perspective, we investigate the problem of optimal testing of structured knowledge - the canonical example being a qualifying examination of a graduate student. The setting is characterized by several factors: examinee's knowledge structured around several inter-dependent topics, a limited \"budget\" of questions available to the examiner, a decision to be made (pass/fail), and an utility for good and bad decisions. The existence of multiple professors brings up additional issues such as committee formation, and the existence of multiple students brings up issues such as fairness."},
{"Title": "Game theory pragmatics", "URL": "https://dl.acm.org/doi/10.5555/1620270.1620343", "Full Abstract": "Game theory has been playing an increasingly visible role in computer science in general and AI in particular, most notably in the area of multi agent systems. I briefly list the areas where most of the action has been in the past decade or so. I then suggest that going forward, the most dramatic interaction between computer science and game theory - with a special role for AI - could be around what might be called game theory pragmatics."},
{"Title": "Computer science and game theory", "URL": "https://dl.acm.org/doi/10.1145/1378704.1378721", "Full Abstract": "The most dramatic interaction between CS and GT may involve game-theory pragmatics."},
{"Title": "Fault tolerant mechanism design", "URL": "https://dl.acm.org/doi/10.1016/j.artint.2008.06.004", "Full Abstract": "We introduce the notion of fault tolerant mechanism design, which extends the standard game theoretic framework of mechanism design to allow for uncertainty about execution. Specifically, we define the problem of task allocation in which the private information of the agents is not only their costs of attempting the tasks but also their probabilities of failure. For several different instances of this setting we present both, positive results in the form of mechanisms that are incentive compatible, individually rational, and efficient, and negative results in the form of impossibility theorems."},
{"Title": "Book announcements", "URL": "https://dl.acm.org/doi/10.1145/1486877.1486881", "Full Abstract": "Copyright © 2008 Authors."},
{"Title": "Eliciting properties of probability distributions", "URL": "https://dl.acm.org/doi/10.1145/1486877.1486886", "Full Abstract": "We investigate the problem of incentivizing an expert to truthfully reveal probabilistic information about a random event. Probabilistic information consists of one or more properties, which are any real-valued functions of the distribution, such as the mean and variance. Not all properties can be elicited truthfully. We provide a simple characterization of elicitable properties, and describe the general form of the associated payment functions that induce truthful revelation. We then consider sets of properties, and observe that all properties can be inferred from sets of elicitable properties. This suggests the concept of elicitation complexity for a property, the size of the smallest set implying the property."},
{"Title": "Multiagent Systems", "URL": "https://dl.acm.org/doi/book/10.5555/1483085", "Full Abstract": "This exciting and pioneering new overview of multiagent systems, which are online systems composed of multiple interacting intelligent agents, i.e., online trading, offers a newly seen computer science perspective on multiagent systems, while integrating ideas from operations research, game theory, economics, logic, and even philosophy and linguistics. The authors emphasize foundations to create a broad and rigorous treatment of their subject, with thorough presentations of distributed problem solving, game theory, multiagent communication and learning, social choice, mechanism design, auctions, cooperative game theory, and modal logics of knowledge and belief. For each topic, basic concepts are introduced, examples are given, proofs of key results are offered, and algorithmic considerations are examined. An appendix covers background material in probability theory, classical logic, Markov decision processes and mathematical programming. Written by two of the leading researchers of this engaging field, this book will surely serve as THE reference for researchers in the fastest-growing area of computer science, and be used as a text for advanced undergraduate or graduate courses."},
{"Title": "Truthful Surveys", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-92185-1_23", "Full Abstract": "We consider the problem of truthfully sampling opinions of a population for statistical analysis purposes, such as estimating the population distribution of opinions. To obtain accurate results, the surveyor must incentivize individuals to report unbiased opinions. We present a rewarding scheme to elicit opinions that are representative of the population. In contrast with the related literature, we do not assume a specific information structure. In particular, our method does not rely on a common prior assumption."},
{"Title": "Ranking games", "URL": "https://dl.acm.org/doi/10.1016/j.artint.2008.10.008", "Full Abstract": "The outcomes of many strategic situations such as parlor games or competitive economic scenarios are rankings of the participants, with higher ranks generally at least as desirable as lower ranks. Here we define ranking games as a class of n-player normal-form games with a payoff structure reflecting the players' von Neumann-Morgenstern preferences over their individual ranks. We investigate the computational complexity of a variety of common game-theoretic solution concepts in ranking games and deliver hardness results for iterated weak dominance and mixed Nash equilibrium when there are more than two players, and for pure Nash equilibrium when the number of players is unbounded but the game is described succinctly. This dashes hope that multi-player ranking games can be solved efficiently, despite their profound structural restrictions. Based on these findings, we provide matching upper and lower bounds for three comparative ratios, each of which relates two different solution concepts: the price of cautiousness, the mediation value, and the enforcement value."},
{"Title": "Modeling billiards games", "URL": "https://dl.acm.org/doi/10.5555/1558013.1558040", "Full Abstract": "Two-player games of billiards, of the sort seen in recent Computer Olympiads held by the International Computer Games Association, are an emerging area with unique challenges for A.I. research. Complementing the heuristic/algorithmic aspect of billiards, of the sort brought to the fore in the ICGA billiards tournaments, we investigate formal models of such games. The modeling is surprisingly subtle. While sharing features with existing models (including stochastic games, games on a square, recursive games, and extensive form games), our model is distinct, and consequently requires novel analysis. We focus on the basic question of whether the game has an equilibrium. For finite versions of the game it is not hard to show the existence of a pure strategy Markov perfect Nash equilibrium. In the infinite case, it can be shown that under certain conditions a stationary pure strategy Markov perfect Nash equilibrium is guaranteed to exist."},
{"Title": "On the complexity of schedule control problems for knockout tournaments", "URL": "https://dl.acm.org/doi/10.5555/1558013.1558044", "Full Abstract": "Knockout tournaments constitute a common format of sporting events, and also model a specific type of election scheme (namely, sequential pairwise elimination election). In such tournaments the designer controls the shape of the tournament (a binary tree) and the seeding of the players (their assignment to the tree leaves). In this paper we investigate the computational complexity of tournament schedule control, i.e., designing a tournament that maximizes the winning probability a target player. We start with a generic probabilistic model consisting of a matrix of pairwise winning probabilities, and then investigate the problem under two types of constraint: constraints on the probability matrix, and constraints on the allowable tournament structure. While the complexity of the general problem is as yet unknown, these various constraints -- all naturally occurring in practice -- serve to push to the problem to one side or the other: easy (polynomial) or hard (NP-complete)."},
{"Title": "Team competition", "URL": "https://dl.acm.org/doi/10.5555/1558013.1558046", "Full Abstract": "In a team competition, two participating teams have an equal number of players, and each team orders its players linearly based on their strengths. A mechanism then specifies how the players from the two teams are matched up and how to score them. There are two types of manipulations by a team: Misreporting the strength ordering and deliberately losing a match. To identify these strategically behaviors, we model the team competition problem in a game-theoretical framework, under which we prove necessary and sufficient conditions which ensure that truthful reporting and maximal effort in matches are equilibrium strategies, and which further ensure certain fairness conditions described by choice functions."},
{"Title": "GFlowNet-EM for learning compositional latent variable models", "URL": "https://dl.acm.org/doi/10.5555/3618408.3618957", "Full Abstract": "Latent variable models (LVMs) with discrete compositional latents are an important but challenging setting due to a combinatorially large number of possible configurations of the latents. A key tradeoff in modeling the posteriors over latents is between expressivity and tractable optimization. For algorithms based on expectationmaximization (EM), the E-step is often intractable without restrictive approximations to the posterior. We propose the use of GFlowNets, algorithms for sampling from an unnormalized density by learning a stochastic policy for sequential construction of samples, for this intractable E-step. By training GFlowNets to sample from the posterior over latents, we take advantage of their strengths as amortized variational inference algorithms for complex distributions over discrete structures. Our approach, GFlowNet-EM, enables the training of expressive LVMs with discrete compositional latents, as shown by experiments on non-context-free grammar induction and on images using discrete variational autoencoders (VAEs) without conditional independence enforced in the encoder. Code: github.com/GFNOrg/GFlowNet-EM."},
{"Title": "Multi-objective GFlowNets", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619004", "Full Abstract": "We study the problem of generating"},
{"Title": "Equivariance with learned canonicalization functions", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619042", "Full Abstract": "Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification,"},
{"Title": "Synergies between disentanglement and sparsity", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619159", "Full Abstract": "Although disentangled representations are often said to be beneficial for downstream tasks, current empirical and theoretical understanding is limited. In this work, we provide evidence that disentangled representations coupled with sparse task-specific predictors improve generalization. In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse predictors yield disentangled representations. Motivated by this theoretical result, we propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem. Finally, we explore a metalearning version of this algorithm based on group Lasso multiclass SVM predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations."},
{"Title": "A theory of continuous generative flow networks", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619162", "Full Abstract": "Generative flow networks (GFlowNets) are amortized variational inference algorithms that are trained to sample from unnormalized target distributions over compositional objects. A key limitation of GFlowNets until this time has been that they are restricted to discrete spaces. We present a theory for generalized GFlowNets, which encompasses both existing discrete GFlowNets and ones with continuous or hybrid state spaces, and perform experiments with two goals in mind. First, we illustrate critical points of the theory and the importance of various assumptions. Second, we empirically demonstrate how observations about discrete GFlowNets transfer to the continuous case and show strong results compared to non-GFlowNet baselines on several previously studied tasks. This work greatly widens the perspectives for the application of GFlowNets in probabilistic inference and various modeling settings."},
{"Title": "GFlowOut", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619306", "Full Abstract": "Bayesian inference offers principled tools to tackle many critical problems with modern neural networks such as poor calibration and generalization, and data inefficiency. However, scaling Bayesian inference to large architectures is challenging and requires restrictive approximations. Monte Carlo Dropout has been widely used as a relatively cheap way to approximate inference and estimate uncertainty with deep neural networks. Traditionally, the dropout mask is sampled independently from a fixed distribution. Recent research shows that the dropout mask can be seen as a latent variable, which can be inferred with variational inference. These methods face two important challenges: (a) the posterior distribution over masks can be highly multi-modal which can be difficult to approximate with standard variational inference and (b) it is not trivial to fully utilize sample-dependent information and correlation among dropout masks to improve posterior estimation. In this work, we propose GFlowOut to address these issues. GFlowOut leverages the recently proposed probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks. We empirically demonstrate that GFlowOut results in predictive distributions that generalize better to out-of-distribution data and provide uncertainty estimates which lead to better performance in downstream tasks."},
{"Title": "Learning GFlowNets from partial episodes for improved convergence and stability", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619387", "Full Abstract": "Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks. Existing training objectives for GFlowNets are either local to states or transitions, or propagate a reward signal over an entire sampling trajectory. We argue that these alternatives represent opposite ends of a gradient bias-variance tradeoff and propose a way to exploit this tradeoff to mitigate its harmful effects. Inspired by the TD(λ) algorithm in reinforcement learning, we introduce"},
{"Title": "Better training of GFlowNets with local credit and incomplete trajectories", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619527", "Full Abstract": "Generative Flow Networks or GFlowNets are related to Monte-Carlo Markov chain methods (as they sample from a distribution specified by an energy function), reinforcement learning (as they learn a policy to sample composed objects through a sequence of steps), generative models (as they learn to represent and sample from a distribution) and amortized variational methods (as they can be used to learn to approximate and sample from an otherwise intractable posterior, given a prior and a likelihood). They are trained to generate an object"},
{"Title": "Hyena hierarchy", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619572", "Full Abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WIKITEXT103 and THE PILE), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100× faster at sequence length 64K."},
{"Title": "Discrete key-value bottleneck", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619842", "Full Abstract": "Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to"},
{"Title": "Stochastic generative flow networks", "URL": "https://dl.acm.org/doi/10.5555/3625834.3625987", "Full Abstract": "Generative Flow Networks (or GFlowNets for short) are a family of probabilistic agents that learn to sample complex combinatorial structures through the lens of \"inference as control\". They have shown great potential in generating high-quality and diverse candidates from a given energy landscape. However existing GFlowNets can be applied only to deterministic environments, and fail in more general tasks with stochastic dynamics, which can limit their applicability. To overcome this challenge, this paper introduces Stochastic GFlowNets, a new algorithm that extends GFlowNets to stochastic environments. By decomposing state transitions into two steps, Stochastic GFlowNets isolate environmental stochasticity and learn a dynamics model to capture it. Extensive experimental results demonstrate that Stochastic GFlowNets offer significant advantages over standard GFlowNets as well as MCMC-and RL-based approaches, on a variety of standard benchmarks with stochastic dynamics."},
{"Title": "MixupE", "URL": "https://dl.acm.org/doi/10.5555/3625834.3626076", "Full Abstract": "Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels. This technique is known to improve the generalization performance in many learning paradigms and applications. In this work, we first analyze Mixup and show that it implicitly regularizes infinitely many directional derivatives of all orders. Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup. To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs. Our results show that the proposed method improves Mixup across multiple datasets using a variety of architectures, for instance, exhibiting an improvement over Mixup by 0.8% in ImageNet top-1 accuracy."},
{"Title": "Benchmarking graph neural networks", "URL": "https://dl.acm.org/doi/10.5555/3648699.3648742", "Full Abstract": "In the last few years, graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. This emerging field has witnessed an extensive growth of promising techniques that have been applied with success to computer science, mathematics, biology, physics and chemistry. But for any successful field to become mainstream and reliable, benchmarks must be developed to quantify progress. This led us in March 2020 to release a benchmark framework that i) comprises of a diverse collection of mathematical and real-world graphs, ii) enables fair model comparison with the same parameter budget to identify key architectures, iii) has an open-source, easy-to-use and reproducible code infrastructure, and iv) is flexible for researchers to experiment with new theoretical ideas. As of December 2022, the GitHub repository1 has reached 2,000 stars and 380 forks, which demonstrates the utility of the proposed open-source framework through the wide usage by the GNN community. In this paper, we present an updated version of our benchmark with a concise presentation of the aforementioned framework characteristics, an additional medium-sized molecular dataset AQSOL, similar to the popular ZINC, but with a real-world measured chemical target, and discuss how this framework can be leveraged to explore new GNN designs and insights. As a proof of value of our benchmark, we study the case of graph positional encoding (PE) in GNNs, which was introduced with this benchmark and has since spurred interest of exploring more powerful PE for Transformers and GNNs in a robust experimental setting."},
{"Title": "GFlowNet foundations", "URL": "https://dl.acm.org/doi/10.5555/3648699.3648909", "Full Abstract": "Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets, including a new local and efficient training objective called detailed balance for the analogy with MCMC. GFlowNets can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, continuous actions and modular energy functions."},
{"Title": "Controlled sparsity via constrained optimization or", "URL": "https://dl.acm.org/doi/10.5555/3600270.3600362", "Full Abstract": "The performance of trained neural networks is robust to harsh levels of pruning. Coupled with the ever-growing size of deep learning models, this observation has motivated extensive research on learning sparse models. In this work, we focus on the task of controlling the level of sparsity when performing sparse learning. Existing methods based on sparsity-inducing penalties involve expensive trial-and-error tuning of the penalty factor, thus lacking direct control of the resulting model sparsity. In response, we adopt a constrained formulation: using the gate mechanism proposed by Louizos et al. [31], we formulate a constrained optimization problem where sparsification is guided by the training objective and the desired sparsity target in an end-to-end fashion. Experiments on CIFAR-{10, 100, Tiny-ImageNet, and ImageNet using WideResNet and ResNet{18, 50 models validate the effectiveness of our proposal and demonstrate that we can reliably achieve pre-determined sparsity targets without compromising on predictive performance."},
{"Title": "Discrete factorial representations as an abstraction for goal conditioned RL", "URL": "https://dl.acm.org/doi/10.5555/3600270.3600551", "Full Abstract": "Goal-conditioned reinforcement learning (RL) is a promising direction for training agents that are capable of solving multiple tasks and reach a diverse set of objectives. How to"},
{"Title": "Trajectory balance", "URL": "https://dl.acm.org/doi/10.5555/3600270.3600701", "Full Abstract": "Generative flow networks (GFlowNets) are a method for learning a stochastic policy for generating compositional objects, such as graphs or strings, from a given unnormalized density by sequences of actions, where many possible action sequences may lead to the same object. We find previously proposed learning objectives for GFlowNets,"},
{"Title": "Neural attentive circuits", "URL": "https://dl.acm.org/doi/10.5555/3600270.3600832", "Full Abstract": "Recent work has seen the development of general purpose neural architectures that can be trained to perform tasks across diverse data modalities. General purpose models typically make few assumptions about the underlying data-structure and are known to perform well in the large-data regime. At the same time, there has been growing interest in modular neural architectures that represent the data using sparsely interacting modules. These models can be more robust out-of-distribution, computationally efficient, and capable of sample-efficient adaptation to new data. However, they tend to make domain-specific assumptions about the data, and present challenges in how module behavior (i.e., parameterization) and connectivity (i.e., their layout) can be jointly learned. In this work, we introduce a general purpose, yet modular neural architecture called Neural Attentive Circuits (NACs) that jointly learns the parameterization and a sparse connectivity of neural modules without using domain knowledge. NACs are best understood as the combination of two systems that are jointly trained end-to-end: one that determines the module configuration and the other that executes it on an input. We demonstrate qualitatively that NACs learn diverse and meaningful module configurations on the Natural Language and Visual Reasoning for Real (NLVR2) dataset without additional supervision. Quantitatively, we show that by incorporating modularity in this way, NACs improve upon a strong non-modular baseline in terms of low-shot adaptation on CIFAR and Caltech-UCSD Birds dataset (CUB) by about 10 percent, and OOD robustness on Tiny ImageNet-R by about 2.5 percent. Further, we find that NACs can achieve an 8x speedup at inference time while losing less than 3 percent performance. Finally, we find NACs to yield competitive results on diverse data modalities spanning point-cloud classification, symbolic processing and text-classification from ASCII bytes, thereby confirming its general purpose nature."},
{"Title": "Temporal latent bottleneck", "URL": "https://dl.acm.org/doi/10.5555/3600270.3601033", "Full Abstract": "Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of K time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of K time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks."},
{"Title": "Weakly supervised representation learning with sparse perturbations", "URL": "https://dl.acm.org/doi/10.5555/3600270.3601399", "Full Abstract": "The theory of representation learning aims to build methods that provably invert the data generating process with minimal domain knowledge or any source of supervision. Most prior approaches require strong distributional assumptions on the latent variables and weak supervision (auxiliary information such as timestamps) to provide provable identification guarantees. In this work, we show that if one has weak supervision from observations generated by sparse perturbations of the latent variables-e.g. images in a reinforcement learning environment where actions move individual sprites-identification is achievable under unknown continuous latent distributions. We show that if the perturbations are applied only on mutually exclusive blocks of latents, we identify the latents up to those blocks. We also show that if these perturbation blocks overlap, we identify latents up to the smallest blocks shared across perturbations. Consequently, if there are blocks that intersect in one latent variable only, then such latents are identified up to permutation and scaling. We propose a natural estimation procedure based on this theory and illustrate it on low-dimensional synthetic and image-based experiments."},
{"Title": "Online mixed packing and covering", "URL": "https://dl.acm.org/doi/10.5555/2627817.2627823", "Full Abstract": "Recent work has shown that the classical framework of solving optimization problems by obtaining a fractional solution to a linear program (LP) and rounding it to an integer solution can be extended to the online setting using primal-dual techniques. The success of this new framework for online optimization can be gauged from the fact that it has led to progress in several longstanding open questions. However, to the best of our knowledge, this framework has previously been applied to LPs containing only packing or only covering constraints, or minor variants of these. We extend this framework in a fundamental way by demonstrating that it can be used to solve mixed packing and covering LPs online, where packing constraints are given offline and covering constraints are received online. The objective is to minimize the maximum multiplicative factor by which any packing constraint is violated, while satisfying the covering constraints. Our results represent the first algorithm that obtains a polylogarithmic competitive ratio for solving mixed LPs online."},
{"Title": "The loss of serving in the dark", "URL": "https://dl.acm.org/doi/10.1145/2488608.2488729", "Full Abstract": "We study the following balls and bins stochastic process: There is a buffer with B bins, and there is a stream of balls X = {X"},
{"Title": "Tight bounds for online vector bin packing", "URL": "https://dl.acm.org/doi/10.1145/2488608.2488730", "Full Abstract": "In the d-dimensional bin packing problem (VBP), one is given vectors x"},
{"Title": "Cloud scheduling with setup cost", "URL": "https://dl.acm.org/doi/10.1145/2486159.2486195", "Full Abstract": "In this paper, we investigate the problem of online task scheduling of jobs such as MapReduce jobs, Monte Carlo simulations and generating search index from web documents, on cloud computing infrastructures. We consider the virtualized cloud computing setup comprising machines that host multiple identical virtual machines (VMs) under pay-as-you-go charging, and that booting a VM requires a constant setup time. The cost of job computation depends on the number of VMs activated, and the VMs can be activated and shutdown on demand. We propose a new bi-objective algorithm to minimize the maximum task delay, and the total cost of the computation. We study both the clairvoyant case, where the duration of each task is known upon its arrival, and the more realistic non-clairvoyant case."},
{"Title": "Sequential decision making with vector outcomes", "URL": "https://dl.acm.org/doi/10.1145/2554797.2554817", "Full Abstract": "We study a multi-round optimization setting in which in each round a player may select one of several actions, and each action produces an outcome"},
{"Title": "Co-Location-Resistant Clouds", "URL": "https://dl.acm.org/doi/10.1145/2664168.2664179", "Full Abstract": "We consider the problem of designing multi-tenant public infrastructure clouds resistant to cross-VM attacks without relying on single-tenancy or on assumptions about the cloud's servers. In a cross-VM attack (which have been demonstrated recently in Amazon EC2) an adversary launches malicious virtual machines (VM) that perform side-channel attacks against co-located VMs in order to recover their contents."},
{"Title": "Speed Scaling in the Non-clairvoyant Model", "URL": "https://dl.acm.org/doi/10.1145/2755573.2755582", "Full Abstract": "In recent years, there has been a growing interest in speed scaling algorithms, where a set of jobs need to be scheduled on a machine with variable speed so as to optimize the flow-times of the jobs and the energy consumed by the machine. A series of results have culminated in constant-competitive algorithms for this problem in the clairvoyant model, i.e., when job parameters are revealed on releasing a job (Bansal, Pruhs, and Stein, SODA 2007; Bansal, Chan, and Pruhs, SODA 2009). Our main contribution in this paper is the first constant-competitive speed scaling algorithm in the non-clairvoyant model, which is typically used in the scheduling literature to model practical settings where job volume is revealed only after the job has been completely processed. Unlike in the clairvoyant model, the speed scaling problem in the non-clairvoyant model is non-trivial"},
{"Title": "Optimal Coordination Mechanisms for Unrelated Machine Scheduling", "URL": "https://dl.acm.org/doi/10.5555/3215755.3215757", "Full Abstract": "We investigate the influence of different algorithmic choices on the approximation ratio in selfish scheduling. Our goal is to design local policies that minimize the inefficiency of resulting equilibria. In particular, we design optimal coordination mechanisms for unrelated machine scheduling, and improve the known approximation ratio from Îï"},
{"Title": "Truthful Mechanism Design via Correlated Tree Rounding", "URL": "https://dl.acm.org/doi/10.1145/2764468.2764503", "Full Abstract": "One of the most powerful algorithmic techniques for truthful mechanism design are maximal-in-distributional-range (MIDR) mechanisms. Unfortunately, many algorithms using this paradigm rely on heavy algorithmic machinery and require the ellipsoid method or (approximate) solution of convex programs. In this paper, we present a simple and natural correlated rounding technique for designing mechanisms that are truthful in expectation. Our technique is elementary and can be implemented quickly. The main property we rely on is that the domain offers fractional optimum solutions with a tree structure. In auctions based on the generalized assignment problem, each bidder has a publicly known knapsack constraint that captures the subsets of items that are of value to him. He has a private valuation for each item and strives to maximize the value of assigned items minus payment. For this domain we design a mechanism for social welfare maximization. Our technique gives a truthful 2-approximate MIDR mechanism without using the ellipsoid method or convex programming. In contrast to some previous work, our mechanism achieves exact truthfulness."},
{"Title": "Truthful Online Scheduling with Commitments", "URL": "https://dl.acm.org/doi/10.1145/2764468.2764535", "Full Abstract": "We study online mechanisms for preemptive scheduling with deadlines, with the goal of maximizing the total value of completed jobs. This problem is fundamental to deadline-aware cloud scheduling, but there are strong lower bounds even for the algorithmic problem without incentive constraints. However, these lower bounds can be circumvented under the natural assumption of deadline slackness, i.e., that there is a guaranteed lower bound s > 1 on the ratio between a job's size and the time window in which it can be executed. In this paper, we construct a truthful scheduling mechanism with a constant competitive ratio, given slackness s > 1. Furthermore, we show that if s is large enough then we can construct a mechanism that also satisfies a commitment property: it can be determined whether or not a job will finish, and the requisite payment if so, well in advance of each job's deadline. This is notable because, in practice, users with strict deadlines may find it unacceptable to discover only very close to their deadline that their job has been rejected."},
{"Title": "Make-to-order integrated scheduling and distribution", "URL": "https://dl.acm.org/doi/10.5555/2884435.2884446", "Full Abstract": "Production and distribution are fundamental operational functions in supply chains. The main challenge is to design algorithms that optimize operational performance by jointly scheduling production and delivery of customer orders. In this paper we study a model of scheduling customer orders on multiple identical machines and their distribution to customers afterwards. The goal is to minimize the total time from release to distribution plus total distribution cost to the customers. We design the first poly-logarithmic competitive algorithm for the problem, improving upon previous algorithms with linear competitive ratios. Our model generalizes two fundamental problems: scheduling of jobs on multiple identical machines (where the goal function is to minimize the total flow time) as well as the TCP Acknowledgment problem."},
{"Title": "Packing small vectors", "URL": "https://dl.acm.org/doi/10.5555/2884435.2884538", "Full Abstract": "Online"},
{"Title": "How to Allocate Goods in an Online Market?", "URL": "https://dl.acm.org/doi/10.1007/s00453-014-9964-7", "Full Abstract": "We study an online version of linear Fisher market. In this market there are $$m$$m buyers and a set of $$n$$n dividable goods to be allocated to the buyers. The utility that buyer $$i$$i derives from good $$j$$j is $$u_{ij$$uij. Given an allocation $$\\hat{U$$U^ in which buyer $$i$$i has utility $$\\hat{U_i$$U^i we study a quality measure that is based on taking an average of the ratios $$U_{i/\\hat{U_i$$Ui/U^i with respect to any other allocation $$U$$U. Market equilibrium allocation is the optimal solution with respect to this measure. Our setting is online and so the allocation of each good should be done without any knowledge of the upcoming goods. We design an online algorithm for the problem that is only worse by a logarithmic factor than any other solution with respect to this quality measure, and in particular competes with the market equilibrium allocation. We prove a tight lower bound which shows that our algorithm is optimal up to constants. Our algorithm uses a primal dual convex programming scheme. To the best of our knowledge this is the first time that such a scheme is used in the online framework."},
{"Title": "When Should an Expert Make a Prediction?", "URL": "https://dl.acm.org/doi/10.1145/2940716.2940729", "Full Abstract": "We consider a setting where in a known future time, a certain continuous random variable will be realized. There is a public prediction that gradually converges to its realized value, and an expert that has access to a more accurate prediction. Our goal is to study"},
{"Title": "Online lower bounds via duality", "URL": "https://dl.acm.org/doi/10.5555/3039686.3039752", "Full Abstract": "In this paper, we exploit linear programming duality in the online setting, where input arrives on the fly, from the unique perspective of designing lower bounds (i.e., hardness results) on the competitive ratio. In particular, we provide a systematic method (as opposed to ad hoc case analysis that is typically done) for obtaining online deterministic and randomized lower bounds on the competitive ratio for a wide variety of problems. We show the usefulness of our approach by providing new, tight hardness results for three diverse online problems: the Vector Bin Packing problem, Ad-auctions (and various online matching problems), and the Capital Investment problem. Our methods are sufficiently general that they can also be used to reconstruct existing lower bounds."},
{"Title": "Polylogarithmic bounds on the competitiveness of min-cost perfect matching with delays", "URL": "https://dl.acm.org/doi/10.5555/3039686.3039753", "Full Abstract": "We consider the problem of online Min-cost Perfect Matching with Delays (MPMD) recently introduced by Emek et al, (STOC 2016). This problem is defined on an underlying"},
{"Title": "Truthful mechanism design via correlated tree rounding", "URL": "https://dl.acm.org/doi/10.1007/s10107-016-1068-5", "Full Abstract": "A powerful algorithmic technique for truthful mechanism design is the maximal-in-distributional-range (MIDR) paradigm. Unfortunately, many such algorithms use heavy algorithmic machinery, e.g., the ellipsoid method and (approximate) solution of convex programs. In this paper, we present a correlated rounding technique for designing mechanisms that are truthful in expectation. It is elementary and can be implemented quickly. The main property we rely on is that the domain offers fractional optimum solutions with a tree structure. In auctions based on the generalized assignment problem, each bidder has a publicly known knapsack constraint that captures the subsets of items that are of value to him. He has a private valuation for each item and strives to maximize the value of assigned items minus payment. For this domain we design a truthful 2-approximate MIDR mechanism for social welfare maximization. It avoids using the ellipsoid method or convex programming. In contrast to some previous work, our mechanism achieves exact truthfulness. In restricted-related scheduling with selfish machines, each job comes with a public weight, and it must be assigned to a machine from a public job-specific subset. Each machine has a private speed and strives to maximize payments minus workload of jobs assigned to it. Here we design a mechanism for makespan minimization. This is a single-parameter domain, but the approximation status of the optimization problem is similar to unrelated machine scheduling: The best known algorithm obtains a (non-truthful) 2-approximation for unrelated machines, and there is 1.5-hardness. Our mechanism matches this bound with a truthful 2-approximation."},
{"Title": "Online service with delay", "URL": "https://dl.acm.org/doi/10.1145/3055399.3055475", "Full Abstract": "In this paper, we introduce the"},
{"Title": "Tight Bounds for Clairvoyant Dynamic Bin Packing", "URL": "https://dl.acm.org/doi/10.1145/3087556.3087570", "Full Abstract": "In this paper we focus on the Clairvoyant Dynamic Bin Packing (DBP) problem, which extends the classical online bin packing problem in that items arrive and depart over time and the departure time of an item is known upon its arrival. The problem naturally arises when handling cloud-based networks. We focus specifically on the MinUsageTime cost function which aims to minimize the overall usage time of all bins that are opened during the packing process. Earlier work has shown a O(\\frac{\\log \\mu{\\log \\log \\mu) upper bound where \\mu is defined as the ratio between the maximal and minimal durations of all items. We improve the upper bound by giving an O(\\sqrt{\\log \\mu)-competitive algorithm. We then provide a matching lower bound of \\Omega(\\sqrt{\\log \\mu) on the competitive ratio of any online algorithm, thus closing the gap with regards to this problem. We then focus on what we call the class of aligned inputs and give a O(\\log \\log \\mu)-competitive algorithm for this case, beating the lower bound of the general case by an exponential factor. Surprisingly enough, the analysis of our algorithm that we present, is closely related to various properties of binary strings."},
{"Title": "On secure and pseudonymous client-relationships with multiple servers", "URL": "https://dl.acm.org/doi/10.1145/330382.330386", "Full Abstract": "This paper introduces a cryptographic engine, Janus, which assists clients in establishing and maintaining secure and pseudonymous relationships with multiple servers. The setting is such that clients reside on a particular subnet (e.g., corporate intranet, ISP) and the servers reside anywhere on the Internet. The Janus engine allows each client-server relationship to use either weak or strong authentication on each interaction. At the same time, each interaction preserves privacy by neither revealing a clients true identity (except for the subnet) nor the set of servers with which a particular client interacts. Furthermore, clients do not need any secure long-term memory, enabling scalability and mobility. The interaction model extends to allow servers to send data back to clients via e-mail at a later date. Hence, our results complement the functionality of current network anonymity tools and remailers. The paper also describes the design and implementation of the Lucent Personalized Web Assistant (LPWA), which is a practical system that provides secure and pseudonymous relations with multiple servers on the Internet. LPWA employs the Janus function to generate site-specific personæ, which consist of alias usernames, passwords, and e-mail addresses."},
{"Title": "Synopsis data structures for massive data sets", "URL": "https://dl.acm.org/doi/10.5555/327766.327776", "Full Abstract": "No abstract available."},
{"Title": "On the temporal HZY compression scheme", "URL": "https://dl.acm.org/doi/10.5555/338219.338250", "Full Abstract": "No abstract available."},
{"Title": "Efficient bundle sorting", "URL": "https://dl.acm.org/doi/10.5555/338219.338647", "Full Abstract": "Abstract not found or failed to load."},
{"Title": "Dynamic Maintenance of Wavelet-Based Histograms", "URL": "https://dl.acm.org/doi/10.5555/645926.672011", "Full Abstract": "No abstract available."},
{"Title": "Placing search in context", "URL": "https://dl.acm.org/doi/10.1145/371920.372094", "Full Abstract": "Copyright © 2001 ACM."},
{"Title": "The Effect of Flexible Parsing for Dynamic Dictionary-Based Data Compression", "URL": "https://dl.acm.org/doi/10.1145/945394.945404", "Full Abstract": "We report on the performance evaluation of greedy parsing with a single step lookahead (which we call flexible Parsing or"},
{"Title": "Online Subpath Profiling", "URL": "https://dl.acm.org/doi/10.5555/647478.727934", "Full Abstract": "We present an efficient online subpath profiling algorithm, OSP, that reports hot subpaths executed by a program in a given run. The hot subpaths can start at arbitrary basic block boundaries, and their identification is important for code optimization; e.g., to locate program traces in which optimizations could be most fruitful, and to help programmers in identifying performance bottlenecks.The OSP algorithm is online in the sense that it reports at any point during execution the hot subpaths as observed so far. It has very low memory and runtime overheads, and exhibits high accuracy in reports for benchmarks such as JLex and FFT. These features make the OSP algorithm potentially attractive for use in just-in-time (JIT) optimizing compilers, in which profiling performance is crucial and it is useful to locate hot subpaths as early as possible.The OSP algorithm is based on an adaptive sampling technique that makes effective utilization of memory with small overhead. Both memory and runtime overheads can be controlled, and the OSP algorithm can therefore be used for arbitrarily large applications, realizing a tradeoff between report accuracy and performance.We have implemented a Java prototype of the OSP algorithm for Java programs. The implementation was tested on programs from the Java Grande benchmark suite and exhibited a low average runtime overhead."},
{"Title": "Tracking Join and Self-Join Sizes in Limited Storage", "URL": "https://dl.acm.org/doi/10.1006/jcss.2001.1813", "Full Abstract": "This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we consider two approaches proposed in N. Alon et al. (The space complexity of approximating the frequency moments, J. Comput. System Sci.58 (1999), 137 147), which we denote tug-of-war and sample-count . We present fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the first experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4-256 memory words, depending on the data set, in order to estimate the self-join size to within a 15% relative error; on average, this is over 4 times (50 times) fewer memory words than needed by sample-count (standard sampling, resp.) to obtain a similar accuracy. For joins, we propose schemes based on maintaining a small signature of each relation independently, such that join sizes can be quickly and accurately estimated between any pair of relations using only these signatures. We show that taking random samples for join signatures can lead to an inaccurate estimation unless the sample size is quite large; moreover, we show that no other signature scheme can significantly improve upon sampling without further assumptions. These negative results are shown to hold even in the presence of sanity bounds. On the other hand, we present a fast join signature scheme based on tug-of-war signatures that provides guarantees on join size estimation as a function of the self-join sizes of the joining relations; this scheme can significantly improve upon the sampling scheme."},
{"Title": "Fast incremental maintenance of approximate histograms", "URL": "https://dl.acm.org/doi/10.1145/581751.581753", "Full Abstract": "Many commercial database systems maintain histograms to summarize the contents of large relations and permit efficient estimation of query result sizes for use in query optimizers. Delaying the propagation of database updates to the histogram often introduces errors into the estimation. This article presents new sampling-based approaches for"},
{"Title": "Spectral bloom filters", "URL": "https://dl.acm.org/doi/10.1145/872757.872787", "Full Abstract": "A Bloom Filter is a space-efficient randomized data structure allowing membership queries over sets with certain allowable errors. It is widely used in many applications which take advantage of its ability to compactly represent a set, and filter out effectively any element that does not belong to the set, with small error probability. This paper introduces the Spectral Bloom Filter (SBF), an extension of the original Bloom Filter to multi-sets, allowing the filtering of elements whose multiplicities are below a threshold given at query time. Using memory only slightly larger than that of the original Bloom Filter, the SBF supports queries on the multiplicities of individual keys with a guaranteed, small error probability. The SBF also supports insertions and deletions over the data set. We present novel methods for reducing the probability and magnitude of errors. We also present an efficient data structure and algorithms to build it incrementally and maintain it over streaming data, as well as over materialized data with arbitrary insertions and deletions. The SBF does not assume any a priori filtering threshold and effectively and efficiently maintains information over the entire data-set, allowing for ad-hoc queries with arbitrary parameters and enabling a range of new applications."},
{"Title": "Efficient pebbling for list traversal synopses", "URL": "https://dl.acm.org/doi/10.5555/1759210.1759299", "Full Abstract": "We show how to support efficient back traversal in a unidirectional list, using small memory and with essentially no slowdown in forward steps. Using"},
{"Title": "\"-Synopses", "URL": "https://dl.acm.org/doi/10.5555/977401.978153", "Full Abstract": "No abstract available."},
{"Title": "Optimal workload-based weighted wavelet synopses", "URL": "https://dl.acm.org/doi/10.1007/978-3-540-30570-5_25", "Full Abstract": "In recent years wavelets were shown to be effective data synopses. We are concerned with the problem of finding efficiently wavelet synopses for massive data sets, in situations where information about query workload is available. We present linear time, I/O optimal algorithms for building optimal workload-based wavelet synopses for point queries. The synopses are based on a novel construction of weighted inner-products and use weighted wavelets that are adapted to those products. The synopses are optimal in the sense that the subset of retained coefficients is the best possible for the bases in use with respect to either the mean-squared absolute or relative errors. For the latter, this is the first optimal wavelet synopsis even for the regular, non-workload-based case. Experimental results demonstrate the advantage obtained by the new optimal wavelet synopses, as well as the robustness of the synopses to deviations in the actual query workload."},
{"Title": "Data streams and data synopses for massive data sets", "URL": "https://dl.acm.org/doi/10.1007/11564096_6", "Full Abstract": "With the proliferation of data intensive applications, it has become necessary to develop new techniques to handle massive data sets. Traditional algorithmic techniques and data structures are not always suitable to handle the amount of data that is required and the fact that the data often streams by and cannot be accessed again. A field of research established over the past decade is that of handling massive data sets using data synopses, and developing algorithmic techniques for data stream models. We will discuss some of the research work that has been done in the field, and provide a decades' perspective to data synopses and data streams."},
{"Title": "Data streams and data synopses for massive data sets", "URL": "https://dl.acm.org/doi/10.5555/3121445.3121452", "Full Abstract": "With the proliferation of data intensive applications, it has become necessary to develop new techniques to handle massive data sets. Traditional algorithmic techniques and data structures are not always suitable to handle the amount of data that is required and the fact that the data often streams by and cannot be accessed again. A field of research established over the past decade is that of handling massive data sets using data synopses, and developing algorithmic techniques for data stream models. We will discuss some of the research work that has been done in the field, and provide a decades' perspective to data synopses and data streams."},
{"Title": "The design and architecture of the τ-synopses system", "URL": "https://dl.acm.org/doi/10.1007/11687238_66", "Full Abstract": "Data synopses are concise representations of data sets, that enable effective processing of approximate queries to the data sets. The"},
{"Title": "Synopses reconciliation via calibration in the τ-synopses system", "URL": "https://dl.acm.org/doi/10.1007/11687238_77", "Full Abstract": "The"},
{"Title": "Trends in high performance analytics", "URL": "https://dl.acm.org/doi/10.1145/1142473.1142559", "Full Abstract": "With the proliferation of analytic and business intelligence applications, and with the persistent growth in data sizes, there is an ever increasing need to support high performance analytics. This talk will present recent technological trends in addressing this need, and will particularly highlight the approach of facilitating high performance analytics in a relational database via a novel dichotomous combination with a non-relational aggregation-server."},
{"Title": "τ-xSynopses - a system for run-time management of XML synopses", "URL": "https://dl.acm.org/doi/10.1007/11780991_34", "Full Abstract": "We introduce the"},
{"Title": "Computing curricula 1991", "URL": "https://dl.acm.org/doi/book/10.1145/126633", "Full Abstract": "No abstract available."},
{"Title": "Flexible Design", "URL": "https://dl.acm.org/doi/10.1109/2.116851", "Full Abstract": "A summary is given of Computing Curricula 1991, which provides curricular guidance for implementing undergraduate programs to faculties of all institutions that offer concentration in computing. Program goals and graduate profiles are discussed. Underlying principles for curriculum design are examined. The implementation of these principles is considered."},
{"Title": "Fundamentals of computing I", "URL": "https://dl.acm.org/doi/book/10.5555/129102", "Full Abstract": "No abstract available."},
{"Title": "Fundamentals of Computing II", "URL": "https://dl.acm.org/doi/book/10.5555/573152", "Full Abstract": "No abstract available."},
{"Title": "Introducing object-orientedness into a breadth-first introductory curriculum", "URL": "https://dl.acm.org/doi/10.1145/157710.157835", "Full Abstract": "Copyright © 1992 ACM."},
{"Title": "Pascal laboratory manual: fundamentals of computing I (Pascal ed. revised)", "URL": "https://dl.acm.org/doi/book/10.5555/174559", "Full Abstract": "No abstract available."},
{"Title": "Fundamentals of computing I (Pascal ed. revised)", "URL": "https://dl.acm.org/doi/book/10.5555/174560", "Full Abstract": "No abstract available."},
{"Title": "New directions in the introductory computer science curriculum", "URL": "https://dl.acm.org/doi/10.1145/191029.191038", "Full Abstract": "Copyright © 1994 ACM."},
{"Title": "Class testing the breadth-first curriculum (abstract)", "URL": "https://dl.acm.org/doi/10.1145/191029.191214", "Full Abstract": "No abstract available."},
{"Title": "Fundamentals of Computing II", "URL": "https://dl.acm.org/doi/book/10.5555/560998", "Full Abstract": "From the Publisher:"},
{"Title": "The role of ACM's Computing Surveys (panel)", "URL": "https://dl.acm.org/doi/10.1145/259526.277432", "Full Abstract": "No abstract available."},
{"Title": "Fundamentals of Computing I", "URL": "https://dl.acm.org/doi/book/10.5555/560306", "Full Abstract": "From the Publisher:"},
{"Title": "Visions of breadth in introductory computing curricula (abstract)", "URL": "https://dl.acm.org/doi/10.1145/199688.199893", "Full Abstract": "No abstract available."},
{"Title": "The first-course conundrum", "URL": "https://dl.acm.org/doi/10.1145/203241.203266", "Full Abstract": "Recently, the College Entrance Examination Board (CEEB) has decided to redesign the Advanced Placement (AP) examination in computer science (CS) so that AP courses will be forced to switch from Pascal to C++ starting around 1998."},
{"Title": "Strategic directions in computer science education", "URL": "https://dl.acm.org/doi/10.1145/242223.246876", "Full Abstract": "Copyright © 1996 ACM."},
{"Title": "Crisis in computer science education", "URL": "https://dl.acm.org/doi/10.1145/242224.242359", "Full Abstract": "No abstract available."},
{"Title": "Strategic directions in computer science education (panel)", "URL": "https://dl.acm.org/doi/10.1145/268084.268228", "Full Abstract": "This panel will discuss major issues and challenges in computer science education across a wide range of institutions. It originates from a report developed by the Education Working Group of the Strategic Directions in Computing Research (SDCR) Workshop. That report appears in its entirety in the"},
{"Title": "The crisis in academic hiring in computer science", "URL": "https://dl.acm.org/doi/10.1145/299649.299814", "Full Abstract": "No abstract available."},
{"Title": "Has our curriculum become math-phobic? (an American perspective)", "URL": "https://dl.acm.org/doi/10.1145/343048.343143", "Full Abstract": "We are concerned about a view in undergraduate computer science education, especially in the early courses, that it's okay to be math-phobic and still prepare oneself to become a computer scientist. Our view is the contrary: that any serious study of computer science requires students to achieve mathematical maturity (especially in discrete mathematics) early in their undergraduate studies, thus becoming well-prepared to integrate mathematical ideas, notations, and methodologies throughout their study of computer science. A major curricular implication of this theme is that the prerequisite expectations and conceptual level of the first discrete mathematics course should be the same as it is for the first calculus course --- secondary school pre-calculus and trigonometry. Ultimately, calculus, linear algebra, and statistics are also essential for computer science majors, but none should occur earlier than discrete mathematics. This paper explains our concerns and outlines our response as a series of examples and recommendations for future action."},
{"Title": "Our curriculum has become math-phobic!", "URL": "https://dl.acm.org/doi/10.1145/364447.364593", "Full Abstract": "The paper [2] argued that mathematical ideas play an important role in the computer science curriculum, and that Discrete Mathematics needs to be taught early in the computer science curriculum. In this follow-up paper, we present evidence that computer science curricula are drifting away from a fundamental commitment to theoretical and mathematical ideas. We propose some actions that can be taken to help reverse this drift."},
{"Title": "Multiphysics simulations", "URL": "https://dl.acm.org/doi/10.1177/1094342012468181", "Full Abstract": "We consider multiphysics applications from algorithmic and architectural perspectives, where â algorithmicâ includes both mathematical analysis and computational complexity, and â architecturalâ includes both software and hardware environments. Many diverse multiphysics applications can be reduced, en route to their computational simulation, to a common algebraic coupling paradigm. Mathematical analysis of multiphysics coupling in this form is not always practical for realistic applications, but model problems representative of applications discussed herein can provide insight. A variety of software frameworks for multiphysics applications have been constructed and refined within disciplinary communities and executed on leading-edge computer systems. We examine several of these, expose some commonalities among them, and attempt to extrapolate best practices to future systems. From our study, we summarize challenges and forecast opportunities."},
{"Title": "Massively Parallel Model of Extended Memory Use in Evolutionary Game Dynamics", "URL": "https://dl.acm.org/doi/10.1109/IPDPS.2013.102", "Full Abstract": "To study the emergence of cooperative behavior, we have developed a scalable parallel framework for evolutionary game dynamics. This is a critical computational tool enabling large-scale agent simulation research. An important aspect is the amount of history, or memory steps, that each agent can keep. When six memory steps are taken into account, the strategy space spans 24096 potential strategies, requiring large populations of agents. We introduce a multi-level decomposition method that allows us to exploit both multi-node and thread-level parallel scaling while minimizing communication overhead. We present the results of a production run modeling up to six memory steps for populations consisting of up to 10^18 agents, making this study one of the largest yet undertaken. The high rate of mutation within the population results in a non-trivial parallel implementation. The strong and weak scaling studies provide insight into parallel scalability and programmability trade-offs for large-scale simulations, while exhibiting near perfect weak and strong scaling on 16,384 tasks on Blue Gene/Q. We further show 99% weak scaling up to 294,912 processors 82% strong scaling efficiency up to 262,144 processors of Blue Gene/P. Our framework marks an important step in the study of game dynamics with potential applications in fields ranging from biology to economics and sociology."},
{"Title": "Performance Analysis of the Lattice Boltzmann Model Beyond Navier-Stokes", "URL": "https://dl.acm.org/doi/10.1109/IPDPS.2013.109", "Full Abstract": "The lattice Boltzmann method is increasingly important in facilitating large-scale fluid dynamics simulations. To date, these simulations have been built on discretized velocity models of up to 27 neighbors. Recent work has shown that higher order approximations of the continuum Boltzmann equation enable not only recovery of the Navier-Stokes hydro-dynamics, but also simulations for a wider range of Knudsen numbers, which is especially important in micro- and nano-scale flows. These higher-order models have significant impact on both the communication and computational complexity of the application. We present a performance study of the higher-order models as compared to the traditional ones, on both the IBM Blue Gene/P and Blue Gene/Q architectures. We study the tradeoffs of many optimizations methods such as the use of deep halo level ghost cells that, alongside hybrid programming models, reduce the impact of extended models and enable efficient modeling of extreme regimes of computational fluid dynamics."},
{"Title": "A Spatio-temporal Coupling Method to Reduce the Time-to-Solution of Cardiovascular Simulations", "URL": "https://dl.acm.org/doi/10.1109/IPDPS.2014.68", "Full Abstract": "We present a new parallel-in-time method designed to reduce the overall time-to-solution of a patient-specific cardiovascular flow simulation. Using a modified Para real algorithm, our approach extends strong scalability beyond spatial parallelism with fully controllable accuracy and no decrease in stability. We discuss the coupling of spatial and temporal domain decompositions used in our implementation, and showcase the use of the method on a study of blood flow through the aorta. We observe an additional 40% reduction in overall wall clock time with no significant loss of accuracy, in agreement with a predictive performance model."},
{"Title": "MIC-SVM", "URL": "https://dl.acm.org/doi/10.1109/IPDPS.2014.88", "Full Abstract": "Support Vector Machine (SVM) has been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design. To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools. MIC-SVM achieves 4.4-84x and 18-47x speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, run on a top of the line NVIDIA k20x GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns."},
{"Title": "Locality-Optimized Mixed Static/Dynamic Scheduling for Improving Load Balancing on SMPs", "URL": "https://dl.acm.org/doi/10.1145/2642769.2642788", "Full Abstract": "Application performance can be degraded significantly due to node-local load imbalances during application execution. Prior work suggested using a mixed static/dynamic scheduling approach for handling this problem, specifically in the context of fine-grained, transient load imbalances. Here, we consider an alternate strategy for more general load imbalances where fine-grained, transient load imbalance may be coupled with coarse-grained load imbalance. Specifically, we implement a scheduling scheme in which we modify the data layout in mixed static/dynamic scheduling, and add an additional tuned constraint in the dequeue function of our scheduler. Through experimentation of an n-body particle simulation code on modern multi-core architectures, our technique gives a 19.4% performance gain over dynamic scheduling, and an overall 48.6% performance gain over standard static scheduling."},
{"Title": "Scaling Support Vector Machines on modern HPC platforms", "URL": "https://dl.acm.org/doi/10.1016/j.jpdc.2014.09.005", "Full Abstract": "Support Vector Machines (SVM) have been widely used in data-mining and Big Data applications as modern commercial databases start to attach an increasing importance to the analytic capabilities. In recent years, SVM was adapted to the field of High Performance Computing for power/performance prediction, auto-tuning, and runtime scheduling. However, even at the risk of losing prediction accuracy due to insufficient runtime information, researchers can only afford to apply offline model training to avoid significant runtime training overhead. Advanced multi- and many-core architectures offer massive parallelism with complex memory hierarchies which can make runtime training possible, but form a barrier to efficient parallel SVM design.To address the challenges above, we designed and implemented MIC-SVM, a highly efficient parallel SVM for x86 based multi-core and many-core architectures, such as the Intel Ivy Bridge CPUs and Intel Xeon Phi co-processor (MIC). We propose various novel analysis methods and optimization techniques to fully utilize the multilevel parallelism provided by these architectures and serve as general optimization methods for other machine learning tools.MIC-SVM achieves 4.4-84í and 18-47í speedups against the popular LIBSVM, on MIC and Ivy Bridge CPUs respectively, for several real-world data-mining datasets. Even compared with GPUSVM, running on the NVIDIA k20x GPU, the performance of our MIC-SVM is competitive. We also conduct a cross-platform performance comparison analysis, focusing on Ivy Bridge CPUs, MIC and GPUs, and provide insights on how to select the most suitable advanced architectures for specific algorithms and input data patterns. An efficient parallel support vector machine for x86 based multi-core platforms.The novel optimization techniques to fully utilize the multi-level parallelism.The improvement for the deficiencies of the current SVM tools.Select the best architectures for input data patterns to achieve best performance.The large-scale distributed algorithm and power-efficient approach."},
{"Title": "Massively parallel models of the human circulatory system", "URL": "https://dl.acm.org/doi/10.1145/2807591.2807676", "Full Abstract": "The potential impact of blood flow simulations on the diagnosis and treatment of patients suffering from vascular disease is tremendous. Empowering models of the full arterial tree can provide insight into diseases such as arterial hypertension and enables the study of the influence of local factors on global hemodynamics. We present a new, highly scalable implementation of the lattice Boltzmann method which addresses key challenges such as multiscale coupling, limited memory capacity and bandwidth, and robust load balancing in complex geometries. We demonstrate the strong scaling of a three-dimensional, high-resolution simulation of hemodynamics in the systemic arterial tree on 1,572,864 cores of Blue Gene/Q. Faster calculation of flow in full arterial networks enables unprecedented risk stratification on a perpatient basis. In pursuit of this goal, we have introduced computational advances that significantly reduce time-to-solution for biofluidic simulations."},
{"Title": "A Computational Framework to Assess the Influence of Changes in Vascular Geometry on Blood Flow", "URL": "https://dl.acm.org/doi/10.1145/3093172.3093227", "Full Abstract": "Many vascular abnormalities, such as aneurysms or stenoses, develop gradually over time. In the early stages of their development, they require monitoring but do not pose sufficient risk to the patient for a clinician to recommend invasive treatment. With a better understanding of the interplay between hemodynamic factors and changes in blood vessel geometry, there is an opportunity to improve clinical care by earlier identification of aneurysms or stenoses with significant potential for further development. Computational fluid dynamics has shown great promise for investigating this interplay and identifying the associated underlying mechanisms, by using patient-derived geometries and modifying them to represent potential evolution of the vascular disease. However, a general, extensible framework for comparing simulation results from different vascular geometries in a direct, quantitative manner does not currently exist. As a first step toward the development of such a framework, we present a method for quantifying the relationship between changes in vascular geometry and hemodynamic factors, such as wall shear stress. We apply this framework to study the correlation between wall shear stress and geometric changes in two opposite settings: when flow properties are associated with consequent changes in the vascular geometry, as in a thoracic aortic aneurysm, and when geometric changes alter the flow, as in a worsening aortic stenosis."},
{"Title": "Immersed Boundary Method Halo Exchange in a Hemodynamics Application", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-22734-0_32", "Full Abstract": "In recent years, highly parallelized simulations of blood flow resolving individual blood cells have been demonstrated. Simulating such dense suspensions of deformable particles in flow often involves a partitioned fluid-structure interaction (FSI) algorithm, with separate solvers for Eulerian fluid and Lagrangian cell grids, plus a solver - e.g., immersed boundary method - for their interaction. Managing data motion in parallel FSI implementations is increasingly important, particularly for inhomogeneous systems like vascular geometries. In this study, we evaluate the influence of Eulerian and Lagrangian halo exchanges on efficiency and scalability of a partitioned FSI algorithm for blood flow. We describe an MPI+OpenMP implementation of the immersed boundary method coupled with lattice Boltzmann and finite element methods. We consider how communication and recomputation costs influence the optimization of halo exchanges with respect to three factors: immersed boundary interaction distance, cell suspension density, and relative fluid/cell solver costs."},
{"Title": "Performance portability study for massively parallel computational fluid dynamics application on scalable heterogeneous architectures", "URL": "https://dl.acm.org/doi/10.1016/j.jpdc.2019.02.005", "Full Abstract": "Patient-specific hemodynamic simulations have the potential to greatly improve both the diagnosis and treatment of a variety of vascular diseases. Portability will enable wider adoption of computational fluid dynamics (CFD) applications in the biomedical research community and targeting to platforms ideally suited to different vascular regions. In this work, we present a case study in performance portability that assesses (1) the ease of porting an MPI application optimized for one specific architecture to new platforms using variants of hybrid MPI＋X programming models; (2) performance portability seen when simulating blood flow in three different vascular regions on diverse heterogeneous architectures; (3) model-based performance prediction for future architectures; and (4) performance scaling of the hybrid MPI＋X programming on parallel heterogeneous systems. We discuss the lessons learned in porting HARVEY, a massively parallel CFD application, from traditional multicore CPUs to diverse heterogeneous architectures ranging from NVIDIA/AMD GPUs to Intel MICs and Altera FPGAs."},
{"Title": "Combating link dynamics for reliable lora connection in urban settings", "URL": "https://dl.acm.org/doi/10.1145/3447993.3483250", "Full Abstract": "LoRa, as a representative Low-Power Wide-Area Network (LPWAN) technology, can provide long-range communication for battery-powered IoT devices with a 10-year lifetime. LoRa links in practice, however, experience high dynamics in various environments. When the SNR falls below the threshold (e.g., in the building), a LoRa device disconnects from the network. We propose Falcon, which addresses the link dynamics by enabling data transmission for very low SNR or even disconnected LoRa links. At the heart of Falcon, we reveal that low SNR LoRa links that cannot deliver packets can still introduce interference to other LoRa transmissions. Therefore, Falcon transmits data bits on the low SNR link by selectively interfering with other LoRa transmissions. We address practical challenges in Falcon design. We propose a low-power channel activity detection method to detect other LoRa transmissions for selective interference. To interfere with the so-called interference-resilient LoRa, we accurately estimate the time and frequency offsets on LoRa packets and propose an adaptive frequency adjusting strategy to maximize the interference. We implement Falcon, all using commercial off-the-shelf LoRa devices, and extensively evaluate its performance. The results show that Falcon can provide reliable communication links for disconnected LoRa devices and achieves the SNR boundary upto 7.5 dB lower than that of standard LoRa."},
{"Title": "Efficient Ambient LoRa Backscatter With On-Off Keying Modulation", "URL": "https://dl.acm.org/doi/10.1109/TNET.2021.3121787", "Full Abstract": "Backscatter communication holds potential for ubiquitous and low-cost connectivity among low-power IoT devices. To avoid interference between the carrier signal and the backscatter signal, recent works propose a frequency-shifting technique to separate these two signals in the frequency domain. Such proposals, however, have to occupy the precious wireless spectrum that is already overcrowded, and increase the power, cost, and complexity of the backscatter tag. In this paper, we revisit the classic ON-OFF Keying (OOK) modulation and propose Aloba, a backscatter system that takes the ambient LoRa transmissions as the excitation and piggybacks the in- band OOK modulated signals over the LoRa transmissions. Our design enables the backscatter signal to work in the same frequency band of the carrier signal, meanwhile achieving flexible data rate at different transmission range. The key contributions of Aloba include: i) the design of a low-power backscatter tag that can pick up the ambient LoRa signals from other signals; ii) a novel decoding algorithm to demodulate both the carrier signal and the backscatter signal from their superposition. We further adopt link coding mechanism and interleave operation to enhance the reliability of backscatter signal decoding. We implement Aloba and conduct head-to-head comparison with the state-of-the-art LoRa backscatter system PLoRa in various settings. The experiment results show Aloba can achieve 39.5&#x2013;199.4 Kbps data rate at various distances, 10.4&#x2013;<inline-formula> <tex-math notation=\"LaTeX\">$52.4\\times $ </tex-math></inline-formula> higher than PLoRa."},
{"Title": "NELoRa", "URL": "https://dl.acm.org/doi/10.1145/3485730.3485928", "Full Abstract": "Low-Power Wide-Area Networks (LPWANs) are an emerging Internet-of-Things (IoT) paradigm marked by low-power and long-distance communication. Among them, LoRa is widely deployed for its unique characteristics and open-source technology. By adopting the Chirp Spread Spectrum (CSS) modulation, LoRa enables low signal-to-noise ratio (SNR) communication. However, the standard demodulation method does not fully exploit the properties of chirp signals, thus yields a sub-optimal SNR threshold under which the decoding fails. Consequently, the communication range and energy consumption have to be compromised for robust transmission. This paper presents NELoRa, a neural-enhanced LoRa demodulation method, exploiting the feature abstraction ability of deep learning to support ultra-low SNR LoRa communication. Taking the spectrogram of both amplitude and phase as input, we first design a mask-enabled Deep Neural Network (DNN) filter that extracts multi-dimension features to capture clean chirp symbols. Second, we develop a spectrogram-based DNN decoder to decode these chirp symbols accurately. Finally, we propose a generic packet demodulation system by incorporating a method that generates high-quality chirp symbols from received signals. We implement and evaluate NELoRa on both indoor and campus-scale outdoor testbeds. The results show that NELoRa achieves 1.84-2.35 dB SNR gains and extends the battery life up to 272% (~0.38-1.51 years) in average for various LoRa configurations."},
{"Title": "Combating Packet Collisions Using Non-Stationary Signal Scaling in LPWANs", "URL": "https://dl.acm.org/doi/10.1109/TNET.2021.3131704", "Full Abstract": "LoRa, a representative Low-Power Wide Area Network (LPWAN) technology, has been shown as a promising platform to connect Internet of Things. Practical LoRa deployments, however, suffer from collisions, especially in dense networks and wide coverage areas expected by LoRa applications. Existing collision resolving approaches do not exploit the modulation properties of LoRa and thus cannot work well for low-SNR LoRa signals. We propose <italic>NScale</italic> to decompose concurrent transmissions by leveraging subtle inter-packet time offsets for low SNR LoRa collisions. NScale (1) translates subtle time offsets, which are vulnerable to noise, to robust frequency features, and (2) further amplifies the time offsets by non-stationary signal scaling, i.e., scaling the amplitude of a symbol differently at different positions. In practical implementation, we propose a noise resistant iterative symbol recovery method to combat symbol distortion in low SNR, and address frequency shifts incurred by CFO and packet time offsets in decoding. We propose optimized designs for diminishing the time costs of computation-intensive tasks and meeting the real-time requirements of LoRa collision resolving. We theoretically show that NScale introduces &#x003C; 1.7 dB SNR loss compared with the original LoRa. We implement NScale on USRP N210 and evaluate its performance in both indoor and outdoor networks. NScale is implemented in software at the gateway and can work for COTS LoRa nodes without any modification. The evaluation results show that NScale improves the network throughput by <inline-formula> <tex-math notation=\"LaTeX\">$3.3\\times $ </tex-math></inline-formula> for low SNR collided signals compared with other state-of-the-art methods."},
{"Title": "MotorBeat", "URL": "https://dl.acm.org/doi/10.1145/3517255", "Full Abstract": "More and more home appliances are now connected to the Internet, thus enabling various smart home applications. However, a critical problem that may impede the further development of smart home is overlooked: Small appliances account for the majority of home appliances, but they receive little attention and most of them are cut off from the Internet. To fill this gap, we propose MotorBeat, an acoustic communication approach that connects small appliances to a smart speaker. Our key idea is to exploit direct current (DC) motors, which are common components of small appliances, to transmit acoustic messages. We design a novel scheme named Variable Pulse Width Modulation (V-PWM) to drive DC motors. MotorBeat achieves the following 3C goals: (1) Comfortable to hear, (2) Compatible with multiple motor modes, and (3) Concurrent transmission. We implement MotorBeat with commercial devices and evaluate its performance on three small appliances and ten DC motors. The results show that the communication range can be up to 10 m."},
{"Title": "LSync: A Universal Event-synchronizing Solution for Live Streaming", "URL": "https://dl.acm.org/doi/10.1109/INFOCOM48880.2022.9796933", "Full Abstract": "The widespread of smart devices and the development of mobile networks brings the growing popularity of live streaming services worldwide. In addition to the video and audio transmission, a lot more media content is sent to the audiences as well, including player statistics for a sports stream, subtitles for living news, etc. However, due to the diverse transmission process between live streams and other media content, the synchronization of them has grown to be a great challenge. Unfortunately, the existing commercial solutions are not universal, which require specific server cloud services or CDN and limit the users&#x2019; free choices of web infrastructures. To address the issue, we propose a lightweight universal event-synchronizing solution for live streaming, called LSync, which inserts a series of audio signals containing metadata into the original audio stream. It brings no modification to the original live broadcast process and thus fits prevalent live broadcast infrastructure. Evaluations on real system show that the proposed solution reduces the signal processing delay by at most 5.62% of an audio buffer length in mobile phones and ensures real-time signal processing. It also achieves a data rate of 156.25 bps in a specific configuration and greatly outperforms recent works."},
{"Title": "Wi-drone", "URL": "https://dl.acm.org/doi/10.1145/3498361.3538936", "Full Abstract": "After years of boom, drones and their applications are now entering indoors. Six-degree-of-freedom (6-DoF) pose tracking is the core of drone flight control, but existing solutions cannot be directly applied to indoor scenarios due to insufficient accuracy, low robustness to adverse texture and light conditions, and signal obstruction in indoor scenarios. To overcome the above limitations, we propose Wi-Drone, a Wi-Fi standalone 6-DoF tracking system for indoor drone flight control. Wi-Drone takes full advantage of both"},
{"Title": "Edge Assisted Mobile Semantic Visual SLAM", "URL": "https://dl.acm.org/doi/10.1109/TMC.2022.3201000", "Full Abstract": "Localization and navigation play a key role in many location-based services and have attracted numerous research efforts. In recent years, visual SLAM has been prevailing for autonomous driving. However, the ever-growing computation resources demanded by SLAM impede its applications to resource-constrained mobile devices. In this paper, we present the design, implementation, and evaluation of <italic>edgeSLAM</italic>, an edge-assisted real-time semantic visual SLAM service running on mobile devices. <italic>edgeSLAM</italic> leverages the state-of-the-art semantic segmentation algorithm to enhance localization and mapping accuracy, and speeds up the computation-intensive SLAM and semantic segmentation algorithms by computation offloading. The key innovations of <italic>edgeSLAM</italic> include an efficient computation offloading strategy, an opportunistic data sharing method, an adaptive task scheduling algorithm, and a multi-user support mechanism. We fully implement <italic>edgeSLAM</italic> and plan to open-source it. Extensive experiments are conducted under 3 datasets. The results show that <italic>edgeSLAM</italic> can run on mobile devices at 35fps and achieve 5cm localization accuracy from real-world experiments, outperforming existing solutions by more than 15&#x0025;. We also demonstrate the usability of <italic>edgeSLAM</italic> through 2 case studies of pedestrian localization and robot navigation. To the best of our knowledge, <italic>edgeSLAM</italic> is the first edge-assisted real-time semantic visual SLAM for mobile devices."},
{"Title": "WebAssembly-based Delta Sync for Cloud Storage Services", "URL": "https://dl.acm.org/doi/10.1145/3502847", "Full Abstract": "Delta synchronization (sync) is crucial to the network-level efficiency of cloud storage services, especially when handling large files with small increments. Practical delta sync techniques are, however, only available for PC clients and mobile apps, but not web browsers—the most pervasive and OS-independent access method. To bridge this gap, prior work concentrates on either reversing the delta sync protocol or utilizing the native client, all striving around the tradeoffs among efficiency, applicability, and usability"},
{"Title": "StreamingTag", "URL": "https://dl.acm.org/doi/10.1145/3495243.3560521", "Full Abstract": "Streaming services have billions of mobile subscribers, yet video piracy has cost service providers billions. Digital Rights Management (DRM), however, is still far from satisfactory. Unlike DRM, which attempts to prohibit the creation of pirated copies, fingerprinting may be used to track out the source of piracy. Nevertheless, the idea of piracy tracing is not widely used at the moment, since existing fingerprinting-based streaming systems fail to serve numerous users. In this paper, we present the design and evaluation of StreamingTag, a scalable piracy tracing system for mobile streaming services. StreamingTag adopts a segment-level fingerprint embedding scheme to remove the need of re-embedding the fingerprint into the video for each new viewer. The key innovations of StreamingTag include a scalable and CDN-friendly delivery framework, a polarized and randomized SVD watermarking scheme suitable for short segments, and a collusion-resistant fingerprinting scheme optimized for large-scale streaming services. Experiment results show the good QoS of StreamingTag in terms of preparation latency, bandwidth consumption, and video fidelity. Compared with existing SVD watermarking schemes, the proposed watermarking scheme improves the watermark extraction accuracy by 2.25x at most and 1.5x on average. Compared with existing collusion-resistant fingerprinting schemes, the proposed scheme catches more colluders and improves the recall rate by 26%."},
{"Title": "De-spreading over the air", "URL": "https://dl.acm.org/doi/10.1145/3495243.3560524", "Full Abstract": "Unlicensed LPWANs on ISM bands share the spectrum with various wireless techniques, such as Wi-Fi, Bluetooth, and ZigBee. The explosion of IoT deployments calls for an increasing need for long-range cross-technology communication (CTC) between LPWANs and other techniques. Yet, existing technologies cannot achieve real long-range CTC for commodity wireless. We propose L2X, which provides long-range CTC to diverse receivers with LoRa transmitters. At the heart of L2X, we design an energy-concentrating demodulation mechanism that de-spreads LoRa chirps over the air. Therefore, L2X enables non-LoRa receivers to detect and demodulate LoRa signals even under extremely low SNR. We address practical challenges in L2X design. We propose a packet detection method to detect low-SNR LoRa transmissions at non-LoRa receivers. To decode LoRa transmissions, we accurately synchronize the demodulation window with incoming packets and propose a cross-domain demodulation approach to enhance the demodulation SNR. We implement L2X, all using commodity devices, and extensively evaluate its performance. The results show that L2X achieves 1.2 km CTC with the signal -9 dB below the noise floor, improving the distance by 30X compared with state-of-the-arts."},
{"Title": "RF-transformer", "URL": "https://dl.acm.org/doi/10.1145/3495243.3560549", "Full Abstract": "This paper presents RF-Transformer, a unified backscatter radio hardware abstraction that allows a low-power IoT device to directly communicate with heterogeneous wireless receivers at the minimum power consumption. Unlike existing backscatter systems that are tailored to a specific wireless communication protocol, RF-Transformer provides a programmable interface to the micro-controller, allowing IoT devices to synthesize different types of protocol-compliant backscatter signals sharing radically different PHY-layer designs. To show the efficacy of our design, we implement a PCB prototype of RF-Transformer on 2.4 GHz ISM band and showcase its capability on generating standard ZigBee, Bluetooth, LoRa, and Wi-Fi 802.11b/g/n/ac packets. Our extensive field studies show that RF-Transformer achieves 23.8 Mbps, 247.1 Kbps, 986.5 Kbps, and 27.3 Kbps throughput when generating standard Wi-Fi, ZigBee, Bluetooth, and LoRa signals while consuming 7.6--74.2X less power than their active counterparts. Our ASIC simulation based on the 65-nm CMOS process shows that the power gain of RF-Transformer can further grow to 92--678X. We further integrate RF-Transformer with pressure sensors and present a case study on detecting foot traffic density in hallways. Our 7-day case studies demonstrate RF-Transformer can reliably transmit sensor data to a commodity gateway by synthesizing LoRa packets on top of Wi-Fi signals. Our experimental results also verify the compatibility of RF-Transformer with commodity receivers. Code and hardware schematics can be found at: https://github.com/LeFsCC/RF-Transformer."},
{"Title": "Trustworthy AI: A Computational Perspective", "URL": "https://dl.acm.org/doi/10.1145/3546872", "Full Abstract": "In the past few decades,"},
{"Title": "Overlay-Based Android Malware Detection at Market Scales: Systematically Adapting to the New Technological Landscape", "URL": "https://dl.acm.org/doi/10.1109/TMC.2021.3079433", "Full Abstract": "Android <italic>overlay</italic> enables one app to draw over other apps by creating an extra <monospace>View</monospace> layer atop the host <monospace>View</monospace>, which nevertheless can be exploited by malicious apps (malware) to attack users. To combat this threat, prior countermeasures concentrate on restricting the capabilities of overlays at the OS level while sacrificing overlays&#x2019; usability; recently, the overlay mechanism has been substantially updated to prevent a variety of attacks, which however can still be evaded by considerable adversaries. To address these shortcomings, a more pragmatic approach is to enable <italic>early detection</italic> of overlay-based malware during the app market review process, so that all the capabilities of overlays can stay unchanged. For this purpose, in this paper we first conduct a large-scale comparative study of overlay characteristics in benign and malicious apps, and then implement the OverlayChecker system to automatically detect overlay-based malware for one of the world&#x2019;s largest Android app stores. In particular, we have made systematic efforts in feature engineering, UI exploration, emulation architecture, and run-time environment, thus maintaining high detection accuracy (97 percent precision and 97 percent recall) and short per-app scan time (<inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math><alternatives><mml:math><mml:mo>&#x223C;</mml:mo></mml:math><inline-graphic xlink:href=\"gong-ieq1-3079433.gif\"/></alternatives></inline-formula>1.7 minutes) with only two commodity servers, under an intensive workload of <inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math><alternatives><mml:math><mml:mo>&#x223C;</mml:mo></mml:math><inline-graphic xlink:href=\"gong-ieq2-3079433.gif\"/></alternatives></inline-formula>10K newly submitted apps per day."},
{"Title": "Fast Uplink Bandwidth Testing for Internet Users", "URL": "https://dl.acm.org/doi/10.1109/TNET.2023.3234265", "Full Abstract": "Access bandwidth measurement is crucial to emerging Internet applications for network-aware content delivery. However, today&#x2019;s bandwidth testing services (BTSes) are slow and costly&#x2014;the tests take a long time to run, consume a great deal of data usage, and usually require large-scale test server deployments. The inefficiency and high cost of BTSes root in their methodologies that use excessive temporal/spatial redundancies for combating noises in Internet measurement. In particular, compared to downlink BTSes, uplink BTSes are subject to more severe performance problems and technical challenges. This paper presents FastUpBTS to make uplink BTS fast and cheap while maintaining high accuracy. The key idea is to strategically accommodate and exploit the noise rather than repetitively and exhaustively suppress the impact of noise. This is achieved by a novel statistical sampling framework termed fuzzy rejection sampling. We build FastUpBTS as an end-to-end BTS that implements fuzzy rejection sampling based on memorization-reinforced throughput denoising, data-driven server selection, and informed multi-homing support. Our evaluation shows that with only 30 test servers, FastUpBTS achieves the same level of accuracy compared to the state-of-the-art BTS (<monospace>SpeedTest.net</monospace>) that deploys &#x007E;16,000 servers. Most importantly, FastUpBTS makes bandwidth tests <inline-formula> <tex-math notation=\"LaTeX\">$5.4\\times $ </tex-math></inline-formula> faster and <inline-formula> <tex-math notation=\"LaTeX\">$6.8\\times $ </tex-math></inline-formula> more data-efficient."},
{"Title": "Aging or Glitching? What Leads to Poor Android Responsiveness and What Can We Do About It?", "URL": "https://dl.acm.org/doi/10.1109/TMC.2023.3237716", "Full Abstract": "Almost all Android users have ever experienced poor responsiveness, including the common frame dropping events&#x2014;slow rendering (SR) and frozen frames (FF), as well as the uncommon Application Not Responding (ANR) and System Not Responding (SNR) that directly disrupt user experience. This work takes two complementary approaches, <italic>controlled benchmarking</italic> and <italic>in-the-wild crowdsourcing</italic>, to comprehensively understand their prevalence, characteristics, and root causes, which turn out to be significantly different from common understandings and prior studies. We find that SR, FF, ANR, and SNR all occur prevalently on all the studied hardware models of Android phones, and better hardware does not seem to relieve ANR/SNR. Most surprisingly, they are oftentimes ascribed to defective software design that incurs substantial resource overuse&#x2014;lightweight apps can experience severe SR/FF events due to <italic>redundant UI rendering</italic>, and the most ANR/SNR events stem from Android&#x0027;s aggressive implementation of <italic>write amplification mitigation</italic>. In fact, the former can be effectively overcome by simplifying the apps&#x2019; UI hierarchy, and we design a practical approach to address almost all (<inline-formula><tex-math notation=\"LaTeX\">$&#x003E;$</tex-math><alternatives><mml:math><mml:mo>&#x003E;</mml:mo></mml:math><inline-graphic xlink:href=\"lin-ieq1-3237716.gif\"/></alternatives></inline-formula>99&#x0025;) of the latter while only decreasing 3&#x0025; of the data write speed with large-scale deployment. We have released our measurement code/data to the research community."},
{"Title": "Measuring Micrometer-Level Vibrations With mmWave Radar", "URL": "https://dl.acm.org/doi/10.1109/TMC.2021.3118349", "Full Abstract": "Vibration measurement is a crucial task in industrial systems, where vibration characteristics reflect health conditions and indicate anomalies of the devices. Previous approaches either work in an intrusive manner or fail to capture the micrometer-level vibrations. In this work, we propose mmVib, a practical approach to measure micrometer-level vibrations with mmWave radar. First, we derive a metric called <italic>Vibration Signal-to-Noise Ratio</italic> (VSNR) that highlights the directions of reducing measurement errors of tiny vibrations. Then, we introduce the design of mmVib based on the concept of <italic>Multi-Signal Consolidation</italic> (MSC) for the error reduction and multi-object measurement. We implement a prototype of mmVib, and the experiments show that it achieves <inline-formula><tex-math notation=\"LaTeX\">$3.946\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>946</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"he-ieq1-3118349.gif\"/></alternatives></inline-formula> relative amplitude error and <inline-formula><tex-math notation=\"LaTeX\">$0.02487\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>02487</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"he-ieq2-3118349.gif\"/></alternatives></inline-formula> relative frequency error in median. Typically, the average amplitude error is only <inline-formula><tex-math notation=\"LaTeX\">$3.174um$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mn>174</mml:mn><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"he-ieq3-3118349.gif\"/></alternatives></inline-formula> when measuring the <inline-formula><tex-math notation=\"LaTeX\">$100um$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>100</mml:mn><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"he-ieq4-3118349.gif\"/></alternatives></inline-formula>-amplitude vibration at around 5 meters. Compared to two existing mmWave-based approaches, mmVib reduces the 80th-percentile amplitude error by <inline-formula><tex-math notation=\"LaTeX\">$69.21\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>69</mml:mn><mml:mo>.</mml:mo><mml:mn>21</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"he-ieq5-3118349.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$97.99\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>97</mml:mn><mml:mo>.</mml:mo><mml:mn>99</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"he-ieq6-3118349.gif\"/></alternatives></inline-formula> respectively."},
{"Title": "Visual-Aware Testing and Debugging for Web Performance Optimization", "URL": "https://dl.acm.org/doi/10.1145/3543507.3583323", "Full Abstract": "Web performance optimization services, or web performance optimizers (WPOs), play a critical role in today’s web ecosystem by improving page load speed and saving network traffic. However, WPOs are known for introducing visual distortions that disrupt the users’ web experience. Unfortunately, visual distortions are hard to analyze, test, and debug, due to their subjective measure, dynamic content, and sophisticated WPO implementations."},
{"Title": "Leggiero: Analog WiFi Backscatter with Payload Transparency", "URL": "https://dl.acm.org/doi/10.1145/3581791.3596835", "Full Abstract": "Backscatter is an enabling technology for battery-free sensing in today's Artificial Intelligence of Things (AIOT). Building a backscatter-based sensing system, however, is a daunting task, due to two obstacles: the unaffordable power consumption of the microprocessor and the coexistence with the ambient carrier's traffic. In order to address the above issues, in this paper, we present Leggiero, the first-of-its-kind analog WiFi backscatter with payload transparency. Leveraging a specially designed circuit with a varactor diode, this design avoids using a microprocessor to interface between the radio and the sensor, and directly converts the analog sensor signal into the phase of RF (radio frequency) signal. By carefully designing the reference circuit on the tag and precisely locating the extra long training field (LTF) section of a WiFi packet, Leggiero embeds the analog phase value into the channel state information (CSI). A commodity WiFi receiver without hardware modification can simultaneously decode the WiFi and the sensor data. We implement Leggiero design and evaluate its performance under varied settings. The results show that the power consumption of the Leggiero tag (excluding the power of the peripheral sensor module) is 30"},
{"Title": "LocRa: Enable Practical Long-Range Backscatter Localization for Low-Cost Tags", "URL": "https://dl.acm.org/doi/10.1145/3581791.3596863", "Full Abstract": "Long-range backscatter localization is a promising technology for the Internet of Things. Existing works cannot work well for distributed base stations and low-cost tags. We present LocRa, which provides accurate localization for long-range backscatter with distributed base stations. We present a novel method to extract accurate channel information and synchronize the phase of different base stations. To compensate for the frequency and phase error on low-cost tags, we combine multiple channel measurements and eliminate the error by aligning different channels. Finally, we exploit frequency domain characteristics of the backscatter signal to extend its bandwidth and improve the SNR, thereby enhancing the localization accuracy. We prototype LocRa tags using custom low-cost hardware and implement LocRa base stations on USRP. Through extensive experiments, we show that the localization error of LocRa is 6.8 cm and 88 cm when the tag is 5m and 50m away from the base station, which is 3.1× and 2.3× better than the state-of-the-arts methods."},
{"Title": "Validating More Loop Optimizations", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2005.02.044", "Full Abstract": "Translation validation is a technique for ensuring that a translator, such as a compiler, produces correct results. Because complete verification of the translator itself is often infeasible, translation validation advocates coupling the verification task with the translation task, so that each run of the translator produces verification conditions which, if valid, prove the correctness of the translation. In previous work, the translation validation approach was used to give a framework for proving the correctness of a variety of compiler optimizations, with a recent focus on loop transformations. However, some of these ideas were preliminary and had not been implemented. Additionally, there were examples of common loop transformations which could not be handled by our previous approaches. This paper addresses these issues. We introduce a new rule Reduce for loop reduction transformations, and we generalize our previous rule Validate so that it can handle more transformations involving loops. We then describe how all of this (including some previous theoretical work) is implemented in our compiler validation tool TVOC."},
{"Title": "Refining the undecidability frontier of hybrid automata", "URL": "https://dl.acm.org/doi/10.1007/11590156_21", "Full Abstract": "Reachability becomes undecidable in hybrid automata (HA) that can simulate a Turing (TM) or Minsky (MM) machine. Asarin and Schneider have shown that, between the decidable 2-dim Piecewise Constant Derivative (PCD) class and the undecidable 3-dim PCD class, there lies the “open” class 2-dim Hierarchical PCD (HPCD). This class was shown to be equivalent to the class of 1-dim Piecewise Affine Maps (PAM). In this paper, we first explore 2-dim HPCD's proximity to decidability, by showing that they are equivalent to 2-dim PCDs with translational resets, and to HPCDs without resets. A hierarchy of intermediates also equivalent to the HPCD class is presented, revealing semblance to timed and initialized rectangular automata. We then explore the proximity to the undecidability frontier. We show that 2-dim HPCDs with zeno executions or integer-checks can simulate the 2-counter MM. We conclude by retreating HPCDs as PAMs, to derive a simple over-approximating algorithm for reachability. This also defines a decidable subclass 1-dim Onto PAM (oPAM). The novel non-trivial transformation of 2-dim HPCDs into “almost decidable” systems, is likely to pave the way for approximate reachability algorithms, and the characterization of decidable subclasses. It is hoped that these ideas eventually coalesce into a complete understanding of the reachability problem for the class 2-dim HPCD (1-dim PAM)."},
{"Title": "Model Checking with Strong Fairness", "URL": "https://dl.acm.org/doi/10.1007/s10703-006-4342-y", "Full Abstract": "In this paper we present a coherent framework for symbolic model checking of linear-time temporal logic ("},
{"Title": "Reduced Functional Consistency of Uninterpreted Functions", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2005.12.006", "Full Abstract": "A reduction of Equality Logic with Uninterpreted Functions (EUF) to Equality Logic with Ackermann's method suffers from a quadratic growth in the number of functional consistency constraints (constraints of the form x=y->F(x)=F(y)). We propose a framework in which syntactic characteristics of function instances (their signature) is used for guessing which constraints will possibly be needed for the proof. This framework can be either combined in an abstraction-refinement loop, or, in some cases, be used without refinement iterations. The framework is suitable for equivalence verification problems, which is one of the typical uses of Uninterpreted Functions. It enabled us to verify dozens of verification conditions resulting from Translation Validation that we could not prove otherwise."},
{"Title": "Ranking abstraction of recursive programs", "URL": "https://dl.acm.org/doi/10.1007/11609773_18", "Full Abstract": "We present a method for model-checking of safety and liveness properties over procedural programs, by combining state and ranking abstractions with procedure summarization. Our abstraction is an augmented finitary abstraction [KP00,BPZ05], meaning that a concrete procedural program is first augmented with a well founded ranking function, and then abstracted by a finitary state abstraction. This results in a procedural abstract program with strong fairness requirements which is then reduced to a finite-state"},
{"Title": "Synthesis of reactive(1) designs", "URL": "https://dl.acm.org/doi/10.1007/11609773_24", "Full Abstract": "We consider the problem of synthesizing digital designs from their ltl specification. In spite of the theoretical double exponential lower bound for the general case, we show that for many expressive specifications of hardware designs the problem can be solved in time"},
{"Title": "Monitoring Interfaces for Faults", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2006.02.005", "Full Abstract": "We consider the problem of a module interacting with an external interface (environment) where the interaction is expected to satisfy some system specification @F. While we have the full implementation details of the module, we are only given a partial external specification for the interface. The interface specification being partial (incomplete) means that the interface displays only a strict subset of the behaviors allowed by the interface specification. Based on the assumption that interface specifications are typically incomplete, we address the question of whether we can tighten the interface specification into a strategy, consistent with the given partial specification, that will guarantee that all possible interactions resulting from possible behaviors of the module will satisfy the system specification @F. We refer to such a tighter specification as @F-guaranteeing specification. Rather than verifying whether the interface, which is often an off-the-shelf component, satisfies the tighter specification, the paper proposes a construction of a run-time monitor which continuously checks the existence of a @F-guaranteeing interface. We view the module and the external interface as players in a 2-player game. The interface has a winning strategy if it can guarantee that no matter what the module does, the overall specification @F is met. The problem of incomplete specifications is resolved by allowing the interface to follow any strategy consistent with the interface specification. Our approach essentially combines traditional run-time monitoring and static analysis. This allows going beyond the focus of traditional run-time monitoring tools - error detection in the execution trace, towards the focus of the static analysis - bug detection in the programs."},
{"Title": "Liveness with invisible ranking", "URL": "https://dl.acm.org/doi/10.5555/3220880.3220973", "Full Abstract": "The method of invisible invariants was developed originally in order to verify safety properties of parameterized systems in a fully automatic manner. The method is based on (1) a project&generalize heuristic to generate auxiliary constructs for parameterized systems and (2) a small-model theorem, implying that it is sufficient to check the validity of logical assertions of a certain syntactic form on small instantiations of a parameterized system. The approach can be generalized to any deductive proof rule that (1) requires auxiliary constructs that can be generated by project&generalize, and (2) the premises resulting when using the constructs are of the form covered by the small-model theorem."},
{"Title": "Invisible safety of distributed protocols", "URL": "https://dl.acm.org/doi/10.1007/11787006_45", "Full Abstract": "The method of “Invisible Invariants” has been applied successfully to protocols that assume a “symmetric” underlying topology, be it cliques, stars, or rings. In this paper we show how the method can be applied to proving safety properties of distributed protocols running under arbitrary topologies. Many safety properties of such protocols have reachability predicates, which, at first glance, are beyond the scope of the Invisible Invariants method. To overcome this difficulty, we present a technique, called “coloring,” that allows, in many instances, to replace the second order reachability predicates by first order predicates, resulting in properties that are amenable to Invisible Invariants.We demonstrate our techniques on several distributed protocols, including a variant on Luby's Maximal Independent Set protocol, the Leader Election protocol used in the IEEE 1394 (Firewire) distributed bus protocol, and various distributed spanning tree algorithms. All examples have been tested using the symbolic model checker tlv"},
{"Title": "Faster Solutions of Rabin and Streett Games", "URL": "https://dl.acm.org/doi/10.1109/LICS.2006.23", "Full Abstract": "In this paper we improve the complexity of solving Rabin and Streett games to approximately the square root of previous bounds. We introduce direct Rabin and Streett ranking that are a sound and complete way to characterize the winning sets in the respective games."},
{"Title": "PSL model checking and run-time verification via testers", "URL": "https://dl.acm.org/doi/10.1007/11813040_38", "Full Abstract": "The paper introduces the construct of"},
{"Title": "From MITL to timed automata", "URL": "https://dl.acm.org/doi/10.1007/11867340_20", "Full Abstract": "We show how to transform formulae written in the real-time temporal logic MITL into timed automata that recognize their satisfying models. This compositional construction is much simpler than previously known and can be easily implemented."},
{"Title": "Liveness by invisible invariants", "URL": "https://dl.acm.org/doi/10.1007/11888116_26", "Full Abstract": "The method of Invisible Invariants was developed in order to verify safety properties of parametrized systems in a fully automatic manner. In this paper, we apply the method of invisible invariant to “bounded response” properties, i.e., liveness properties of the type $ p \\Rightarrow \\diamondsuit q$ that are bounded – once a"},
{"Title": "Shape analysis of single-parent heaps", "URL": "https://dl.acm.org/doi/10.5555/1763048.1763059", "Full Abstract": "We define the class of single-parent heap systems, which rely on a singly-linked heap in order to model destructive updates on tree structures. This encoding has the advantage of relying on a relatively simple theory of linked lists in order to support abstraction computation. To facilitate the application of this encoding, we provide a program transformation that, given a program operating on a multi-linked heap without sharing, transforms it into one over a single-parent heap. It is then possible to apply shape analysis by predicate and ranking abstraction as in [3]. The technique has been successfully applied on examples with trees of fixed arity (balancing of and insertion into a binary sort tree)."},
{"Title": "\"Don't care\" modeling", "URL": "https://dl.acm.org/doi/10.5555/1763507.1763542", "Full Abstract": "Analysis of biological data often requires an understanding of components of pathways and/or networks and their mutual dependency relationships. Such systems are often analyzed and understood from datasets made up of the states of the relevant components and a set of discrete outcomes or results. The analysis of these systems can be assisted by models that are consistent with the available data while being maximally predictive for untested conditions. Here, we present a method to construct such models for these types of systems. To maximize predictive capability, we introduce a set of \"don't care\" (dc) Boolean variables that must be assigned values in order to obtain a concrete model. When a dc variable is set to 1, this indicates that the information from the corresponding component does not contribute to the observed result. Intuitively, more dc variables that are set to 1 maximizes both the potential predictive capability as well as the possibility of obtaining an inconsistent model. We thus formulate our problemas maximizing the number of dc variables that are set to 1, while retaining a model solution that is consistent and can explain all the given known data. This amounts to solving a quantified Boolean formula (QBF) with three levels of quantifier alternations, with a maximization goal for the dc variables. We have developed a prototype implementation to support our new modeling approach and are applying our method to part of a classical system in developmental biology describing fate specification of vulval precursor cells in the"},
{"Title": "Interactive presentation: Automatic hardware synthesis from specifications", "URL": "https://dl.acm.org/doi/10.5555/1266366.1266622", "Full Abstract": "We propose to use a formal specification language as a high-level hardware description language. Formal languages allow for compact, unambiguous representations and yield designs that are correct by construction. The idea of automatic synthesis from specifications is old, but used to be completely impractical. Recently, great strides towards efficient synthesis from specifications have been made. In this paper we extend these recent methods to generate compact circuits and we show their practicality by synthesizing an arbiter for ARM's AMBA AHB bus and a generalized-buffer from specifications given in PSL. These are the first industrial examples that have been synthesized automatically from their specifications."},
{"Title": "On synthesizing controllers from bounded-response properties", "URL": "https://dl.acm.org/doi/10.5555/1770351.1770368", "Full Abstract": "In this paper we propose a complete chain for synthesizing controllers from high-level specifications. From real-time properties expressed in the logic MTL we generate, under bounded-variability assumptions, deterministic timed automata to which we apply safety synthesis algorithms to derive a controller that satisfies the properties by construction. Some preliminary experimental results are reported."},
{"Title": "Synthesizing reactive systems from LSC requirements using the play-engine", "URL": "https://dl.acm.org/doi/10.1145/1297846.1297895", "Full Abstract": "Live Sequence Charts (LSCs) is a scenario-based language for modeling object-based reactive systems with liveness properties. A tool called the Play-Engine allows users to create LSC requirements using a point-and-click interface and generate executable traces using features called play-out and smart play-out. Finite executable trace fragments called super-steps are generated by smart play-out in response to user inputs. Each super-step is guaranteed not to violate the LSC requirements, provided one exists. However, non-violation is not guaranteed beyond each individual super-step. In this work, we demonstrate a powerful extension to smart play-out which produces only traces that are guaranteed not to violate the LSC requirements, provided the requirements are realizable. Using this method, we may synthesize correct executable programs directly from LSC requirements."},
{"Title": "Specify, Compile, Run", "URL": "https://dl.acm.org/doi/10.1016/j.entcs.2007.09.004", "Full Abstract": "We propose to use a formal specification language as a high-level hardware description language. Formal languages allow for compact, unambiguous representations and yield designs that are correct by construction. The idea of automatic synthesis from specifications is old, but used to be completely impractical. Recently, great strides towards efficient synthesis from specifications have been made. In this paper we extend these recent methods to generate compact circuits and we show their practicality by synthesizing a generalized buffer and an arbiter for ARM's AMBA AHB bus from specifications given in PSL. These are the first industrial examples that have been synthesized automatically from their specifications."},
{"Title": "Verifying Correctness of Transactional Memories", "URL": "https://dl.acm.org/doi/10.5555/1333874.1334162", "Full Abstract": "We show how to verify the correctness of transactional memory implementations with a model checker. We show how to specify transactional memory in terms of the admissible interchange of transaction operations, and give proof rules for showing that an implementation satisfies this specification. This notion of an admissible interchange is a key to our ability to use a model checker, and lets us capture the various notions of transaction conflict as characterized by Scott. We demonstrate our work using the TLC model checker to verify several well-known implementations described abstractly in the TLA+ specification language."},
{"Title": "Memristor-based approximated computation", "URL": "https://dl.acm.org/doi/10.5555/2648668.2648729", "Full Abstract": "The cessation of Moore's Law has limited further improvements in power efficiency. In recent years, the physical realization of the memristor has demonstrated a promising solution to ultra-integrated hardware realization of neural networks, which can be leveraged for better performance and power efficiency gains. In this work, we introduce a power efficient framework for approximated computations by taking advantage of the memristor-based multilayer neural networks. A programmable memristor approximated computation unit (Memristor ACU) is introduced first to accelerate approximated computation and a memristor-based approximated computation framework with scalability is proposed on top of the Memristor ACU. We also introduce a parameter configuration algorithm of the Memristor ACU and a feedback state tuning circuit to program the Memristor ACU effectively. Our simulation results show that the maximum error of the Memristor ACU for 6 common complex functions is only 1.87% while the state tuning circuit can achieve 12-bit precision. The implementation of HMAX model atop our proposed memristor-based approximated computation framework demonstrates 22X power efficiency improvements than its pure digital implementation counterpart."},
{"Title": "Hardware Acceleration for an Accurate Stereo Vision System Using Mini-Census Adaptive Support Region", "URL": "https://dl.acm.org/doi/10.1145/2584659", "Full Abstract": "Domain of stereo vision is highly important in the fields of autonomous cars, video tolling, robotics, and aerial surveys. The specific feature of this domain is that we should handle not only the pixel-by-pixel 2D processing in one image but also the 3D processing for depth estimation by comparing information about a scene from several images with different perspectives. This feature brings challenges to memory resource utilization, because an extra dimension of data has to be buffered. Due to the memory limitation, few of previous stereo vision implementations provide both accurate and high-speed processing for high-resolution images at the same time."},
{"Title": "Enabling FPGAs in the cloud", "URL": "https://dl.acm.org/doi/10.1145/2597917.2597929", "Full Abstract": "Cloud computing is becoming a major trend for delivering and accessing infrastructure on demand via the network. Meanwhile, the usage of FPGAs (Field Programmable Gate Arrays) for computation acceleration has made significant inroads into multiple application domains due to their ability to achieve high throughput and predictable latency, while providing programmability, low power consumption and time-to-value. Many types of workloads, e.g. databases, big data analytics, and high performance computing, can be and have been accelerated by FPGAs. As more and more workloads are being deployed in the cloud, it is appropriate to consider how to make FPGAs and their capabilities available in the cloud. However, such integration is non-trivial due to issues related to FPGA resource abstraction and sharing, compatibility with applications and accelerator logics, and security, among others. In this paper, a general framework for integrating FPGAs into the cloud is proposed and a prototype of the framework is implemented based on OpenStack, Linux-KVM and Xilinx FPGAs. The prototype enables isolation between multiple processes in multiple VMs, precise quantitative acceleration resource allocation, and priority-based workload scheduling. Experimental results demonstrate the effectiveness of this prototype, an acceptable overhead, and good scalability when hosting multiple VMs and processes."},
{"Title": "RRAM-Based Analog Approximate Computing", "URL": "https://dl.acm.org/doi/10.1109/TCAD.2015.2445741", "Full Abstract": "Approximate computing is a promising design paradigm for better performance and power efficiency. In this paper, we propose a power efficient framework for analog approximate computing with the emerging metal-oxide resistive switching random-access memory (RRAM) devices. A programmable RRAM-based approximate computing unit (RRAM-ACU) is introduced first to accelerate approximated computation, and an approximate computing framework with scalability is then proposed on top of the RRAM-ACU. In order to program the RRAM-ACU efficiently, we also present a detailed configuration flow, which includes a customized approximator training scheme, an approximator-parameter-to-RRAM-state mapping algorithm, and an RRAM state tuning scheme. Finally, the proposed RRAM-based computing framework is modeled at system level. A predictive compact model is developed to estimate the configuration overhead of RRAM-ACU and help explore the application scenarios of RRAM-based analog approximate computing. The simulation results on a set of diverse benchmarks demonstrate that, compared with a x86-64 CPU at 2 GHz, the RRAM-ACU is able to achieve 4.06-196.41&#x00D7; speedup and power efficiency of 24.59-567.98 GFLOPS/W with quality loss of 8.72% on average. And the implementation of hierarchical model and X application demonstrates that the proposed RRAM-based approximate computing framework can achieve 12.8&#x00D7; power efficiency than its pure digital implementation counterparts (CPU, graphics processing unit, and field- programmable gate arrays)."},
{"Title": "A Fine-Grained Sparse Accelerator for Multi-Precision DNN", "URL": "https://dl.acm.org/doi/10.1145/3289602.3293964", "Full Abstract": "Neural Networks (NNs) have made a significant breakthrough in many fields, while they also pose a great challenge to hardware platforms since the state-of-the-art neural networks are both communicational- and computational-intensive. Researchers proposed model compression algorithms using sparsification and quantization, along with specific hardware architecture designs, to accelerate various applications. However, the irregularity of memory access caused by the sparsity severely damages the regularity of intensive computation loops. Therefore, the architecture design for sparse neural networks is crucial to better software and hardware co-design for neural network applications. To face these challenges, this paper first analyzes the computation patterns of different NN structures and unify them into the form of sparse matrix-vector multiplication, sparse matrix-matrix multiplication, and element-wise multiplication. On the basis of the EIE which supports only the fully-connected network and recurrent neural network (RNN), we expand it to support the convolution neural network (CNN) using the input vector transform unit. This paper designs a multi-precision multiplier with supporting datapath, which makes the proposed architecture have a better acceleration effect in the low-bit quantization with the same hardware architecture. The proposed accelerator architecture can achieve the equivalent performance and energy efficiency up to 574.2 GOPS, 42.8 GOPS/W for CNN and 110.4 GOPS, 8.24 GOPS/W for RNN under 4-bit quantization on Xilinx XCKU115 FPGA running at 200MHz. And it is the state-of-the-art accelerator supporting CNN-RNN-based models like the long-term recurrent convolutional network with 571.1 GOPS performance and 42.6 GOPS/W energy efficiency under 4-bit data format."},
{"Title": "DNNVM", "URL": "https://dl.acm.org/doi/10.1145/3289602.3293972", "Full Abstract": "In recent years, Convolutional Neural Network(CNN) is becoming the state-of-the-art method in a wide range of Artificial Intelligence(AI) domains. The increasingly large and complex CNN models are both computation bound and I/O bound. FPGA-based accelerators driven by custom Instruction Set Architecture(ISA) achieve a balance between generality and efficiency, and leave much room for optimization. Operation fusion which fuses adjacent operations without saving intermediate results back to off-chip DDR can greatly alleviate bandwidth pressure, operations can be executed by different computation engines concurrently for latency hiding. To leverage optimizations, especially operation fusion on custom instruction-based accelerators, we propose a full-stack compiler DNNVM(Deep Neural Network Virtual Machine). DNNVM is an integration of optimizers for framework-independent computing graph, loops and data layouts, an assembler, a runtime supporter and a validation environment. DNNVM works in the context of deep learning frameworks and transforms CNN models into a directed acyclic graph, XGraph. After analyzing the interaction among fusion depth, tiling across multiple stages and on-chip memory capacity, DNNVM enumerates all potentially profitable fusion opportunities according to custom fusion templates upon XGraph, by a subgraph isomorphism algorithm. In addition, DNNVM searches for the optimal execution strategies by a heuristic shortest-path algorithm. On Xilinx ZU2@330MHz, we achieve up to 1.26x speedup than naïve implementations without fusion on GoogLeNet. On Xilinx ZU9@330MHz, we achieve the throughput of 2.82 TOPs/s for VGG, 1.38 TOPs/s for ResNet50 - he fastest ever reported on comparable FPGAs."},
{"Title": "Black Box Search Space Profiling for Accelerator-Aware Neural Architecture Search", "URL": "https://dl.acm.org/doi/10.1109/ASP-DAC47756.2020.9045179", "Full Abstract": "Neural Architecture Search (NAS) is a promising approach to discover good neural network architectures for given applications. Among the three basic components in a NAS system (search space, search strategy, and evaluation), prior work mainly focused on the development of different search strategies and evaluation methods. As most of the previous hardware-aware search space designs aimed at CPUs and GPUs, it still remains a challenge to design a suitable search space for Deep Neural Network (DNN) accelerators. Besides, the architectures and compilers of DNN accelerators vary greatly, so it is quite difficult to get a unified and accurate evaluation of the latency of DNN across different platforms. To address these issues, we propose a black box profiling-based search space tuning method and further improve the latency evaluation by introducing a layer adaptive latency correction method. Used as the first stage in our general accelerator-aware NAS pipeline, our proposed methods could provide a smaller and dynamic search space with a controllable trade-off between accuracy and latency for DNN accelerators. Experimental results on CIFAR-10 and ImageNet demonstrate our search space is effective with up to 12.7% improvement in accuracy and 2.2x reduction of latency, and also efficient by reducing the search time and GPU memory up to 4.35x and 6.25x, respectively."},
{"Title": "LPAC: A Low-Precision Accelerator for CNN on FPGAs", "URL": "https://dl.acm.org/doi/10.1145/3373087.3375343", "Full Abstract": "Low bit quantization of neural network is required on edge devices to achieve lower power consumption and higher performance. 8bit or binary network either consumes a lot of resources or has accuracy degradation. Thus, a full-process hardware-friendly quantization solution of 4A4W (activations 4bit and weights 4bit) is proposed to achieve better accuracy/resource trade-off. It doesn't contain any additional floating operations and achieve accuracy comparable to full-precision. We also implement a low-precision accelerator for CNN (LPAC) on the Xilinx FPGA, which takes full advantage of its DSP by efficiently mapping convolutional computations. Through on-chip reassign management and resource-saving analysis, high performance can be achieved on small chips. Our 4A4W solution achieves 1.8x higher performance than 8A8W and 2.42x increase in power efficiency under the same resource. On ImageNet classification, the accuracy has a gap less than 1% to full-precision in Top-5. On the human pose estimation, we achieve 261 frames per second on ZU2EG, which is 1.78x speed up compared to 8A8W and the accuracy has only 1.62% gap to full-precision. This proves that our solution has better universality."},
{"Title": "ProgressFace: Scale-Aware Progressive Learning for Face Detection", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-58539-6_21", "Full Abstract": "Scale variation stands out as one of key challenges in face detection. Recent attempts have been made to cope with this issue by incorporating image/feature pyramids or adjusting anchor sampling/matching strategies. In this work, we propose a novel scale-aware progressive training mechanism to address large scale variations across faces. Inspired by curriculum learning, our method gradually learns large-to-small face instances. The preceding models learned with easier samples (i.e., large faces) can provide good initialization for succeeding learning with harder samples (i.e., small faces), ultimately deriving a better optimum of face detectors. Moreover, we propose an auxiliary anchor-free enhancement module to facilitate the learning of small faces by supplying positive anchors that may be not covered according to the criterion of IoU overlap. Such anchor-free module will be removed during inference and hence no extra computation cost is introduced. Extensive experimental results demonstrate the superiority of our method compared to the state-of-the-arts on the standard FDDB and WIDER FACE benchmarks. Especially, our ProgressFace-Light with MobileNet-0.25 backbone achieves 87.9% AP on the hard set of WIDER FACE, surpassing largely RetinaFace with the same backbone by 9.7%. Code and our trained face detection models are available at"},
{"Title": "MTNAS: Search Multi-task Networks for Autonomous Driving", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-69535-4_41", "Full Abstract": "Multi-task learning (MTL) aims to learn shared representations from multiple tasks simultaneously, which has yielded outstanding performance in widespread applications of computer vision. However, existing multi-task approaches often demand manual design on network architectures, including shared backbone and individual branches. In this work, we propose MTNAS, a practical and principled neural architecture search algorithm for multi-task learning. We focus on searching for the overall optimized network architecture with task-specific branches and task-shared backbone. Specifically, the MTNAS pipeline consists of two searching stages: branch search and backbone search. For branch search, we separately optimize each branch structure for each target task. For backbone search, we first design a pre-searching procedure t1o pre-optimize the backbone structure on ImageNet. We observe that searching on such auxiliary large-scale data can not only help learn low-/mid-level features but also offer good initialization of backbone structure. After backbone pre-searching, we further optimize the backbone structure for learning task-shared knowledge under the overall multi-task guidance. We apply MTNAS to joint learning of object detection and semantic segmentation for autonomous driving. Extensive experimental results demonstrate that our searched multi-task model achieves superior performance for each task and consumes less computation complexity compared to prior hand-crafted MTL baselines. Code and searched models will be released at"},
{"Title": "A parameter-free hedging algorithm", "URL": "https://dl.acm.org/doi/10.5555/2984093.2984127", "Full Abstract": "We study the problem of decision-theoretic online learning (DTOL). Motivated by practical applications, we focus on DTOL when the number of actions is very large. Previous algorithms for learning in this framework have a tunable learning rate parameter, and a barrier to using online-learning in practical applications is that it is not understood how to set this parameter optimally, particularly when the number of actions is large."},
{"Title": "An online learning-based framework for tracking", "URL": "https://dl.acm.org/doi/10.5555/3023549.3023561", "Full Abstract": "We study the tracking problem, namely, estimating the hidden state of an object over time, from unreliable and noisy measurements. The standard framework for the tracking problem is the generative framework, which is the basis of solutions such as the Bayesian algorithm and its approximation, the particle filters. However, these solutions can be very sensitive to model mismatches. In this paper, motivated by online learning, we introduce a new framework for tracking. We provide an efficient tracking algorithm for this framework. We provide experimental results comparing our algorithm to the Bayesian algorithm on simulated data. Our experiments show that when there are slight model mismatches, our algorithm outperforms the Bayesian algorithm."},
{"Title": "Data winnowing", "URL": "https://dl.acm.org/doi/10.1145/1835804.1835806", "Full Abstract": "Massive quantities of digital data are being collected in every aspect of modern life. Examples include Personal photos and videos, biological and medical images and recordings from sensor arrays. To transform these massive data streams into useful information we use a sequence of \"winnowing\" stages. Each step reduces the size of the data by an order of magnitude; extracting the wheat form the chaff. In this talk I will describe this approach in a variety of contexts, ranging from the analysis of genetic pathways in fruit-fly embryos and C-Elegans worms to counting birds and helping elderly people living alone keep in touch with their family and caregivers."},
{"Title": "Learning a board Balanced Scorecard to improve corporate performance", "URL": "https://dl.acm.org/doi/10.1016/j.dss.2010.04.004", "Full Abstract": "The objective of this paper is to demonstrate how the boosting approach can be used to define a data-driven board Balanced Scorecard (BSC) with applications to S&P 500 companies. Using Adaboost, we can generate alternating decision trees (ADTs) that explain the relationship between corporate governance variables, and firm performance. We also propose an algorithm to build a representative ADT based on cross-validation experiments. The representative ADT selects the most important indicators for the board BSC. As a final result, we propose a partially automated strategic planning system combining Adaboost with the board BSC for board-level or investment decisions."},
{"Title": "An Online Learning Approach to Occlusion Boundary Detection", "URL": "https://dl.acm.org/doi/10.1109/TIP.2011.2162420", "Full Abstract": "We propose a novel online learning-based framework for occlusion boundary detection in video sequences. This approach does not require any prior training and instead “learns” occlusion boundaries by updating a set of weights for the online learning Hedge algorithm at each frame instance. Whereas previous training-based methods perform well only on data similar to the trained examples, the proposed method is well suited for any video sequence. We demonstrate the performance of the proposed detector both for the CMU data set, which includes hand-labeled occlusion boundaries, and for a novel video sequence. In addition to occlusion boundary detection, the proposed algorithm is capable of classifying occlusion boundaries by angle and by whether the occluding object is covering or uncovering the background."},
{"Title": "RIFFA", "URL": "https://dl.acm.org/doi/10.1109/FCCM.2012.44", "Full Abstract": "We present RIFFA, a reusable integration framework for FPGA accelerators. RIFFA provides communication and synchronization for FPGA accelerated software using a standard interface. Our goal is to expand the use of FPGAs as an acceleration platform by releasing, as open source, a no cost framework that easily integrates software on traditional CPUs with FPGA based IP cores, over PCIe, with minimal custom configuration. RIFFA requires no specialized hardware or fee licensed IP cores. It can be deployed on common Linux workstations with a PCIe bus and has been tested on two different Linux distributions using Xilinx FPGAs."},
{"Title": "Boosting", "URL": "https://dl.acm.org/doi/book/10.5555/2207821", "Full Abstract": "Boosting is an approach to machine learning based on the idea of creating a highly accurate predictor by combining many weak and inaccurate \"rules of thumb.\" A remarkably rich theory has evolved around boosting, with connections to a range of topics, including statistics, game theory, convex optimization, and information geometry. Boosting algorithms have also enjoyed practical success in such fields as biology, vision, and speech processing. At various times in its history, boosting has been perceived as mysterious, controversial, even paradoxical.This book, written by the inventors of the method, brings together, organizes, simplifies, and substantially extends two decades of research on boosting, presenting both theory and applications in a way that is accessible to readers from diverse backgrounds while also providing an authoritative reference for advanced researchers. With its introductory treatment of all material and its inclusion of exercises in every chapter, the book is appropriate for course use as well. The book begins with a general introduction to machine learning algorithms and their analysis; then explores the core theory of boosting, especially its ability to generalize; examines some of the myriad other theoretical viewpoints that help to explain and understand boosting; provides practical extensions of boosting for more complex learning problems; and finally presents a number of advanced theoretical topics. Numerous applications and practical illustrations are offered throughout."},
{"Title": "The fast convergence of incremental PCA", "URL": "https://dl.acm.org/doi/10.5555/2999792.2999966", "Full Abstract": "We consider a situation in which we see samples"},
{"Title": "A system for sending the right hint at the right time", "URL": "https://dl.acm.org/doi/10.1145/2556325.2567864", "Full Abstract": "Hints are sometimes used in online learning system to help students when they are having difficulties. However, in all of the systems we are aware of, the hints are fixed ahead of time and do not depend on the unsuccessful attempts the student has already made. This severely limits the effectiveness of the hints."},
{"Title": "Improved kNN Rule for Small Training Sets", "URL": "https://dl.acm.org/doi/10.1109/ICMLA.2014.37", "Full Abstract": "The traditional k-NN classification rule predicts a label based on the most common label of the k nearest neighbors (the plurality rule). It is known that the plurality rule is optimal when the number of examples tends to infinity. In this paper we show that the plurality rule is sub-optimal when the number of labels is large and the number of examples is small. We propose a simple k-NN rule that takes into account the labels of all of the neighbors, rather than just the most common label. We present a number of experiments on both synthetic datasets and real-world datasets, including MNIST and SVHN. We show that our new rule can achieve lower error rates compared to the majority rule in many cases."},
{"Title": "Scalable Semi-supervised aggregation of classifiers", "URL": "https://dl.acm.org/doi/10.5555/2969239.2969390", "Full Abstract": "We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests."},
{"Title": "Optimal binary classifier aggregation for general losses", "URL": "https://dl.acm.org/doi/10.5555/3157382.3157660", "Full Abstract": "We address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting, to minimize prediction loss incurred on the unlabeled data. We find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses, extending a recent analysis of the problem for misclassification error. The result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization, but are minimax optimal without any relaxations. Their decision rules take a form familiar in decision theory - applying sigmoid functions to a notion of ensemble margin - without the assumptions typically made in margin-based learning."},
{"Title": "The Active Atlas: Combining 3D Anatomical Models with Texture Detectors", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-66182-7_1", "Full Abstract": "While modern imaging technologies such as fMRI have opened exciting possibilities for studying the brain in vivo, histological sections remain the best way to study brain anatomy at the level of neurons. The procedure for building histological atlas changed little since 1909 and identifying brain regions is a still a labor intensive process performed only by experienced neuroanatomists. Existing digital atlases such as the Allen Reference Atlas are constructed using downsampled images and can not reliably map low-contrast parts such as brainstem, which is usually annotated based on high-resolution cellular texture."},
{"Title": "An adaptive nearest neighbor rule for classification", "URL": "https://dl.acm.org/doi/10.5555/3454287.3454968", "Full Abstract": "We introduce a variant of the"},
{"Title": "Faster boosting with smaller memory", "URL": "https://dl.acm.org/doi/10.5555/3454287.3455307", "Full Abstract": "State-of-the-art implementations of boosting, such as XGBoost and LightGBM, can process large training sets extremely fast. However, this performance requires that the memory size is sufficient to hold a 2-3 multiple of the training set size. This paper presents an alternative approach to implementing the boosted trees, which achieves a significant speedup over XGBoost and LightGBM, especially when the memory size is small. This is achieved using a combination of three techniques: early stopping, effective sample size, and stratified sampling. Our experiments demonstrate a 10-100 speedup over XGBoost when the training data is too large to fit in memory."},
{"Title": "When is the convergence time of Langevin algorithms dimension independent? a composite optimization viewpoint", "URL": "https://dl.acm.org/doi/10.5555/3586589.3586803", "Full Abstract": "There has been a surge of works bridging MCMC sampling and optimization, with a specific focus on translating non-asymptotic convergence guarantees for optimization problems into the analysis of Langevin algorithms in MCMC sampling. A conspicuous distinction between the convergence analysis of Langevin sampling and that of optimization is that all known convergence rates for Langevin algorithms depend on the dimensionality of the problem, whereas the convergence rates for optimization are dimension-free for convex problems. Whether a dimension independent convergence rate can be achieved by the Langevin algorithm is thus a long-standing open problem. This paper provides an affirmative answer to this problem for the case of either Lipschitz or smooth convex functions with normal priors. By viewing Langevin algorithm as composite optimization, we develop a new analysis technique that leads to dimension independent convergence rates for such problems."},
{"Title": "         StreaMRAK a streaming multi-resolution adaptive kernel algorithm", "URL": "https://dl.acm.org/doi/10.1016/j.amc.2022.127112", "Full Abstract": "Kernel ridge regression (KRR) is a popular scheme for non-linear non-parametric learning. However, existing implementations of KRR require that all the data is stored in the main memory, which severely limits the use of KRR in contexts where data size far exceeds the memory size. Such applications are increasingly common in data mining, bioinformatics, and control. A powerful paradigm for computing on data sets that are too large for memory is the"},
{"Title": "Towards Explainable Automated Neuroanatomy", "URL": "https://dl.acm.org/doi/10.1007/978-3-031-72384-1_45", "Full Abstract": "We present a novel method for quantifying the microscopic structure of brain tissue. It is based on the automated recognition of interpretable features obtained by analyzing the shapes of cells. This contrasts with prevailing methods of brain anatomical analysis in two ways. First, contemporary methods use gray-scale values derived from smoothed version of the anatomical images, which dissipated valuable information from the texture of the images. Second, contemporary analysis uses the output of black-box Convolutional Neural Networks, while our system makes decisions based on interpretable features obtained by analyzing the shapes of individual cells. An important benefit of this open-box approach is that the anatomist can understand and correct the decisions made by the computer. Our proposed system can accurately localize and identify existing brain structures. This can be used to align and coregistar brains and will facilitate connectomic studies for reverse engineering of brain circuitry."},
{"Title": "On the ", "URL": "https://dl.acm.org/doi/10.1016/j.ipl.2022.106252", "Full Abstract": "We try to understand how the optimal"},
{"Title": "Empirical hardness models", "URL": "https://dl.acm.org/doi/10.1145/1538902.1538906", "Full Abstract": "Is it possible to predict how long an algorithm will take to solve a previously-unseen instance of an NP-complete problem? If so, what uses can be found for models that make such predictions? This article provides answers to these questions and evaluates the answers experimentally."},
{"Title": "Considerations on the logic of intention", "URL": "https://dl.acm.org/doi/10.1145/1562814.1562818", "Full Abstract": "While logical theories of informational attitudes, such as knowledge, certainty and belief, have flourished in the past two decades, logical formalization of other facets of rational behavior has lagged behind significantly. One intriguing line of research concerns the concept of intention. I will discuss one approach to tackling the notion within a logical framework."},
{"Title": "Eliciting truthful answers to multiple-choice questions", "URL": "https://dl.acm.org/doi/10.1145/1566374.1566391", "Full Abstract": "Motivated by the prevalence of online questionnaires in electronic commerce, and of multiple-choice questions in such questionnaires, we consider the problem of eliciting truthful answers to multiple-choice questions from a knowledgeable respondent. Specifically, each question is a statement regarding an uncertain future event, and is multiple-choice -- the responder must select exactly one of the given answers. The principal offers a payment, whose amount is a function of the answer selected and the true outcome (which the principal will eventually observe). This problem significantly generalizes recent work on truthful elicitation of distribution properties, which itself generalized a long line of work in elicitation of complete distributions. We provide necessary and sufficient conditions for the existence of payments that induce truthful answers, and give a characterization of those payments. We also study in greater details the common case of questions with ordinal answers, and illustrate our results with several examples of practical interest."},
{"Title": "Analysis of a winning computational billiards player", "URL": "https://dl.acm.org/doi/10.5555/1661445.1661666", "Full Abstract": "We discuss CUECARD, the program that won the 2008 Computer Olympiad computational pool tournament. Beside addressing intrinsic interest in a complex competitive environment with unique features, our goal is to isolate the factors that contributed to the performance so that the lessons can be transferred to other, similar domains. Specifically, we distinguish among pure engineering factors (such as using a computer cluster), domain-specific factors (such as optimized break shots), and domain-independent factors (such as state clustering). Our conclusion is that each type of factor contributed to the performance of the program."},
{"Title": "Invited presentations at the twelfth international conference on principles of knowledge representation and reasoning", "URL": "https://dl.acm.org/doi/10.5555/3031748.3031750", "Full Abstract": "No abstract available."},
{"Title": "Joint revision of belief and intention", "URL": "https://dl.acm.org/doi/10.5555/3031748.3031825", "Full Abstract": "We present a formal semantical model to capture action, belief and intention, based on the \"database perspective\" (Shoham 2009). We then provide postulates for belief and intention revision, and state a representation theorem relating our postulates to the formal model. Our belief postulates are in the spirit of the AGM theory; the intention postulates stand in rough correspondence with the belief postulates."},
{"Title": "Internal implementation", "URL": "https://dl.acm.org/doi/10.5555/1838206.1838233", "Full Abstract": "We introduce a constrained mechanism design setting called"},
{"Title": "Joint process games", "URL": "https://dl.acm.org/doi/10.5555/1838206.1838319", "Full Abstract": "We introduce a game setting called a joint process, where the history of actions determine the state, and the state and agent properties determine the payoff. This setting is a special case of stochastic games and is a natural model for situations with alternating control. Joint process games have applications as diverse as aggregate rating sites and wiki page updates. These games are related to Black's median voter theorem and also strongly connected to Moulin's strategy-proof voting schemes. When each agent has a personal goal, we look at how the play converges under a simple myopic action rule, and prove that not only do these simple dynamics converge, but the actions selected also form a Nash equilibrium. The convergence point is not the mean or the median of the set of agent goals; instead we prove the convergence point is the median of the set of agent goals and a set of focal points. This work provides the first theoretical model of wiki-type behavior and opens the door to more questions about the properties of these games."},
{"Title": "Success, strategy and skill", "URL": "https://dl.acm.org/doi/10.5555/1838206.1838353", "Full Abstract": "In many AI settings an agent is comprised of both action-planning and action-execution components. We examine the relationship between the precision of the execution component, the intelligence of the planning component, and the overall success of the agent. Our motivation lies in determining whether higher execution skill rewards more strategic playing. We present a computational billiards framework in which the interaction between skill and strategy can be experimentally investigated. By comparing the performance of different agents with varying levels of skill and strategic intelligence we show that intelligent planning can contribute most to an agent's success when that agent has"},
{"Title": "Optimal seeding in knockout tournaments", "URL": "https://dl.acm.org/doi/10.5555/1838206.1838490", "Full Abstract": "Optimal seeding in balanced knockout tournaments has only been studied in very limited settings, for example, maximizing predictive power for up to 8 players using only the relative ranking of the players (ordinal information). We broaden the scope of the analysis along several dimensions: tournaments of size up to 128, different player models, ordinal as well as cardinal solutions, and two additional objective functions."},
{"Title": "Designing competitions between teams of individuals", "URL": "https://dl.acm.org/doi/10.1016/j.artint.2010.04.025", "Full Abstract": "We consider a setting with two teams, each with a number of players. There is an ordering of all players that determines outcome of matches between any two players from the opposing teams. Neither the teams nor the competition designer know this ordering, but each team knows the derived ordering of strengths among its own players. Each team announces an ordering of its players, and the competition designer schedules matches according to the announced orderings. This setting in general allows for two types of manipulations by a team: Misreporting the strength ordering (lack of truthfulness), and deliberately losing a match (moral hazard). We prove necessary and sufficient conditions for a set of competition rules to have the properties that truthful reporting are dominant strategies and maximum effort in matches are Nash equilibrium strategies, and certain fairness conditions are met. Extensions of the original setting are discussed."},
{"Title": "Hustling in repeated zero-sum games with imperfect execution", "URL": "https://dl.acm.org/doi/10.5555/2283396.2283403", "Full Abstract": "We study repeated games in which players have imperfect execution skill and one player's true skill is not common knowledge. In these settings the possibility arises of a player \"hustling\", or pretending to have lower execution skill than they actually have. Focusing on repeated zero-sum games, we provide a hustle-proof strategy; this strategy maximizes a player's payoff, regardless of the true skill level of the other player."},
{"Title": "Fair Seeding in Knockout Tournaments", "URL": "https://dl.acm.org/doi/10.1145/2036264.2036273", "Full Abstract": "We investigated the existence of fair seeding in knockout tournaments. We define two fairness criteria, both adapted from the literature: envy-freeness and order preservation. We show how to achieve the first criterion in tournaments whose structure is unconstrained, and prove an impossibility result for balanced tournaments. For the second criterion we have a similar result for unconstrained tournaments, but not for the balanced case. We provide instead a heuristic algorithm which we show through experiments to be efficient and effective. This suggests that the criterion is achievable also in balanced tournaments. However, we prove that it again becomes impossible to achieve when we add a weak condition guarding against the phenomenon of tournament dropout."},
{"Title": "Turning personal calendars into scheduling assistants", "URL": "https://dl.acm.org/doi/10.1145/2212776.2223854", "Full Abstract": "Personal calendars have long played a major role in time management, but they have evolved little over the years, and their contribution to productivity has stagnated. Inspired by logical theories of intention as well as experimental results on human productivity, and leveraging the power of optimization algorithms, we seek to reinvent the digital calendar. First, we increase the expressive power of calendar systems by deriving new entity types that go beyond simple events to better represent human intentions, plans, and goals. Next, we build on social psychological research to characterize the properties of a schedule best engineered for human productivity. Finally, we develop an optimization framework and algorithm to generate these schedules from a set of entities. With these tools combined, we transform the digital calendar from a passive repository into an active scheduling assistant."},
{"Title": "Price manipulation in prediction markets", "URL": "https://dl.acm.org/doi/10.5555/2615731.2615768", "Full Abstract": "We consider the possible existence of a manipulator in a prediction market, whose incentive is to maximally increase the predicted probability of an event, and for whom profit or loss in the market is immaterial. We characterize the equilibria in a single-round market scoring rule (MSR), showing that the manipulator will play a strategy that mixes between pretending to have received one of the top signals. We propose a modification to the MSR in the form of trade limits, a maximum amount by which the price of the security can change at a given round. We show analytically that without a manipulator, this process converges to the true posterior, and computationally that in a market with a manipulator, the limits help reduce the distortion by the manipulator when traders do not know about the manipulator's existence. Specifically, we show through simulations that with high probability the honest traders will fully reveal their signals before the manipulator does, and that the price at this point of full revelation by the honest traders can be a significantly better approximation of the true posterior than the ultimate price reached, suggesting a rule by which the market should be stopped at that point."},
{"Title": "Optimizing time and convenience in group scheduling", "URL": "https://dl.acm.org/doi/10.5555/2615731.2617465", "Full Abstract": "We consider the situation in which a group of agents are trying to find an agreeable time for a meeting. Each agent's availability is governed by a known prior probability distribution, but the realization of it is private information. The goal (of the convener) is to find an agreeable time slot while optimizing in two dimensions: Expected time to reach agreement and expected inconvenience caused during the process."},
{"Title": "Stable group scheduling", "URL": "https://dl.acm.org/doi/10.5555/2615731.2617466", "Full Abstract": "We consider the situation in which an organizer is trying to convene an event, and needs to decide on a schedule: a time slot and a set of invitees from a given set of agents. For each possible time slot, each agent has a single-peaked preference over the number of attendees at the event. Agent also has the outside option of not attending, which she prefers in some situations. The task of the organizer is to issue a maximum stable schedule -- the invited agents prefer attending to not attending, the agents not invited do not regret not being invited, and the event has the maximum number of attendees subject to these stability requirements. We consider both the non-strategic and strategic cases. In the former, in which agents truthfully reveal their preferences, we provide a polynomial-time algorithm for determining whether a stable schedule exists, and if it does, determining the maximum such schedule. In the strategic case we provide a truthful mechanism for the case in which the preferences of the agents are monotonically increasing, and an impossibility result for the general case."},
{"Title": "The right to obscure", "URL": "https://dl.acm.org/doi/10.5555/2832249.2832409", "Full Abstract": "The recent landmark \"right to be forgotten\" ruling by the EU Court gives EU citizens the right to remove certain links that are \"inaccurate, inadequate, irrelevant or excessive\" from search results under their names. While we agree with the spirit of the ruling--to empower individuals to manage their personal data while keeping a balance between such right and the freedom of expression, we believe that the ruling is impractical as it provides neither precise criteria for evaluating removal requests nor concrete guidelines for implementation. Consequently, Google's current implementation has several problems concerning scalability, objectivity, and responsiveness."},
{"Title": "Why knowledge representation matters", "URL": "https://dl.acm.org/doi/10.1145/2803170", "Full Abstract": "A personal story: From philosophy to software."},
{"Title": "Toward the AI Index", "URL": "https://dl.acm.org/doi/10.1609/aimag.v38i4.2761", "Full Abstract": "The AI Index is a new effort to track key developments in AI in a factual and objective way, and in doing so inform discussion and decision making both within AI and outside it. Since the project is early on, the goal of this article is not to present a final product, but rather to convey the current state of the index and invite the community's participation in helping to shape it."},
{"Title": "Is a modular architecture enough?", "URL": "https://dl.acm.org/doi/10.5555/3600270.3602354", "Full Abstract": "Inspired from human cognition, machine learning systems are gradually revealing advantages of sparser and more modular architectures. Recent work demonstrates that not only do some modular architectures generalize well, but they also lead to better out-of-distribution generalization, scaling properties, learning speed, and interpretability. A key intuition behind the success of such systems is that the data generating system for most real-world settings is considered to consist of sparsely interacting parts, and endowing models with similar inductive biases will be helpful. However, the field has been lacking in a rigorous quantitative assessment of such systems because these real-world data distributions are complex and unknown. In this work, we provide a thorough assessment of common modular architectures, through the lens of simple and known modular data distributions. We highlight the benefits of modularity and sparsity and reveal insights on the challenges faced while optimizing modular systems. In doing so, we propose evaluation metrics that highlight the benefits of modularity, the regimes in which these benefits are substantial, as well as the sub-optimality of current end-to-end learned modular systems as opposed to their claimed potential."},
{"Title": "MAgNet", "URL": "https://dl.acm.org/doi/10.5555/3600270.3602587", "Full Abstract": "The computational complexity of classical numerical methods for solving Partial Differential Equations (PDE) scales significantly as the resolution increases. As an important example, climate predictions require fine spatio-temporal resolutions to resolve all turbulent scales in the fluid simulations. This makes the task of accurately resolving these scales computationally out of reach even with modern supercomputers. As a result, current numerical modelers solve PDEs on grids that are too coarse (3km to 200km on each side), which hinders the accuracy and usefulness of the predictions. In this paper, we leverage the recent advances in Implicit Neural Representations (INR) to design a novel architecture that predicts the spatially continuous solution of a PDE given a spatial position query. By augmenting coordinate-based architectures with Graph Neural Networks (GNN), we enable zero-shot generalization to new non-uniform meshes and long-term predictions up to 250 frames ahead that are physically consistent. Our Mesh Agnostic Neural PDE Solver (MAgNet) is able to make accurate predictions across a variety of PDE simulation datasets and compares favorably with existing baselines. Moreover, MAgNet generalizes well to different meshes and resolutions up to four times those trained on."},
{"Title": "Improving ", "URL": "https://dl.acm.org/doi/10.5555/3666122.3666231", "Full Abstract": "Solar power harbors immense potential in mitigating climate change by substantially reducing CO"},
{"Title": "Let the flows tell", "URL": "https://dl.acm.org/doi/10.5555/3666122.3666644", "Full Abstract": "Combinatorial optimization (CO) problems are often NP-hard and thus out of reach for exact algorithms, making them a tempting domain to apply machine learning methods. The highly structured constraints in these problems can hinder either optimization or sampling directly in the solution space. On the other hand, GFlowNets have recently emerged as a powerful machinery to efficiently sample from composite unnormalized densities sequentially and have the potential to amortize such solution-searching processes in CO, as well as generate diverse solution candidates. In this paper, we design Markov decision processes (MDPs) for different combinatorial problems and propose to train conditional GFlowNets to sample from the solution space. Efficient training techniques are also developed to benefit long-range credit assignment. Through extensive experiments on a variety of different CO tasks with synthetic and realistic data, we demonstrate that GFlowNet policies can efficiently find high-quality solutions. Our implementation is open-sourced at https://github.com/zdhNarsil/GFlowNet-CombOpt."},
{"Title": "Laughing Hyena Distillery", "URL": "https://dl.acm.org/doi/10.5555/3666122.3666869", "Full Abstract": "Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular,"},
{"Title": "Reusable slotwise mechanisms", "URL": "https://dl.acm.org/doi/10.5555/3666122.3667143", "Full Abstract": "Agents with the ability to comprehend and reason about the dynamics of objects would be expected to exhibit improved robustness and generalization in novel scenarios. However, achieving this capability necessitates not only an effective scene representation but also an understanding of the mechanisms governing interactions among object subsets. Recent studies have made significant progress in representing scenes using object slots. In this work, we introduce Reusable Slotwise Mechanisms, or RSM, a framework that models object dynamics by leveraging communication among slots along with a modular architecture capable of dynamically selecting reusable mechanisms for predicting the future states of each object slot. Crucially, RSM leverages the"},
{"Title": "Contrastive Retrospection", "URL": "https://dl.acm.org/doi/10.5555/3666122.3667478", "Full Abstract": "In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec."},
{"Title": "Joint Bayesian inference of graphical structure and parameters with a single generative flow network", "URL": "https://dl.acm.org/doi/10.5555/3666122.3667482", "Full Abstract": "Generative Flow Networks (GFlowNets), a class of generative models over discrete and structured sample spaces, have been previously applied to the problem of inferring the marginal posterior distribution over the directed acyclic graph (DAG) of a Bayesian Network, given a dataset of observations. Based on recent advances extending this framework to non-discrete sample spaces, we propose in this paper to approximate the joint posterior over not only the structure of a Bayesian Network, but also the parameters of its conditional probability distributions. We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known. Since the parameters are included in the posterior distribution, this leaves more flexibility for the local probability models of the Bayesian Network, making our approach applicable even to non-linear models parametrized by neural networks. We show that our method, called JSP-GFN, offers an accurate approximation of the joint posterior, while comparing favorably against existing methods on both simulated and real data."},
{"Title": "HyenaDNA", "URL": "https://dl.acm.org/doi/10.5555/3666122.3667994", "Full Abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation, protein synthesis, and numerous other cellular properties. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution (i.e. DNA \"characters\") where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA,"},
{"Title": "GEO-bench", "URL": "https://dl.acm.org/doi/10.5555/3666122.3668345", "Full Abstract": "Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. Such models, recently coined"},
{"Title": "DynGFN", "URL": "https://dl.acm.org/doi/10.5555/3666122.3669375", "Full Abstract": "One of the grand challenges of cell biology is inferring the gene regulatory network (GRN) which describes interactions between genes and their products that control gene expression and cellular function. We can treat this as a causal discovery problem but with two non-standard challenges: (1) regulatory networks are inherently cyclic so we should not model a GRN as a directed acyclic graph (DAG), and (2) observations have significant measurement noise, so for typical sample sizes there will always be a large equivalence class of graphs that are likely given the data, and we want methods that capture this uncertainty. Existing methods either focus on challenge (1), identifying"},
{"Title": "SatBird", "URL": "https://dl.acm.org/doi/10.5555/3666122.3669439", "Full Abstract": "Biodiversity is declining at an unprecedented rate, impacting ecosystem services necessary to ensure food, water, and human health and well-being. Understanding the distribution of species and their habitats is crucial for conservation policy planning. However, traditional methods in ecology for species distribution models (SDMs) generally focus either on narrow sets of species or narrow geographical areas and there remain significant knowledge gaps about the distribution of species. A major reason for this is the limited availability of data traditionally used, due to the prohibitive amount of effort and expertise required for traditional field monitoring. The wide availability of remote sensing data and the growing adoption of citizen science tools to collect species observations data at low cost offer an opportunity for improving biodiversity monitoring and enabling the modelling of complex ecosystems. We introduce a novel task for mapping bird species to their habitats by predicting species encounter rates from satellite images, and present SatBird, a satellite dataset of locations in the USA with labels derived from presence-absence observation data from the citizen science database eBird, considering summer (breeding) and winter seasons. We also provide a dataset in Kenya representing low-data regimes. We additionally provide environmental data and species range maps for each location. We benchmark a set of baselines on our dataset, including SOTA models for remote sensing tasks. SatBird opens up possibilities for scalably modelling properties of ecosystems worldwide."},
{"Title": "Gradient starvation", "URL": "https://dl.acm.org/doi/10.5555/3540261.3540358", "Full Abstract": "We identify and formalize a fundamental gradient descent phenomenon leading to a learning proclivity in over-parameterized neural networks."},
{"Title": "A consciousness-inspired planning agent for model-based reinforcement learning", "URL": "https://dl.acm.org/doi/10.5555/3540261.3540382", "Full Abstract": "We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the numberofentitiestowhichtheagent attends during planningtobesmall. In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring different challenges. We consistently observethat the design allows the planning agentstogeneralize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance."},
{"Title": "Discrete-valued neural communication in structured architectures enhances generalization", "URL": "https://dl.acm.org/doi/10.5555/3540261.3540423", "Full Abstract": "Deep learning has advanced from fully connected architectures to structured models organized into components, e.g., the transformer composed of positional elements, modular architectures divided into slots, and graph neural nets made up of nodes. The nature of structured models is that communication among the components has a bottleneck, typically achieved by restricted connectivity and attention. In this work, we further tighten the bottleneck via discreteness of the representations transmitted between components. We hypothesize that this constraint serves as a useful form of inductive bias. Our hypothesis is motivated by past empirical work showing the benefits of discretization in non-structured architectures as well as our own theoretical results showing that discretization increases noise robustness and reduces the underlying dimensionality of the model. Building on an existing technique for discretization from the VQ-VAE, we consider multi-headed discretization with shared codebooks as the output of each architectural component. One motivating intuition is human language in which communication occurs through multiple discrete symbols. This form of communication is hypothesized to facilitate transmission of information between functional components of the brain by providing a common interlingua, just as it does for human-to-human communication. Our experiments show that"},
{"Title": "Invariance principle meets information bottleneck for out-of-distribution generalization", "URL": "https://dl.acm.org/doi/10.5555/3540261.3540524", "Full Abstract": "The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classification tasks, where invariant (causal) features capture all the information about the label. Are these failures due to the methods failing to capture the invariance? Or is the invariance principle itself insufficient? To answer these questions, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD. In contrast to the linear regression tasks, we show that for linear classification tasks we need much stronger restrictions on the distribution shifts, or otherwise OOD generalization is impossible. Furthermore, even with appropriate restrictions on distribution shifts in place, we show that the invariance principle alone is insufficient. We prove that a form of the information bottleneck constraint along with invariance helps address key failures when invariant features capture all the information about the label and also retains the existing success when they do not. We propose an approach that incorporates both of these principles and demonstrate its effectiveness in several experiments."},
{"Title": "The causal-neural connection", "URL": "https://dl.acm.org/doi/10.5555/3540261.3541089", "Full Abstract": "One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is"},
{"Title": "Dynamic inference with neural interpreters", "URL": "https://dl.acm.org/doi/10.5555/3540261.3541101", "Full Abstract": "Modern neural network architectures can leverage large amounts of data to generalize well within the training distribution. However, they are less capable of systematic generalization to data drawn from unseen but related distributions, a feat that is hypothesized to require compositional reasoning and reuse of knowledge. In this work, we present Neural Interpreters, an architecture that factorizes inference in a self-attention network as a system of modules, which we call"},
{"Title": "Neural production systems", "URL": "https://dl.acm.org/doi/10.5555/3540261.3542227", "Full Abstract": "Visual environments are structured, consisting of distinct objects or"},
{"Title": "Flow network based generative models for non-iterative diverse candidate generation", "URL": "https://dl.acm.org/doi/10.5555/3540261.3542358", "Full Abstract": "This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task."},
{"Title": "Scheduling with Deadlines and Buffer Management with Processing Requirements", "URL": "https://dl.acm.org/doi/10.1007/s00453-016-0257-1", "Full Abstract": "We discuss the well known online job scheduling problem with release times and deadlines, alongside an extended model--buffer management for packets with processing requirements. For job scheduling, an $${\\varOmega \\left( \\sqrt{\\frac{\\log {\\kappa {\\log {\\log {\\kappa \\right) $$Ωlogźloglogź lower bound on the competitive ratio of any randomized preemptive algorithm was shown by Canetti and Irani (Proceedings of the 27th annual ACM symposium on Theory of computing, ACM, pp 606---615, 1995), where $$\\kappa $$ź is the the maximum job duration or the maximum job value (the minimum is assumed to be 1). The proof of this well-known result is fairly elaborate and involved. In contrast, we show a significantly improved lower bound of $${\\varOmega (\\log {\\kappa )$$Ω(logź) using a simple proof. Our result matches the easy upper bound and closes a gap which was supposedly open for 20 years. We also discuss the problem of handling a FIFO buffer of a limited capacity, where packets arrive over time and may be preempted. Most of the work in buffer management considers the case where each packet has unit processing requirement. We consider a model where packets require some number of processing cycles before they can be transmitted. We aim to maximize the value of transmitted packets. We show an $${\\varOmega \\left( \\frac{\\log {\\kappa {\\log {\\log {\\kappa \\right) $$Ωlogźloglogź lower bound on the competitive ratio of randomized algorithms in this setting. We also present bounds for several special cases. For packets with unit values we also show a $$\\varphi \\approx 1.618$$źź1.618 lower bound on the competitive ratio of deterministic algorithms, and a 2-competitive algorithm. For the case of packets with constant densities we present a 4-competitive algorithm."},
{"Title": "The Strategy of Experts for Repeated Predictions", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-71924-5_4", "Full Abstract": "We investigate the behavior of experts who seek to make predictions with maximum impact on an audience. At a known future time, a certain continuous random variable will be realized. A public prediction gradually converges to the outcome, and an expert has access to a more accurate prediction. We study when the expert should reveal his information, when his reward is based on a proper scoring rule (e.g., is proportional to the change in log-likelihood of the outcome)."},
{"Title": "Randomized algorithms for online vector load balancing", "URL": "https://dl.acm.org/doi/10.5555/3174304.3175333", "Full Abstract": "We study randomized algorithms for the online vector bin packing and vector scheduling problems. For vector bin packing, we achieve a competitive ratio of"},
{"Title": "Prophet Secretary", "URL": "https://dl.acm.org/doi/10.1145/3219166.3219182", "Full Abstract": "In the Prophet Secretary problem, samples from a known set of probability distributions arrive one by one in a uniformly random order, and an algorithm must irrevocably pick one of the samples as soon as it arrives. The goal is to maximize the expected value of the sample picked relative to the expected maximum of the distributions. This is one of the most simple and fundamental problems in online decision making that models the process selling one item to a sequence of costumers. For a closely related problem called the Prophet Inequality where the order of the random variables is adversarial, it is known that one can achieve in expectation 1/2 of the expected maximum, and no better ratio is possible. For the Prophet Secretary problem, that is, when the variables arrive in a random order, Esfandiari et al. (2015) showed that one can actually get 1-1/e of the maximum. The 1-1/e bound was recently extended to more general settings by Ehsani et al. (2018). Given these results, one might be tempted to believe that 1-1/e is the correct bound. We show that this is not the case by providing an algorithm for the Prophet Secretary problem that beats the 1-1/e bound and achieves 1-1/e+1/400 times the expected maximum. We also prove a hardness result on the performance of algorithms under a natural restriction which we call deterministic distribution-insensitivity."},
{"Title": "The Price of Bounded Preemption", "URL": "https://dl.acm.org/doi/10.1145/3210377.3210407", "Full Abstract": "In this paper we provide a tight bound for the price of preemption for scheduling jobs on a single machine (or multiple machines). The input consists of a set of jobs to be scheduled and of an integer parameter $k \\ge 1$. Each job has a release time, deadline, length (also called processing time) and value associated with it. The goal is to feasibly schedule a subset of the jobs so that their total value is maximal; while preemption of a job is permitted, a job may be preempted no more than k times. The price of preemption is the worst possible (i.e., largest) ratio of the optimal non-bounded-preemptive scheduling to the optimal k-bounded-preemptive scheduling. Our results show that allowing at most k preemptions suffices to guarantee a Θ(\\min\\łog_k+1 n, łog_k+1 P\\ )$ fraction of the total value achieved when the number of preemptions is unrestricted (where n is the number of the jobs and P the ratio of the maximal length to the minimal length), giving us an upper bound for the price; a specific scenario serves to prove the tightness of this bound. We further show that when no preemptions are permitted at all (i.e., k=0), the price is Θ(\\min\\n, łog P\\ )$. As part of the proof, we introduce the notion of the Bounded-Degree Ancestor-Free Sub-Forest (BAS). We investigate the problem of computing the maximal-value BAS of a given forest and give a tight bound for the loss factor, which is Θ(łog_k+1 n)$ as well, where n is the size of the original forest and k is the bound on the degree of the sub-forest."},
{"Title": "2-Approximation algorithm for a generalization of scheduling on unrelated parallel machines", "URL": "https://dl.acm.org/doi/10.1016/j.ipl.2018.07.005", "Full Abstract": "In their seminal work"},
{"Title": "Deterministic Min-Cost Matching with Delays", "URL": "https://dl.acm.org/doi/10.1007/978-3-030-04693-4_2", "Full Abstract": "We consider the online Minimum-Cost Perfect Matching with Delays (MPMD) problem introduced by Emek et al. (STOC 2016), in which a general metric space is given, and requests are submitted in different times in this space by an adversary. The goal is to match requests, while minimizing the sum of distances between matched pairs in addition to the time intervals passed from the moment each request appeared until it is matched."},
{"Title": "Efficient Allocation of Free Stuff", "URL": "https://dl.acm.org/doi/10.5555/3306127.3331785", "Full Abstract": "We study online matching settings with selfish agents when everything is free. Inconsiderate agents break ties arbitrarily amongst equal maximal value available choices, even if the maximal value is equal to zero. \\par Even for the simplest case of zero/one valuations, where agents arrive online in an arbitrary order, and agents are restricted to taking at most one item, the resulting social welfare may be negligible for a deterministic algorithm. This may be surprising when contrasted with the 1/2 approximation of the greedy algorithm, analogous to this setting, except that agents are considerate (i.e., they don't take zero-valued items). \\par We overcome this challenge by introducing a new class of algorithms, which we refer to as prioritization algorithms. We show that upgrading a random subset of the agents to \"business class\" already improves the approximation to a constant. For more general valuations, we achieve a constant approximation using $łog n$ priority classes, when the valuations are known in advance. We extend these results to settings where agents have additive valuations and are restricted to taking up to some $q\\geq 1$ items. Our results are tight up to a constant."},
{"Title": "The Price of Clustering in Bin-Packing with Applications to Bin-Packingwith Delays", "URL": "https://dl.acm.org/doi/10.1145/3323165.3323180", "Full Abstract": "One of the most significant algorithmic challenges in the \"big data era\" is handling instances that are too large to be processed by a single machine. The common practice in this regard is to partition the massive problem instance into smaller ones and process each one of them separately. In some cases, the solutions for the smaller instances are later on assembled into a solution for the whole instance, but in many cases this last stage cannot be pursued (e.g., because it is too costly, because of locality issues, or due to privacy considerations). Motivated by this phenomenon, we consider the following natural combinatorial question: Given a bin-packing instance (namely, a set of items with sizes in (0, 1] that should be packed into unit capacity bins) I and a partition I"},
{"Title": "Tight Bounds for Clairvoyant Dynamic Bin Packing", "URL": "https://dl.acm.org/doi/10.1145/3364214", "Full Abstract": "In this article, we focus on the Clairvoyant Dynamic Bin Packing (DBP) problem, which extends the Classical Online Bin Packing problem in that items arrive and depart over time and the departure time of an item is known upon its arrival. The problem naturally arises when handling cloud-based networks. We focus specifically on the MinUsageTime objective function, which aims to minimize the overall usage time of all bins that are opened during the packing process. Earlier work has shown a"},
{"Title": "Deterministic Min-Cost Matching with Delays", "URL": "https://dl.acm.org/doi/10.1007/s00224-019-09963-7", "Full Abstract": "We consider the online Minimum-Cost Perfect Matching with Delays (MPMD) problem introduced by Emek et al. (STOC 2016), in which a general metric space is given, and requests for points in space are submitted in different times in this space by an adversary. The goal is to match requests, while minimizing the sum of distances between matched pairs in addition to the time intervals passed from the moment each request appeared until it is matched. In the online Minimum-Cost Bipartite Perfect Matching with Delays (MBPMD) problem introduced by Ashlagi et al. (APPROX/RANDOM 2017), each request is also associated with one of two classes, and requests can only be matched with requests of the other class. Previous algorithms for the problems mentioned above, include randomized"},
{"Title": "The Price of Bounded Preemption", "URL": "https://dl.acm.org/doi/10.1145/3434377", "Full Abstract": "In this article we provide a tight bound for the"},
{"Title": "The min-cost matching with concave delays problem", "URL": "https://dl.acm.org/doi/10.5555/3458064.3458084", "Full Abstract": "We consider the problem of online min-cost perfect matching with concave delays. We begin with the single location variant. Specifically, requests arrive in an online fashion at a single location. The algorithm must then choose between matching a pair of requests or delaying them to be matched later on. The cost is defined by a concave function on the delay. Given linear or even convex delay functions, matching any two available requests is trivially optimal. However, this does not extend to concave delays. We solve this by providing an"},
{"Title": "Flow time scheduling with uncertain processing time", "URL": "https://dl.acm.org/doi/10.1145/3406325.3451023", "Full Abstract": "We consider the problem of online scheduling on a single machine in order to minimize weighted flow time. The existing algorithms for this problem (STOC ’01, SODA ’03, FOCS ’18) all require exact knowledge of the processing time of each job. This assumption is crucial, as even a slight perturbation of the processing time would lead to polynomial competitive ratio. However, this assumption very rarely holds in real-life scenarios."},
{"Title": "Online Service with Delay", "URL": "https://dl.acm.org/doi/10.1145/3459925", "Full Abstract": "In this article, we introduce the"},
{"Title": "Competitive Vertex Recoloring", "URL": "https://dl.acm.org/doi/10.1007/s00453-022-01076-x", "Full Abstract": "Motivated by placement of jobs in physical machines, we introduce and analyze the problem of online recoloring, or online disengagement. In this problem, we are given a set of"},
{"Title": "The loss of serving in the dark", "URL": "https://dl.acm.org/doi/10.1016/j.ipl.2022.106334", "Full Abstract": "We study the following balls and bins stochastic process: There is a buffer with"},
{"Title": "Introduction to the Special Issue for SPAA’21", "URL": "https://dl.acm.org/doi/10.1145/3630608", "Full Abstract": "This special issue of the ACM Transactions on Parallel Computing (TOPC) contains selected papers from the 33rd ACM Symposium on Parallelism in Algorithms and Architectures (SPAA 2021), which was held July 6–8, 2021. Based on the conference reviews, we invited a selection of the papers accepted at the conference for this special issue. The papers submitted for this special issue then went through TOPC's own review process. The end result is the following set of outstanding papers in this issue:"},
{"Title": "An ", "URL": "https://dl.acm.org/doi/10.5555/3600270.3600393", "Full Abstract": "We study sequential bilateral trade where sellers and buyers valuations are completely arbitrary ("},
{"Title": "Discrete-smoothness in online algorithms with predictions", "URL": "https://dl.acm.org/doi/10.5555/3666122.3667939", "Full Abstract": "In recent years, there has been an increasing focus on designing online algorithms with (machine-learned) predictions. The ideal learning-augmented algorithm is comparable to the optimum when given perfect predictions ("},
{"Title": "LTS", "URL": "https://dl.acm.org/doi/10.1007/11780991_35", "Full Abstract": "The List-Traversal Synopses (LTS) system enables the efficient execution, utilization, and integration of memory-efficient algorithms that can support efficient forward and backward traversal of unidirectional lists. It has applications ranging from traversal of linked structures, garbage collection, hash-chain computations and program rollback. The system provides anima-tion and performance testing platform for traversal algorithms. We will demonstrate the algorithms animation, and will demonstrate how computer programs can be made reversible, and actually run backwards, using the LTS system."},
{"Title": "Efficient Bundle Sorting", "URL": "https://dl.acm.org/doi/10.1137/S0097539704446554", "Full Abstract": "Many data sets to be sorted consist of a limited number of distinct keys. Sorting such data sets can be thought of as bundling together identical keys and having the bundles placed in order; we therefore denote this as"},
{"Title": "Inner-product based wavelet synopses for range-sum queries", "URL": "https://dl.acm.org/doi/10.1007/11841036_46", "Full Abstract": "In recent years wavelet based synopses were shown to be effective for approximate queries in database systems. The simplest wavelet synopses are constructed by computing the Haar transform over a vector consisting of either the raw-data or the prefix-sums of the data, and using a greedy-euristic to select the wavelet coefficients that are kept in the synopsis. The greedy-heuristic is known to be optimal for point queries w.r.t. the mean-squared-error, but no similar efficient optimality result was known for range-sum queries, for which the effectiveness of such synopses was only shown experimentally."},
{"Title": "Optimal workload-based weighted wavelet synopses", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2006.11.018", "Full Abstract": "In recent years wavelets were shown to be effective data synopses. We are concerned with the problem of finding efficiently wavelet synopses for massive data sets, in situations where information about query workload is available. We present linear time, I/O optimal algorithms for building optimal workload-based wavelet synopses for point queries. The synopses are based on a novel construction of weighted inner products and use weighted wavelets that are adapted to those products. The synopses are optimal in the sense that the subset of retained coefficients is the best possible for the bases in use with respect to either the mean-squared absolute or relative errors. For the latter, this is the first optimal wavelet synopsis even for the regular, non-workload-based case. Experimental results demonstrate the advantage obtained by the new optimal wavelet synopses."},
{"Title": "Efficient pebbling for list traversal synopses with application to program rollback", "URL": "https://dl.acm.org/doi/10.1016/j.tcs.2007.02.048", "Full Abstract": "We show how to support efficient back traversal in a unidirectional list, using small memory and with essentially no slowdown in forward steps. Using O(lgn) memory for a list of size n, the i'th back-step from the farthest point reached so far takes O(lgi) time in the worst case, while the overhead per forward step is at most @e for arbitrary small constant @e>0. An arbitrary sequence of forward and back steps is allowed. A full trade-off between memory usage and time per back-step is presented: k vs. kn^1^/^k and vice versa. Our algorithms are based on a novel pebbling technique which moves pebbles on a virtual binary, or n^1^/^k-ary, tree that can only be traversed in a pre-order fashion. The compact data structures used by the pebbling algorithms, called list traversal synopses, extend to general directed graphs, and have other interesting applications, including memory efficient hash-chain implementation. Perhaps the most surprising application is in showing that for any program, arbitrary rollback steps can be efficiently supported with small overhead in memory, and marginal overhead in its ordinary execution. More concretely: let P be a program that runs for at most T steps, using memory of size M. Then, at the cost of recording the input used by the program, and increasing the memory by a factor of O(lgT) to O(MlgT), the program P can be extended to support an arbitrary sequence of forward execution and rollback steps: the i'th rollback step takes O(lgi) time in the worst case, while forward steps take O(1) time in the worst case, and 1+@e amortized time per step."},
{"Title": "Google's Auction for TV Ads", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-02930-1_26", "Full Abstract": "This document describes the auction system used by Google for allocation and pricing of TV ads. It is based on a simultaneous ascending auction, and has been in use since September 2008."},
{"Title": "Suggesting friends using the implicit social graph", "URL": "https://dl.acm.org/doi/10.1145/1835804.1835836", "Full Abstract": "Although users of online communication tools rarely categorize their contacts into groups such as \"family\", \"co-workers\", or \"jogging buddies\", they nonetheless implicitly cluster contacts, by virtue of their interactions with them, forming"},
{"Title": "Mind the (gender) gap", "URL": "https://dl.acm.org/doi/10.1145/2003616.2003637", "Full Abstract": "This paper presents the \"Mind the Gap\" initiative that aims to encourage female high school pupils to study computer science (CS) in high school. This is achieved by increasing their awareness to what CS is, and exposing them to the essence of a hi-tech environment and to same gender role models. Female software engineers at Google's Israel R&D Center in collaboration with the Israeli National Center undertook the initiative for computer science teachers. We describe the initiative and its impact on the female pupils' interest in CS. One of our conclusions is that even a short visit to a hi-tech company, in this case - Google, has the potential to change pupils' perception of what CS is and to increase their interest in CS and their desire to study it. One could easily adapt this initiative to other companies and scale it to infl uence a rather large population."},
{"Title": "Contextual OTP", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-31284-7_3", "Full Abstract": "OTP (One Time Password) devices are highly deployed trust enhancing (password entropy increasing) devices which are used to authenticate a user with a second factor (a pseudorandom sequence of digits produced by a device the user owns) and to cope with off-line phishing of password information. Wireless connection adds usability to OTP protocols in an obvious way: instead of the person copying the information between machines, the wireless (say, Bluetooth) mechanism can transfer the value directly. Indeed, OTP devices implemented in a smartphone and communicating with the browser over Bluetooth can act in usable fashion (and this extension was implemented in our organization and got very positive usability feedback). What we then noticed as a key observation is that this mode of OTP wireless transfer has turned the \"man to machine\" nature of the OTP tokens to a \"(mobile) device to machine (the browser on the computer)\" method, so we can now employ protocols between the two interacting computers. Thus, we asked what can this new mode contribute to security (rather than to usability only) and cope with increased set of attacks. Specifically, the question we are dealing with is whether wireless OTP devices (i.e., smartphones) can be hardened at a reasonable cost (i.e., without costly OTP infrastructural changes, public-key infrastructure/ operations, and with small modification to browsers) so as to be useful against one type of interesting and currently growing and highly publicized Man in the Middle (MITM) attacks. The work herein summarizes our study which is based on our proposed new notion of Contextual OTP (XOTP for short), which exploits session contexts to break the symmetry between the \"user-MITM\" and the \"MITM-server\" sessions."},
{"Title": "On big data algorithmics", "URL": "https://dl.acm.org/doi/10.1007/978-3-642-33090-2_1", "Full Abstract": "The extensive use of Big Data has now become common in plethora of technologies and industries. From massive data bases to business intelligence and datamining applications; from search engines to recommendation systems; advancing the state of the art of voice recognition, translation and more. The design, analysis and engineering of Big Data algorithms has multiple flavors, including massive parallelism, streaming algorithms, sketches and synopses, cloud technologies, and more. We will discuss some of these aspects, and reflect on their evolution and on the interplay between the theory and practice of Big Data algorithmics."},
{"Title": "Nowcasting with Google Trends", "URL": "https://dl.acm.org/doi/10.1007/978-3-319-02432-5_4", "Full Abstract": "Since launching Google Trends we have seen extensive interest in what can be learned from search trends. A plethora of studies have shown how to use search trends data for effective nowcasting in diverse areas such as health, finance, economics, politics and more."},
{"Title": "Dynamic Composition for Conversational Domain Exploration", "URL": "https://dl.acm.org/doi/10.1145/3366423.3380167", "Full Abstract": "We study conversational domain exploration (CODEX), where the user’s goal is to enrich her knowledge of a given domain by conversing with an informative bot. Such conversations should be well grounded in high-quality domain knowledge as well as engaging and open-ended. A CODEX bot should be proactive and introduce relevant information even if not directly asked for by the user. The bot should also appropriately pivot the conversation to undiscovered regions of the domain. To address these dialogue characteristics, we introduce a novel approach termed dynamic composition that decouples candidate content generation from the flexible composition of bot responses. This allows the bot to control the source, correctness and quality of the offered content, while achieving flexibility via a dialogue manager that selects the most appropriate contents in a compositional manner. We implemented a CODEX bot based on dynamic composition and integrated it into the Google Assistant . As an example domain, the bot conversed about the NBA basketball league in a seamless experience, such that users were not aware whether they were conversing with the vanilla system or the one augmented with our CODEX bot. Results are positive and offer insights into what makes for a good conversation. To the best of our knowledge, this is the first real user experiment of open-ended dialogues as part of a commercial assistant system."},
{"Title": "Adversarially robust streaming algorithms via differential privacy", "URL": "https://dl.acm.org/doi/10.5555/3495724.3495737", "Full Abstract": "A streaming algorithm is said to be"},
{"Title": "Adversarially Robust Streaming Algorithms via Differential Privacy", "URL": "https://dl.acm.org/doi/10.1145/3556972", "Full Abstract": "A streaming algorithm is said to be"},
{"Title": "Fast inference from transformers via speculative decoding", "URL": "https://dl.acm.org/doi/10.5555/3618408.3619203", "Full Abstract": "Inference from large autoregressive models like Transformers is slow - decoding"},
{"Title": "UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image", "URL": "https://dl.acm.org/doi/10.1145/3592451", "Full Abstract": "Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit while maintaining high fidelity to the input image. UniTune does not require additional inputs, like masks or sketches, and can perform multiple edits on the same image without retraining. We test our method using the Imagen model in a range of different use cases. We demonstrate that it is broadly applicable and can perform a surprisingly wide range of expressive editing operations, including those requiring significant visual changes that were previously impossible."},
{"Title": "Face0: Instantaneously Conditioning a Text-to-Image Model on a Face", "URL": "https://dl.acm.org/doi/10.1145/3610548.3618249", "Full Abstract": "We present Face0, a novel way to instantaneously condition a text-to-image generation model on a face without any optimization procedures such as fine-tuning or inversions. We augment a dataset of annotated images with embeddings of the included faces and train an image generation model on the augmented dataset. Once trained, our system is practically identical at inference time to the underlying base model, and is therefore able to generate face-conditioned images in just a couple of seconds. Our method achieves pleasing results, is remarkably simple, extremely fast, and equips the underlying model with new capabilities, like controlling the generated images both via text or via direct manipulation of the input face embeddings. In addition, when using a fixed random vector instead of a face embedding from a user supplied image, our method essentially solves the problem of consistent character generation across images. Finally, our method decouples the model’s textual biases from its biases on faces. While requiring further research, we hope that this may help reduce biases in future text-to-image models."},
{"Title": "Conversational AI in health: Design considerations from a Wizard-of-Oz dermatology case study with users, clinicians and a medical LLM", "URL": "https://dl.acm.org/doi/10.1145/3613905.3651891", "Full Abstract": "Although skin concerns are common, access to specialist care is limited. Artificial intelligence (AI)-assisted tools to support medical decisions may provide patients with feedback on their concerns while also helping ensure the most urgent cases are routed to dermatologists. Although AI-based conversational agents have been explored recently, how they are perceived by patients and clinicians is not well understood. We conducted a Wizard-of-Oz study involving 18 participants with real skin concerns. Participants were randomly assigned to interact with either a clinician agent (portrayed by a dermatologist) or an LLM agent (supervised by a dermatologist) via synchronous multimodal chat. In both conditions, participants found the conversation to be helpful in understanding their medical situation and alleviate their concerns. Through qualitative coding of the conversation transcripts, we provide insight on the importance of empathy and effective information-seeking. We conclude with design considerations for future AI-based conversational agents in healthcare settings."},
{"Title": "Physics-aware downsampling with deep learning for scalable flood modeling", "URL": "https://dl.acm.org/doi/10.5555/3540261.3540367", "Full Abstract": ". Floods are the most common natural disaster in the world, affecting the lives of hundreds of millions. Flood forecasting is therefore a vitally important endeavor, typically achieved using physical water flow simulations, which rely on accurate terrain elevation maps. However, such simulations, based on solving partial differential equations, are computationally prohibitive on a large scale. This scalability issue is commonly alleviated using a coarse grid representation of the elevation map, though this representation may distort crucial terrain details, leading to significant inaccuracies in the simulation."},
{"Title": "Adversarial robustness of streaming algorithms through importance sampling", "URL": "https://dl.acm.org/doi/10.5555/3540261.3540532", "Full Abstract": "Robustness against adversarial attacks has recently been at the forefront of algorithmic design for machine learning tasks. In the adversarial streaming model, an adversary gives an algorithm a sequence of adaptively chosen updates"},
